Inizio training IRL
Using device: cuda
Using cuda device
=== Iterazione IRL 0 ===
Loss reward (iter 0): 6.846870422363281
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 137      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.18    |
|    critic_loss     | 0.046    |
|    ent_coef        | 0.915    |
|    ent_coef_loss   | -0.145   |
|    learning_rate   | 0.0003   |
|    n_updates       | 299      |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 124      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.09    |
|    critic_loss     | 0.014    |
|    ent_coef        | 0.812    |
|    ent_coef_loss   | -0.344   |
|    learning_rate   | 0.0003   |
|    n_updates       | 699      |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 121      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -6.86    |
|    critic_loss     | 0.0189   |
|    ent_coef        | 0.721    |
|    ent_coef_loss   | -0.53    |
|    learning_rate   | 0.0003   |
|    n_updates       | 1099     |
---------------------------------
=== Iterazione IRL 1 ===
Loss reward (iter 1): 6.68127965927124
=== Iterazione IRL 2 ===
Loss reward (iter 2): 6.458652496337891
=== Iterazione IRL 3 ===
Loss reward (iter 3): 5.840271949768066
=== Iterazione IRL 4 ===
Loss reward (iter 4): 5.2506513595581055
=== Iterazione IRL 5 ===
Loss reward (iter 5): 4.358055114746094
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 149      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -17.2    |
|    critic_loss     | 8.52     |
|    ent_coef        | 0.634    |
|    ent_coef_loss   | -0.121   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1699     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 129      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -35.3    |
|    critic_loss     | 7.56     |
|    ent_coef        | 0.651    |
|    ent_coef_loss   | 0.328    |
|    learning_rate   | 0.0003   |
|    n_updates       | 2099     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 124      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -65.7    |
|    critic_loss     | 8.17     |
|    ent_coef        | 0.717    |
|    ent_coef_loss   | 0.451    |
|    learning_rate   | 0.0003   |
|    n_updates       | 2499     |
---------------------------------
=== Iterazione IRL 6 ===
Loss reward (iter 6): 21.49879264831543
=== Iterazione IRL 7 ===
Loss reward (iter 7): 14.071613311767578
=== Iterazione IRL 8 ===
Loss reward (iter 8): 8.88242244720459
=== Iterazione IRL 9 ===
Loss reward (iter 9): 5.074146270751953
=== Iterazione IRL 10 ===
Loss reward (iter 10): 2.0409998893737793
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 149      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -102     |
|    critic_loss     | 39.7     |
|    ent_coef        | 0.884    |
|    ent_coef_loss   | 0.159    |
|    learning_rate   | 0.0003   |
|    n_updates       | 3099     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 129      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -117     |
|    critic_loss     | 67.9     |
|    ent_coef        | 0.998    |
|    ent_coef_loss   | 0.00171  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3499     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 124      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -141     |
|    critic_loss     | 83.2     |
|    ent_coef        | 1.12     |
|    ent_coef_loss   | -0.095   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3899     |
---------------------------------
=== Iterazione IRL 11 ===
Loss reward (iter 11): -0.29780200123786926
=== Iterazione IRL 12 ===
Loss reward (iter 12): -3.8596737384796143
=== Iterazione IRL 13 ===
Loss reward (iter 13): -8.123231887817383
=== Iterazione IRL 14 ===
Loss reward (iter 14): -12.75701904296875
=== Iterazione IRL 15 ===
Loss reward (iter 15): -18.050724029541016
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 149      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -149     |
|    critic_loss     | 138      |
|    ent_coef        | 1.28     |
|    ent_coef_loss   | -0.15    |
|    learning_rate   | 0.0003   |
|    n_updates       | 4499     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 129      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -201     |
|    critic_loss     | 210      |
|    ent_coef        | 1.34     |
|    ent_coef_loss   | -0.222   |
|    learning_rate   | 0.0003   |
|    n_updates       | 4899     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 124      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -250     |
|    critic_loss     | 303      |
|    ent_coef        | 1.54     |
|    ent_coef_loss   | -0.507   |
|    learning_rate   | 0.0003   |
|    n_updates       | 5299     |
---------------------------------
=== Iterazione IRL 16 ===
Loss reward (iter 16): 82.7431869506836
=== Iterazione IRL 17 ===
Loss reward (iter 17): 53.39441680908203
=== Iterazione IRL 18 ===
Loss reward (iter 18): 34.90918731689453
=== Iterazione IRL 19 ===
Loss reward (iter 19): 22.598276138305664
=== Iterazione IRL 20 ===
Loss reward (iter 20): 13.396539688110352
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -351     |
|    critic_loss     | 524      |
|    ent_coef        | 1.95     |
|    ent_coef_loss   | -1.08    |
|    learning_rate   | 0.0003   |
|    n_updates       | 5899     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -431     |
|    critic_loss     | 580      |
|    ent_coef        | 2.23     |
|    ent_coef_loss   | -0.991   |
|    learning_rate   | 0.0003   |
|    n_updates       | 6299     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -478     |
|    critic_loss     | 491      |
|    ent_coef        | 2.5      |
|    ent_coef_loss   | -1.06    |
|    learning_rate   | 0.0003   |
|    n_updates       | 6699     |
---------------------------------
=== Iterazione IRL 21 ===
Loss reward (iter 21): 5.301582336425781
=== Iterazione IRL 22 ===
Loss reward (iter 22): -2.288461685180664
=== Iterazione IRL 23 ===
Loss reward (iter 23): -10.353110313415527
=== Iterazione IRL 24 ===
Loss reward (iter 24): -19.73223876953125
=== Iterazione IRL 25 ===
Loss reward (iter 25): -30.42033576965332
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -548     |
|    critic_loss     | 852      |
|    ent_coef        | 2.89     |
|    ent_coef_loss   | -0.803   |
|    learning_rate   | 0.0003   |
|    n_updates       | 7299     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -612     |
|    critic_loss     | 707      |
|    ent_coef        | 3.05     |
|    ent_coef_loss   | -0.685   |
|    learning_rate   | 0.0003   |
|    n_updates       | 7699     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -585     |
|    critic_loss     | 751      |
|    ent_coef        | 3.11     |
|    ent_coef_loss   | 0.0698   |
|    learning_rate   | 0.0003   |
|    n_updates       | 8099     |
---------------------------------
=== Iterazione IRL 26 ===
Loss reward (iter 26): 8.494837760925293
=== Iterazione IRL 27 ===
Loss reward (iter 27): 7.141053676605225
=== Iterazione IRL 28 ===
Loss reward (iter 28): 5.624241828918457
=== Iterazione IRL 29 ===
Loss reward (iter 29): 3.932269811630249
=== Iterazione IRL 30 ===
Loss reward (iter 30): 2.8976500034332275
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -624     |
|    critic_loss     | 1e+03    |
|    ent_coef        | 3.05     |
|    ent_coef_loss   | 0.198    |
|    learning_rate   | 0.0003   |
|    n_updates       | 8699     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -685     |
|    critic_loss     | 908      |
|    ent_coef        | 2.87     |
|    ent_coef_loss   | 0.198    |
|    learning_rate   | 0.0003   |
|    n_updates       | 9099     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -708     |
|    critic_loss     | 919      |
|    ent_coef        | 2.6      |
|    ent_coef_loss   | 0.454    |
|    learning_rate   | 0.0003   |
|    n_updates       | 9499     |
---------------------------------
=== Iterazione IRL 31 ===
Loss reward (iter 31): 4.84287691116333
=== Iterazione IRL 32 ===
Loss reward (iter 32): 3.6567466259002686
=== Iterazione IRL 33 ===
Loss reward (iter 33): 4.63215446472168
=== Iterazione IRL 34 ===
Loss reward (iter 34): 2.0342679023742676
=== Iterazione IRL 35 ===
Loss reward (iter 35): -2.557661294937134
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -680     |
|    critic_loss     | 694      |
|    ent_coef        | 2.19     |
|    ent_coef_loss   | 0.431    |
|    learning_rate   | 0.0003   |
|    n_updates       | 10099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -630     |
|    critic_loss     | 712      |
|    ent_coef        | 2.07     |
|    ent_coef_loss   | 0.152    |
|    learning_rate   | 0.0003   |
|    n_updates       | 10499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -675     |
|    critic_loss     | 847      |
|    ent_coef        | 1.98     |
|    ent_coef_loss   | -0.0714  |
|    learning_rate   | 0.0003   |
|    n_updates       | 10899    |
---------------------------------
=== Iterazione IRL 36 ===
Loss reward (iter 36): 7.550387382507324
=== Iterazione IRL 37 ===
Loss reward (iter 37): 6.541426181793213
=== Iterazione IRL 38 ===
Loss reward (iter 38): 7.498056411743164
=== Iterazione IRL 39 ===
Loss reward (iter 39): 5.036480903625488
=== Iterazione IRL 40 ===
Loss reward (iter 40): 5.782134532928467
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -673     |
|    critic_loss     | 641      |
|    ent_coef        | 1.89     |
|    ent_coef_loss   | 0.0842   |
|    learning_rate   | 0.0003   |
|    n_updates       | 11499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 130      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -659     |
|    critic_loss     | 793      |
|    ent_coef        | 1.87     |
|    ent_coef_loss   | 0.0944   |
|    learning_rate   | 0.0003   |
|    n_updates       | 11899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 124      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -685     |
|    critic_loss     | 664      |
|    ent_coef        | 1.9      |
|    ent_coef_loss   | -0.129   |
|    learning_rate   | 0.0003   |
|    n_updates       | 12299    |
---------------------------------
=== Iterazione IRL 41 ===
Loss reward (iter 41): 5.1763081550598145
=== Iterazione IRL 42 ===
Loss reward (iter 42): 3.8083913326263428
=== Iterazione IRL 43 ===
Loss reward (iter 43): 2.305715560913086
=== Iterazione IRL 44 ===
Loss reward (iter 44): 3.184314727783203
=== Iterazione IRL 45 ===
Loss reward (iter 45): 2.4868080615997314
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -668     |
|    critic_loss     | 749      |
|    ent_coef        | 1.96     |
|    ent_coef_loss   | 0.0886   |
|    learning_rate   | 0.0003   |
|    n_updates       | 12899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 130      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -648     |
|    critic_loss     | 900      |
|    ent_coef        | 2        |
|    ent_coef_loss   | -0.0186  |
|    learning_rate   | 0.0003   |
|    n_updates       | 13299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 124      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -686     |
|    critic_loss     | 608      |
|    ent_coef        | 2.01     |
|    ent_coef_loss   | 0.0445   |
|    learning_rate   | 0.0003   |
|    n_updates       | 13699    |
---------------------------------
=== Iterazione IRL 46 ===
Loss reward (iter 46): 2.8402233123779297
=== Iterazione IRL 47 ===
Loss reward (iter 47): 3.86446213722229
=== Iterazione IRL 48 ===
Loss reward (iter 48): 2.45815110206604
=== Iterazione IRL 49 ===
Loss reward (iter 49): -1.6907016038894653
=== Iterazione IRL 50 ===
Loss reward (iter 50): 4.334104061126709
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -673     |
|    critic_loss     | 959      |
|    ent_coef        | 2        |
|    ent_coef_loss   | 0.0765   |
|    learning_rate   | 0.0003   |
|    n_updates       | 14299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 130      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -662     |
|    critic_loss     | 620      |
|    ent_coef        | 1.89     |
|    ent_coef_loss   | -0.00675 |
|    learning_rate   | 0.0003   |
|    n_updates       | 14699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -655     |
|    critic_loss     | 286      |
|    ent_coef        | 1.8      |
|    ent_coef_loss   | 0.0499   |
|    learning_rate   | 0.0003   |
|    n_updates       | 15099    |
---------------------------------
=== Iterazione IRL 51 ===
Loss reward (iter 51): 7.566239833831787
=== Iterazione IRL 52 ===
Loss reward (iter 52): 7.357877731323242
=== Iterazione IRL 53 ===
Loss reward (iter 53): 7.065776824951172
=== Iterazione IRL 54 ===
Loss reward (iter 54): 5.106949329376221
=== Iterazione IRL 55 ===
Loss reward (iter 55): 6.122925758361816
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -690     |
|    critic_loss     | 704      |
|    ent_coef        | 1.61     |
|    ent_coef_loss   | 0.00697  |
|    learning_rate   | 0.0003   |
|    n_updates       | 15699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 130      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -682     |
|    critic_loss     | 624      |
|    ent_coef        | 1.52     |
|    ent_coef_loss   | -0.0433  |
|    learning_rate   | 0.0003   |
|    n_updates       | 16099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -681     |
|    critic_loss     | 548      |
|    ent_coef        | 1.51     |
|    ent_coef_loss   | 0.017    |
|    learning_rate   | 0.0003   |
|    n_updates       | 16499    |
---------------------------------
=== Iterazione IRL 56 ===
Loss reward (iter 56): 8.568485260009766
=== Iterazione IRL 57 ===
Loss reward (iter 57): 7.580172061920166
=== Iterazione IRL 58 ===
Loss reward (iter 58): 8.759577751159668
=== Iterazione IRL 59 ===
Loss reward (iter 59): 6.856995105743408
=== Iterazione IRL 60 ===
Loss reward (iter 60): 6.847184658050537
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -655     |
|    critic_loss     | 696      |
|    ent_coef        | 1.43     |
|    ent_coef_loss   | 0.0537   |
|    learning_rate   | 0.0003   |
|    n_updates       | 17099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -590     |
|    critic_loss     | 595      |
|    ent_coef        | 1.38     |
|    ent_coef_loss   | 0.0461   |
|    learning_rate   | 0.0003   |
|    n_updates       | 17499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -630     |
|    critic_loss     | 336      |
|    ent_coef        | 1.28     |
|    ent_coef_loss   | -0.0133  |
|    learning_rate   | 0.0003   |
|    n_updates       | 17899    |
---------------------------------
=== Iterazione IRL 61 ===
Loss reward (iter 61): 7.799139976501465
=== Iterazione IRL 62 ===
Loss reward (iter 62): 7.208762168884277
=== Iterazione IRL 63 ===
Loss reward (iter 63): 7.240670680999756
=== Iterazione IRL 64 ===
Loss reward (iter 64): 7.329010963439941
=== Iterazione IRL 65 ===
Loss reward (iter 65): 6.338768482208252
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -614     |
|    critic_loss     | 505      |
|    ent_coef        | 1.26     |
|    ent_coef_loss   | -0.00786 |
|    learning_rate   | 0.0003   |
|    n_updates       | 18499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 130      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -586     |
|    critic_loss     | 696      |
|    ent_coef        | 1.22     |
|    ent_coef_loss   | 0.0138   |
|    learning_rate   | 0.0003   |
|    n_updates       | 18899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -576     |
|    critic_loss     | 543      |
|    ent_coef        | 1.22     |
|    ent_coef_loss   | -0.00418 |
|    learning_rate   | 0.0003   |
|    n_updates       | 19299    |
---------------------------------
=== Iterazione IRL 66 ===
Loss reward (iter 66): 6.191582202911377
=== Iterazione IRL 67 ===
Loss reward (iter 67): 6.596826553344727
=== Iterazione IRL 68 ===
Loss reward (iter 68): 6.3155598640441895
=== Iterazione IRL 69 ===
Loss reward (iter 69): 6.4418206214904785
=== Iterazione IRL 70 ===
Loss reward (iter 70): 6.2523956298828125
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -551     |
|    critic_loss     | 617      |
|    ent_coef        | 1.21     |
|    ent_coef_loss   | -0.00744 |
|    learning_rate   | 0.0003   |
|    n_updates       | 19899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 130      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -521     |
|    critic_loss     | 557      |
|    ent_coef        | 1.2      |
|    ent_coef_loss   | 0.0104   |
|    learning_rate   | 0.0003   |
|    n_updates       | 20299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -540     |
|    critic_loss     | 468      |
|    ent_coef        | 1.18     |
|    ent_coef_loss   | 0.00964  |
|    learning_rate   | 0.0003   |
|    n_updates       | 20699    |
---------------------------------
=== Iterazione IRL 71 ===
Loss reward (iter 71): 5.544070243835449
=== Iterazione IRL 72 ===
Loss reward (iter 72): 5.022982597351074
=== Iterazione IRL 73 ===
Loss reward (iter 73): 7.380791664123535
=== Iterazione IRL 74 ===
Loss reward (iter 74): 6.3110575675964355
=== Iterazione IRL 75 ===
Loss reward (iter 75): 5.819164276123047
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -524     |
|    critic_loss     | 425      |
|    ent_coef        | 1.15     |
|    ent_coef_loss   | 0.00694  |
|    learning_rate   | 0.0003   |
|    n_updates       | 21299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -498     |
|    critic_loss     | 385      |
|    ent_coef        | 1.09     |
|    ent_coef_loss   | 0.0132   |
|    learning_rate   | 0.0003   |
|    n_updates       | 21699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -501     |
|    critic_loss     | 430      |
|    ent_coef        | 1.07     |
|    ent_coef_loss   | -0.0104  |
|    learning_rate   | 0.0003   |
|    n_updates       | 22099    |
---------------------------------
=== Iterazione IRL 76 ===
Loss reward (iter 76): 7.149556636810303
=== Iterazione IRL 77 ===
Loss reward (iter 77): 6.483486175537109
=== Iterazione IRL 78 ===
Loss reward (iter 78): 6.1975812911987305
=== Iterazione IRL 79 ===
Loss reward (iter 79): 6.414561748504639
=== Iterazione IRL 80 ===
Loss reward (iter 80): 5.096764087677002
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -499     |
|    critic_loss     | 466      |
|    ent_coef        | 1.08     |
|    ent_coef_loss   | -0.00278 |
|    learning_rate   | 0.0003   |
|    n_updates       | 22699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -463     |
|    critic_loss     | 483      |
|    ent_coef        | 1.08     |
|    ent_coef_loss   | 0.002    |
|    learning_rate   | 0.0003   |
|    n_updates       | 23099    |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -452      |
|    critic_loss     | 469       |
|    ent_coef        | 1.05      |
|    ent_coef_loss   | -0.000975 |
|    learning_rate   | 0.0003    |
|    n_updates       | 23499     |
----------------------------------
=== Iterazione IRL 81 ===
Loss reward (iter 81): 6.414270401000977
=== Iterazione IRL 82 ===
Loss reward (iter 82): 6.098991870880127
=== Iterazione IRL 83 ===
Loss reward (iter 83): 6.944594860076904
=== Iterazione IRL 84 ===
Loss reward (iter 84): 6.927232265472412
=== Iterazione IRL 85 ===
Loss reward (iter 85): 6.503971576690674
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -454     |
|    critic_loss     | 350      |
|    ent_coef        | 0.983    |
|    ent_coef_loss   | -0.00158 |
|    learning_rate   | 0.0003   |
|    n_updates       | 24099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -445     |
|    critic_loss     | 453      |
|    ent_coef        | 0.964    |
|    ent_coef_loss   | -0.00678 |
|    learning_rate   | 0.0003   |
|    n_updates       | 24499    |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 125       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -433      |
|    critic_loss     | 274       |
|    ent_coef        | 0.954     |
|    ent_coef_loss   | -0.000719 |
|    learning_rate   | 0.0003    |
|    n_updates       | 24899     |
----------------------------------
=== Iterazione IRL 86 ===
Loss reward (iter 86): 7.281743049621582
=== Iterazione IRL 87 ===
Loss reward (iter 87): 6.7196574211120605
=== Iterazione IRL 88 ===
Loss reward (iter 88): 6.613436698913574
=== Iterazione IRL 89 ===
Loss reward (iter 89): 6.976774215698242
=== Iterazione IRL 90 ===
Loss reward (iter 90): 6.798495292663574
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -379     |
|    critic_loss     | 405      |
|    ent_coef        | 0.944    |
|    ent_coef_loss   | -0.00639 |
|    learning_rate   | 0.0003   |
|    n_updates       | 25499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -398     |
|    critic_loss     | 399      |
|    ent_coef        | 0.895    |
|    ent_coef_loss   | 0.00399  |
|    learning_rate   | 0.0003   |
|    n_updates       | 25899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -368     |
|    critic_loss     | 356      |
|    ent_coef        | 0.847    |
|    ent_coef_loss   | 0.0129   |
|    learning_rate   | 0.0003   |
|    n_updates       | 26299    |
---------------------------------
=== Iterazione IRL 91 ===
Loss reward (iter 91): 6.4869232177734375
=== Iterazione IRL 92 ===
Loss reward (iter 92): 6.261989593505859
=== Iterazione IRL 93 ===
Loss reward (iter 93): 6.513289928436279
=== Iterazione IRL 94 ===
Loss reward (iter 94): 6.05668306350708
=== Iterazione IRL 95 ===
Loss reward (iter 95): 6.239173889160156
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -340     |
|    critic_loss     | 327      |
|    ent_coef        | 0.838    |
|    ent_coef_loss   | -0.0251  |
|    learning_rate   | 0.0003   |
|    n_updates       | 26899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -353     |
|    critic_loss     | 428      |
|    ent_coef        | 0.867    |
|    ent_coef_loss   | 0.0187   |
|    learning_rate   | 0.0003   |
|    n_updates       | 27299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -362     |
|    critic_loss     | 373      |
|    ent_coef        | 0.872    |
|    ent_coef_loss   | -0.00785 |
|    learning_rate   | 0.0003   |
|    n_updates       | 27699    |
---------------------------------
=== Iterazione IRL 96 ===
Loss reward (iter 96): 6.823969841003418
=== Iterazione IRL 97 ===
Loss reward (iter 97): 6.175544261932373
=== Iterazione IRL 98 ===
Loss reward (iter 98): 6.314205646514893
=== Iterazione IRL 99 ===
Loss reward (iter 99): 6.37163782119751
=== Iterazione IRL 100 ===
Loss reward (iter 100): 6.736374378204346
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -344     |
|    critic_loss     | 316      |
|    ent_coef        | 0.886    |
|    ent_coef_loss   | 0.000997 |
|    learning_rate   | 0.0003   |
|    n_updates       | 28299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -341     |
|    critic_loss     | 348      |
|    ent_coef        | 0.881    |
|    ent_coef_loss   | 0.00362  |
|    learning_rate   | 0.0003   |
|    n_updates       | 28699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -328     |
|    critic_loss     | 326      |
|    ent_coef        | 0.958    |
|    ent_coef_loss   | 0.00495  |
|    learning_rate   | 0.0003   |
|    n_updates       | 29099    |
---------------------------------
=== Iterazione IRL 101 ===
Loss reward (iter 101): 5.552770614624023
=== Iterazione IRL 102 ===
Loss reward (iter 102): 6.10930871963501
=== Iterazione IRL 103 ===
Loss reward (iter 103): 5.646390438079834
=== Iterazione IRL 104 ===
Loss reward (iter 104): 5.487358570098877
=== Iterazione IRL 105 ===
Loss reward (iter 105): 5.915340900421143
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -319     |
|    critic_loss     | 335      |
|    ent_coef        | 1.08     |
|    ent_coef_loss   | -0.00767 |
|    learning_rate   | 0.0003   |
|    n_updates       | 29699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -312     |
|    critic_loss     | 562      |
|    ent_coef        | 1.12     |
|    ent_coef_loss   | -0.00184 |
|    learning_rate   | 0.0003   |
|    n_updates       | 30099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -314     |
|    critic_loss     | 258      |
|    ent_coef        | 1.14     |
|    ent_coef_loss   | 0.0166   |
|    learning_rate   | 0.0003   |
|    n_updates       | 30499    |
---------------------------------
=== Iterazione IRL 106 ===
Loss reward (iter 106): 6.131396293640137
=== Iterazione IRL 107 ===
Loss reward (iter 107): 5.605033874511719
=== Iterazione IRL 108 ===
Loss reward (iter 108): 4.37519645690918
=== Iterazione IRL 109 ===
Loss reward (iter 109): 4.262104034423828
=== Iterazione IRL 110 ===
Loss reward (iter 110): 7.437633037567139
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -329     |
|    critic_loss     | 455      |
|    ent_coef        | 1.19     |
|    ent_coef_loss   | -0.015   |
|    learning_rate   | 0.0003   |
|    n_updates       | 31099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -303     |
|    critic_loss     | 331      |
|    ent_coef        | 1.2      |
|    ent_coef_loss   | 0.0289   |
|    learning_rate   | 0.0003   |
|    n_updates       | 31499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -333     |
|    critic_loss     | 493      |
|    ent_coef        | 1.25     |
|    ent_coef_loss   | -0.0392  |
|    learning_rate   | 0.0003   |
|    n_updates       | 31899    |
---------------------------------
=== Iterazione IRL 111 ===
Loss reward (iter 111): 5.476047515869141
=== Iterazione IRL 112 ===
Loss reward (iter 112): 5.127996444702148
=== Iterazione IRL 113 ===
Loss reward (iter 113): 3.4407198429107666
=== Iterazione IRL 114 ===
Loss reward (iter 114): 2.765016794204712
=== Iterazione IRL 115 ===
Loss reward (iter 115): 4.191004753112793
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -285     |
|    critic_loss     | 185      |
|    ent_coef        | 1.46     |
|    ent_coef_loss   | 0.0231   |
|    learning_rate   | 0.0003   |
|    n_updates       | 32499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -269     |
|    critic_loss     | 263      |
|    ent_coef        | 1.48     |
|    ent_coef_loss   | 0.0649   |
|    learning_rate   | 0.0003   |
|    n_updates       | 32899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -300     |
|    critic_loss     | 511      |
|    ent_coef        | 1.5      |
|    ent_coef_loss   | -0.0245  |
|    learning_rate   | 0.0003   |
|    n_updates       | 33299    |
---------------------------------
=== Iterazione IRL 116 ===
Loss reward (iter 116): 3.618210792541504
=== Iterazione IRL 117 ===
Loss reward (iter 117): 1.5264338254928589
=== Iterazione IRL 118 ===
Loss reward (iter 118): 1.863246202468872
=== Iterazione IRL 119 ===
Loss reward (iter 119): 2.233759880065918
=== Iterazione IRL 120 ===
Loss reward (iter 120): -0.4192350506782532
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -302     |
|    critic_loss     | 302      |
|    ent_coef        | 1.55     |
|    ent_coef_loss   | -0.0769  |
|    learning_rate   | 0.0003   |
|    n_updates       | 33899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -263     |
|    critic_loss     | 288      |
|    ent_coef        | 1.52     |
|    ent_coef_loss   | 0.0167   |
|    learning_rate   | 0.0003   |
|    n_updates       | 34299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -263     |
|    critic_loss     | 193      |
|    ent_coef        | 1.46     |
|    ent_coef_loss   | -0.0128  |
|    learning_rate   | 0.0003   |
|    n_updates       | 34699    |
---------------------------------
=== Iterazione IRL 121 ===
Loss reward (iter 121): 5.438792705535889
=== Iterazione IRL 122 ===
Loss reward (iter 122): 4.895527362823486
=== Iterazione IRL 123 ===
Loss reward (iter 123): 4.301557540893555
=== Iterazione IRL 124 ===
Loss reward (iter 124): 3.3467700481414795
=== Iterazione IRL 125 ===
Loss reward (iter 125): 0.5399969220161438
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -266     |
|    critic_loss     | 471      |
|    ent_coef        | 1.34     |
|    ent_coef_loss   | -0.0133  |
|    learning_rate   | 0.0003   |
|    n_updates       | 35299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -240     |
|    critic_loss     | 338      |
|    ent_coef        | 1.32     |
|    ent_coef_loss   | 0.0199   |
|    learning_rate   | 0.0003   |
|    n_updates       | 35699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -252     |
|    critic_loss     | 236      |
|    ent_coef        | 1.28     |
|    ent_coef_loss   | -0.0447  |
|    learning_rate   | 0.0003   |
|    n_updates       | 36099    |
---------------------------------
=== Iterazione IRL 126 ===
Loss reward (iter 126): 3.048997402191162
=== Iterazione IRL 127 ===
Loss reward (iter 127): 1.10819673538208
=== Iterazione IRL 128 ===
Loss reward (iter 128): 1.698694109916687
=== Iterazione IRL 129 ===
Loss reward (iter 129): 2.3109350204467773
=== Iterazione IRL 130 ===
Loss reward (iter 130): -0.20720723271369934
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -278     |
|    critic_loss     | 362      |
|    ent_coef        | 1.29     |
|    ent_coef_loss   | -0.0283  |
|    learning_rate   | 0.0003   |
|    n_updates       | 36699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -219     |
|    critic_loss     | 324      |
|    ent_coef        | 1.27     |
|    ent_coef_loss   | -0.0132  |
|    learning_rate   | 0.0003   |
|    n_updates       | 37099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -239     |
|    critic_loss     | 258      |
|    ent_coef        | 1.21     |
|    ent_coef_loss   | -0.00438 |
|    learning_rate   | 0.0003   |
|    n_updates       | 37499    |
---------------------------------
=== Iterazione IRL 131 ===
Loss reward (iter 131): -0.2982102334499359
=== Iterazione IRL 132 ===
Loss reward (iter 132): -7.656832218170166
=== Iterazione IRL 133 ===
Loss reward (iter 133): 1.2086310386657715
=== Iterazione IRL 134 ===
Loss reward (iter 134): 1.0607755184173584
=== Iterazione IRL 135 ===
Loss reward (iter 135): -1.4982324838638306
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -183     |
|    critic_loss     | 359      |
|    ent_coef        | 1.12     |
|    ent_coef_loss   | 0.0175   |
|    learning_rate   | 0.0003   |
|    n_updates       | 38099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -204     |
|    critic_loss     | 181      |
|    ent_coef        | 1.04     |
|    ent_coef_loss   | 0.0106   |
|    learning_rate   | 0.0003   |
|    n_updates       | 38499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -212     |
|    critic_loss     | 438      |
|    ent_coef        | 0.931    |
|    ent_coef_loss   | -0.02    |
|    learning_rate   | 0.0003   |
|    n_updates       | 38899    |
---------------------------------
=== Iterazione IRL 136 ===
Loss reward (iter 136): -0.21780532598495483
=== Iterazione IRL 137 ===
Loss reward (iter 137): 7.252828121185303
=== Iterazione IRL 138 ===
Loss reward (iter 138): 2.567556858062744
=== Iterazione IRL 139 ===
Loss reward (iter 139): -0.7581593990325928
=== Iterazione IRL 140 ===
Loss reward (iter 140): 1.157296061515808
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -178     |
|    critic_loss     | 363      |
|    ent_coef        | 0.81     |
|    ent_coef_loss   | -0.0393  |
|    learning_rate   | 0.0003   |
|    n_updates       | 39499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -162     |
|    critic_loss     | 348      |
|    ent_coef        | 0.748    |
|    ent_coef_loss   | -0.043   |
|    learning_rate   | 0.0003   |
|    n_updates       | 39899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -146     |
|    critic_loss     | 448      |
|    ent_coef        | 0.718    |
|    ent_coef_loss   | 0.0563   |
|    learning_rate   | 0.0003   |
|    n_updates       | 40299    |
---------------------------------
=== Iterazione IRL 141 ===
Loss reward (iter 141): 0.5751357078552246
=== Iterazione IRL 142 ===
Loss reward (iter 142): 0.4222756028175354
=== Iterazione IRL 143 ===
Loss reward (iter 143): -0.1666032373905182
=== Iterazione IRL 144 ===
Loss reward (iter 144): -1.382144808769226
=== Iterazione IRL 145 ===
Loss reward (iter 145): 0.2669923007488251
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -104     |
|    critic_loss     | 353      |
|    ent_coef        | 0.745    |
|    ent_coef_loss   | 0.0142   |
|    learning_rate   | 0.0003   |
|    n_updates       | 40899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -124     |
|    critic_loss     | 413      |
|    ent_coef        | 0.804    |
|    ent_coef_loss   | 0.00859  |
|    learning_rate   | 0.0003   |
|    n_updates       | 41299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -92.1    |
|    critic_loss     | 623      |
|    ent_coef        | 0.905    |
|    ent_coef_loss   | -0.00515 |
|    learning_rate   | 0.0003   |
|    n_updates       | 41699    |
---------------------------------
=== Iterazione IRL 146 ===
Loss reward (iter 146): 6.199975967407227
=== Iterazione IRL 147 ===
Loss reward (iter 147): 5.064670085906982
=== Iterazione IRL 148 ===
Loss reward (iter 148): 3.943843126296997
=== Iterazione IRL 149 ===
Loss reward (iter 149): 1.0537055730819702
=== Iterazione IRL 150 ===
Loss reward (iter 150): 6.866515636444092
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -66.9    |
|    critic_loss     | 533      |
|    ent_coef        | 0.922    |
|    ent_coef_loss   | 0.0093   |
|    learning_rate   | 0.0003   |
|    n_updates       | 42299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -57      |
|    critic_loss     | 427      |
|    ent_coef        | 0.923    |
|    ent_coef_loss   | -0.0059  |
|    learning_rate   | 0.0003   |
|    n_updates       | 42699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -10.7    |
|    critic_loss     | 433      |
|    ent_coef        | 0.945    |
|    ent_coef_loss   | -0.0046  |
|    learning_rate   | 0.0003   |
|    n_updates       | 43099    |
---------------------------------
=== Iterazione IRL 151 ===
Loss reward (iter 151): 6.0442047119140625
=== Iterazione IRL 152 ===
Loss reward (iter 152): 5.141757965087891
=== Iterazione IRL 153 ===
Loss reward (iter 153): 5.493888854980469
=== Iterazione IRL 154 ===
Loss reward (iter 154): 5.585693836212158
=== Iterazione IRL 155 ===
Loss reward (iter 155): 5.613491058349609
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 153       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -13       |
|    critic_loss     | 676       |
|    ent_coef        | 0.786     |
|    ent_coef_loss   | -0.000305 |
|    learning_rate   | 0.0003    |
|    n_updates       | 43699     |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 9.94     |
|    critic_loss     | 473      |
|    ent_coef        | 0.741    |
|    ent_coef_loss   | 0.00988  |
|    learning_rate   | 0.0003   |
|    n_updates       | 44099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 13.8     |
|    critic_loss     | 530      |
|    ent_coef        | 0.74     |
|    ent_coef_loss   | 0.0417   |
|    learning_rate   | 0.0003   |
|    n_updates       | 44499    |
---------------------------------
=== Iterazione IRL 156 ===
Loss reward (iter 156): 3.9193968772888184
=== Iterazione IRL 157 ===
Loss reward (iter 157): 5.189467430114746
=== Iterazione IRL 158 ===
Loss reward (iter 158): 5.269522190093994
=== Iterazione IRL 159 ===
Loss reward (iter 159): 3.9622421264648438
=== Iterazione IRL 160 ===
Loss reward (iter 160): 2.996154308319092
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 37.6     |
|    critic_loss     | 557      |
|    ent_coef        | 0.749    |
|    ent_coef_loss   | 0.0418   |
|    learning_rate   | 0.0003   |
|    n_updates       | 45099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 46.4     |
|    critic_loss     | 671      |
|    ent_coef        | 0.745    |
|    ent_coef_loss   | 0.0595   |
|    learning_rate   | 0.0003   |
|    n_updates       | 45499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 70.3     |
|    critic_loss     | 603      |
|    ent_coef        | 0.799    |
|    ent_coef_loss   | 0.0259   |
|    learning_rate   | 0.0003   |
|    n_updates       | 45899    |
---------------------------------
=== Iterazione IRL 161 ===
Loss reward (iter 161): 5.790713787078857
=== Iterazione IRL 162 ===
Loss reward (iter 162): 6.181026458740234
=== Iterazione IRL 163 ===
Loss reward (iter 163): 5.969468593597412
=== Iterazione IRL 164 ===
Loss reward (iter 164): 5.0755414962768555
=== Iterazione IRL 165 ===
Loss reward (iter 165): 5.599857330322266
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 89.9     |
|    critic_loss     | 518      |
|    ent_coef        | 0.832    |
|    ent_coef_loss   | 0.0025   |
|    learning_rate   | 0.0003   |
|    n_updates       | 46499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 97.6     |
|    critic_loss     | 611      |
|    ent_coef        | 0.858    |
|    ent_coef_loss   | 0.00149  |
|    learning_rate   | 0.0003   |
|    n_updates       | 46899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 136      |
|    critic_loss     | 455      |
|    ent_coef        | 0.879    |
|    ent_coef_loss   | 0.00157  |
|    learning_rate   | 0.0003   |
|    n_updates       | 47299    |
---------------------------------
=== Iterazione IRL 166 ===
Loss reward (iter 166): 6.980666637420654
=== Iterazione IRL 167 ===
Loss reward (iter 167): 5.507402420043945
=== Iterazione IRL 168 ===
Loss reward (iter 168): 5.909763813018799
=== Iterazione IRL 169 ===
Loss reward (iter 169): 5.733581066131592
=== Iterazione IRL 170 ===
Loss reward (iter 170): 6.147881031036377
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 139      |
|    critic_loss     | 398      |
|    ent_coef        | 0.868    |
|    ent_coef_loss   | -0.0213  |
|    learning_rate   | 0.0003   |
|    n_updates       | 47899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 162      |
|    critic_loss     | 270      |
|    ent_coef        | 0.873    |
|    ent_coef_loss   | -0.00992 |
|    learning_rate   | 0.0003   |
|    n_updates       | 48299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 147      |
|    critic_loss     | 756      |
|    ent_coef        | 0.871    |
|    ent_coef_loss   | 0.0204   |
|    learning_rate   | 0.0003   |
|    n_updates       | 48699    |
---------------------------------
=== Iterazione IRL 171 ===
Loss reward (iter 171): 6.052048206329346
=== Iterazione IRL 172 ===
Loss reward (iter 172): 5.7494425773620605
=== Iterazione IRL 173 ===
Loss reward (iter 173): 5.090041637420654
=== Iterazione IRL 174 ===
Loss reward (iter 174): 6.864168167114258
=== Iterazione IRL 175 ===
Loss reward (iter 175): 6.327802658081055
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | 184       |
|    critic_loss     | 422       |
|    ent_coef        | 0.89      |
|    ent_coef_loss   | -0.000227 |
|    learning_rate   | 0.0003    |
|    n_updates       | 49299     |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 192      |
|    critic_loss     | 381      |
|    ent_coef        | 0.876    |
|    ent_coef_loss   | 0.0244   |
|    learning_rate   | 0.0003   |
|    n_updates       | 49699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 214      |
|    critic_loss     | 431      |
|    ent_coef        | 0.879    |
|    ent_coef_loss   | 0.0118   |
|    learning_rate   | 0.0003   |
|    n_updates       | 50099    |
---------------------------------
=== Iterazione IRL 176 ===
Loss reward (iter 176): 5.266053676605225
=== Iterazione IRL 177 ===
Loss reward (iter 177): 5.677430629730225
=== Iterazione IRL 178 ===
Loss reward (iter 178): 5.343099594116211
=== Iterazione IRL 179 ===
Loss reward (iter 179): 5.565720558166504
=== Iterazione IRL 180 ===
Loss reward (iter 180): 4.984930992126465
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 221      |
|    critic_loss     | 246      |
|    ent_coef        | 0.873    |
|    ent_coef_loss   | 0.00912  |
|    learning_rate   | 0.0003   |
|    n_updates       | 50699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 235      |
|    critic_loss     | 496      |
|    ent_coef        | 0.892    |
|    ent_coef_loss   | 0.0325   |
|    learning_rate   | 0.0003   |
|    n_updates       | 51099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 240      |
|    critic_loss     | 272      |
|    ent_coef        | 0.898    |
|    ent_coef_loss   | 0.00755  |
|    learning_rate   | 0.0003   |
|    n_updates       | 51499    |
---------------------------------
=== Iterazione IRL 181 ===
Loss reward (iter 181): 5.479073524475098
=== Iterazione IRL 182 ===
Loss reward (iter 182): 5.328982353210449
=== Iterazione IRL 183 ===
Loss reward (iter 183): 5.172196865081787
=== Iterazione IRL 184 ===
Loss reward (iter 184): 4.263542175292969
=== Iterazione IRL 185 ===
Loss reward (iter 185): 3.4518795013427734
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 262      |
|    critic_loss     | 326      |
|    ent_coef        | 0.91     |
|    ent_coef_loss   | -0.00987 |
|    learning_rate   | 0.0003   |
|    n_updates       | 52099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 257      |
|    critic_loss     | 399      |
|    ent_coef        | 0.913    |
|    ent_coef_loss   | -0.0183  |
|    learning_rate   | 0.0003   |
|    n_updates       | 52499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 279      |
|    critic_loss     | 234      |
|    ent_coef        | 0.923    |
|    ent_coef_loss   | 0.0086   |
|    learning_rate   | 0.0003   |
|    n_updates       | 52899    |
---------------------------------
=== Iterazione IRL 186 ===
Loss reward (iter 186): 6.498259544372559
=== Iterazione IRL 187 ===
Loss reward (iter 187): 5.615540027618408
=== Iterazione IRL 188 ===
Loss reward (iter 188): 5.159289836883545
=== Iterazione IRL 189 ===
Loss reward (iter 189): 6.938075065612793
=== Iterazione IRL 190 ===
Loss reward (iter 190): 5.630488872528076
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 288      |
|    critic_loss     | 332      |
|    ent_coef        | 0.929    |
|    ent_coef_loss   | 0.00353  |
|    learning_rate   | 0.0003   |
|    n_updates       | 53499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 292      |
|    critic_loss     | 427      |
|    ent_coef        | 0.935    |
|    ent_coef_loss   | 0.0141   |
|    learning_rate   | 0.0003   |
|    n_updates       | 53899    |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | 308       |
|    critic_loss     | 432       |
|    ent_coef        | 0.949     |
|    ent_coef_loss   | -6.38e-05 |
|    learning_rate   | 0.0003    |
|    n_updates       | 54299     |
----------------------------------
=== Iterazione IRL 191 ===
Loss reward (iter 191): 5.477814674377441
=== Iterazione IRL 192 ===
Loss reward (iter 192): 4.912052631378174
=== Iterazione IRL 193 ===
Loss reward (iter 193): 5.3833441734313965
=== Iterazione IRL 194 ===
Loss reward (iter 194): 5.316208362579346
=== Iterazione IRL 195 ===
Loss reward (iter 195): 5.523766994476318
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 331      |
|    critic_loss     | 350      |
|    ent_coef        | 0.933    |
|    ent_coef_loss   | -0.00619 |
|    learning_rate   | 0.0003   |
|    n_updates       | 54899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 334      |
|    critic_loss     | 247      |
|    ent_coef        | 0.923    |
|    ent_coef_loss   | 0.00628  |
|    learning_rate   | 0.0003   |
|    n_updates       | 55299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 343      |
|    critic_loss     | 435      |
|    ent_coef        | 0.899    |
|    ent_coef_loss   | -0.0219  |
|    learning_rate   | 0.0003   |
|    n_updates       | 55699    |
---------------------------------
=== Iterazione IRL 196 ===
Loss reward (iter 196): 5.393007755279541
=== Iterazione IRL 197 ===
Loss reward (iter 197): 6.101705074310303
=== Iterazione IRL 198 ===
Loss reward (iter 198): 5.102112293243408
=== Iterazione IRL 199 ===
Loss reward (iter 199): 5.885799407958984
=== Iterazione IRL 200 ===
Loss reward (iter 200): 5.666289329528809
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 360      |
|    critic_loss     | 441      |
|    ent_coef        | 0.854    |
|    ent_coef_loss   | 0.00998  |
|    learning_rate   | 0.0003   |
|    n_updates       | 56299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 365      |
|    critic_loss     | 347      |
|    ent_coef        | 0.846    |
|    ent_coef_loss   | -0.0112  |
|    learning_rate   | 0.0003   |
|    n_updates       | 56699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 381      |
|    critic_loss     | 395      |
|    ent_coef        | 0.828    |
|    ent_coef_loss   | -0.00385 |
|    learning_rate   | 0.0003   |
|    n_updates       | 57099    |
---------------------------------
=== Iterazione IRL 201 ===
Loss reward (iter 201): 4.688925743103027
=== Iterazione IRL 202 ===
Loss reward (iter 202): 5.826207160949707
=== Iterazione IRL 203 ===
Loss reward (iter 203): 5.297098636627197
=== Iterazione IRL 204 ===
Loss reward (iter 204): 3.96304988861084
=== Iterazione IRL 205 ===
Loss reward (iter 205): 4.755237102508545
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 398      |
|    critic_loss     | 312      |
|    ent_coef        | 0.798    |
|    ent_coef_loss   | -0.0244  |
|    learning_rate   | 0.0003   |
|    n_updates       | 57699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 403      |
|    critic_loss     | 354      |
|    ent_coef        | 0.777    |
|    ent_coef_loss   | 0.00505  |
|    learning_rate   | 0.0003   |
|    n_updates       | 58099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 401      |
|    critic_loss     | 434      |
|    ent_coef        | 0.735    |
|    ent_coef_loss   | -0.0255  |
|    learning_rate   | 0.0003   |
|    n_updates       | 58499    |
---------------------------------
=== Iterazione IRL 206 ===
Loss reward (iter 206): 4.456098556518555
=== Iterazione IRL 207 ===
Loss reward (iter 207): 5.718929767608643
=== Iterazione IRL 208 ===
Loss reward (iter 208): 6.441550254821777
=== Iterazione IRL 209 ===
Loss reward (iter 209): 6.001075267791748
=== Iterazione IRL 210 ===
Loss reward (iter 210): 5.493868827819824
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 404      |
|    critic_loss     | 336      |
|    ent_coef        | 0.675    |
|    ent_coef_loss   | 0.0208   |
|    learning_rate   | 0.0003   |
|    n_updates       | 59099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 427      |
|    critic_loss     | 283      |
|    ent_coef        | 0.684    |
|    ent_coef_loss   | 0.0631   |
|    learning_rate   | 0.0003   |
|    n_updates       | 59499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 435      |
|    critic_loss     | 438      |
|    ent_coef        | 0.669    |
|    ent_coef_loss   | 0.0153   |
|    learning_rate   | 0.0003   |
|    n_updates       | 59899    |
---------------------------------
=== Iterazione IRL 211 ===
Loss reward (iter 211): 5.693110942840576
=== Iterazione IRL 212 ===
Loss reward (iter 212): 5.760164260864258
=== Iterazione IRL 213 ===
Loss reward (iter 213): 5.751944065093994
=== Iterazione IRL 214 ===
Loss reward (iter 214): 5.918389797210693
=== Iterazione IRL 215 ===
Loss reward (iter 215): 5.622553825378418
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 444      |
|    critic_loss     | 421      |
|    ent_coef        | 0.642    |
|    ent_coef_loss   | -0.125   |
|    learning_rate   | 0.0003   |
|    n_updates       | 60499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 454      |
|    critic_loss     | 477      |
|    ent_coef        | 0.599    |
|    ent_coef_loss   | -0.0312  |
|    learning_rate   | 0.0003   |
|    n_updates       | 60899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 472      |
|    critic_loss     | 477      |
|    ent_coef        | 0.561    |
|    ent_coef_loss   | -0.0459  |
|    learning_rate   | 0.0003   |
|    n_updates       | 61299    |
---------------------------------
=== Iterazione IRL 216 ===
Loss reward (iter 216): 4.107371807098389
=== Iterazione IRL 217 ===
Loss reward (iter 217): 4.114589691162109
=== Iterazione IRL 218 ===
Loss reward (iter 218): 5.8984198570251465
=== Iterazione IRL 219 ===
Loss reward (iter 219): 3.5121445655822754
=== Iterazione IRL 220 ===
Loss reward (iter 220): 6.429099082946777
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 493      |
|    critic_loss     | 427      |
|    ent_coef        | 0.546    |
|    ent_coef_loss   | -0.0232  |
|    learning_rate   | 0.0003   |
|    n_updates       | 61899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 494      |
|    critic_loss     | 280      |
|    ent_coef        | 0.544    |
|    ent_coef_loss   | -0.0159  |
|    learning_rate   | 0.0003   |
|    n_updates       | 62299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 502      |
|    critic_loss     | 267      |
|    ent_coef        | 0.553    |
|    ent_coef_loss   | 0.0248   |
|    learning_rate   | 0.0003   |
|    n_updates       | 62699    |
---------------------------------
=== Iterazione IRL 221 ===
Loss reward (iter 221): 5.513874053955078
=== Iterazione IRL 222 ===
Loss reward (iter 222): 4.9645304679870605
=== Iterazione IRL 223 ===
Loss reward (iter 223): 5.0941972732543945
=== Iterazione IRL 224 ===
Loss reward (iter 224): 5.760714054107666
=== Iterazione IRL 225 ===
Loss reward (iter 225): 5.305402755737305
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 503      |
|    critic_loss     | 358      |
|    ent_coef        | 0.575    |
|    ent_coef_loss   | -0.0376  |
|    learning_rate   | 0.0003   |
|    n_updates       | 63299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 514      |
|    critic_loss     | 357      |
|    ent_coef        | 0.605    |
|    ent_coef_loss   | -0.094   |
|    learning_rate   | 0.0003   |
|    n_updates       | 63699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 523      |
|    critic_loss     | 361      |
|    ent_coef        | 0.639    |
|    ent_coef_loss   | -0.054   |
|    learning_rate   | 0.0003   |
|    n_updates       | 64099    |
---------------------------------
=== Iterazione IRL 226 ===
Loss reward (iter 226): 5.153136253356934
=== Iterazione IRL 227 ===
Loss reward (iter 227): 4.9335784912109375
=== Iterazione IRL 228 ===
Loss reward (iter 228): 4.308107376098633
=== Iterazione IRL 229 ===
Loss reward (iter 229): 4.579977989196777
=== Iterazione IRL 230 ===
Loss reward (iter 230): 3.9442710876464844
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 527      |
|    critic_loss     | 582      |
|    ent_coef        | 0.674    |
|    ent_coef_loss   | 0.113    |
|    learning_rate   | 0.0003   |
|    n_updates       | 64699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 547      |
|    critic_loss     | 248      |
|    ent_coef        | 0.717    |
|    ent_coef_loss   | 0.0263   |
|    learning_rate   | 0.0003   |
|    n_updates       | 65099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 545      |
|    critic_loss     | 215      |
|    ent_coef        | 0.726    |
|    ent_coef_loss   | -0.00545 |
|    learning_rate   | 0.0003   |
|    n_updates       | 65499    |
---------------------------------
=== Iterazione IRL 231 ===
Loss reward (iter 231): 5.274689197540283
=== Iterazione IRL 232 ===
Loss reward (iter 232): 5.582816123962402
=== Iterazione IRL 233 ===
Loss reward (iter 233): 4.840663909912109
=== Iterazione IRL 234 ===
Loss reward (iter 234): 4.794839859008789
=== Iterazione IRL 235 ===
Loss reward (iter 235): 4.3122100830078125
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 556      |
|    critic_loss     | 520      |
|    ent_coef        | 0.706    |
|    ent_coef_loss   | -0.051   |
|    learning_rate   | 0.0003   |
|    n_updates       | 66099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 569      |
|    critic_loss     | 307      |
|    ent_coef        | 0.681    |
|    ent_coef_loss   | -0.0365  |
|    learning_rate   | 0.0003   |
|    n_updates       | 66499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 580      |
|    critic_loss     | 222      |
|    ent_coef        | 0.701    |
|    ent_coef_loss   | 0.0188   |
|    learning_rate   | 0.0003   |
|    n_updates       | 66899    |
---------------------------------
=== Iterazione IRL 236 ===
Loss reward (iter 236): 3.9138071537017822
=== Iterazione IRL 237 ===
Loss reward (iter 237): 3.373117685317993
=== Iterazione IRL 238 ===
Loss reward (iter 238): 4.29025411605835
=== Iterazione IRL 239 ===
Loss reward (iter 239): 4.5867600440979
=== Iterazione IRL 240 ===
Loss reward (iter 240): 3.529805898666382
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 592      |
|    critic_loss     | 283      |
|    ent_coef        | 0.687    |
|    ent_coef_loss   | 0.0925   |
|    learning_rate   | 0.0003   |
|    n_updates       | 67499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 593      |
|    critic_loss     | 379      |
|    ent_coef        | 0.731    |
|    ent_coef_loss   | -0.0371  |
|    learning_rate   | 0.0003   |
|    n_updates       | 67899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 604      |
|    critic_loss     | 339      |
|    ent_coef        | 0.739    |
|    ent_coef_loss   | -0.0407  |
|    learning_rate   | 0.0003   |
|    n_updates       | 68299    |
---------------------------------
=== Iterazione IRL 241 ===
Loss reward (iter 241): 4.276916980743408
=== Iterazione IRL 242 ===
Loss reward (iter 242): 2.8350563049316406
=== Iterazione IRL 243 ===
Loss reward (iter 243): 4.736271858215332
=== Iterazione IRL 244 ===
Loss reward (iter 244): 3.9571096897125244
=== Iterazione IRL 245 ===
Loss reward (iter 245): 3.242493152618408
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 614      |
|    critic_loss     | 239      |
|    ent_coef        | 0.714    |
|    ent_coef_loss   | -0.0281  |
|    learning_rate   | 0.0003   |
|    n_updates       | 68899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 614      |
|    critic_loss     | 335      |
|    ent_coef        | 0.694    |
|    ent_coef_loss   | -0.00967 |
|    learning_rate   | 0.0003   |
|    n_updates       | 69299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 617      |
|    critic_loss     | 237      |
|    ent_coef        | 0.689    |
|    ent_coef_loss   | -0.0802  |
|    learning_rate   | 0.0003   |
|    n_updates       | 69699    |
---------------------------------
=== Iterazione IRL 246 ===
Loss reward (iter 246): 4.221001148223877
=== Iterazione IRL 247 ===
Loss reward (iter 247): 5.1808037757873535
=== Iterazione IRL 248 ===
Loss reward (iter 248): 5.378134727478027
=== Iterazione IRL 249 ===
Loss reward (iter 249): 4.522327423095703
=== Iterazione IRL 250 ===
Loss reward (iter 250): 5.302590847015381
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 635      |
|    critic_loss     | 286      |
|    ent_coef        | 0.667    |
|    ent_coef_loss   | -0.00807 |
|    learning_rate   | 0.0003   |
|    n_updates       | 70299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 633      |
|    critic_loss     | 271      |
|    ent_coef        | 0.649    |
|    ent_coef_loss   | -0.0954  |
|    learning_rate   | 0.0003   |
|    n_updates       | 70699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 649      |
|    critic_loss     | 276      |
|    ent_coef        | 0.652    |
|    ent_coef_loss   | -0.0125  |
|    learning_rate   | 0.0003   |
|    n_updates       | 71099    |
---------------------------------
=== Iterazione IRL 251 ===
Loss reward (iter 251): 6.992369651794434
=== Iterazione IRL 252 ===
Loss reward (iter 252): 6.202877044677734
=== Iterazione IRL 253 ===
Loss reward (iter 253): 4.991018772125244
=== Iterazione IRL 254 ===
Loss reward (iter 254): 4.996077537536621
=== Iterazione IRL 255 ===
Loss reward (iter 255): 5.322890281677246
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 671      |
|    critic_loss     | 171      |
|    ent_coef        | 0.67     |
|    ent_coef_loss   | -0.0379  |
|    learning_rate   | 0.0003   |
|    n_updates       | 71699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 678      |
|    critic_loss     | 198      |
|    ent_coef        | 0.7      |
|    ent_coef_loss   | 0.0394   |
|    learning_rate   | 0.0003   |
|    n_updates       | 72099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 682      |
|    critic_loss     | 400      |
|    ent_coef        | 0.71     |
|    ent_coef_loss   | 0.069    |
|    learning_rate   | 0.0003   |
|    n_updates       | 72499    |
---------------------------------
=== Iterazione IRL 256 ===
Loss reward (iter 256): 4.170756816864014
=== Iterazione IRL 257 ===
Loss reward (iter 257): 3.2144737243652344
=== Iterazione IRL 258 ===
Loss reward (iter 258): 2.9731175899505615
=== Iterazione IRL 259 ===
Loss reward (iter 259): 1.5018547773361206
=== Iterazione IRL 260 ===
Loss reward (iter 260): 5.890799045562744
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 691      |
|    critic_loss     | 306      |
|    ent_coef        | 0.744    |
|    ent_coef_loss   | 0.0655   |
|    learning_rate   | 0.0003   |
|    n_updates       | 73099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 690      |
|    critic_loss     | 554      |
|    ent_coef        | 0.761    |
|    ent_coef_loss   | 0.0684   |
|    learning_rate   | 0.0003   |
|    n_updates       | 73499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 714      |
|    critic_loss     | 338      |
|    ent_coef        | 0.786    |
|    ent_coef_loss   | 0.0605   |
|    learning_rate   | 0.0003   |
|    n_updates       | 73899    |
---------------------------------
=== Iterazione IRL 261 ===
Loss reward (iter 261): 5.0163798332214355
=== Iterazione IRL 262 ===
Loss reward (iter 262): 4.855238437652588
=== Iterazione IRL 263 ===
Loss reward (iter 263): 3.9808707237243652
=== Iterazione IRL 264 ===
Loss reward (iter 264): 5.155368328094482
=== Iterazione IRL 265 ===
Loss reward (iter 265): 4.982652187347412
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 149      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 717      |
|    critic_loss     | 388      |
|    ent_coef        | 0.793    |
|    ent_coef_loss   | 0.042    |
|    learning_rate   | 0.0003   |
|    n_updates       | 74499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 715      |
|    critic_loss     | 353      |
|    ent_coef        | 0.804    |
|    ent_coef_loss   | 0.023    |
|    learning_rate   | 0.0003   |
|    n_updates       | 74899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 717      |
|    critic_loss     | 259      |
|    ent_coef        | 0.808    |
|    ent_coef_loss   | -0.0112  |
|    learning_rate   | 0.0003   |
|    n_updates       | 75299    |
---------------------------------
=== Iterazione IRL 266 ===
Loss reward (iter 266): 4.677797794342041
=== Iterazione IRL 267 ===
Loss reward (iter 267): 5.084041118621826
=== Iterazione IRL 268 ===
Loss reward (iter 268): 3.1821889877319336
=== Iterazione IRL 269 ===
Loss reward (iter 269): 4.141404151916504
=== Iterazione IRL 270 ===
Loss reward (iter 270): 4.489424705505371
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 739      |
|    critic_loss     | 230      |
|    ent_coef        | 0.813    |
|    ent_coef_loss   | 0.00442  |
|    learning_rate   | 0.0003   |
|    n_updates       | 75899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 748      |
|    critic_loss     | 196      |
|    ent_coef        | 0.804    |
|    ent_coef_loss   | -0.0477  |
|    learning_rate   | 0.0003   |
|    n_updates       | 76299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 748      |
|    critic_loss     | 360      |
|    ent_coef        | 0.803    |
|    ent_coef_loss   | 0.0271   |
|    learning_rate   | 0.0003   |
|    n_updates       | 76699    |
---------------------------------
=== Iterazione IRL 271 ===
Loss reward (iter 271): 3.3904597759246826
=== Iterazione IRL 272 ===
Loss reward (iter 272): 4.457076549530029
=== Iterazione IRL 273 ===
Loss reward (iter 273): 4.485431671142578
=== Iterazione IRL 274 ===
Loss reward (iter 274): 4.629696846008301
=== Iterazione IRL 275 ===
Loss reward (iter 275): 5.1598429679870605
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 759      |
|    critic_loss     | 378      |
|    ent_coef        | 0.798    |
|    ent_coef_loss   | -0.019   |
|    learning_rate   | 0.0003   |
|    n_updates       | 77299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 756      |
|    critic_loss     | 289      |
|    ent_coef        | 0.803    |
|    ent_coef_loss   | -0.0292  |
|    learning_rate   | 0.0003   |
|    n_updates       | 77699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 755      |
|    critic_loss     | 422      |
|    ent_coef        | 0.802    |
|    ent_coef_loss   | 0.00125  |
|    learning_rate   | 0.0003   |
|    n_updates       | 78099    |
---------------------------------
=== Iterazione IRL 276 ===
Loss reward (iter 276): 4.254335403442383
=== Iterazione IRL 277 ===
Loss reward (iter 277): 3.4711334705352783
=== Iterazione IRL 278 ===
Loss reward (iter 278): 4.7136430740356445
=== Iterazione IRL 279 ===
Loss reward (iter 279): 3.7032697200775146
=== Iterazione IRL 280 ===
Loss reward (iter 280): 3.6934452056884766
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 783      |
|    critic_loss     | 347      |
|    ent_coef        | 0.804    |
|    ent_coef_loss   | 0.0039   |
|    learning_rate   | 0.0003   |
|    n_updates       | 78699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 783      |
|    critic_loss     | 423      |
|    ent_coef        | 0.818    |
|    ent_coef_loss   | 0.00203  |
|    learning_rate   | 0.0003   |
|    n_updates       | 79099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 766      |
|    critic_loss     | 371      |
|    ent_coef        | 0.835    |
|    ent_coef_loss   | 0.0387   |
|    learning_rate   | 0.0003   |
|    n_updates       | 79499    |
---------------------------------
=== Iterazione IRL 281 ===
Loss reward (iter 281): 6.211091041564941
=== Iterazione IRL 282 ===
Loss reward (iter 282): 4.692407608032227
=== Iterazione IRL 283 ===
Loss reward (iter 283): 4.503367900848389
=== Iterazione IRL 284 ===
Loss reward (iter 284): 4.5442423820495605
=== Iterazione IRL 285 ===
Loss reward (iter 285): 4.825260162353516
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 153       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | 786       |
|    critic_loss     | 348       |
|    ent_coef        | 0.87      |
|    ent_coef_loss   | -0.000452 |
|    learning_rate   | 0.0003    |
|    n_updates       | 80099     |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 781      |
|    critic_loss     | 220      |
|    ent_coef        | 0.91     |
|    ent_coef_loss   | 0.021    |
|    learning_rate   | 0.0003   |
|    n_updates       | 80499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 797      |
|    critic_loss     | 601      |
|    ent_coef        | 0.988    |
|    ent_coef_loss   | 0.000295 |
|    learning_rate   | 0.0003   |
|    n_updates       | 80899    |
---------------------------------
=== Iterazione IRL 286 ===
Loss reward (iter 286): 3.152549982070923
=== Iterazione IRL 287 ===
Loss reward (iter 287): 2.7173519134521484
=== Iterazione IRL 288 ===
Loss reward (iter 288): 1.9040805101394653
=== Iterazione IRL 289 ===
Loss reward (iter 289): 4.91760778427124
=== Iterazione IRL 290 ===
Loss reward (iter 290): 4.206079483032227
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 790      |
|    critic_loss     | 764      |
|    ent_coef        | 1.06     |
|    ent_coef_loss   | 0.00038  |
|    learning_rate   | 0.0003   |
|    n_updates       | 81499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 819      |
|    critic_loss     | 524      |
|    ent_coef        | 1.07     |
|    ent_coef_loss   | 0.00255  |
|    learning_rate   | 0.0003   |
|    n_updates       | 81899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 810      |
|    critic_loss     | 807      |
|    ent_coef        | 1.07     |
|    ent_coef_loss   | -0.0102  |
|    learning_rate   | 0.0003   |
|    n_updates       | 82299    |
---------------------------------
=== Iterazione IRL 291 ===
Loss reward (iter 291): 5.411491870880127
=== Iterazione IRL 292 ===
Loss reward (iter 292): 4.284280300140381
=== Iterazione IRL 293 ===
Loss reward (iter 293): 4.980644702911377
=== Iterazione IRL 294 ===
Loss reward (iter 294): 5.000760078430176
=== Iterazione IRL 295 ===
Loss reward (iter 295): 5.3371052742004395
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 814      |
|    critic_loss     | 556      |
|    ent_coef        | 1.04     |
|    ent_coef_loss   | -0.00124 |
|    learning_rate   | 0.0003   |
|    n_updates       | 82899    |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 135       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | 829       |
|    critic_loss     | 654       |
|    ent_coef        | 1         |
|    ent_coef_loss   | -0.000431 |
|    learning_rate   | 0.0003    |
|    n_updates       | 83299     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 129       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | 847       |
|    critic_loss     | 631       |
|    ent_coef        | 0.978     |
|    ent_coef_loss   | -0.000657 |
|    learning_rate   | 0.0003    |
|    n_updates       | 83699     |
----------------------------------
=== Iterazione IRL 296 ===
Loss reward (iter 296): 4.723597049713135
=== Iterazione IRL 297 ===
Loss reward (iter 297): 4.973021984100342
=== Iterazione IRL 298 ===
Loss reward (iter 298): 2.6183218955993652
=== Iterazione IRL 299 ===
Loss reward (iter 299): 5.3958563804626465
=== Iterazione IRL 300 ===
Loss reward (iter 300): 3.158731460571289
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 855      |
|    critic_loss     | 599      |
|    ent_coef        | 0.91     |
|    ent_coef_loss   | -0.0171  |
|    learning_rate   | 0.0003   |
|    n_updates       | 84299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 850      |
|    critic_loss     | 402      |
|    ent_coef        | 0.891    |
|    ent_coef_loss   | 0.00863  |
|    learning_rate   | 0.0003   |
|    n_updates       | 84699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 855      |
|    critic_loss     | 498      |
|    ent_coef        | 0.864    |
|    ent_coef_loss   | -0.0103  |
|    learning_rate   | 0.0003   |
|    n_updates       | 85099    |
---------------------------------
=== Iterazione IRL 301 ===
Loss reward (iter 301): 5.083679676055908
=== Iterazione IRL 302 ===
Loss reward (iter 302): 5.462357521057129
=== Iterazione IRL 303 ===
Loss reward (iter 303): 4.783759117126465
=== Iterazione IRL 304 ===
Loss reward (iter 304): 4.993236064910889
=== Iterazione IRL 305 ===
Loss reward (iter 305): 5.090003490447998
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 870      |
|    critic_loss     | 883      |
|    ent_coef        | 0.807    |
|    ent_coef_loss   | -0.0164  |
|    learning_rate   | 0.0003   |
|    n_updates       | 85699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 882      |
|    critic_loss     | 579      |
|    ent_coef        | 0.826    |
|    ent_coef_loss   | 0.0349   |
|    learning_rate   | 0.0003   |
|    n_updates       | 86099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 886      |
|    critic_loss     | 655      |
|    ent_coef        | 0.893    |
|    ent_coef_loss   | 0.00316  |
|    learning_rate   | 0.0003   |
|    n_updates       | 86499    |
---------------------------------
=== Iterazione IRL 306 ===
Loss reward (iter 306): 4.384731769561768
=== Iterazione IRL 307 ===
Loss reward (iter 307): 4.238571643829346
=== Iterazione IRL 308 ===
Loss reward (iter 308): 3.899606227874756
=== Iterazione IRL 309 ===
Loss reward (iter 309): 3.668921709060669
=== Iterazione IRL 310 ===
Loss reward (iter 310): 4.7891082763671875
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 904      |
|    critic_loss     | 1.17e+03 |
|    ent_coef        | 0.946    |
|    ent_coef_loss   | -0.00241 |
|    learning_rate   | 0.0003   |
|    n_updates       | 87099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 906      |
|    critic_loss     | 611      |
|    ent_coef        | 0.913    |
|    ent_coef_loss   | 0.00728  |
|    learning_rate   | 0.0003   |
|    n_updates       | 87499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 922      |
|    critic_loss     | 695      |
|    ent_coef        | 0.909    |
|    ent_coef_loss   | -0.00113 |
|    learning_rate   | 0.0003   |
|    n_updates       | 87899    |
---------------------------------
=== Iterazione IRL 311 ===
Loss reward (iter 311): 5.183864116668701
=== Iterazione IRL 312 ===
Loss reward (iter 312): 3.7083582878112793
=== Iterazione IRL 313 ===
Loss reward (iter 313): 5.750561237335205
=== Iterazione IRL 314 ===
Loss reward (iter 314): 5.059136867523193
=== Iterazione IRL 315 ===
Loss reward (iter 315): 3.681467294692993
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 935      |
|    critic_loss     | 476      |
|    ent_coef        | 0.887    |
|    ent_coef_loss   | 0.0108   |
|    learning_rate   | 0.0003   |
|    n_updates       | 88499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 946      |
|    critic_loss     | 768      |
|    ent_coef        | 0.894    |
|    ent_coef_loss   | 0.00304  |
|    learning_rate   | 0.0003   |
|    n_updates       | 88899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 958      |
|    critic_loss     | 614      |
|    ent_coef        | 0.893    |
|    ent_coef_loss   | 0.0133   |
|    learning_rate   | 0.0003   |
|    n_updates       | 89299    |
---------------------------------
=== Iterazione IRL 316 ===
Loss reward (iter 316): 5.098962783813477
=== Iterazione IRL 317 ===
Loss reward (iter 317): 3.054602861404419
=== Iterazione IRL 318 ===
Loss reward (iter 318): 4.32210111618042
=== Iterazione IRL 319 ===
Loss reward (iter 319): 4.481010913848877
=== Iterazione IRL 320 ===
Loss reward (iter 320): 4.728095531463623
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 967      |
|    critic_loss     | 751      |
|    ent_coef        | 0.904    |
|    ent_coef_loss   | -0.00501 |
|    learning_rate   | 0.0003   |
|    n_updates       | 89899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 961      |
|    critic_loss     | 456      |
|    ent_coef        | 0.925    |
|    ent_coef_loss   | 0.00266  |
|    learning_rate   | 0.0003   |
|    n_updates       | 90299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 991      |
|    critic_loss     | 790      |
|    ent_coef        | 0.941    |
|    ent_coef_loss   | 0.00706  |
|    learning_rate   | 0.0003   |
|    n_updates       | 90699    |
---------------------------------
=== Iterazione IRL 321 ===
Loss reward (iter 321): 5.67366886138916
=== Iterazione IRL 322 ===
Loss reward (iter 322): 5.171414375305176
=== Iterazione IRL 323 ===
Loss reward (iter 323): 3.5648813247680664
=== Iterazione IRL 324 ===
Loss reward (iter 324): 5.490327835083008
=== Iterazione IRL 325 ===
Loss reward (iter 325): 5.276575565338135
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 990      |
|    critic_loss     | 601      |
|    ent_coef        | 0.972    |
|    ent_coef_loss   | -0.00123 |
|    learning_rate   | 0.0003   |
|    n_updates       | 91299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 993      |
|    critic_loss     | 890      |
|    ent_coef        | 0.97     |
|    ent_coef_loss   | -0.00162 |
|    learning_rate   | 0.0003   |
|    n_updates       | 91699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.01e+03 |
|    critic_loss     | 593      |
|    ent_coef        | 0.948    |
|    ent_coef_loss   | -0.00369 |
|    learning_rate   | 0.0003   |
|    n_updates       | 92099    |
---------------------------------
=== Iterazione IRL 326 ===
Loss reward (iter 326): 3.769981622695923
=== Iterazione IRL 327 ===
Loss reward (iter 327): 3.532737970352173
=== Iterazione IRL 328 ===
Loss reward (iter 328): 3.8408164978027344
=== Iterazione IRL 329 ===
Loss reward (iter 329): 3.878154993057251
=== Iterazione IRL 330 ===
Loss reward (iter 330): 5.543971061706543
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.02e+03 |
|    critic_loss     | 740      |
|    ent_coef        | 0.928    |
|    ent_coef_loss   | 0.0102   |
|    learning_rate   | 0.0003   |
|    n_updates       | 92699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.04e+03 |
|    critic_loss     | 467      |
|    ent_coef        | 0.906    |
|    ent_coef_loss   | -0.0215  |
|    learning_rate   | 0.0003   |
|    n_updates       | 93099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.05e+03 |
|    critic_loss     | 875      |
|    ent_coef        | 0.897    |
|    ent_coef_loss   | -0.0105  |
|    learning_rate   | 0.0003   |
|    n_updates       | 93499    |
---------------------------------
=== Iterazione IRL 331 ===
Loss reward (iter 331): 5.123905658721924
=== Iterazione IRL 332 ===
Loss reward (iter 332): 5.413205146789551
=== Iterazione IRL 333 ===
Loss reward (iter 333): 5.115532875061035
=== Iterazione IRL 334 ===
Loss reward (iter 334): 4.837487697601318
=== Iterazione IRL 335 ===
Loss reward (iter 335): 5.745718002319336
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.03e+03 |
|    critic_loss     | 690      |
|    ent_coef        | 0.894    |
|    ent_coef_loss   | 0.00431  |
|    learning_rate   | 0.0003   |
|    n_updates       | 94099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.01e+03 |
|    critic_loss     | 615      |
|    ent_coef        | 0.891    |
|    ent_coef_loss   | 0.0168   |
|    learning_rate   | 0.0003   |
|    n_updates       | 94499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.04e+03 |
|    critic_loss     | 625      |
|    ent_coef        | 0.885    |
|    ent_coef_loss   | 0.0217   |
|    learning_rate   | 0.0003   |
|    n_updates       | 94899    |
---------------------------------
=== Iterazione IRL 336 ===
Loss reward (iter 336): 5.5919718742370605
=== Iterazione IRL 337 ===
Loss reward (iter 337): 5.31321382522583
=== Iterazione IRL 338 ===
Loss reward (iter 338): 5.078569412231445
=== Iterazione IRL 339 ===
Loss reward (iter 339): 4.414413928985596
=== Iterazione IRL 340 ===
Loss reward (iter 340): 5.243031024932861
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.08e+03 |
|    critic_loss     | 421      |
|    ent_coef        | 0.884    |
|    ent_coef_loss   | 0.0163   |
|    learning_rate   | 0.0003   |
|    n_updates       | 95499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.08e+03 |
|    critic_loss     | 615      |
|    ent_coef        | 0.87     |
|    ent_coef_loss   | -0.0257  |
|    learning_rate   | 0.0003   |
|    n_updates       | 95899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.07e+03 |
|    critic_loss     | 854      |
|    ent_coef        | 0.867    |
|    ent_coef_loss   | 0.0191   |
|    learning_rate   | 0.0003   |
|    n_updates       | 96299    |
---------------------------------
=== Iterazione IRL 341 ===
Loss reward (iter 341): 4.968178749084473
=== Iterazione IRL 342 ===
Loss reward (iter 342): 4.594554424285889
=== Iterazione IRL 343 ===
Loss reward (iter 343): 5.31217622756958
=== Iterazione IRL 344 ===
Loss reward (iter 344): 5.329176902770996
=== Iterazione IRL 345 ===
Loss reward (iter 345): 4.301910400390625
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.1e+03  |
|    critic_loss     | 823      |
|    ent_coef        | 0.857    |
|    ent_coef_loss   | 0.00865  |
|    learning_rate   | 0.0003   |
|    n_updates       | 96899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.09e+03 |
|    critic_loss     | 874      |
|    ent_coef        | 0.842    |
|    ent_coef_loss   | 0.0155   |
|    learning_rate   | 0.0003   |
|    n_updates       | 97299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.07e+03 |
|    critic_loss     | 164      |
|    ent_coef        | 0.824    |
|    ent_coef_loss   | -0.0528  |
|    learning_rate   | 0.0003   |
|    n_updates       | 97699    |
---------------------------------
=== Iterazione IRL 346 ===
Loss reward (iter 346): 5.460344314575195
=== Iterazione IRL 347 ===
Loss reward (iter 347): 4.950064182281494
=== Iterazione IRL 348 ===
Loss reward (iter 348): 4.869898319244385
=== Iterazione IRL 349 ===
Loss reward (iter 349): 5.538292407989502
=== Iterazione IRL 350 ===
Loss reward (iter 350): 5.702717304229736
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.1e+03  |
|    critic_loss     | 556      |
|    ent_coef        | 0.768    |
|    ent_coef_loss   | -0.0147  |
|    learning_rate   | 0.0003   |
|    n_updates       | 98299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.08e+03 |
|    critic_loss     | 790      |
|    ent_coef        | 0.763    |
|    ent_coef_loss   | 0.0594   |
|    learning_rate   | 0.0003   |
|    n_updates       | 98699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.08e+03 |
|    critic_loss     | 355      |
|    ent_coef        | 0.772    |
|    ent_coef_loss   | -0.0172  |
|    learning_rate   | 0.0003   |
|    n_updates       | 99099    |
---------------------------------
=== Iterazione IRL 351 ===
Loss reward (iter 351): 5.280635356903076
=== Iterazione IRL 352 ===
Loss reward (iter 352): 5.649334907531738
=== Iterazione IRL 353 ===
Loss reward (iter 353): 5.982721328735352
=== Iterazione IRL 354 ===
Loss reward (iter 354): 5.713308334350586
=== Iterazione IRL 355 ===
Loss reward (iter 355): 5.22686243057251
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.14e+03 |
|    critic_loss     | 986      |
|    ent_coef        | 0.802    |
|    ent_coef_loss   | 0.0356   |
|    learning_rate   | 0.0003   |
|    n_updates       | 99699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.12e+03 |
|    critic_loss     | 876      |
|    ent_coef        | 0.824    |
|    ent_coef_loss   | 0.00131  |
|    learning_rate   | 0.0003   |
|    n_updates       | 100099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.09e+03 |
|    critic_loss     | 453      |
|    ent_coef        | 0.855    |
|    ent_coef_loss   | 0.011    |
|    learning_rate   | 0.0003   |
|    n_updates       | 100499   |
---------------------------------
=== Iterazione IRL 356 ===
Loss reward (iter 356): 6.221573352813721
=== Iterazione IRL 357 ===
Loss reward (iter 357): 6.008195400238037
=== Iterazione IRL 358 ===
Loss reward (iter 358): 6.217023849487305
=== Iterazione IRL 359 ===
Loss reward (iter 359): 5.573450088500977
=== Iterazione IRL 360 ===
Loss reward (iter 360): 6.321560382843018
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.11e+03 |
|    critic_loss     | 388      |
|    ent_coef        | 0.878    |
|    ent_coef_loss   | -0.017   |
|    learning_rate   | 0.0003   |
|    n_updates       | 101099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.09e+03 |
|    critic_loss     | 463      |
|    ent_coef        | 0.9      |
|    ent_coef_loss   | 0.0261   |
|    learning_rate   | 0.0003   |
|    n_updates       | 101499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.09e+03 |
|    critic_loss     | 541      |
|    ent_coef        | 0.909    |
|    ent_coef_loss   | -0.0215  |
|    learning_rate   | 0.0003   |
|    n_updates       | 101899   |
---------------------------------
=== Iterazione IRL 361 ===
Loss reward (iter 361): 5.982350826263428
=== Iterazione IRL 362 ===
Loss reward (iter 362): 5.289482593536377
=== Iterazione IRL 363 ===
Loss reward (iter 363): 5.759903907775879
=== Iterazione IRL 364 ===
Loss reward (iter 364): 5.946012496948242
=== Iterazione IRL 365 ===
Loss reward (iter 365): 5.545769214630127
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.16e+03 |
|    critic_loss     | 622      |
|    ent_coef        | 0.889    |
|    ent_coef_loss   | 0.000243 |
|    learning_rate   | 0.0003   |
|    n_updates       | 102499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.09e+03 |
|    critic_loss     | 486      |
|    ent_coef        | 0.88     |
|    ent_coef_loss   | 0.00718  |
|    learning_rate   | 0.0003   |
|    n_updates       | 102899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.1e+03  |
|    critic_loss     | 439      |
|    ent_coef        | 0.881    |
|    ent_coef_loss   | -0.0106  |
|    learning_rate   | 0.0003   |
|    n_updates       | 103299   |
---------------------------------
=== Iterazione IRL 366 ===
Loss reward (iter 366): 5.542462348937988
=== Iterazione IRL 367 ===
Loss reward (iter 367): 5.376458644866943
=== Iterazione IRL 368 ===
Loss reward (iter 368): 5.769526958465576
=== Iterazione IRL 369 ===
Loss reward (iter 369): 5.753708362579346
=== Iterazione IRL 370 ===
Loss reward (iter 370): 5.731012344360352
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.09e+03 |
|    critic_loss     | 462      |
|    ent_coef        | 0.91     |
|    ent_coef_loss   | 0.00311  |
|    learning_rate   | 0.0003   |
|    n_updates       | 103899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.08e+03 |
|    critic_loss     | 354      |
|    ent_coef        | 0.911    |
|    ent_coef_loss   | -0.00169 |
|    learning_rate   | 0.0003   |
|    n_updates       | 104299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.12e+03 |
|    critic_loss     | 460      |
|    ent_coef        | 0.888    |
|    ent_coef_loss   | -0.014   |
|    learning_rate   | 0.0003   |
|    n_updates       | 104699   |
---------------------------------
=== Iterazione IRL 371 ===
Loss reward (iter 371): 5.772470951080322
=== Iterazione IRL 372 ===
Loss reward (iter 372): 6.301284313201904
=== Iterazione IRL 373 ===
Loss reward (iter 373): 5.784252643585205
=== Iterazione IRL 374 ===
Loss reward (iter 374): 5.065430164337158
=== Iterazione IRL 375 ===
Loss reward (iter 375): 6.543210983276367
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.13e+03 |
|    critic_loss     | 286      |
|    ent_coef        | 0.871    |
|    ent_coef_loss   | 0.0241   |
|    learning_rate   | 0.0003   |
|    n_updates       | 105299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.18e+03 |
|    critic_loss     | 773      |
|    ent_coef        | 0.889    |
|    ent_coef_loss   | 0.000692 |
|    learning_rate   | 0.0003   |
|    n_updates       | 105699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.09e+03 |
|    critic_loss     | 365      |
|    ent_coef        | 0.903    |
|    ent_coef_loss   | 0.0119   |
|    learning_rate   | 0.0003   |
|    n_updates       | 106099   |
---------------------------------
=== Iterazione IRL 376 ===
Loss reward (iter 376): 5.459023952484131
=== Iterazione IRL 377 ===
Loss reward (iter 377): 4.853961944580078
=== Iterazione IRL 378 ===
Loss reward (iter 378): 5.221548557281494
=== Iterazione IRL 379 ===
Loss reward (iter 379): 5.363189697265625
=== Iterazione IRL 380 ===
Loss reward (iter 380): 5.489939212799072
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.13e+03 |
|    critic_loss     | 471      |
|    ent_coef        | 0.92     |
|    ent_coef_loss   | 0.0108   |
|    learning_rate   | 0.0003   |
|    n_updates       | 106699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.15e+03 |
|    critic_loss     | 365      |
|    ent_coef        | 0.9      |
|    ent_coef_loss   | -0.00959 |
|    learning_rate   | 0.0003   |
|    n_updates       | 107099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.08e+03 |
|    critic_loss     | 424      |
|    ent_coef        | 0.894    |
|    ent_coef_loss   | 0.00125  |
|    learning_rate   | 0.0003   |
|    n_updates       | 107499   |
---------------------------------
=== Iterazione IRL 381 ===
Loss reward (iter 381): 5.80503511428833
=== Iterazione IRL 382 ===
Loss reward (iter 382): 5.140004634857178
=== Iterazione IRL 383 ===
Loss reward (iter 383): 5.698025226593018
=== Iterazione IRL 384 ===
Loss reward (iter 384): 5.4581708908081055
=== Iterazione IRL 385 ===
Loss reward (iter 385): 5.446206569671631
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.11e+03 |
|    critic_loss     | 369      |
|    ent_coef        | 0.907    |
|    ent_coef_loss   | 0.00175  |
|    learning_rate   | 0.0003   |
|    n_updates       | 108099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.1e+03  |
|    critic_loss     | 576      |
|    ent_coef        | 0.898    |
|    ent_coef_loss   | -0.00886 |
|    learning_rate   | 0.0003   |
|    n_updates       | 108499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.13e+03 |
|    critic_loss     | 362      |
|    ent_coef        | 0.891    |
|    ent_coef_loss   | -0.00461 |
|    learning_rate   | 0.0003   |
|    n_updates       | 108899   |
---------------------------------
=== Iterazione IRL 386 ===
Loss reward (iter 386): 6.169610977172852
=== Iterazione IRL 387 ===
Loss reward (iter 387): 5.606048583984375
=== Iterazione IRL 388 ===
Loss reward (iter 388): 5.553379058837891
=== Iterazione IRL 389 ===
Loss reward (iter 389): 5.858832359313965
=== Iterazione IRL 390 ===
Loss reward (iter 390): 5.346075057983398
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 153       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | 1.14e+03  |
|    critic_loss     | 628       |
|    ent_coef        | 0.905     |
|    ent_coef_loss   | -0.000723 |
|    learning_rate   | 0.0003    |
|    n_updates       | 109499    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.19e+03 |
|    critic_loss     | 648      |
|    ent_coef        | 0.902    |
|    ent_coef_loss   | 0.0155   |
|    learning_rate   | 0.0003   |
|    n_updates       | 109899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.17e+03 |
|    critic_loss     | 620      |
|    ent_coef        | 0.895    |
|    ent_coef_loss   | 0.0291   |
|    learning_rate   | 0.0003   |
|    n_updates       | 110299   |
---------------------------------
=== Iterazione IRL 391 ===
Loss reward (iter 391): 5.697332382202148
=== Iterazione IRL 392 ===
Loss reward (iter 392): 5.796017646789551
=== Iterazione IRL 393 ===
Loss reward (iter 393): 5.908658504486084
=== Iterazione IRL 394 ===
Loss reward (iter 394): 5.494747161865234
=== Iterazione IRL 395 ===
Loss reward (iter 395): 5.593376636505127
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.16e+03 |
|    critic_loss     | 659      |
|    ent_coef        | 0.885    |
|    ent_coef_loss   | 0.0138   |
|    learning_rate   | 0.0003   |
|    n_updates       | 110899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.14e+03 |
|    critic_loss     | 423      |
|    ent_coef        | 0.878    |
|    ent_coef_loss   | 0.00861  |
|    learning_rate   | 0.0003   |
|    n_updates       | 111299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.09e+03 |
|    critic_loss     | 341      |
|    ent_coef        | 0.866    |
|    ent_coef_loss   | -0.0105  |
|    learning_rate   | 0.0003   |
|    n_updates       | 111699   |
---------------------------------
=== Iterazione IRL 396 ===
Loss reward (iter 396): 5.009359836578369
=== Iterazione IRL 397 ===
Loss reward (iter 397): 5.163271427154541
=== Iterazione IRL 398 ===
Loss reward (iter 398): 5.052117347717285
=== Iterazione IRL 399 ===
Loss reward (iter 399): 5.8385820388793945
=== Iterazione IRL 400 ===
Loss reward (iter 400): 5.008711338043213
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.11e+03 |
|    critic_loss     | 282      |
|    ent_coef        | 0.873    |
|    ent_coef_loss   | 0.0118   |
|    learning_rate   | 0.0003   |
|    n_updates       | 112299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.18e+03 |
|    critic_loss     | 947      |
|    ent_coef        | 0.858    |
|    ent_coef_loss   | 0.0031   |
|    learning_rate   | 0.0003   |
|    n_updates       | 112699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.17e+03 |
|    critic_loss     | 467      |
|    ent_coef        | 0.87     |
|    ent_coef_loss   | 0.0194   |
|    learning_rate   | 0.0003   |
|    n_updates       | 113099   |
---------------------------------
=== Iterazione IRL 401 ===
Loss reward (iter 401): 5.251526832580566
=== Iterazione IRL 402 ===
Loss reward (iter 402): 5.693843841552734
=== Iterazione IRL 403 ===
Loss reward (iter 403): 4.676631450653076
=== Iterazione IRL 404 ===
Loss reward (iter 404): 5.138977527618408
=== Iterazione IRL 405 ===
Loss reward (iter 405): 5.2029924392700195
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.1e+03  |
|    critic_loss     | 404      |
|    ent_coef        | 0.859    |
|    ent_coef_loss   | -0.0103  |
|    learning_rate   | 0.0003   |
|    n_updates       | 113699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.09e+03 |
|    critic_loss     | 487      |
|    ent_coef        | 0.875    |
|    ent_coef_loss   | 0.0157   |
|    learning_rate   | 0.0003   |
|    n_updates       | 114099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.1e+03  |
|    critic_loss     | 446      |
|    ent_coef        | 0.854    |
|    ent_coef_loss   | -0.00343 |
|    learning_rate   | 0.0003   |
|    n_updates       | 114499   |
---------------------------------
=== Iterazione IRL 406 ===
Loss reward (iter 406): 5.237799167633057
=== Iterazione IRL 407 ===
Loss reward (iter 407): 6.491404056549072
=== Iterazione IRL 408 ===
Loss reward (iter 408): 5.029481887817383
=== Iterazione IRL 409 ===
Loss reward (iter 409): 5.6880998611450195
=== Iterazione IRL 410 ===
Loss reward (iter 410): 5.752721309661865
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.13e+03 |
|    critic_loss     | 632      |
|    ent_coef        | 0.852    |
|    ent_coef_loss   | -0.00481 |
|    learning_rate   | 0.0003   |
|    n_updates       | 115099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.16e+03 |
|    critic_loss     | 1.05e+03 |
|    ent_coef        | 0.863    |
|    ent_coef_loss   | -0.015   |
|    learning_rate   | 0.0003   |
|    n_updates       | 115499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.13e+03 |
|    critic_loss     | 287      |
|    ent_coef        | 0.852    |
|    ent_coef_loss   | 0.00254  |
|    learning_rate   | 0.0003   |
|    n_updates       | 115899   |
---------------------------------
=== Iterazione IRL 411 ===
Loss reward (iter 411): 5.078144073486328
=== Iterazione IRL 412 ===
Loss reward (iter 412): 5.759907245635986
=== Iterazione IRL 413 ===
Loss reward (iter 413): 5.786687850952148
=== Iterazione IRL 414 ===
Loss reward (iter 414): 5.0964484214782715
=== Iterazione IRL 415 ===
Loss reward (iter 415): 5.908181667327881
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.06e+03 |
|    critic_loss     | 227      |
|    ent_coef        | 0.841    |
|    ent_coef_loss   | -0.0369  |
|    learning_rate   | 0.0003   |
|    n_updates       | 116499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.15e+03 |
|    critic_loss     | 568      |
|    ent_coef        | 0.855    |
|    ent_coef_loss   | -0.0099  |
|    learning_rate   | 0.0003   |
|    n_updates       | 116899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.13e+03 |
|    critic_loss     | 491      |
|    ent_coef        | 0.858    |
|    ent_coef_loss   | -0.011   |
|    learning_rate   | 0.0003   |
|    n_updates       | 117299   |
---------------------------------
=== Iterazione IRL 416 ===
Loss reward (iter 416): 5.054713249206543
=== Iterazione IRL 417 ===
Loss reward (iter 417): 5.179467678070068
=== Iterazione IRL 418 ===
Loss reward (iter 418): 5.191391944885254
=== Iterazione IRL 419 ===
Loss reward (iter 419): 5.127427101135254
=== Iterazione IRL 420 ===
Loss reward (iter 420): 5.047122955322266
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.14e+03 |
|    critic_loss     | 426      |
|    ent_coef        | 0.882    |
|    ent_coef_loss   | 0.00771  |
|    learning_rate   | 0.0003   |
|    n_updates       | 117899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.1e+03  |
|    critic_loss     | 561      |
|    ent_coef        | 0.883    |
|    ent_coef_loss   | 0.000185 |
|    learning_rate   | 0.0003   |
|    n_updates       | 118299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.08e+03 |
|    critic_loss     | 601      |
|    ent_coef        | 0.882    |
|    ent_coef_loss   | 0.00444  |
|    learning_rate   | 0.0003   |
|    n_updates       | 118699   |
---------------------------------
=== Iterazione IRL 421 ===
Loss reward (iter 421): 4.993563175201416
=== Iterazione IRL 422 ===
Loss reward (iter 422): 5.798945426940918
=== Iterazione IRL 423 ===
Loss reward (iter 423): 5.6071672439575195
=== Iterazione IRL 424 ===
Loss reward (iter 424): 5.2861223220825195
=== Iterazione IRL 425 ===
Loss reward (iter 425): 5.864577293395996
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.14e+03 |
|    critic_loss     | 472      |
|    ent_coef        | 0.885    |
|    ent_coef_loss   | 0.00646  |
|    learning_rate   | 0.0003   |
|    n_updates       | 119299   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | 1.07e+03  |
|    critic_loss     | 332       |
|    ent_coef        | 0.905     |
|    ent_coef_loss   | -0.000887 |
|    learning_rate   | 0.0003    |
|    n_updates       | 119699    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.08e+03 |
|    critic_loss     | 470      |
|    ent_coef        | 0.904    |
|    ent_coef_loss   | 0.0115   |
|    learning_rate   | 0.0003   |
|    n_updates       | 120099   |
---------------------------------
=== Iterazione IRL 426 ===
Loss reward (iter 426): 4.725159168243408
=== Iterazione IRL 427 ===
Loss reward (iter 427): 4.621109485626221
=== Iterazione IRL 428 ===
Loss reward (iter 428): 5.111799240112305
=== Iterazione IRL 429 ===
Loss reward (iter 429): 5.342520713806152
=== Iterazione IRL 430 ===
Loss reward (iter 430): 5.259417533874512
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.1e+03  |
|    critic_loss     | 321      |
|    ent_coef        | 0.905    |
|    ent_coef_loss   | -0.00609 |
|    learning_rate   | 0.0003   |
|    n_updates       | 120699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.14e+03 |
|    critic_loss     | 997      |
|    ent_coef        | 0.929    |
|    ent_coef_loss   | 0.00471  |
|    learning_rate   | 0.0003   |
|    n_updates       | 121099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.07e+03 |
|    critic_loss     | 563      |
|    ent_coef        | 0.934    |
|    ent_coef_loss   | -0.00865 |
|    learning_rate   | 0.0003   |
|    n_updates       | 121499   |
---------------------------------
=== Iterazione IRL 431 ===
Loss reward (iter 431): 4.993795394897461
=== Iterazione IRL 432 ===
Loss reward (iter 432): 5.191633701324463
=== Iterazione IRL 433 ===
Loss reward (iter 433): 3.548712730407715
=== Iterazione IRL 434 ===
Loss reward (iter 434): 5.0016255378723145
=== Iterazione IRL 435 ===
Loss reward (iter 435): 5.128962993621826
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.14e+03 |
|    critic_loss     | 328      |
|    ent_coef        | 0.934    |
|    ent_coef_loss   | 0.00142  |
|    learning_rate   | 0.0003   |
|    n_updates       | 122099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.13e+03 |
|    critic_loss     | 465      |
|    ent_coef        | 0.938    |
|    ent_coef_loss   | 0.00401  |
|    learning_rate   | 0.0003   |
|    n_updates       | 122499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.11e+03 |
|    critic_loss     | 353      |
|    ent_coef        | 0.942    |
|    ent_coef_loss   | 0.00145  |
|    learning_rate   | 0.0003   |
|    n_updates       | 122899   |
---------------------------------
=== Iterazione IRL 436 ===
Loss reward (iter 436): 4.820087909698486
=== Iterazione IRL 437 ===
Loss reward (iter 437): 5.080710411071777
=== Iterazione IRL 438 ===
Loss reward (iter 438): 4.846607208251953
=== Iterazione IRL 439 ===
Loss reward (iter 439): 4.85978364944458
=== Iterazione IRL 440 ===
Loss reward (iter 440): 4.718039512634277
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.08e+03 |
|    critic_loss     | 758      |
|    ent_coef        | 0.928    |
|    ent_coef_loss   | 0.00854  |
|    learning_rate   | 0.0003   |
|    n_updates       | 123499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.17e+03 |
|    critic_loss     | 653      |
|    ent_coef        | 0.939    |
|    ent_coef_loss   | 0.00671  |
|    learning_rate   | 0.0003   |
|    n_updates       | 123899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.07e+03 |
|    critic_loss     | 318      |
|    ent_coef        | 0.951    |
|    ent_coef_loss   | 0.00109  |
|    learning_rate   | 0.0003   |
|    n_updates       | 124299   |
---------------------------------
=== Iterazione IRL 441 ===
Loss reward (iter 441): 5.498697280883789
=== Iterazione IRL 442 ===
Loss reward (iter 442): 5.141017436981201
=== Iterazione IRL 443 ===
Loss reward (iter 443): 5.713665008544922
=== Iterazione IRL 444 ===
Loss reward (iter 444): 5.50005578994751
=== Iterazione IRL 445 ===
Loss reward (iter 445): 4.964954376220703
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.1e+03  |
|    critic_loss     | 286      |
|    ent_coef        | 0.962    |
|    ent_coef_loss   | 0.00377  |
|    learning_rate   | 0.0003   |
|    n_updates       | 124899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.13e+03 |
|    critic_loss     | 646      |
|    ent_coef        | 0.97     |
|    ent_coef_loss   | 5.3e-05  |
|    learning_rate   | 0.0003   |
|    n_updates       | 125299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.13e+03 |
|    critic_loss     | 440      |
|    ent_coef        | 0.961    |
|    ent_coef_loss   | 0.00238  |
|    learning_rate   | 0.0003   |
|    n_updates       | 125699   |
---------------------------------
=== Iterazione IRL 446 ===
Loss reward (iter 446): 4.671162128448486
=== Iterazione IRL 447 ===
Loss reward (iter 447): 4.658919811248779
=== Iterazione IRL 448 ===
Loss reward (iter 448): 4.919497489929199
=== Iterazione IRL 449 ===
Loss reward (iter 449): 4.511795997619629
=== Iterazione IRL 450 ===
Loss reward (iter 450): 4.812342643737793
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.04e+03 |
|    critic_loss     | 438      |
|    ent_coef        | 0.984    |
|    ent_coef_loss   | -0.00188 |
|    learning_rate   | 0.0003   |
|    n_updates       | 126299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.07e+03 |
|    critic_loss     | 359      |
|    ent_coef        | 0.984    |
|    ent_coef_loss   | -0.00299 |
|    learning_rate   | 0.0003   |
|    n_updates       | 126699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.1e+03  |
|    critic_loss     | 744      |
|    ent_coef        | 0.961    |
|    ent_coef_loss   | 0.000835 |
|    learning_rate   | 0.0003   |
|    n_updates       | 127099   |
---------------------------------
=== Iterazione IRL 451 ===
Loss reward (iter 451): 5.181577205657959
=== Iterazione IRL 452 ===
Loss reward (iter 452): 5.161872386932373
=== Iterazione IRL 453 ===
Loss reward (iter 453): 5.562316417694092
=== Iterazione IRL 454 ===
Loss reward (iter 454): 6.036445617675781
=== Iterazione IRL 455 ===
Loss reward (iter 455): 5.648655891418457
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.05e+03 |
|    critic_loss     | 582      |
|    ent_coef        | 0.974    |
|    ent_coef_loss   | -0.00121 |
|    learning_rate   | 0.0003   |
|    n_updates       | 127699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.03e+03 |
|    critic_loss     | 672      |
|    ent_coef        | 0.973    |
|    ent_coef_loss   | 0.00072  |
|    learning_rate   | 0.0003   |
|    n_updates       | 128099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.08e+03 |
|    critic_loss     | 239      |
|    ent_coef        | 1        |
|    ent_coef_loss   | 0.000149 |
|    learning_rate   | 0.0003   |
|    n_updates       | 128499   |
---------------------------------
=== Iterazione IRL 456 ===
Loss reward (iter 456): 6.057894229888916
=== Iterazione IRL 457 ===
Loss reward (iter 457): 5.273166656494141
=== Iterazione IRL 458 ===
Loss reward (iter 458): 6.008540153503418
=== Iterazione IRL 459 ===
Loss reward (iter 459): 6.460484504699707
=== Iterazione IRL 460 ===
Loss reward (iter 460): 6.241906642913818
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.04e+03 |
|    critic_loss     | 460      |
|    ent_coef        | 0.987    |
|    ent_coef_loss   | 0.000228 |
|    learning_rate   | 0.0003   |
|    n_updates       | 129099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.12e+03 |
|    critic_loss     | 781      |
|    ent_coef        | 0.995    |
|    ent_coef_loss   | 0.000728 |
|    learning_rate   | 0.0003   |
|    n_updates       | 129499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.01e+03 |
|    critic_loss     | 220      |
|    ent_coef        | 0.996    |
|    ent_coef_loss   | 0.000101 |
|    learning_rate   | 0.0003   |
|    n_updates       | 129899   |
---------------------------------
=== Iterazione IRL 461 ===
Loss reward (iter 461): 5.1493916511535645
=== Iterazione IRL 462 ===
Loss reward (iter 462): 4.797927379608154
=== Iterazione IRL 463 ===
Loss reward (iter 463): 4.933877944946289
=== Iterazione IRL 464 ===
Loss reward (iter 464): 5.152706146240234
=== Iterazione IRL 465 ===
Loss reward (iter 465): 4.870856761932373
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.09e+03 |
|    critic_loss     | 536      |
|    ent_coef        | 0.967    |
|    ent_coef_loss   | 0.00331  |
|    learning_rate   | 0.0003   |
|    n_updates       | 130499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.1e+03  |
|    critic_loss     | 416      |
|    ent_coef        | 0.969    |
|    ent_coef_loss   | 0.00229  |
|    learning_rate   | 0.0003   |
|    n_updates       | 130899   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | 1.02e+03  |
|    critic_loss     | 448       |
|    ent_coef        | 0.984     |
|    ent_coef_loss   | -7.02e-05 |
|    learning_rate   | 0.0003    |
|    n_updates       | 131299    |
----------------------------------
=== Iterazione IRL 466 ===
Loss reward (iter 466): 6.067394256591797
=== Iterazione IRL 467 ===
Loss reward (iter 467): 5.9019694328308105
=== Iterazione IRL 468 ===
Loss reward (iter 468): 5.778255939483643
=== Iterazione IRL 469 ===
Loss reward (iter 469): 5.544543266296387
=== Iterazione IRL 470 ===
Loss reward (iter 470): 5.224917411804199
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.07e+03 |
|    critic_loss     | 694      |
|    ent_coef        | 0.983    |
|    ent_coef_loss   | 0.000957 |
|    learning_rate   | 0.0003   |
|    n_updates       | 131899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.03e+03 |
|    critic_loss     | 429      |
|    ent_coef        | 0.995    |
|    ent_coef_loss   | 0.0004   |
|    learning_rate   | 0.0003   |
|    n_updates       | 132299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.07e+03 |
|    critic_loss     | 381      |
|    ent_coef        | 0.985    |
|    ent_coef_loss   | -0.00135 |
|    learning_rate   | 0.0003   |
|    n_updates       | 132699   |
---------------------------------
=== Iterazione IRL 471 ===
Loss reward (iter 471): 5.914821624755859
=== Iterazione IRL 472 ===
Loss reward (iter 472): 5.472087383270264
=== Iterazione IRL 473 ===
Loss reward (iter 473): 5.200000286102295
=== Iterazione IRL 474 ===
Loss reward (iter 474): 5.396279811859131
=== Iterazione IRL 475 ===
Loss reward (iter 475): 6.26794958114624
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.03e+03 |
|    critic_loss     | 380      |
|    ent_coef        | 0.994    |
|    ent_coef_loss   | 0.000298 |
|    learning_rate   | 0.0003   |
|    n_updates       | 133299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.02e+03 |
|    critic_loss     | 246      |
|    ent_coef        | 1.01     |
|    ent_coef_loss   | -0.00174 |
|    learning_rate   | 0.0003   |
|    n_updates       | 133699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.04e+03 |
|    critic_loss     | 327      |
|    ent_coef        | 1.02     |
|    ent_coef_loss   | -0.00383 |
|    learning_rate   | 0.0003   |
|    n_updates       | 134099   |
---------------------------------
=== Iterazione IRL 476 ===
Loss reward (iter 476): 6.728794097900391
=== Iterazione IRL 477 ===
Loss reward (iter 477): 6.096607208251953
=== Iterazione IRL 478 ===
Loss reward (iter 478): 6.059483528137207
=== Iterazione IRL 479 ===
Loss reward (iter 479): 6.0051422119140625
=== Iterazione IRL 480 ===
Loss reward (iter 480): 5.303474426269531
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.01e+03 |
|    critic_loss     | 549      |
|    ent_coef        | 1.03     |
|    ent_coef_loss   | -0.00069 |
|    learning_rate   | 0.0003   |
|    n_updates       | 134699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.03e+03 |
|    critic_loss     | 305      |
|    ent_coef        | 1.03     |
|    ent_coef_loss   | 0.00283  |
|    learning_rate   | 0.0003   |
|    n_updates       | 135099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 974      |
|    critic_loss     | 234      |
|    ent_coef        | 1.03     |
|    ent_coef_loss   | 0.0074   |
|    learning_rate   | 0.0003   |
|    n_updates       | 135499   |
---------------------------------
=== Iterazione IRL 481 ===
Loss reward (iter 481): 5.017995834350586
=== Iterazione IRL 482 ===
Loss reward (iter 482): 4.49772310256958
=== Iterazione IRL 483 ===
Loss reward (iter 483): 4.538201808929443
=== Iterazione IRL 484 ===
Loss reward (iter 484): 5.997097969055176
=== Iterazione IRL 485 ===
Loss reward (iter 485): 4.986544132232666
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.08e+03 |
|    critic_loss     | 390      |
|    ent_coef        | 1.01     |
|    ent_coef_loss   | 0.000276 |
|    learning_rate   | 0.0003   |
|    n_updates       | 136099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 973      |
|    critic_loss     | 241      |
|    ent_coef        | 1.02     |
|    ent_coef_loss   | -0.00211 |
|    learning_rate   | 0.0003   |
|    n_updates       | 136499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.1e+03  |
|    critic_loss     | 795      |
|    ent_coef        | 1.05     |
|    ent_coef_loss   | -0.00454 |
|    learning_rate   | 0.0003   |
|    n_updates       | 136899   |
---------------------------------
=== Iterazione IRL 486 ===
Loss reward (iter 486): 6.139796257019043
=== Iterazione IRL 487 ===
Loss reward (iter 487): 5.410691261291504
=== Iterazione IRL 488 ===
Loss reward (iter 488): 5.142220973968506
=== Iterazione IRL 489 ===
Loss reward (iter 489): 6.031490325927734
=== Iterazione IRL 490 ===
Loss reward (iter 490): 5.917004585266113
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 153       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | 1.02e+03  |
|    critic_loss     | 451       |
|    ent_coef        | 1.05      |
|    ent_coef_loss   | -0.000884 |
|    learning_rate   | 0.0003    |
|    n_updates       | 137499    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 997      |
|    critic_loss     | 630      |
|    ent_coef        | 1.07     |
|    ent_coef_loss   | -0.00713 |
|    learning_rate   | 0.0003   |
|    n_updates       | 137899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.09e+03 |
|    critic_loss     | 323      |
|    ent_coef        | 1.09     |
|    ent_coef_loss   | -0.0105  |
|    learning_rate   | 0.0003   |
|    n_updates       | 138299   |
---------------------------------
=== Iterazione IRL 491 ===
Loss reward (iter 491): 6.549438953399658
=== Iterazione IRL 492 ===
Loss reward (iter 492): 5.793596267700195
=== Iterazione IRL 493 ===
Loss reward (iter 493): 5.733559608459473
=== Iterazione IRL 494 ===
Loss reward (iter 494): 6.070089817047119
=== Iterazione IRL 495 ===
Loss reward (iter 495): 5.903399467468262
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.01e+03 |
|    critic_loss     | 296      |
|    ent_coef        | 1.1      |
|    ent_coef_loss   | 0.00418  |
|    learning_rate   | 0.0003   |
|    n_updates       | 138899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.02e+03 |
|    critic_loss     | 651      |
|    ent_coef        | 1.1      |
|    ent_coef_loss   | -0.0223  |
|    learning_rate   | 0.0003   |
|    n_updates       | 139299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 990      |
|    critic_loss     | 234      |
|    ent_coef        | 1.11     |
|    ent_coef_loss   | -0.0082  |
|    learning_rate   | 0.0003   |
|    n_updates       | 139699   |
---------------------------------
=== Iterazione IRL 496 ===
Loss reward (iter 496): 4.822293281555176
=== Iterazione IRL 497 ===
Loss reward (iter 497): 5.203033447265625
=== Iterazione IRL 498 ===
Loss reward (iter 498): 5.1430535316467285
=== Iterazione IRL 499 ===
Loss reward (iter 499): 5.5642924308776855
=== Iterazione IRL 500 ===
Loss reward (iter 500): 5.160783767700195
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 981      |
|    critic_loss     | 265      |
|    ent_coef        | 1.13     |
|    ent_coef_loss   | -0.00612 |
|    learning_rate   | 0.0003   |
|    n_updates       | 140299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.04e+03 |
|    critic_loss     | 312      |
|    ent_coef        | 1.14     |
|    ent_coef_loss   | -0.0222  |
|    learning_rate   | 0.0003   |
|    n_updates       | 140699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 990      |
|    critic_loss     | 342      |
|    ent_coef        | 1.16     |
|    ent_coef_loss   | -0.00757 |
|    learning_rate   | 0.0003   |
|    n_updates       | 141099   |
---------------------------------
=== Iterazione IRL 501 ===
Loss reward (iter 501): 5.491949558258057
=== Iterazione IRL 502 ===
Loss reward (iter 502): 5.181056022644043
=== Iterazione IRL 503 ===
Loss reward (iter 503): 5.252302169799805
=== Iterazione IRL 504 ===
Loss reward (iter 504): 5.380295753479004
=== Iterazione IRL 505 ===
Loss reward (iter 505): 4.984949588775635
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 986      |
|    critic_loss     | 390      |
|    ent_coef        | 1.18     |
|    ent_coef_loss   | -0.0268  |
|    learning_rate   | 0.0003   |
|    n_updates       | 141699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 979      |
|    critic_loss     | 230      |
|    ent_coef        | 1.18     |
|    ent_coef_loss   | -0.0119  |
|    learning_rate   | 0.0003   |
|    n_updates       | 142099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 987      |
|    critic_loss     | 617      |
|    ent_coef        | 1.19     |
|    ent_coef_loss   | 0.03     |
|    learning_rate   | 0.0003   |
|    n_updates       | 142499   |
---------------------------------
=== Iterazione IRL 506 ===
Loss reward (iter 506): 6.168493747711182
=== Iterazione IRL 507 ===
Loss reward (iter 507): 6.136148929595947
=== Iterazione IRL 508 ===
Loss reward (iter 508): 6.587018013000488
=== Iterazione IRL 509 ===
Loss reward (iter 509): 6.389087677001953
=== Iterazione IRL 510 ===
Loss reward (iter 510): 6.369307041168213
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 995      |
|    critic_loss     | 453      |
|    ent_coef        | 1.17     |
|    ent_coef_loss   | 0.0157   |
|    learning_rate   | 0.0003   |
|    n_updates       | 143099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 949      |
|    critic_loss     | 486      |
|    ent_coef        | 1.17     |
|    ent_coef_loss   | 0.0218   |
|    learning_rate   | 0.0003   |
|    n_updates       | 143499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1e+03    |
|    critic_loss     | 485      |
|    ent_coef        | 1.17     |
|    ent_coef_loss   | 0.00375  |
|    learning_rate   | 0.0003   |
|    n_updates       | 143899   |
---------------------------------
=== Iterazione IRL 511 ===
Loss reward (iter 511): 5.9456892013549805
=== Iterazione IRL 512 ===
Loss reward (iter 512): 5.236674785614014
=== Iterazione IRL 513 ===
Loss reward (iter 513): 6.1778106689453125
=== Iterazione IRL 514 ===
Loss reward (iter 514): 5.935233116149902
=== Iterazione IRL 515 ===
Loss reward (iter 515): 5.805995464324951
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.02e+03 |
|    critic_loss     | 875      |
|    ent_coef        | 1.16     |
|    ent_coef_loss   | -0.00875 |
|    learning_rate   | 0.0003   |
|    n_updates       | 144499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 963      |
|    critic_loss     | 331      |
|    ent_coef        | 1.16     |
|    ent_coef_loss   | 0.00294  |
|    learning_rate   | 0.0003   |
|    n_updates       | 144899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 942      |
|    critic_loss     | 407      |
|    ent_coef        | 1.16     |
|    ent_coef_loss   | -0.0138  |
|    learning_rate   | 0.0003   |
|    n_updates       | 145299   |
---------------------------------
=== Iterazione IRL 516 ===
Loss reward (iter 516): 6.597813129425049
=== Iterazione IRL 517 ===
Loss reward (iter 517): 5.509352207183838
=== Iterazione IRL 518 ===
Loss reward (iter 518): 5.718528747558594
=== Iterazione IRL 519 ===
Loss reward (iter 519): 5.528568267822266
=== Iterazione IRL 520 ===
Loss reward (iter 520): 5.660023212432861
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 993      |
|    critic_loss     | 386      |
|    ent_coef        | 1.17     |
|    ent_coef_loss   | 0.00156  |
|    learning_rate   | 0.0003   |
|    n_updates       | 145899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 988      |
|    critic_loss     | 662      |
|    ent_coef        | 1.17     |
|    ent_coef_loss   | 0.00269  |
|    learning_rate   | 0.0003   |
|    n_updates       | 146299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 960      |
|    critic_loss     | 473      |
|    ent_coef        | 1.19     |
|    ent_coef_loss   | 0.0198   |
|    learning_rate   | 0.0003   |
|    n_updates       | 146699   |
---------------------------------
=== Iterazione IRL 521 ===
Loss reward (iter 521): 5.575162887573242
=== Iterazione IRL 522 ===
Loss reward (iter 522): 5.951940059661865
=== Iterazione IRL 523 ===
Loss reward (iter 523): 6.679462432861328
=== Iterazione IRL 524 ===
Loss reward (iter 524): 6.142605304718018
=== Iterazione IRL 525 ===
Loss reward (iter 525): 6.319852828979492
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 919      |
|    critic_loss     | 323      |
|    ent_coef        | 1.19     |
|    ent_coef_loss   | 0.0287   |
|    learning_rate   | 0.0003   |
|    n_updates       | 147299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 935      |
|    critic_loss     | 376      |
|    ent_coef        | 1.21     |
|    ent_coef_loss   | 0.0036   |
|    learning_rate   | 0.0003   |
|    n_updates       | 147699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 903      |
|    critic_loss     | 395      |
|    ent_coef        | 1.23     |
|    ent_coef_loss   | 0.0137   |
|    learning_rate   | 0.0003   |
|    n_updates       | 148099   |
---------------------------------
=== Iterazione IRL 526 ===
Loss reward (iter 526): 5.806719779968262
=== Iterazione IRL 527 ===
Loss reward (iter 527): 5.701198577880859
=== Iterazione IRL 528 ===
Loss reward (iter 528): 5.745047569274902
=== Iterazione IRL 529 ===
Loss reward (iter 529): 6.032190322875977
=== Iterazione IRL 530 ===
Loss reward (iter 530): 5.4158854484558105
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 957      |
|    critic_loss     | 189      |
|    ent_coef        | 1.23     |
|    ent_coef_loss   | 0.0193   |
|    learning_rate   | 0.0003   |
|    n_updates       | 148699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.05e+03 |
|    critic_loss     | 1.15e+03 |
|    ent_coef        | 1.23     |
|    ent_coef_loss   | -0.00609 |
|    learning_rate   | 0.0003   |
|    n_updates       | 149099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 960      |
|    critic_loss     | 240      |
|    ent_coef        | 1.23     |
|    ent_coef_loss   | 0.0117   |
|    learning_rate   | 0.0003   |
|    n_updates       | 149499   |
---------------------------------
=== Iterazione IRL 531 ===
Loss reward (iter 531): 5.735498428344727
=== Iterazione IRL 532 ===
Loss reward (iter 532): 4.932496070861816
=== Iterazione IRL 533 ===
Loss reward (iter 533): 4.565068244934082
=== Iterazione IRL 534 ===
Loss reward (iter 534): 5.250071048736572
=== Iterazione IRL 535 ===
Loss reward (iter 535): 5.015289783477783
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 943      |
|    critic_loss     | 501      |
|    ent_coef        | 1.23     |
|    ent_coef_loss   | 0.0105   |
|    learning_rate   | 0.0003   |
|    n_updates       | 150099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 956      |
|    critic_loss     | 443      |
|    ent_coef        | 1.23     |
|    ent_coef_loss   | 0.0223   |
|    learning_rate   | 0.0003   |
|    n_updates       | 150499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 929      |
|    critic_loss     | 303      |
|    ent_coef        | 1.21     |
|    ent_coef_loss   | 0.00619  |
|    learning_rate   | 0.0003   |
|    n_updates       | 150899   |
---------------------------------
=== Iterazione IRL 536 ===
Loss reward (iter 536): 5.814830780029297
=== Iterazione IRL 537 ===
Loss reward (iter 537): 5.8643412590026855
=== Iterazione IRL 538 ===
Loss reward (iter 538): 6.001325607299805
=== Iterazione IRL 539 ===
Loss reward (iter 539): 5.387529373168945
=== Iterazione IRL 540 ===
Loss reward (iter 540): 5.39747428894043
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 939      |
|    critic_loss     | 242      |
|    ent_coef        | 1.24     |
|    ent_coef_loss   | 0.0215   |
|    learning_rate   | 0.0003   |
|    n_updates       | 151499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 929      |
|    critic_loss     | 297      |
|    ent_coef        | 1.24     |
|    ent_coef_loss   | 0.00324  |
|    learning_rate   | 0.0003   |
|    n_updates       | 151899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 913      |
|    critic_loss     | 325      |
|    ent_coef        | 1.24     |
|    ent_coef_loss   | -0.0196  |
|    learning_rate   | 0.0003   |
|    n_updates       | 152299   |
---------------------------------
=== Iterazione IRL 541 ===
Loss reward (iter 541): 5.992195129394531
=== Iterazione IRL 542 ===
Loss reward (iter 542): 5.37459659576416
=== Iterazione IRL 543 ===
Loss reward (iter 543): 5.741628646850586
=== Iterazione IRL 544 ===
Loss reward (iter 544): 5.95595121383667
=== Iterazione IRL 545 ===
Loss reward (iter 545): 6.254980087280273
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.01e+03 |
|    critic_loss     | 776      |
|    ent_coef        | 1.2      |
|    ent_coef_loss   | -0.0104  |
|    learning_rate   | 0.0003   |
|    n_updates       | 152899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 917      |
|    critic_loss     | 282      |
|    ent_coef        | 1.19     |
|    ent_coef_loss   | 0.0193   |
|    learning_rate   | 0.0003   |
|    n_updates       | 153299   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | 947       |
|    critic_loss     | 339       |
|    ent_coef        | 1.22      |
|    ent_coef_loss   | -0.000186 |
|    learning_rate   | 0.0003    |
|    n_updates       | 153699    |
----------------------------------
=== Iterazione IRL 546 ===
Loss reward (iter 546): 5.842337608337402
=== Iterazione IRL 547 ===
Loss reward (iter 547): 5.676194190979004
=== Iterazione IRL 548 ===
Loss reward (iter 548): 5.484786033630371
=== Iterazione IRL 549 ===
Loss reward (iter 549): 5.937629699707031
=== Iterazione IRL 550 ===
Loss reward (iter 550): 5.646792411804199
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 935      |
|    critic_loss     | 435      |
|    ent_coef        | 1.24     |
|    ent_coef_loss   | -0.0546  |
|    learning_rate   | 0.0003   |
|    n_updates       | 154299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 961      |
|    critic_loss     | 336      |
|    ent_coef        | 1.25     |
|    ent_coef_loss   | 0.00949  |
|    learning_rate   | 0.0003   |
|    n_updates       | 154699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 905      |
|    critic_loss     | 247      |
|    ent_coef        | 1.24     |
|    ent_coef_loss   | 0.00117  |
|    learning_rate   | 0.0003   |
|    n_updates       | 155099   |
---------------------------------
=== Iterazione IRL 551 ===
Loss reward (iter 551): 5.98995304107666
=== Iterazione IRL 552 ===
Loss reward (iter 552): 5.695918560028076
=== Iterazione IRL 553 ===
Loss reward (iter 553): 5.815394401550293
=== Iterazione IRL 554 ===
Loss reward (iter 554): 5.7582879066467285
=== Iterazione IRL 555 ===
Loss reward (iter 555): 5.978464126586914
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 938      |
|    critic_loss     | 325      |
|    ent_coef        | 1.25     |
|    ent_coef_loss   | -0.0267  |
|    learning_rate   | 0.0003   |
|    n_updates       | 155699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 957      |
|    critic_loss     | 374      |
|    ent_coef        | 1.22     |
|    ent_coef_loss   | -0.0072  |
|    learning_rate   | 0.0003   |
|    n_updates       | 156099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 859      |
|    critic_loss     | 380      |
|    ent_coef        | 1.23     |
|    ent_coef_loss   | 0.00804  |
|    learning_rate   | 0.0003   |
|    n_updates       | 156499   |
---------------------------------
=== Iterazione IRL 556 ===
Loss reward (iter 556): 5.896259307861328
=== Iterazione IRL 557 ===
Loss reward (iter 557): 5.917370319366455
=== Iterazione IRL 558 ===
Loss reward (iter 558): 5.990922927856445
=== Iterazione IRL 559 ===
Loss reward (iter 559): 6.201322078704834
=== Iterazione IRL 560 ===
Loss reward (iter 560): 6.135704517364502
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 899      |
|    critic_loss     | 519      |
|    ent_coef        | 1.24     |
|    ent_coef_loss   | 0.0204   |
|    learning_rate   | 0.0003   |
|    n_updates       | 157099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 890      |
|    critic_loss     | 325      |
|    ent_coef        | 1.26     |
|    ent_coef_loss   | -0.0231  |
|    learning_rate   | 0.0003   |
|    n_updates       | 157499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 888      |
|    critic_loss     | 409      |
|    ent_coef        | 1.25     |
|    ent_coef_loss   | -0.0385  |
|    learning_rate   | 0.0003   |
|    n_updates       | 157899   |
---------------------------------
=== Iterazione IRL 561 ===
Loss reward (iter 561): 6.20730447769165
=== Iterazione IRL 562 ===
Loss reward (iter 562): 6.370916366577148
=== Iterazione IRL 563 ===
Loss reward (iter 563): 6.144900798797607
=== Iterazione IRL 564 ===
Loss reward (iter 564): 5.701790809631348
=== Iterazione IRL 565 ===
Loss reward (iter 565): 5.7095160484313965
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 913      |
|    critic_loss     | 322      |
|    ent_coef        | 1.26     |
|    ent_coef_loss   | -0.0332  |
|    learning_rate   | 0.0003   |
|    n_updates       | 158499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 875      |
|    critic_loss     | 377      |
|    ent_coef        | 1.27     |
|    ent_coef_loss   | 0.0551   |
|    learning_rate   | 0.0003   |
|    n_updates       | 158899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 882      |
|    critic_loss     | 641      |
|    ent_coef        | 1.25     |
|    ent_coef_loss   | -0.00416 |
|    learning_rate   | 0.0003   |
|    n_updates       | 159299   |
---------------------------------
=== Iterazione IRL 566 ===
Loss reward (iter 566): 5.69373893737793
=== Iterazione IRL 567 ===
Loss reward (iter 567): 4.6871724128723145
=== Iterazione IRL 568 ===
Loss reward (iter 568): 5.668062210083008
=== Iterazione IRL 569 ===
Loss reward (iter 569): 5.524723052978516
=== Iterazione IRL 570 ===
Loss reward (iter 570): 5.211280345916748
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 893      |
|    critic_loss     | 699      |
|    ent_coef        | 1.28     |
|    ent_coef_loss   | 0.0126   |
|    learning_rate   | 0.0003   |
|    n_updates       | 159899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 864      |
|    critic_loss     | 260      |
|    ent_coef        | 1.25     |
|    ent_coef_loss   | -0.00201 |
|    learning_rate   | 0.0003   |
|    n_updates       | 160299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 855      |
|    critic_loss     | 303      |
|    ent_coef        | 1.28     |
|    ent_coef_loss   | 0.0296   |
|    learning_rate   | 0.0003   |
|    n_updates       | 160699   |
---------------------------------
=== Iterazione IRL 571 ===
Loss reward (iter 571): 5.933618068695068
=== Iterazione IRL 572 ===
Loss reward (iter 572): 6.218717575073242
=== Iterazione IRL 573 ===
Loss reward (iter 573): 6.096166133880615
=== Iterazione IRL 574 ===
Loss reward (iter 574): 6.204950332641602
=== Iterazione IRL 575 ===
Loss reward (iter 575): 6.021519184112549
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 919      |
|    critic_loss     | 390      |
|    ent_coef        | 1.26     |
|    ent_coef_loss   | -0.0162  |
|    learning_rate   | 0.0003   |
|    n_updates       | 161299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 895      |
|    critic_loss     | 306      |
|    ent_coef        | 1.24     |
|    ent_coef_loss   | -0.00382 |
|    learning_rate   | 0.0003   |
|    n_updates       | 161699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 800      |
|    critic_loss     | 353      |
|    ent_coef        | 1.25     |
|    ent_coef_loss   | 0.0185   |
|    learning_rate   | 0.0003   |
|    n_updates       | 162099   |
---------------------------------
=== Iterazione IRL 576 ===
Loss reward (iter 576): 5.327350616455078
=== Iterazione IRL 577 ===
Loss reward (iter 577): 5.623807907104492
=== Iterazione IRL 578 ===
Loss reward (iter 578): 4.970506191253662
=== Iterazione IRL 579 ===
Loss reward (iter 579): 5.730659008026123
=== Iterazione IRL 580 ===
Loss reward (iter 580): 5.424957752227783
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 842      |
|    critic_loss     | 551      |
|    ent_coef        | 1.25     |
|    ent_coef_loss   | 0.0274   |
|    learning_rate   | 0.0003   |
|    n_updates       | 162699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 914      |
|    critic_loss     | 580      |
|    ent_coef        | 1.27     |
|    ent_coef_loss   | -0.0248  |
|    learning_rate   | 0.0003   |
|    n_updates       | 163099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 873      |
|    critic_loss     | 353      |
|    ent_coef        | 1.26     |
|    ent_coef_loss   | -0.00245 |
|    learning_rate   | 0.0003   |
|    n_updates       | 163499   |
---------------------------------
=== Iterazione IRL 581 ===
Loss reward (iter 581): 5.032349109649658
=== Iterazione IRL 582 ===
Loss reward (iter 582): 4.8882737159729
=== Iterazione IRL 583 ===
Loss reward (iter 583): 5.357026100158691
=== Iterazione IRL 584 ===
Loss reward (iter 584): 6.394000053405762
=== Iterazione IRL 585 ===
Loss reward (iter 585): 5.0177998542785645
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 814      |
|    critic_loss     | 232      |
|    ent_coef        | 1.25     |
|    ent_coef_loss   | 0.0446   |
|    learning_rate   | 0.0003   |
|    n_updates       | 164099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 820      |
|    critic_loss     | 222      |
|    ent_coef        | 1.25     |
|    ent_coef_loss   | 0.0381   |
|    learning_rate   | 0.0003   |
|    n_updates       | 164499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 817      |
|    critic_loss     | 298      |
|    ent_coef        | 1.27     |
|    ent_coef_loss   | -0.0209  |
|    learning_rate   | 0.0003   |
|    n_updates       | 164899   |
---------------------------------
=== Iterazione IRL 586 ===
Loss reward (iter 586): 6.4466447830200195
=== Iterazione IRL 587 ===
Loss reward (iter 587): 6.365664958953857
=== Iterazione IRL 588 ===
Loss reward (iter 588): 6.117990493774414
=== Iterazione IRL 589 ===
Loss reward (iter 589): 5.663167953491211
=== Iterazione IRL 590 ===
Loss reward (iter 590): 6.045870304107666
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 885      |
|    critic_loss     | 324      |
|    ent_coef        | 1.28     |
|    ent_coef_loss   | -0.0429  |
|    learning_rate   | 0.0003   |
|    n_updates       | 165499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 808      |
|    critic_loss     | 237      |
|    ent_coef        | 1.27     |
|    ent_coef_loss   | -0.0092  |
|    learning_rate   | 0.0003   |
|    n_updates       | 165899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 827      |
|    critic_loss     | 281      |
|    ent_coef        | 1.27     |
|    ent_coef_loss   | -0.0219  |
|    learning_rate   | 0.0003   |
|    n_updates       | 166299   |
---------------------------------
=== Iterazione IRL 591 ===
Loss reward (iter 591): 5.583395957946777
=== Iterazione IRL 592 ===
Loss reward (iter 592): 5.98013973236084
=== Iterazione IRL 593 ===
Loss reward (iter 593): 6.049708366394043
=== Iterazione IRL 594 ===
Loss reward (iter 594): 5.94091272354126
=== Iterazione IRL 595 ===
Loss reward (iter 595): 5.6046977043151855
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 794      |
|    critic_loss     | 250      |
|    ent_coef        | 1.26     |
|    ent_coef_loss   | 0.0523   |
|    learning_rate   | 0.0003   |
|    n_updates       | 166899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 853      |
|    critic_loss     | 306      |
|    ent_coef        | 1.29     |
|    ent_coef_loss   | -0.0306  |
|    learning_rate   | 0.0003   |
|    n_updates       | 167299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 821      |
|    critic_loss     | 871      |
|    ent_coef        | 1.32     |
|    ent_coef_loss   | 0.0471   |
|    learning_rate   | 0.0003   |
|    n_updates       | 167699   |
---------------------------------
=== Iterazione IRL 596 ===
Loss reward (iter 596): 6.051163673400879
=== Iterazione IRL 597 ===
Loss reward (iter 597): 5.856872081756592
=== Iterazione IRL 598 ===
Loss reward (iter 598): 5.635313034057617
=== Iterazione IRL 599 ===
Loss reward (iter 599): 5.490729808807373
=== Iterazione IRL 600 ===
Loss reward (iter 600): 5.733577251434326
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 767      |
|    critic_loss     | 576      |
|    ent_coef        | 1.31     |
|    ent_coef_loss   | -0.00288 |
|    learning_rate   | 0.0003   |
|    n_updates       | 168299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 809      |
|    critic_loss     | 447      |
|    ent_coef        | 1.29     |
|    ent_coef_loss   | -0.0313  |
|    learning_rate   | 0.0003   |
|    n_updates       | 168699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 778      |
|    critic_loss     | 347      |
|    ent_coef        | 1.3      |
|    ent_coef_loss   | -0.0493  |
|    learning_rate   | 0.0003   |
|    n_updates       | 169099   |
---------------------------------
=== Iterazione IRL 601 ===
Loss reward (iter 601): 5.930248260498047
=== Iterazione IRL 602 ===
Loss reward (iter 602): 5.908551216125488
=== Iterazione IRL 603 ===
Loss reward (iter 603): 6.186139106750488
=== Iterazione IRL 604 ===
Loss reward (iter 604): 5.8451409339904785
=== Iterazione IRL 605 ===
Loss reward (iter 605): 5.894806385040283
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 764      |
|    critic_loss     | 251      |
|    ent_coef        | 1.3      |
|    ent_coef_loss   | 0.0386   |
|    learning_rate   | 0.0003   |
|    n_updates       | 169699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 833      |
|    critic_loss     | 484      |
|    ent_coef        | 1.3      |
|    ent_coef_loss   | -0.0475  |
|    learning_rate   | 0.0003   |
|    n_updates       | 170099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 791      |
|    critic_loss     | 861      |
|    ent_coef        | 1.34     |
|    ent_coef_loss   | -0.017   |
|    learning_rate   | 0.0003   |
|    n_updates       | 170499   |
---------------------------------
=== Iterazione IRL 606 ===
Loss reward (iter 606): 5.855761528015137
=== Iterazione IRL 607 ===
Loss reward (iter 607): 6.165075302124023
=== Iterazione IRL 608 ===
Loss reward (iter 608): 5.848759174346924
=== Iterazione IRL 609 ===
Loss reward (iter 609): 6.09335470199585
=== Iterazione IRL 610 ===
Loss reward (iter 610): 5.592887878417969
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 764      |
|    critic_loss     | 416      |
|    ent_coef        | 1.33     |
|    ent_coef_loss   | -0.00736 |
|    learning_rate   | 0.0003   |
|    n_updates       | 171099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 770      |
|    critic_loss     | 253      |
|    ent_coef        | 1.32     |
|    ent_coef_loss   | -0.0299  |
|    learning_rate   | 0.0003   |
|    n_updates       | 171499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 775      |
|    critic_loss     | 350      |
|    ent_coef        | 1.33     |
|    ent_coef_loss   | -0.0341  |
|    learning_rate   | 0.0003   |
|    n_updates       | 171899   |
---------------------------------
=== Iterazione IRL 611 ===
Loss reward (iter 611): 6.3467698097229
=== Iterazione IRL 612 ===
Loss reward (iter 612): 6.27653694152832
=== Iterazione IRL 613 ===
Loss reward (iter 613): 6.18355131149292
=== Iterazione IRL 614 ===
Loss reward (iter 614): 5.999888896942139
=== Iterazione IRL 615 ===
Loss reward (iter 615): 6.049347400665283
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 838      |
|    critic_loss     | 416      |
|    ent_coef        | 1.34     |
|    ent_coef_loss   | -0.0247  |
|    learning_rate   | 0.0003   |
|    n_updates       | 172499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 713      |
|    critic_loss     | 304      |
|    ent_coef        | 1.33     |
|    ent_coef_loss   | 0.0623   |
|    learning_rate   | 0.0003   |
|    n_updates       | 172899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 773      |
|    critic_loss     | 185      |
|    ent_coef        | 1.35     |
|    ent_coef_loss   | -0.00667 |
|    learning_rate   | 0.0003   |
|    n_updates       | 173299   |
---------------------------------
=== Iterazione IRL 616 ===
Loss reward (iter 616): 5.80261754989624
=== Iterazione IRL 617 ===
Loss reward (iter 617): 5.771870136260986
=== Iterazione IRL 618 ===
Loss reward (iter 618): 6.085395812988281
=== Iterazione IRL 619 ===
Loss reward (iter 619): 5.542089939117432
=== Iterazione IRL 620 ===
Loss reward (iter 620): 5.542715549468994
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 774      |
|    critic_loss     | 246      |
|    ent_coef        | 1.35     |
|    ent_coef_loss   | 0.052    |
|    learning_rate   | 0.0003   |
|    n_updates       | 173899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 755      |
|    critic_loss     | 441      |
|    ent_coef        | 1.34     |
|    ent_coef_loss   | -0.0203  |
|    learning_rate   | 0.0003   |
|    n_updates       | 174299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 710      |
|    critic_loss     | 194      |
|    ent_coef        | 1.35     |
|    ent_coef_loss   | 0.0497   |
|    learning_rate   | 0.0003   |
|    n_updates       | 174699   |
---------------------------------
=== Iterazione IRL 621 ===
Loss reward (iter 621): 5.329023838043213
=== Iterazione IRL 622 ===
Loss reward (iter 622): 5.286428928375244
=== Iterazione IRL 623 ===
Loss reward (iter 623): 5.326221942901611
=== Iterazione IRL 624 ===
Loss reward (iter 624): 5.664149761199951
=== Iterazione IRL 625 ===
Loss reward (iter 625): 5.1055402755737305
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 708      |
|    critic_loss     | 378      |
|    ent_coef        | 1.36     |
|    ent_coef_loss   | 0.00759  |
|    learning_rate   | 0.0003   |
|    n_updates       | 175299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 719      |
|    critic_loss     | 264      |
|    ent_coef        | 1.35     |
|    ent_coef_loss   | 0.0276   |
|    learning_rate   | 0.0003   |
|    n_updates       | 175699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 813      |
|    critic_loss     | 371      |
|    ent_coef        | 1.36     |
|    ent_coef_loss   | -0.0449  |
|    learning_rate   | 0.0003   |
|    n_updates       | 176099   |
---------------------------------
=== Iterazione IRL 626 ===
Loss reward (iter 626): 5.869412899017334
=== Iterazione IRL 627 ===
Loss reward (iter 627): 5.883749485015869
=== Iterazione IRL 628 ===
Loss reward (iter 628): 6.018777370452881
=== Iterazione IRL 629 ===
Loss reward (iter 629): 5.497443199157715
=== Iterazione IRL 630 ===
Loss reward (iter 630): 5.456563472747803
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 782      |
|    critic_loss     | 733      |
|    ent_coef        | 1.35     |
|    ent_coef_loss   | -0.0317  |
|    learning_rate   | 0.0003   |
|    n_updates       | 176699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 717      |
|    critic_loss     | 210      |
|    ent_coef        | 1.35     |
|    ent_coef_loss   | 0.0058   |
|    learning_rate   | 0.0003   |
|    n_updates       | 177099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 732      |
|    critic_loss     | 179      |
|    ent_coef        | 1.36     |
|    ent_coef_loss   | 0.0144   |
|    learning_rate   | 0.0003   |
|    n_updates       | 177499   |
---------------------------------
=== Iterazione IRL 631 ===
Loss reward (iter 631): 5.965559005737305
=== Iterazione IRL 632 ===
Loss reward (iter 632): 6.227603912353516
=== Iterazione IRL 633 ===
Loss reward (iter 633): 5.573115825653076
=== Iterazione IRL 634 ===
Loss reward (iter 634): 5.607913970947266
=== Iterazione IRL 635 ===
Loss reward (iter 635): 5.621626853942871
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 733      |
|    critic_loss     | 213      |
|    ent_coef        | 1.36     |
|    ent_coef_loss   | 0.0115   |
|    learning_rate   | 0.0003   |
|    n_updates       | 178099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 780      |
|    critic_loss     | 611      |
|    ent_coef        | 1.37     |
|    ent_coef_loss   | 0.0221   |
|    learning_rate   | 0.0003   |
|    n_updates       | 178499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 723      |
|    critic_loss     | 291      |
|    ent_coef        | 1.38     |
|    ent_coef_loss   | 0.00189  |
|    learning_rate   | 0.0003   |
|    n_updates       | 178899   |
---------------------------------
=== Iterazione IRL 636 ===
Loss reward (iter 636): 4.867861270904541
=== Iterazione IRL 637 ===
Loss reward (iter 637): 4.364635467529297
=== Iterazione IRL 638 ===
Loss reward (iter 638): 5.397319316864014
=== Iterazione IRL 639 ===
Loss reward (iter 639): 4.953464984893799
=== Iterazione IRL 640 ===
Loss reward (iter 640): 5.714470386505127
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 711      |
|    critic_loss     | 256      |
|    ent_coef        | 1.4      |
|    ent_coef_loss   | -0.00276 |
|    learning_rate   | 0.0003   |
|    n_updates       | 179499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 728      |
|    critic_loss     | 281      |
|    ent_coef        | 1.4      |
|    ent_coef_loss   | 0.0352   |
|    learning_rate   | 0.0003   |
|    n_updates       | 179899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 698      |
|    critic_loss     | 661      |
|    ent_coef        | 1.38     |
|    ent_coef_loss   | -0.0262  |
|    learning_rate   | 0.0003   |
|    n_updates       | 180299   |
---------------------------------
=== Iterazione IRL 641 ===
Loss reward (iter 641): 5.989012718200684
=== Iterazione IRL 642 ===
Loss reward (iter 642): 5.828235626220703
=== Iterazione IRL 643 ===
Loss reward (iter 643): 6.069256782531738
=== Iterazione IRL 644 ===
Loss reward (iter 644): 5.792631149291992
=== Iterazione IRL 645 ===
Loss reward (iter 645): 5.960153579711914
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 747      |
|    critic_loss     | 523      |
|    ent_coef        | 1.42     |
|    ent_coef_loss   | 0.00299  |
|    learning_rate   | 0.0003   |
|    n_updates       | 180899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 708      |
|    critic_loss     | 508      |
|    ent_coef        | 1.43     |
|    ent_coef_loss   | -0.0167  |
|    learning_rate   | 0.0003   |
|    n_updates       | 181299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 691      |
|    critic_loss     | 322      |
|    ent_coef        | 1.42     |
|    ent_coef_loss   | 0.0401   |
|    learning_rate   | 0.0003   |
|    n_updates       | 181699   |
---------------------------------
=== Iterazione IRL 646 ===
Loss reward (iter 646): 5.108612060546875
=== Iterazione IRL 647 ===
Loss reward (iter 647): 5.4243388175964355
=== Iterazione IRL 648 ===
Loss reward (iter 648): 5.308598041534424
=== Iterazione IRL 649 ===
Loss reward (iter 649): 5.471266746520996
=== Iterazione IRL 650 ===
Loss reward (iter 650): 5.579425811767578
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 736      |
|    critic_loss     | 344      |
|    ent_coef        | 1.43     |
|    ent_coef_loss   | 0.0428   |
|    learning_rate   | 0.0003   |
|    n_updates       | 182299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 733      |
|    critic_loss     | 435      |
|    ent_coef        | 1.42     |
|    ent_coef_loss   | -0.0345  |
|    learning_rate   | 0.0003   |
|    n_updates       | 182699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 670      |
|    critic_loss     | 308      |
|    ent_coef        | 1.43     |
|    ent_coef_loss   | 0.0274   |
|    learning_rate   | 0.0003   |
|    n_updates       | 183099   |
---------------------------------
=== Iterazione IRL 651 ===
Loss reward (iter 651): 6.359553337097168
=== Iterazione IRL 652 ===
Loss reward (iter 652): 6.1410136222839355
=== Iterazione IRL 653 ===
Loss reward (iter 653): 5.675723552703857
=== Iterazione IRL 654 ===
Loss reward (iter 654): 5.924603462219238
=== Iterazione IRL 655 ===
Loss reward (iter 655): 6.158144474029541
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 725      |
|    critic_loss     | 548      |
|    ent_coef        | 1.41     |
|    ent_coef_loss   | -0.0252  |
|    learning_rate   | 0.0003   |
|    n_updates       | 183699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 752      |
|    critic_loss     | 741      |
|    ent_coef        | 1.41     |
|    ent_coef_loss   | -0.0113  |
|    learning_rate   | 0.0003   |
|    n_updates       | 184099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 722      |
|    critic_loss     | 278      |
|    ent_coef        | 1.41     |
|    ent_coef_loss   | 0.043    |
|    learning_rate   | 0.0003   |
|    n_updates       | 184499   |
---------------------------------
=== Iterazione IRL 656 ===
Loss reward (iter 656): 6.1074628829956055
=== Iterazione IRL 657 ===
Loss reward (iter 657): 6.139342308044434
=== Iterazione IRL 658 ===
Loss reward (iter 658): 5.7766337394714355
=== Iterazione IRL 659 ===
Loss reward (iter 659): 5.82150936126709
=== Iterazione IRL 660 ===
Loss reward (iter 660): 5.967945098876953
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 699      |
|    critic_loss     | 210      |
|    ent_coef        | 1.41     |
|    ent_coef_loss   | 0.0719   |
|    learning_rate   | 0.0003   |
|    n_updates       | 185099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 710      |
|    critic_loss     | 294      |
|    ent_coef        | 1.41     |
|    ent_coef_loss   | -0.0521  |
|    learning_rate   | 0.0003   |
|    n_updates       | 185499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 698      |
|    critic_loss     | 489      |
|    ent_coef        | 1.4      |
|    ent_coef_loss   | -0.0231  |
|    learning_rate   | 0.0003   |
|    n_updates       | 185899   |
---------------------------------
=== Iterazione IRL 661 ===
Loss reward (iter 661): 5.636291027069092
=== Iterazione IRL 662 ===
Loss reward (iter 662): 6.0573272705078125
=== Iterazione IRL 663 ===
Loss reward (iter 663): 5.7626423835754395
=== Iterazione IRL 664 ===
Loss reward (iter 664): 5.570587635040283
=== Iterazione IRL 665 ===
Loss reward (iter 665): 5.803000450134277
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 711      |
|    critic_loss     | 390      |
|    ent_coef        | 1.41     |
|    ent_coef_loss   | 0.0143   |
|    learning_rate   | 0.0003   |
|    n_updates       | 186499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 678      |
|    critic_loss     | 265      |
|    ent_coef        | 1.41     |
|    ent_coef_loss   | 0.00361  |
|    learning_rate   | 0.0003   |
|    n_updates       | 186899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 707      |
|    critic_loss     | 549      |
|    ent_coef        | 1.45     |
|    ent_coef_loss   | 0.00834  |
|    learning_rate   | 0.0003   |
|    n_updates       | 187299   |
---------------------------------
=== Iterazione IRL 666 ===
Loss reward (iter 666): 4.862982749938965
=== Iterazione IRL 667 ===
Loss reward (iter 667): 4.9095611572265625
=== Iterazione IRL 668 ===
Loss reward (iter 668): 4.901309967041016
=== Iterazione IRL 669 ===
Loss reward (iter 669): 5.219273090362549
=== Iterazione IRL 670 ===
Loss reward (iter 670): 5.562363624572754
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 644      |
|    critic_loss     | 521      |
|    ent_coef        | 1.44     |
|    ent_coef_loss   | 0.0295   |
|    learning_rate   | 0.0003   |
|    n_updates       | 187899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 646      |
|    critic_loss     | 647      |
|    ent_coef        | 1.42     |
|    ent_coef_loss   | -0.0269  |
|    learning_rate   | 0.0003   |
|    n_updates       | 188299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 589      |
|    critic_loss     | 147      |
|    ent_coef        | 1.43     |
|    ent_coef_loss   | 0.0167   |
|    learning_rate   | 0.0003   |
|    n_updates       | 188699   |
---------------------------------
=== Iterazione IRL 671 ===
Loss reward (iter 671): 5.624751567840576
=== Iterazione IRL 672 ===
Loss reward (iter 672): 6.445547103881836
=== Iterazione IRL 673 ===
Loss reward (iter 673): 6.211346626281738
=== Iterazione IRL 674 ===
Loss reward (iter 674): 5.594965934753418
=== Iterazione IRL 675 ===
Loss reward (iter 675): 5.767728805541992
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 661      |
|    critic_loss     | 375      |
|    ent_coef        | 1.44     |
|    ent_coef_loss   | 0.0131   |
|    learning_rate   | 0.0003   |
|    n_updates       | 189299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 594      |
|    critic_loss     | 137      |
|    ent_coef        | 1.45     |
|    ent_coef_loss   | -0.0101  |
|    learning_rate   | 0.0003   |
|    n_updates       | 189699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 619      |
|    critic_loss     | 304      |
|    ent_coef        | 1.45     |
|    ent_coef_loss   | 0.00263  |
|    learning_rate   | 0.0003   |
|    n_updates       | 190099   |
---------------------------------
=== Iterazione IRL 676 ===
Loss reward (iter 676): 6.2691497802734375
=== Iterazione IRL 677 ===
Loss reward (iter 677): 5.899652481079102
=== Iterazione IRL 678 ===
Loss reward (iter 678): 6.202429294586182
=== Iterazione IRL 679 ===
Loss reward (iter 679): 5.641969680786133
=== Iterazione IRL 680 ===
Loss reward (iter 680): 6.423341751098633
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 677      |
|    critic_loss     | 289      |
|    ent_coef        | 1.47     |
|    ent_coef_loss   | 0.0219   |
|    learning_rate   | 0.0003   |
|    n_updates       | 190699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 661      |
|    critic_loss     | 197      |
|    ent_coef        | 1.44     |
|    ent_coef_loss   | 0.00791  |
|    learning_rate   | 0.0003   |
|    n_updates       | 191099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 673      |
|    critic_loss     | 429      |
|    ent_coef        | 1.44     |
|    ent_coef_loss   | 0.024    |
|    learning_rate   | 0.0003   |
|    n_updates       | 191499   |
---------------------------------
=== Iterazione IRL 681 ===
Loss reward (iter 681): 6.349699020385742
=== Iterazione IRL 682 ===
Loss reward (iter 682): 6.301043510437012
=== Iterazione IRL 683 ===
Loss reward (iter 683): 5.926420211791992
=== Iterazione IRL 684 ===
Loss reward (iter 684): 6.2870893478393555
=== Iterazione IRL 685 ===
Loss reward (iter 685): 6.024576663970947
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 762      |
|    critic_loss     | 471      |
|    ent_coef        | 1.47     |
|    ent_coef_loss   | -0.0885  |
|    learning_rate   | 0.0003   |
|    n_updates       | 192099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 637      |
|    critic_loss     | 458      |
|    ent_coef        | 1.47     |
|    ent_coef_loss   | 0.0296   |
|    learning_rate   | 0.0003   |
|    n_updates       | 192499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 667      |
|    critic_loss     | 591      |
|    ent_coef        | 1.45     |
|    ent_coef_loss   | 0.0252   |
|    learning_rate   | 0.0003   |
|    n_updates       | 192899   |
---------------------------------
=== Iterazione IRL 686 ===
Loss reward (iter 686): 5.832266807556152
=== Iterazione IRL 687 ===
Loss reward (iter 687): 5.619060039520264
=== Iterazione IRL 688 ===
Loss reward (iter 688): 5.936851978302002
=== Iterazione IRL 689 ===
Loss reward (iter 689): 5.7122697830200195
=== Iterazione IRL 690 ===
Loss reward (iter 690): 5.7470784187316895
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 659      |
|    critic_loss     | 536      |
|    ent_coef        | 1.47     |
|    ent_coef_loss   | 0.0199   |
|    learning_rate   | 0.0003   |
|    n_updates       | 193499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 642      |
|    critic_loss     | 572      |
|    ent_coef        | 1.45     |
|    ent_coef_loss   | -0.039   |
|    learning_rate   | 0.0003   |
|    n_updates       | 193899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 634      |
|    critic_loss     | 376      |
|    ent_coef        | 1.43     |
|    ent_coef_loss   | -0.0173  |
|    learning_rate   | 0.0003   |
|    n_updates       | 194299   |
---------------------------------
=== Iterazione IRL 691 ===
Loss reward (iter 691): 6.124920845031738
=== Iterazione IRL 692 ===
Loss reward (iter 692): 6.206149578094482
=== Iterazione IRL 693 ===
Loss reward (iter 693): 5.985257625579834
=== Iterazione IRL 694 ===
Loss reward (iter 694): 5.978979110717773
=== Iterazione IRL 695 ===
Loss reward (iter 695): 6.06231164932251
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 627      |
|    critic_loss     | 621      |
|    ent_coef        | 1.44     |
|    ent_coef_loss   | -0.0327  |
|    learning_rate   | 0.0003   |
|    n_updates       | 194899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 625      |
|    critic_loss     | 233      |
|    ent_coef        | 1.47     |
|    ent_coef_loss   | 0.0285   |
|    learning_rate   | 0.0003   |
|    n_updates       | 195299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 646      |
|    critic_loss     | 388      |
|    ent_coef        | 1.46     |
|    ent_coef_loss   | 0.0401   |
|    learning_rate   | 0.0003   |
|    n_updates       | 195699   |
---------------------------------
=== Iterazione IRL 696 ===
Loss reward (iter 696): 6.110142707824707
=== Iterazione IRL 697 ===
Loss reward (iter 697): 6.069701194763184
=== Iterazione IRL 698 ===
Loss reward (iter 698): 6.450785160064697
=== Iterazione IRL 699 ===
Loss reward (iter 699): 5.8314104080200195
=== Iterazione IRL 700 ===
Loss reward (iter 700): 5.798934459686279
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 623      |
|    critic_loss     | 359      |
|    ent_coef        | 1.46     |
|    ent_coef_loss   | 0.00855  |
|    learning_rate   | 0.0003   |
|    n_updates       | 196299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 615      |
|    critic_loss     | 371      |
|    ent_coef        | 1.45     |
|    ent_coef_loss   | -0.0235  |
|    learning_rate   | 0.0003   |
|    n_updates       | 196699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 630      |
|    critic_loss     | 283      |
|    ent_coef        | 1.45     |
|    ent_coef_loss   | 8.73e-06 |
|    learning_rate   | 0.0003   |
|    n_updates       | 197099   |
---------------------------------
=== Iterazione IRL 701 ===
Loss reward (iter 701): 5.528635501861572
=== Iterazione IRL 702 ===
Loss reward (iter 702): 5.780162811279297
=== Iterazione IRL 703 ===
Loss reward (iter 703): 5.522496700286865
=== Iterazione IRL 704 ===
Loss reward (iter 704): 6.242221832275391
=== Iterazione IRL 705 ===
Loss reward (iter 705): 6.129894256591797
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 598      |
|    critic_loss     | 292      |
|    ent_coef        | 1.47     |
|    ent_coef_loss   | 0.0119   |
|    learning_rate   | 0.0003   |
|    n_updates       | 197699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 610      |
|    critic_loss     | 446      |
|    ent_coef        | 1.46     |
|    ent_coef_loss   | -0.0333  |
|    learning_rate   | 0.0003   |
|    n_updates       | 198099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 591      |
|    critic_loss     | 333      |
|    ent_coef        | 1.48     |
|    ent_coef_loss   | 0.0107   |
|    learning_rate   | 0.0003   |
|    n_updates       | 198499   |
---------------------------------
=== Iterazione IRL 706 ===
Loss reward (iter 706): 5.674162864685059
=== Iterazione IRL 707 ===
Loss reward (iter 707): 6.038852691650391
=== Iterazione IRL 708 ===
Loss reward (iter 708): 5.84061861038208
=== Iterazione IRL 709 ===
Loss reward (iter 709): 5.688448429107666
=== Iterazione IRL 710 ===
Loss reward (iter 710): 5.432531833648682
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 703      |
|    critic_loss     | 599      |
|    ent_coef        | 1.51     |
|    ent_coef_loss   | -0.0983  |
|    learning_rate   | 0.0003   |
|    n_updates       | 199099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 622      |
|    critic_loss     | 283      |
|    ent_coef        | 1.49     |
|    ent_coef_loss   | -0.0269  |
|    learning_rate   | 0.0003   |
|    n_updates       | 199499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 636      |
|    critic_loss     | 593      |
|    ent_coef        | 1.45     |
|    ent_coef_loss   | -0.0613  |
|    learning_rate   | 0.0003   |
|    n_updates       | 199899   |
---------------------------------
=== Iterazione IRL 711 ===
Loss reward (iter 711): 5.81294059753418
=== Iterazione IRL 712 ===
Loss reward (iter 712): 5.77892541885376
=== Iterazione IRL 713 ===
Loss reward (iter 713): 6.002849578857422
=== Iterazione IRL 714 ===
Loss reward (iter 714): 5.950582027435303
=== Iterazione IRL 715 ===
Loss reward (iter 715): 6.029140472412109
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 554      |
|    critic_loss     | 318      |
|    ent_coef        | 1.45     |
|    ent_coef_loss   | 0.0292   |
|    learning_rate   | 0.0003   |
|    n_updates       | 200499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 633      |
|    critic_loss     | 756      |
|    ent_coef        | 1.45     |
|    ent_coef_loss   | 0.0523   |
|    learning_rate   | 0.0003   |
|    n_updates       | 200899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 610      |
|    critic_loss     | 940      |
|    ent_coef        | 1.47     |
|    ent_coef_loss   | 0.0443   |
|    learning_rate   | 0.0003   |
|    n_updates       | 201299   |
---------------------------------
=== Iterazione IRL 716 ===
Loss reward (iter 716): 5.06934118270874
=== Iterazione IRL 717 ===
Loss reward (iter 717): 5.327509880065918
=== Iterazione IRL 718 ===
Loss reward (iter 718): 5.833691596984863
=== Iterazione IRL 719 ===
Loss reward (iter 719): 5.401388645172119
=== Iterazione IRL 720 ===
Loss reward (iter 720): 5.330455780029297
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 569      |
|    critic_loss     | 370      |
|    ent_coef        | 1.49     |
|    ent_coef_loss   | -0.0158  |
|    learning_rate   | 0.0003   |
|    n_updates       | 201899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 582      |
|    critic_loss     | 287      |
|    ent_coef        | 1.49     |
|    ent_coef_loss   | -0.00563 |
|    learning_rate   | 0.0003   |
|    n_updates       | 202299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 696      |
|    critic_loss     | 950      |
|    ent_coef        | 1.48     |
|    ent_coef_loss   | -0.00295 |
|    learning_rate   | 0.0003   |
|    n_updates       | 202699   |
---------------------------------
=== Iterazione IRL 721 ===
Loss reward (iter 721): 6.091882705688477
=== Iterazione IRL 722 ===
Loss reward (iter 722): 5.613127708435059
=== Iterazione IRL 723 ===
Loss reward (iter 723): 5.998589515686035
=== Iterazione IRL 724 ===
Loss reward (iter 724): 5.8083271980285645
=== Iterazione IRL 725 ===
Loss reward (iter 725): 5.8107075691223145
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 654      |
|    critic_loss     | 498      |
|    ent_coef        | 1.44     |
|    ent_coef_loss   | -0.0382  |
|    learning_rate   | 0.0003   |
|    n_updates       | 203299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 518      |
|    critic_loss     | 279      |
|    ent_coef        | 1.46     |
|    ent_coef_loss   | 0.0272   |
|    learning_rate   | 0.0003   |
|    n_updates       | 203699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 597      |
|    critic_loss     | 532      |
|    ent_coef        | 1.44     |
|    ent_coef_loss   | 0.0123   |
|    learning_rate   | 0.0003   |
|    n_updates       | 204099   |
---------------------------------
=== Iterazione IRL 726 ===
Loss reward (iter 726): 5.8531270027160645
=== Iterazione IRL 727 ===
Loss reward (iter 727): 5.8338303565979
=== Iterazione IRL 728 ===
Loss reward (iter 728): 5.762689590454102
=== Iterazione IRL 729 ===
Loss reward (iter 729): 5.753909111022949
=== Iterazione IRL 730 ===
Loss reward (iter 730): 5.913215637207031
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 559      |
|    critic_loss     | 315      |
|    ent_coef        | 1.46     |
|    ent_coef_loss   | 0.0334   |
|    learning_rate   | 0.0003   |
|    n_updates       | 204699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 574      |
|    critic_loss     | 180      |
|    ent_coef        | 1.46     |
|    ent_coef_loss   | 0.0489   |
|    learning_rate   | 0.0003   |
|    n_updates       | 205099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 546      |
|    critic_loss     | 169      |
|    ent_coef        | 1.45     |
|    ent_coef_loss   | -0.00593 |
|    learning_rate   | 0.0003   |
|    n_updates       | 205499   |
---------------------------------
=== Iterazione IRL 731 ===
Loss reward (iter 731): 6.282928466796875
=== Iterazione IRL 732 ===
Loss reward (iter 732): 5.829723358154297
=== Iterazione IRL 733 ===
Loss reward (iter 733): 6.150510787963867
=== Iterazione IRL 734 ===
Loss reward (iter 734): 5.695058345794678
=== Iterazione IRL 735 ===
Loss reward (iter 735): 5.822883605957031
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 608      |
|    critic_loss     | 692      |
|    ent_coef        | 1.46     |
|    ent_coef_loss   | -0.0341  |
|    learning_rate   | 0.0003   |
|    n_updates       | 206099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 564      |
|    critic_loss     | 281      |
|    ent_coef        | 1.47     |
|    ent_coef_loss   | -0.0546  |
|    learning_rate   | 0.0003   |
|    n_updates       | 206499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 539      |
|    critic_loss     | 399      |
|    ent_coef        | 1.47     |
|    ent_coef_loss   | -0.00889 |
|    learning_rate   | 0.0003   |
|    n_updates       | 206899   |
---------------------------------
=== Iterazione IRL 736 ===
Loss reward (iter 736): 5.830091953277588
=== Iterazione IRL 737 ===
Loss reward (iter 737): 6.221770286560059
=== Iterazione IRL 738 ===
Loss reward (iter 738): 6.5364227294921875
=== Iterazione IRL 739 ===
Loss reward (iter 739): 5.994513988494873
=== Iterazione IRL 740 ===
Loss reward (iter 740): 6.130259037017822
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 568      |
|    critic_loss     | 229      |
|    ent_coef        | 1.46     |
|    ent_coef_loss   | 0.00832  |
|    learning_rate   | 0.0003   |
|    n_updates       | 207499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 603      |
|    critic_loss     | 410      |
|    ent_coef        | 1.45     |
|    ent_coef_loss   | -0.0354  |
|    learning_rate   | 0.0003   |
|    n_updates       | 207899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 603      |
|    critic_loss     | 719      |
|    ent_coef        | 1.46     |
|    ent_coef_loss   | -0.0123  |
|    learning_rate   | 0.0003   |
|    n_updates       | 208299   |
---------------------------------
=== Iterazione IRL 741 ===
Loss reward (iter 741): 5.723979949951172
=== Iterazione IRL 742 ===
Loss reward (iter 742): 5.702788829803467
=== Iterazione IRL 743 ===
Loss reward (iter 743): 5.298688888549805
=== Iterazione IRL 744 ===
Loss reward (iter 744): 6.081886291503906
=== Iterazione IRL 745 ===
Loss reward (iter 745): 6.482193946838379
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 593      |
|    critic_loss     | 787      |
|    ent_coef        | 1.43     |
|    ent_coef_loss   | 0.0592   |
|    learning_rate   | 0.0003   |
|    n_updates       | 208899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 577      |
|    critic_loss     | 335      |
|    ent_coef        | 1.42     |
|    ent_coef_loss   | -0.00668 |
|    learning_rate   | 0.0003   |
|    n_updates       | 209299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 636      |
|    critic_loss     | 726      |
|    ent_coef        | 1.45     |
|    ent_coef_loss   | -0.0501  |
|    learning_rate   | 0.0003   |
|    n_updates       | 209699   |
---------------------------------
=== Iterazione IRL 746 ===
Loss reward (iter 746): 6.03789758682251
=== Iterazione IRL 747 ===
Loss reward (iter 747): 5.819750785827637
=== Iterazione IRL 748 ===
Loss reward (iter 748): 6.005484580993652
=== Iterazione IRL 749 ===
Loss reward (iter 749): 5.582849502563477
=== Iterazione IRL 750 ===
Loss reward (iter 750): 5.729441165924072
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 534      |
|    critic_loss     | 312      |
|    ent_coef        | 1.43     |
|    ent_coef_loss   | -0.0232  |
|    learning_rate   | 0.0003   |
|    n_updates       | 210299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 525      |
|    critic_loss     | 288      |
|    ent_coef        | 1.44     |
|    ent_coef_loss   | 0.0156   |
|    learning_rate   | 0.0003   |
|    n_updates       | 210699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 574      |
|    critic_loss     | 315      |
|    ent_coef        | 1.45     |
|    ent_coef_loss   | 0.0102   |
|    learning_rate   | 0.0003   |
|    n_updates       | 211099   |
---------------------------------
=== Iterazione IRL 751 ===
Loss reward (iter 751): 5.216582298278809
=== Iterazione IRL 752 ===
Loss reward (iter 752): 5.336668491363525
=== Iterazione IRL 753 ===
Loss reward (iter 753): 5.883725166320801
=== Iterazione IRL 754 ===
Loss reward (iter 754): 5.987886428833008
=== Iterazione IRL 755 ===
Loss reward (iter 755): 6.395933151245117
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 534      |
|    critic_loss     | 241      |
|    ent_coef        | 1.45     |
|    ent_coef_loss   | 0.024    |
|    learning_rate   | 0.0003   |
|    n_updates       | 211699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 515      |
|    critic_loss     | 366      |
|    ent_coef        | 1.45     |
|    ent_coef_loss   | -0.0473  |
|    learning_rate   | 0.0003   |
|    n_updates       | 212099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 507      |
|    critic_loss     | 161      |
|    ent_coef        | 1.44     |
|    ent_coef_loss   | 0.0652   |
|    learning_rate   | 0.0003   |
|    n_updates       | 212499   |
---------------------------------
=== Iterazione IRL 756 ===
Loss reward (iter 756): 6.412385940551758
=== Iterazione IRL 757 ===
Loss reward (iter 757): 5.989182472229004
=== Iterazione IRL 758 ===
Loss reward (iter 758): 6.281018257141113
=== Iterazione IRL 759 ===
Loss reward (iter 759): 6.213395118713379
=== Iterazione IRL 760 ===
Loss reward (iter 760): 5.822159767150879
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 565      |
|    critic_loss     | 296      |
|    ent_coef        | 1.44     |
|    ent_coef_loss   | 0.0638   |
|    learning_rate   | 0.0003   |
|    n_updates       | 213099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 516      |
|    critic_loss     | 336      |
|    ent_coef        | 1.45     |
|    ent_coef_loss   | 0.0657   |
|    learning_rate   | 0.0003   |
|    n_updates       | 213499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 554      |
|    critic_loss     | 193      |
|    ent_coef        | 1.45     |
|    ent_coef_loss   | -0.00417 |
|    learning_rate   | 0.0003   |
|    n_updates       | 213899   |
---------------------------------
=== Iterazione IRL 761 ===
Loss reward (iter 761): 6.230642318725586
=== Iterazione IRL 762 ===
Loss reward (iter 762): 5.993715286254883
=== Iterazione IRL 763 ===
Loss reward (iter 763): 5.78140115737915
=== Iterazione IRL 764 ===
Loss reward (iter 764): 5.852126121520996
=== Iterazione IRL 765 ===
Loss reward (iter 765): 5.8646955490112305
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 562      |
|    critic_loss     | 740      |
|    ent_coef        | 1.45     |
|    ent_coef_loss   | 0.00135  |
|    learning_rate   | 0.0003   |
|    n_updates       | 214499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 546      |
|    critic_loss     | 378      |
|    ent_coef        | 1.46     |
|    ent_coef_loss   | -0.00185 |
|    learning_rate   | 0.0003   |
|    n_updates       | 214899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 614      |
|    critic_loss     | 505      |
|    ent_coef        | 1.45     |
|    ent_coef_loss   | -0.0217  |
|    learning_rate   | 0.0003   |
|    n_updates       | 215299   |
---------------------------------
=== Iterazione IRL 766 ===
Loss reward (iter 766): 5.6814446449279785
=== Iterazione IRL 767 ===
Loss reward (iter 767): 5.6680097579956055
=== Iterazione IRL 768 ===
Loss reward (iter 768): 5.770288944244385
=== Iterazione IRL 769 ===
Loss reward (iter 769): 5.3106818199157715
=== Iterazione IRL 770 ===
Loss reward (iter 770): 5.411845684051514
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 552      |
|    critic_loss     | 231      |
|    ent_coef        | 1.45     |
|    ent_coef_loss   | 0.0559   |
|    learning_rate   | 0.0003   |
|    n_updates       | 215899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 556      |
|    critic_loss     | 358      |
|    ent_coef        | 1.46     |
|    ent_coef_loss   | -0.023   |
|    learning_rate   | 0.0003   |
|    n_updates       | 216299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 523      |
|    critic_loss     | 485      |
|    ent_coef        | 1.46     |
|    ent_coef_loss   | 0.0524   |
|    learning_rate   | 0.0003   |
|    n_updates       | 216699   |
---------------------------------
=== Iterazione IRL 771 ===
Loss reward (iter 771): 6.012491703033447
=== Iterazione IRL 772 ===
Loss reward (iter 772): 5.7881245613098145
=== Iterazione IRL 773 ===
Loss reward (iter 773): 6.219442844390869
=== Iterazione IRL 774 ===
Loss reward (iter 774): 5.849462985992432
=== Iterazione IRL 775 ===
Loss reward (iter 775): 5.330548286437988
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 559      |
|    critic_loss     | 376      |
|    ent_coef        | 1.47     |
|    ent_coef_loss   | -0.0141  |
|    learning_rate   | 0.0003   |
|    n_updates       | 217299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 556      |
|    critic_loss     | 475      |
|    ent_coef        | 1.47     |
|    ent_coef_loss   | 0.0322   |
|    learning_rate   | 0.0003   |
|    n_updates       | 217699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 502      |
|    critic_loss     | 189      |
|    ent_coef        | 1.46     |
|    ent_coef_loss   | 0.0153   |
|    learning_rate   | 0.0003   |
|    n_updates       | 218099   |
---------------------------------
=== Iterazione IRL 776 ===
Loss reward (iter 776): 5.699590682983398
=== Iterazione IRL 777 ===
Loss reward (iter 777): 5.881598472595215
=== Iterazione IRL 778 ===
Loss reward (iter 778): 5.339752197265625
=== Iterazione IRL 779 ===
Loss reward (iter 779): 5.9670610427856445
=== Iterazione IRL 780 ===
Loss reward (iter 780): 5.414687156677246
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 503      |
|    critic_loss     | 233      |
|    ent_coef        | 1.45     |
|    ent_coef_loss   | -0.0225  |
|    learning_rate   | 0.0003   |
|    n_updates       | 218699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 545      |
|    critic_loss     | 280      |
|    ent_coef        | 1.46     |
|    ent_coef_loss   | 0.0464   |
|    learning_rate   | 0.0003   |
|    n_updates       | 219099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 517      |
|    critic_loss     | 358      |
|    ent_coef        | 1.46     |
|    ent_coef_loss   | -0.0135  |
|    learning_rate   | 0.0003   |
|    n_updates       | 219499   |
---------------------------------
=== Iterazione IRL 781 ===
Loss reward (iter 781): 5.888662338256836
=== Iterazione IRL 782 ===
Loss reward (iter 782): 6.02454137802124
=== Iterazione IRL 783 ===
Loss reward (iter 783): 6.155436038970947
=== Iterazione IRL 784 ===
Loss reward (iter 784): 5.110548973083496
=== Iterazione IRL 785 ===
Loss reward (iter 785): 5.630992412567139
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 516      |
|    critic_loss     | 161      |
|    ent_coef        | 1.47     |
|    ent_coef_loss   | 0.0216   |
|    learning_rate   | 0.0003   |
|    n_updates       | 220099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 494      |
|    critic_loss     | 284      |
|    ent_coef        | 1.49     |
|    ent_coef_loss   | 0.0165   |
|    learning_rate   | 0.0003   |
|    n_updates       | 220499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 513      |
|    critic_loss     | 202      |
|    ent_coef        | 1.47     |
|    ent_coef_loss   | 0.02     |
|    learning_rate   | 0.0003   |
|    n_updates       | 220899   |
---------------------------------
=== Iterazione IRL 786 ===
Loss reward (iter 786): 5.793137073516846
=== Iterazione IRL 787 ===
Loss reward (iter 787): 6.332798480987549
=== Iterazione IRL 788 ===
Loss reward (iter 788): 5.630825042724609
=== Iterazione IRL 789 ===
Loss reward (iter 789): 5.666686058044434
=== Iterazione IRL 790 ===
Loss reward (iter 790): 5.69078254699707
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 542      |
|    critic_loss     | 284      |
|    ent_coef        | 1.48     |
|    ent_coef_loss   | 0.0109   |
|    learning_rate   | 0.0003   |
|    n_updates       | 221499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 534      |
|    critic_loss     | 260      |
|    ent_coef        | 1.47     |
|    ent_coef_loss   | -0.0266  |
|    learning_rate   | 0.0003   |
|    n_updates       | 221899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 502      |
|    critic_loss     | 435      |
|    ent_coef        | 1.48     |
|    ent_coef_loss   | 0.0108   |
|    learning_rate   | 0.0003   |
|    n_updates       | 222299   |
---------------------------------
=== Iterazione IRL 791 ===
Loss reward (iter 791): 5.968600273132324
=== Iterazione IRL 792 ===
Loss reward (iter 792): 5.7921648025512695
=== Iterazione IRL 793 ===
Loss reward (iter 793): 5.978047847747803
=== Iterazione IRL 794 ===
Loss reward (iter 794): 5.74274206161499
=== Iterazione IRL 795 ===
Loss reward (iter 795): 5.866453647613525
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 540      |
|    critic_loss     | 552      |
|    ent_coef        | 1.46     |
|    ent_coef_loss   | -0.0159  |
|    learning_rate   | 0.0003   |
|    n_updates       | 222899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 516      |
|    critic_loss     | 376      |
|    ent_coef        | 1.46     |
|    ent_coef_loss   | 0.0163   |
|    learning_rate   | 0.0003   |
|    n_updates       | 223299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 460      |
|    critic_loss     | 244      |
|    ent_coef        | 1.45     |
|    ent_coef_loss   | -0.0575  |
|    learning_rate   | 0.0003   |
|    n_updates       | 223699   |
---------------------------------
=== Iterazione IRL 796 ===
Loss reward (iter 796): 6.332046985626221
=== Iterazione IRL 797 ===
Loss reward (iter 797): 6.0261945724487305
=== Iterazione IRL 798 ===
Loss reward (iter 798): 5.47788143157959
=== Iterazione IRL 799 ===
Loss reward (iter 799): 5.239999771118164
=== Iterazione IRL 800 ===
Loss reward (iter 800): 5.572781562805176
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 500      |
|    critic_loss     | 234      |
|    ent_coef        | 1.45     |
|    ent_coef_loss   | 0.0368   |
|    learning_rate   | 0.0003   |
|    n_updates       | 224299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 521      |
|    critic_loss     | 581      |
|    ent_coef        | 1.46     |
|    ent_coef_loss   | -0.064   |
|    learning_rate   | 0.0003   |
|    n_updates       | 224699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 516      |
|    critic_loss     | 223      |
|    ent_coef        | 1.47     |
|    ent_coef_loss   | -0.0232  |
|    learning_rate   | 0.0003   |
|    n_updates       | 225099   |
---------------------------------
=== Iterazione IRL 801 ===
Loss reward (iter 801): 5.906224250793457
=== Iterazione IRL 802 ===
Loss reward (iter 802): 6.523326396942139
=== Iterazione IRL 803 ===
Loss reward (iter 803): 6.127375602722168
=== Iterazione IRL 804 ===
Loss reward (iter 804): 5.452160835266113
=== Iterazione IRL 805 ===
Loss reward (iter 805): 5.58771276473999
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 493      |
|    critic_loss     | 451      |
|    ent_coef        | 1.47     |
|    ent_coef_loss   | 0.0526   |
|    learning_rate   | 0.0003   |
|    n_updates       | 225699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 474      |
|    critic_loss     | 259      |
|    ent_coef        | 1.47     |
|    ent_coef_loss   | 0.00087  |
|    learning_rate   | 0.0003   |
|    n_updates       | 226099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 483      |
|    critic_loss     | 320      |
|    ent_coef        | 1.47     |
|    ent_coef_loss   | 0.0348   |
|    learning_rate   | 0.0003   |
|    n_updates       | 226499   |
---------------------------------
=== Iterazione IRL 806 ===
Loss reward (iter 806): 5.367696762084961
=== Iterazione IRL 807 ===
Loss reward (iter 807): 5.735036373138428
=== Iterazione IRL 808 ===
Loss reward (iter 808): 6.046873092651367
=== Iterazione IRL 809 ===
Loss reward (iter 809): 6.071793556213379
=== Iterazione IRL 810 ===
Loss reward (iter 810): 5.730626583099365
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 543      |
|    critic_loss     | 308      |
|    ent_coef        | 1.47     |
|    ent_coef_loss   | -0.0321  |
|    learning_rate   | 0.0003   |
|    n_updates       | 227099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 508      |
|    critic_loss     | 475      |
|    ent_coef        | 1.47     |
|    ent_coef_loss   | -0.0188  |
|    learning_rate   | 0.0003   |
|    n_updates       | 227499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 535      |
|    critic_loss     | 477      |
|    ent_coef        | 1.47     |
|    ent_coef_loss   | -0.0893  |
|    learning_rate   | 0.0003   |
|    n_updates       | 227899   |
---------------------------------
=== Iterazione IRL 811 ===
Loss reward (iter 811): 5.983885765075684
=== Iterazione IRL 812 ===
Loss reward (iter 812): 5.63899564743042
=== Iterazione IRL 813 ===
Loss reward (iter 813): 5.5494160652160645
=== Iterazione IRL 814 ===
Loss reward (iter 814): 5.8542561531066895
=== Iterazione IRL 815 ===
Loss reward (iter 815): 5.1548848152160645
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 492      |
|    critic_loss     | 299      |
|    ent_coef        | 1.48     |
|    ent_coef_loss   | -0.0225  |
|    learning_rate   | 0.0003   |
|    n_updates       | 228499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 453      |
|    critic_loss     | 273      |
|    ent_coef        | 1.48     |
|    ent_coef_loss   | -0.0245  |
|    learning_rate   | 0.0003   |
|    n_updates       | 228899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 517      |
|    critic_loss     | 425      |
|    ent_coef        | 1.46     |
|    ent_coef_loss   | -0.00193 |
|    learning_rate   | 0.0003   |
|    n_updates       | 229299   |
---------------------------------
=== Iterazione IRL 816 ===
Loss reward (iter 816): 6.368066787719727
=== Iterazione IRL 817 ===
Loss reward (iter 817): 5.953000068664551
=== Iterazione IRL 818 ===
Loss reward (iter 818): 5.954780578613281
=== Iterazione IRL 819 ===
Loss reward (iter 819): 5.634912490844727
=== Iterazione IRL 820 ===
Loss reward (iter 820): 5.665649890899658
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 486      |
|    critic_loss     | 500      |
|    ent_coef        | 1.47     |
|    ent_coef_loss   | 0.0364   |
|    learning_rate   | 0.0003   |
|    n_updates       | 229899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 526      |
|    critic_loss     | 230      |
|    ent_coef        | 1.48     |
|    ent_coef_loss   | 0.0275   |
|    learning_rate   | 0.0003   |
|    n_updates       | 230299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 506      |
|    critic_loss     | 247      |
|    ent_coef        | 1.48     |
|    ent_coef_loss   | -0.0429  |
|    learning_rate   | 0.0003   |
|    n_updates       | 230699   |
---------------------------------
=== Iterazione IRL 821 ===
Loss reward (iter 821): 5.56771183013916
=== Iterazione IRL 822 ===
Loss reward (iter 822): 4.867197036743164
=== Iterazione IRL 823 ===
Loss reward (iter 823): 5.677788257598877
=== Iterazione IRL 824 ===
Loss reward (iter 824): 5.181788921356201
=== Iterazione IRL 825 ===
Loss reward (iter 825): 5.839934825897217
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 521      |
|    critic_loss     | 209      |
|    ent_coef        | 1.47     |
|    ent_coef_loss   | 0.0502   |
|    learning_rate   | 0.0003   |
|    n_updates       | 231299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 497      |
|    critic_loss     | 124      |
|    ent_coef        | 1.46     |
|    ent_coef_loss   | -0.00968 |
|    learning_rate   | 0.0003   |
|    n_updates       | 231699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 492      |
|    critic_loss     | 244      |
|    ent_coef        | 1.45     |
|    ent_coef_loss   | -0.0241  |
|    learning_rate   | 0.0003   |
|    n_updates       | 232099   |
---------------------------------
=== Iterazione IRL 826 ===
Loss reward (iter 826): 6.018265247344971
=== Iterazione IRL 827 ===
Loss reward (iter 827): 5.768494606018066
=== Iterazione IRL 828 ===
Loss reward (iter 828): 6.137424468994141
=== Iterazione IRL 829 ===
Loss reward (iter 829): 5.832948207855225
=== Iterazione IRL 830 ===
Loss reward (iter 830): 5.702943801879883
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 490      |
|    critic_loss     | 455      |
|    ent_coef        | 1.45     |
|    ent_coef_loss   | 0.00954  |
|    learning_rate   | 0.0003   |
|    n_updates       | 232699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 484      |
|    critic_loss     | 337      |
|    ent_coef        | 1.46     |
|    ent_coef_loss   | -0.0275  |
|    learning_rate   | 0.0003   |
|    n_updates       | 233099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 475      |
|    critic_loss     | 367      |
|    ent_coef        | 1.46     |
|    ent_coef_loss   | -0.00965 |
|    learning_rate   | 0.0003   |
|    n_updates       | 233499   |
---------------------------------
=== Iterazione IRL 831 ===
Loss reward (iter 831): 6.133275032043457
=== Iterazione IRL 832 ===
Loss reward (iter 832): 6.15388822555542
=== Iterazione IRL 833 ===
Loss reward (iter 833): 5.951639175415039
=== Iterazione IRL 834 ===
Loss reward (iter 834): 5.411677837371826
=== Iterazione IRL 835 ===
Loss reward (iter 835): 5.42697811126709
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 446      |
|    critic_loss     | 221      |
|    ent_coef        | 1.45     |
|    ent_coef_loss   | 0.0533   |
|    learning_rate   | 0.0003   |
|    n_updates       | 234099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 510      |
|    critic_loss     | 443      |
|    ent_coef        | 1.45     |
|    ent_coef_loss   | -0.0268  |
|    learning_rate   | 0.0003   |
|    n_updates       | 234499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 482      |
|    critic_loss     | 370      |
|    ent_coef        | 1.47     |
|    ent_coef_loss   | 0.0363   |
|    learning_rate   | 0.0003   |
|    n_updates       | 234899   |
---------------------------------
=== Iterazione IRL 836 ===
Loss reward (iter 836): 5.925771236419678
=== Iterazione IRL 837 ===
Loss reward (iter 837): 5.751255989074707
=== Iterazione IRL 838 ===
Loss reward (iter 838): 5.512665271759033
=== Iterazione IRL 839 ===
Loss reward (iter 839): 5.845774173736572
=== Iterazione IRL 840 ===
Loss reward (iter 840): 6.202879428863525
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 499      |
|    critic_loss     | 339      |
|    ent_coef        | 1.44     |
|    ent_coef_loss   | 0.0192   |
|    learning_rate   | 0.0003   |
|    n_updates       | 235499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 423      |
|    critic_loss     | 411      |
|    ent_coef        | 1.44     |
|    ent_coef_loss   | 0.00173  |
|    learning_rate   | 0.0003   |
|    n_updates       | 235899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 492      |
|    critic_loss     | 265      |
|    ent_coef        | 1.45     |
|    ent_coef_loss   | -0.0182  |
|    learning_rate   | 0.0003   |
|    n_updates       | 236299   |
---------------------------------
=== Iterazione IRL 841 ===
Loss reward (iter 841): 5.441958904266357
=== Iterazione IRL 842 ===
Loss reward (iter 842): 6.2051591873168945
=== Iterazione IRL 843 ===
Loss reward (iter 843): 5.574519157409668
=== Iterazione IRL 844 ===
Loss reward (iter 844): 5.700576305389404
=== Iterazione IRL 845 ===
Loss reward (iter 845): 5.813562870025635
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 455      |
|    critic_loss     | 444      |
|    ent_coef        | 1.43     |
|    ent_coef_loss   | -0.027   |
|    learning_rate   | 0.0003   |
|    n_updates       | 236899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 478      |
|    critic_loss     | 206      |
|    ent_coef        | 1.44     |
|    ent_coef_loss   | -0.0329  |
|    learning_rate   | 0.0003   |
|    n_updates       | 237299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 516      |
|    critic_loss     | 459      |
|    ent_coef        | 1.43     |
|    ent_coef_loss   | -0.0865  |
|    learning_rate   | 0.0003   |
|    n_updates       | 237699   |
---------------------------------
=== Iterazione IRL 846 ===
Loss reward (iter 846): 5.765692234039307
=== Iterazione IRL 847 ===
Loss reward (iter 847): 5.3719658851623535
=== Iterazione IRL 848 ===
Loss reward (iter 848): 5.765839099884033
=== Iterazione IRL 849 ===
Loss reward (iter 849): 6.029905319213867
=== Iterazione IRL 850 ===
Loss reward (iter 850): 5.66485595703125
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 468      |
|    critic_loss     | 172      |
|    ent_coef        | 1.43     |
|    ent_coef_loss   | 0.00658  |
|    learning_rate   | 0.0003   |
|    n_updates       | 238299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 513      |
|    critic_loss     | 569      |
|    ent_coef        | 1.42     |
|    ent_coef_loss   | 0.0215   |
|    learning_rate   | 0.0003   |
|    n_updates       | 238699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 441      |
|    critic_loss     | 222      |
|    ent_coef        | 1.42     |
|    ent_coef_loss   | -0.0174  |
|    learning_rate   | 0.0003   |
|    n_updates       | 239099   |
---------------------------------
=== Iterazione IRL 851 ===
Loss reward (iter 851): 5.7584662437438965
=== Iterazione IRL 852 ===
Loss reward (iter 852): 6.048497200012207
=== Iterazione IRL 853 ===
Loss reward (iter 853): 5.361668586730957
=== Iterazione IRL 854 ===
Loss reward (iter 854): 5.593204975128174
=== Iterazione IRL 855 ===
Loss reward (iter 855): 6.14505672454834
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 446      |
|    critic_loss     | 435      |
|    ent_coef        | 1.43     |
|    ent_coef_loss   | -0.00992 |
|    learning_rate   | 0.0003   |
|    n_updates       | 239699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 451      |
|    critic_loss     | 235      |
|    ent_coef        | 1.42     |
|    ent_coef_loss   | 0.00831  |
|    learning_rate   | 0.0003   |
|    n_updates       | 240099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 427      |
|    critic_loss     | 287      |
|    ent_coef        | 1.43     |
|    ent_coef_loss   | 0.0184   |
|    learning_rate   | 0.0003   |
|    n_updates       | 240499   |
---------------------------------
=== Iterazione IRL 856 ===
Loss reward (iter 856): 5.809569835662842
=== Iterazione IRL 857 ===
Loss reward (iter 857): 5.0313825607299805
=== Iterazione IRL 858 ===
Loss reward (iter 858): 4.932035446166992
=== Iterazione IRL 859 ===
Loss reward (iter 859): 5.039947509765625
=== Iterazione IRL 860 ===
Loss reward (iter 860): 5.246283531188965
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 504      |
|    critic_loss     | 416      |
|    ent_coef        | 1.45     |
|    ent_coef_loss   | 0.0272   |
|    learning_rate   | 0.0003   |
|    n_updates       | 241099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 515      |
|    critic_loss     | 469      |
|    ent_coef        | 1.45     |
|    ent_coef_loss   | -0.0126  |
|    learning_rate   | 0.0003   |
|    n_updates       | 241499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 479      |
|    critic_loss     | 180      |
|    ent_coef        | 1.46     |
|    ent_coef_loss   | -0.0163  |
|    learning_rate   | 0.0003   |
|    n_updates       | 241899   |
---------------------------------
=== Iterazione IRL 861 ===
Loss reward (iter 861): 5.714025974273682
=== Iterazione IRL 862 ===
Loss reward (iter 862): 5.709144115447998
=== Iterazione IRL 863 ===
Loss reward (iter 863): 5.369400978088379
=== Iterazione IRL 864 ===
Loss reward (iter 864): 5.206338882446289
=== Iterazione IRL 865 ===
Loss reward (iter 865): 4.94849967956543
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 462      |
|    critic_loss     | 650      |
|    ent_coef        | 1.47     |
|    ent_coef_loss   | 0.0136   |
|    learning_rate   | 0.0003   |
|    n_updates       | 242499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 475      |
|    critic_loss     | 194      |
|    ent_coef        | 1.46     |
|    ent_coef_loss   | 0.018    |
|    learning_rate   | 0.0003   |
|    n_updates       | 242899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 437      |
|    critic_loss     | 229      |
|    ent_coef        | 1.47     |
|    ent_coef_loss   | 0.0513   |
|    learning_rate   | 0.0003   |
|    n_updates       | 243299   |
---------------------------------
=== Iterazione IRL 866 ===
Loss reward (iter 866): 5.896890163421631
=== Iterazione IRL 867 ===
Loss reward (iter 867): 5.614398002624512
=== Iterazione IRL 868 ===
Loss reward (iter 868): 5.841914653778076
=== Iterazione IRL 869 ===
Loss reward (iter 869): 5.999527454376221
=== Iterazione IRL 870 ===
Loss reward (iter 870): 5.565890789031982
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 450      |
|    critic_loss     | 227      |
|    ent_coef        | 1.47     |
|    ent_coef_loss   | 0.0463   |
|    learning_rate   | 0.0003   |
|    n_updates       | 243899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 396      |
|    critic_loss     | 255      |
|    ent_coef        | 1.45     |
|    ent_coef_loss   | -0.0196  |
|    learning_rate   | 0.0003   |
|    n_updates       | 244299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 519      |
|    critic_loss     | 517      |
|    ent_coef        | 1.46     |
|    ent_coef_loss   | -0.0391  |
|    learning_rate   | 0.0003   |
|    n_updates       | 244699   |
---------------------------------
=== Iterazione IRL 871 ===
Loss reward (iter 871): 5.348948955535889
=== Iterazione IRL 872 ===
Loss reward (iter 872): 5.420069694519043
=== Iterazione IRL 873 ===
Loss reward (iter 873): 3.311462640762329
=== Iterazione IRL 874 ===
Loss reward (iter 874): 6.75465726852417
=== Iterazione IRL 875 ===
Loss reward (iter 875): 6.056207656860352
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 406      |
|    critic_loss     | 222      |
|    ent_coef        | 1.45     |
|    ent_coef_loss   | 0.0179   |
|    learning_rate   | 0.0003   |
|    n_updates       | 245299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 531      |
|    critic_loss     | 563      |
|    ent_coef        | 1.43     |
|    ent_coef_loss   | -0.0635  |
|    learning_rate   | 0.0003   |
|    n_updates       | 245699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 499      |
|    critic_loss     | 219      |
|    ent_coef        | 1.43     |
|    ent_coef_loss   | 0.0588   |
|    learning_rate   | 0.0003   |
|    n_updates       | 246099   |
---------------------------------
=== Iterazione IRL 876 ===
Loss reward (iter 876): 5.855831146240234
=== Iterazione IRL 877 ===
Loss reward (iter 877): 5.604198932647705
=== Iterazione IRL 878 ===
Loss reward (iter 878): 5.352671146392822
=== Iterazione IRL 879 ===
Loss reward (iter 879): 6.214199542999268
=== Iterazione IRL 880 ===
Loss reward (iter 880): 5.486190319061279
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 503      |
|    critic_loss     | 632      |
|    ent_coef        | 1.41     |
|    ent_coef_loss   | -0.0884  |
|    learning_rate   | 0.0003   |
|    n_updates       | 246699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 486      |
|    critic_loss     | 362      |
|    ent_coef        | 1.43     |
|    ent_coef_loss   | 0.00145  |
|    learning_rate   | 0.0003   |
|    n_updates       | 247099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 428      |
|    critic_loss     | 235      |
|    ent_coef        | 1.44     |
|    ent_coef_loss   | -0.0377  |
|    learning_rate   | 0.0003   |
|    n_updates       | 247499   |
---------------------------------
=== Iterazione IRL 881 ===
Loss reward (iter 881): 5.792820930480957
=== Iterazione IRL 882 ===
Loss reward (iter 882): 6.00453519821167
=== Iterazione IRL 883 ===
Loss reward (iter 883): 5.534427642822266
=== Iterazione IRL 884 ===
Loss reward (iter 884): 4.61102294921875
=== Iterazione IRL 885 ===
Loss reward (iter 885): 4.514455795288086
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 493      |
|    critic_loss     | 399      |
|    ent_coef        | 1.45     |
|    ent_coef_loss   | -0.0209  |
|    learning_rate   | 0.0003   |
|    n_updates       | 248099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 412      |
|    critic_loss     | 91.3     |
|    ent_coef        | 1.45     |
|    ent_coef_loss   | 0.0931   |
|    learning_rate   | 0.0003   |
|    n_updates       | 248499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 410      |
|    critic_loss     | 217      |
|    ent_coef        | 1.46     |
|    ent_coef_loss   | -0.0401  |
|    learning_rate   | 0.0003   |
|    n_updates       | 248899   |
---------------------------------
=== Iterazione IRL 886 ===
Loss reward (iter 886): 5.947628021240234
=== Iterazione IRL 887 ===
Loss reward (iter 887): 5.781836032867432
=== Iterazione IRL 888 ===
Loss reward (iter 888): 5.689827919006348
=== Iterazione IRL 889 ===
Loss reward (iter 889): 5.423423767089844
=== Iterazione IRL 890 ===
Loss reward (iter 890): 5.228288173675537
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 393      |
|    critic_loss     | 187      |
|    ent_coef        | 1.45     |
|    ent_coef_loss   | 0.0252   |
|    learning_rate   | 0.0003   |
|    n_updates       | 249499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 461      |
|    critic_loss     | 266      |
|    ent_coef        | 1.44     |
|    ent_coef_loss   | 0.0274   |
|    learning_rate   | 0.0003   |
|    n_updates       | 249899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 451      |
|    critic_loss     | 351      |
|    ent_coef        | 1.43     |
|    ent_coef_loss   | 0.0689   |
|    learning_rate   | 0.0003   |
|    n_updates       | 250299   |
---------------------------------
=== Iterazione IRL 891 ===
Loss reward (iter 891): 5.64669132232666
=== Iterazione IRL 892 ===
Loss reward (iter 892): 5.5165696144104
=== Iterazione IRL 893 ===
Loss reward (iter 893): 5.201426982879639
=== Iterazione IRL 894 ===
Loss reward (iter 894): 5.354681491851807
=== Iterazione IRL 895 ===
Loss reward (iter 895): 5.31663179397583
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 440      |
|    critic_loss     | 172      |
|    ent_coef        | 1.44     |
|    ent_coef_loss   | -0.0428  |
|    learning_rate   | 0.0003   |
|    n_updates       | 250899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 475      |
|    critic_loss     | 612      |
|    ent_coef        | 1.45     |
|    ent_coef_loss   | 0.0122   |
|    learning_rate   | 0.0003   |
|    n_updates       | 251299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 444      |
|    critic_loss     | 292      |
|    ent_coef        | 1.45     |
|    ent_coef_loss   | 0.00911  |
|    learning_rate   | 0.0003   |
|    n_updates       | 251699   |
---------------------------------
=== Iterazione IRL 896 ===
Loss reward (iter 896): 5.406646251678467
=== Iterazione IRL 897 ===
Loss reward (iter 897): 6.031801700592041
=== Iterazione IRL 898 ===
Loss reward (iter 898): 5.035322189331055
=== Iterazione IRL 899 ===
Loss reward (iter 899): 5.7131428718566895
=== Iterazione IRL 900 ===
Loss reward (iter 900): 5.269862174987793
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 448      |
|    critic_loss     | 437      |
|    ent_coef        | 1.46     |
|    ent_coef_loss   | 0.0241   |
|    learning_rate   | 0.0003   |
|    n_updates       | 252299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 442      |
|    critic_loss     | 376      |
|    ent_coef        | 1.45     |
|    ent_coef_loss   | -0.0068  |
|    learning_rate   | 0.0003   |
|    n_updates       | 252699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 406      |
|    critic_loss     | 105      |
|    ent_coef        | 1.46     |
|    ent_coef_loss   | 0.0242   |
|    learning_rate   | 0.0003   |
|    n_updates       | 253099   |
---------------------------------
=== Iterazione IRL 901 ===
Loss reward (iter 901): 5.461555004119873
=== Iterazione IRL 902 ===
Loss reward (iter 902): 5.064328193664551
=== Iterazione IRL 903 ===
Loss reward (iter 903): 5.193634033203125
=== Iterazione IRL 904 ===
Loss reward (iter 904): 4.863692283630371
=== Iterazione IRL 905 ===
Loss reward (iter 905): 5.9758195877075195
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 405      |
|    critic_loss     | 311      |
|    ent_coef        | 1.46     |
|    ent_coef_loss   | 0.0152   |
|    learning_rate   | 0.0003   |
|    n_updates       | 253699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 438      |
|    critic_loss     | 184      |
|    ent_coef        | 1.46     |
|    ent_coef_loss   | 0.0537   |
|    learning_rate   | 0.0003   |
|    n_updates       | 254099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 423      |
|    critic_loss     | 178      |
|    ent_coef        | 1.46     |
|    ent_coef_loss   | -0.0436  |
|    learning_rate   | 0.0003   |
|    n_updates       | 254499   |
---------------------------------
=== Iterazione IRL 906 ===
Loss reward (iter 906): 5.245023250579834
=== Iterazione IRL 907 ===
Loss reward (iter 907): 5.841081142425537
=== Iterazione IRL 908 ===
Loss reward (iter 908): 5.75640869140625
=== Iterazione IRL 909 ===
Loss reward (iter 909): 5.80012321472168
=== Iterazione IRL 910 ===
Loss reward (iter 910): 5.521501064300537
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 485      |
|    critic_loss     | 243      |
|    ent_coef        | 1.47     |
|    ent_coef_loss   | -0.0468  |
|    learning_rate   | 0.0003   |
|    n_updates       | 255099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 438      |
|    critic_loss     | 522      |
|    ent_coef        | 1.46     |
|    ent_coef_loss   | -0.0151  |
|    learning_rate   | 0.0003   |
|    n_updates       | 255499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 477      |
|    critic_loss     | 189      |
|    ent_coef        | 1.47     |
|    ent_coef_loss   | 0.0385   |
|    learning_rate   | 0.0003   |
|    n_updates       | 255899   |
---------------------------------
=== Iterazione IRL 911 ===
Loss reward (iter 911): 6.065739154815674
=== Iterazione IRL 912 ===
Loss reward (iter 912): 5.851319313049316
=== Iterazione IRL 913 ===
Loss reward (iter 913): 5.533980846405029
=== Iterazione IRL 914 ===
Loss reward (iter 914): 5.155158996582031
=== Iterazione IRL 915 ===
Loss reward (iter 915): 5.720491409301758
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 410      |
|    critic_loss     | 156      |
|    ent_coef        | 1.45     |
|    ent_coef_loss   | 0.00173  |
|    learning_rate   | 0.0003   |
|    n_updates       | 256499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 430      |
|    critic_loss     | 166      |
|    ent_coef        | 1.44     |
|    ent_coef_loss   | 0.00977  |
|    learning_rate   | 0.0003   |
|    n_updates       | 256899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 418      |
|    critic_loss     | 332      |
|    ent_coef        | 1.45     |
|    ent_coef_loss   | -0.0315  |
|    learning_rate   | 0.0003   |
|    n_updates       | 257299   |
---------------------------------
=== Iterazione IRL 916 ===
Loss reward (iter 916): 5.732123374938965
=== Iterazione IRL 917 ===
Loss reward (iter 917): 5.561501979827881
=== Iterazione IRL 918 ===
Loss reward (iter 918): 5.318222999572754
=== Iterazione IRL 919 ===
Loss reward (iter 919): 5.557430744171143
=== Iterazione IRL 920 ===
Loss reward (iter 920): 5.53903865814209
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 397      |
|    critic_loss     | 355      |
|    ent_coef        | 1.46     |
|    ent_coef_loss   | -0.0338  |
|    learning_rate   | 0.0003   |
|    n_updates       | 257899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 419      |
|    critic_loss     | 446      |
|    ent_coef        | 1.46     |
|    ent_coef_loss   | 0.00658  |
|    learning_rate   | 0.0003   |
|    n_updates       | 258299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 427      |
|    critic_loss     | 480      |
|    ent_coef        | 1.46     |
|    ent_coef_loss   | 0.0151   |
|    learning_rate   | 0.0003   |
|    n_updates       | 258699   |
---------------------------------
=== Iterazione IRL 921 ===
Loss reward (iter 921): 5.6321539878845215
=== Iterazione IRL 922 ===
Loss reward (iter 922): 5.521473407745361
=== Iterazione IRL 923 ===
Loss reward (iter 923): 5.762275218963623
=== Iterazione IRL 924 ===
Loss reward (iter 924): 5.737998962402344
=== Iterazione IRL 925 ===
Loss reward (iter 925): 5.430680274963379
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 423      |
|    critic_loss     | 475      |
|    ent_coef        | 1.48     |
|    ent_coef_loss   | 0.0364   |
|    learning_rate   | 0.0003   |
|    n_updates       | 259299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 443      |
|    critic_loss     | 403      |
|    ent_coef        | 1.49     |
|    ent_coef_loss   | -0.0613  |
|    learning_rate   | 0.0003   |
|    n_updates       | 259699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 417      |
|    critic_loss     | 253      |
|    ent_coef        | 1.49     |
|    ent_coef_loss   | -0.0158  |
|    learning_rate   | 0.0003   |
|    n_updates       | 260099   |
---------------------------------
=== Iterazione IRL 926 ===
Loss reward (iter 926): 5.186952114105225
=== Iterazione IRL 927 ===
Loss reward (iter 927): 4.762842178344727
=== Iterazione IRL 928 ===
Loss reward (iter 928): 6.491119384765625
=== Iterazione IRL 929 ===
Loss reward (iter 929): 5.526017665863037
=== Iterazione IRL 930 ===
Loss reward (iter 930): 5.282431125640869
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 449      |
|    critic_loss     | 288      |
|    ent_coef        | 1.47     |
|    ent_coef_loss   | -0.0202  |
|    learning_rate   | 0.0003   |
|    n_updates       | 260699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 428      |
|    critic_loss     | 197      |
|    ent_coef        | 1.48     |
|    ent_coef_loss   | -0.0663  |
|    learning_rate   | 0.0003   |
|    n_updates       | 261099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 450      |
|    critic_loss     | 213      |
|    ent_coef        | 1.47     |
|    ent_coef_loss   | -0.0156  |
|    learning_rate   | 0.0003   |
|    n_updates       | 261499   |
---------------------------------
=== Iterazione IRL 931 ===
Loss reward (iter 931): 6.094329357147217
=== Iterazione IRL 932 ===
Loss reward (iter 932): 5.828181743621826
=== Iterazione IRL 933 ===
Loss reward (iter 933): 5.514405727386475
=== Iterazione IRL 934 ===
Loss reward (iter 934): 6.033102512359619
=== Iterazione IRL 935 ===
Loss reward (iter 935): 5.85049295425415
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 445      |
|    critic_loss     | 266      |
|    ent_coef        | 1.48     |
|    ent_coef_loss   | -0.075   |
|    learning_rate   | 0.0003   |
|    n_updates       | 262099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 419      |
|    critic_loss     | 256      |
|    ent_coef        | 1.47     |
|    ent_coef_loss   | 0.0165   |
|    learning_rate   | 0.0003   |
|    n_updates       | 262499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 460      |
|    critic_loss     | 599      |
|    ent_coef        | 1.47     |
|    ent_coef_loss   | -0.00854 |
|    learning_rate   | 0.0003   |
|    n_updates       | 262899   |
---------------------------------
=== Iterazione IRL 936 ===
Loss reward (iter 936): 5.469496250152588
=== Iterazione IRL 937 ===
Loss reward (iter 937): 5.134109973907471
=== Iterazione IRL 938 ===
Loss reward (iter 938): 6.223418235778809
=== Iterazione IRL 939 ===
Loss reward (iter 939): 5.578099727630615
=== Iterazione IRL 940 ===
Loss reward (iter 940): 5.884337425231934
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 408      |
|    critic_loss     | 272      |
|    ent_coef        | 1.49     |
|    ent_coef_loss   | 0.0535   |
|    learning_rate   | 0.0003   |
|    n_updates       | 263499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 462      |
|    critic_loss     | 139      |
|    ent_coef        | 1.48     |
|    ent_coef_loss   | 0.0373   |
|    learning_rate   | 0.0003   |
|    n_updates       | 263899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 422      |
|    critic_loss     | 252      |
|    ent_coef        | 1.48     |
|    ent_coef_loss   | 0.0432   |
|    learning_rate   | 0.0003   |
|    n_updates       | 264299   |
---------------------------------
=== Iterazione IRL 941 ===
Loss reward (iter 941): 4.999336242675781
=== Iterazione IRL 942 ===
Loss reward (iter 942): 5.076164245605469
=== Iterazione IRL 943 ===
Loss reward (iter 943): 5.03470516204834
=== Iterazione IRL 944 ===
Loss reward (iter 944): 5.091228485107422
=== Iterazione IRL 945 ===
Loss reward (iter 945): 4.577119827270508
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 443      |
|    critic_loss     | 222      |
|    ent_coef        | 1.47     |
|    ent_coef_loss   | -0.0333  |
|    learning_rate   | 0.0003   |
|    n_updates       | 264899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 454      |
|    critic_loss     | 606      |
|    ent_coef        | 1.47     |
|    ent_coef_loss   | -0.0411  |
|    learning_rate   | 0.0003   |
|    n_updates       | 265299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 389      |
|    critic_loss     | 204      |
|    ent_coef        | 1.46     |
|    ent_coef_loss   | 0.0933   |
|    learning_rate   | 0.0003   |
|    n_updates       | 265699   |
---------------------------------
=== Iterazione IRL 946 ===
Loss reward (iter 946): 5.61997127532959
=== Iterazione IRL 947 ===
Loss reward (iter 947): 5.837447166442871
=== Iterazione IRL 948 ===
Loss reward (iter 948): 5.20847225189209
=== Iterazione IRL 949 ===
Loss reward (iter 949): 5.325658798217773
=== Iterazione IRL 950 ===
Loss reward (iter 950): 5.232636451721191
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 448      |
|    critic_loss     | 630      |
|    ent_coef        | 1.45     |
|    ent_coef_loss   | -0.0373  |
|    learning_rate   | 0.0003   |
|    n_updates       | 266299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 411      |
|    critic_loss     | 455      |
|    ent_coef        | 1.45     |
|    ent_coef_loss   | -0.0172  |
|    learning_rate   | 0.0003   |
|    n_updates       | 266699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 368      |
|    critic_loss     | 289      |
|    ent_coef        | 1.47     |
|    ent_coef_loss   | -0.00507 |
|    learning_rate   | 0.0003   |
|    n_updates       | 267099   |
---------------------------------
=== Iterazione IRL 951 ===
Loss reward (iter 951): 5.789273262023926
=== Iterazione IRL 952 ===
Loss reward (iter 952): 5.603710174560547
=== Iterazione IRL 953 ===
Loss reward (iter 953): 4.995292663574219
=== Iterazione IRL 954 ===
Loss reward (iter 954): 5.547088623046875
=== Iterazione IRL 955 ===
Loss reward (iter 955): 5.4905686378479
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 413      |
|    critic_loss     | 472      |
|    ent_coef        | 1.47     |
|    ent_coef_loss   | -0.066   |
|    learning_rate   | 0.0003   |
|    n_updates       | 267699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 417      |
|    critic_loss     | 601      |
|    ent_coef        | 1.47     |
|    ent_coef_loss   | -0.00241 |
|    learning_rate   | 0.0003   |
|    n_updates       | 268099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 444      |
|    critic_loss     | 990      |
|    ent_coef        | 1.46     |
|    ent_coef_loss   | 0.00511  |
|    learning_rate   | 0.0003   |
|    n_updates       | 268499   |
---------------------------------
=== Iterazione IRL 956 ===
Loss reward (iter 956): 5.721874713897705
=== Iterazione IRL 957 ===
Loss reward (iter 957): 5.637774467468262
=== Iterazione IRL 958 ===
Loss reward (iter 958): 6.1847968101501465
=== Iterazione IRL 959 ===
Loss reward (iter 959): 5.690404891967773
=== Iterazione IRL 960 ===
Loss reward (iter 960): 5.823556900024414
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 443      |
|    critic_loss     | 474      |
|    ent_coef        | 1.47     |
|    ent_coef_loss   | -0.0525  |
|    learning_rate   | 0.0003   |
|    n_updates       | 269099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 451      |
|    critic_loss     | 306      |
|    ent_coef        | 1.48     |
|    ent_coef_loss   | -0.0563  |
|    learning_rate   | 0.0003   |
|    n_updates       | 269499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 471      |
|    critic_loss     | 309      |
|    ent_coef        | 1.48     |
|    ent_coef_loss   | 0.00835  |
|    learning_rate   | 0.0003   |
|    n_updates       | 269899   |
---------------------------------
=== Iterazione IRL 961 ===
Loss reward (iter 961): 5.2061614990234375
=== Iterazione IRL 962 ===
Loss reward (iter 962): 5.595841407775879
=== Iterazione IRL 963 ===
Loss reward (iter 963): 4.885753154754639
=== Iterazione IRL 964 ===
Loss reward (iter 964): 5.413871765136719
=== Iterazione IRL 965 ===
Loss reward (iter 965): 5.464004039764404
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 153       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | 479       |
|    critic_loss     | 524       |
|    ent_coef        | 1.44      |
|    ent_coef_loss   | -0.000875 |
|    learning_rate   | 0.0003    |
|    n_updates       | 270499    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 444      |
|    critic_loss     | 582      |
|    ent_coef        | 1.46     |
|    ent_coef_loss   | -0.00812 |
|    learning_rate   | 0.0003   |
|    n_updates       | 270899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 462      |
|    critic_loss     | 193      |
|    ent_coef        | 1.45     |
|    ent_coef_loss   | 0.00377  |
|    learning_rate   | 0.0003   |
|    n_updates       | 271299   |
---------------------------------
=== Iterazione IRL 966 ===
Loss reward (iter 966): 6.271365165710449
=== Iterazione IRL 967 ===
Loss reward (iter 967): 5.746045112609863
=== Iterazione IRL 968 ===
Loss reward (iter 968): 5.994287490844727
=== Iterazione IRL 969 ===
Loss reward (iter 969): 5.410850524902344
=== Iterazione IRL 970 ===
Loss reward (iter 970): 5.566126823425293
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 373      |
|    critic_loss     | 351      |
|    ent_coef        | 1.42     |
|    ent_coef_loss   | 0.0365   |
|    learning_rate   | 0.0003   |
|    n_updates       | 271899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 406      |
|    critic_loss     | 723      |
|    ent_coef        | 1.42     |
|    ent_coef_loss   | 0.00783  |
|    learning_rate   | 0.0003   |
|    n_updates       | 272299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 403      |
|    critic_loss     | 363      |
|    ent_coef        | 1.42     |
|    ent_coef_loss   | -0.0162  |
|    learning_rate   | 0.0003   |
|    n_updates       | 272699   |
---------------------------------
=== Iterazione IRL 971 ===
Loss reward (iter 971): 5.640606880187988
=== Iterazione IRL 972 ===
Loss reward (iter 972): 5.572049617767334
=== Iterazione IRL 973 ===
Loss reward (iter 973): 5.467411041259766
=== Iterazione IRL 974 ===
Loss reward (iter 974): 5.2338104248046875
=== Iterazione IRL 975 ===
Loss reward (iter 975): 5.677574157714844
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 411      |
|    critic_loss     | 352      |
|    ent_coef        | 1.44     |
|    ent_coef_loss   | -0.00754 |
|    learning_rate   | 0.0003   |
|    n_updates       | 273299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 386      |
|    critic_loss     | 484      |
|    ent_coef        | 1.45     |
|    ent_coef_loss   | 0.0345   |
|    learning_rate   | 0.0003   |
|    n_updates       | 273699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 417      |
|    critic_loss     | 429      |
|    ent_coef        | 1.47     |
|    ent_coef_loss   | 0.0309   |
|    learning_rate   | 0.0003   |
|    n_updates       | 274099   |
---------------------------------
=== Iterazione IRL 976 ===
Loss reward (iter 976): 5.026778221130371
=== Iterazione IRL 977 ===
Loss reward (iter 977): 5.701753616333008
=== Iterazione IRL 978 ===
Loss reward (iter 978): 5.555263519287109
=== Iterazione IRL 979 ===
Loss reward (iter 979): 5.637329578399658
=== Iterazione IRL 980 ===
Loss reward (iter 980): 5.637822151184082
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 140      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 455      |
|    critic_loss     | 415      |
|    ent_coef        | 1.45     |
|    ent_coef_loss   | -0.00914 |
|    learning_rate   | 0.0003   |
|    n_updates       | 274699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 121      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 422      |
|    critic_loss     | 385      |
|    ent_coef        | 1.45     |
|    ent_coef_loss   | 0.0274   |
|    learning_rate   | 0.0003   |
|    n_updates       | 275099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 116      |
|    time_elapsed    | 10       |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 393      |
|    critic_loss     | 286      |
|    ent_coef        | 1.45     |
|    ent_coef_loss   | 0.0555   |
|    learning_rate   | 0.0003   |
|    n_updates       | 275499   |
---------------------------------
=== Iterazione IRL 981 ===
Loss reward (iter 981): 5.686916351318359
=== Iterazione IRL 982 ===
Loss reward (iter 982): 6.268835544586182
=== Iterazione IRL 983 ===
Loss reward (iter 983): 5.934604644775391
=== Iterazione IRL 984 ===
Loss reward (iter 984): 6.092121601104736
=== Iterazione IRL 985 ===
Loss reward (iter 985): 5.282060623168945
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 141      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 382      |
|    critic_loss     | 294      |
|    ent_coef        | 1.46     |
|    ent_coef_loss   | 0.0373   |
|    learning_rate   | 0.0003   |
|    n_updates       | 276099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 122      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 469      |
|    critic_loss     | 752      |
|    ent_coef        | 1.46     |
|    ent_coef_loss   | 0.0382   |
|    learning_rate   | 0.0003   |
|    n_updates       | 276499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 117      |
|    time_elapsed    | 10       |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 408      |
|    critic_loss     | 396      |
|    ent_coef        | 1.46     |
|    ent_coef_loss   | 0.0347   |
|    learning_rate   | 0.0003   |
|    n_updates       | 276899   |
---------------------------------
=== Iterazione IRL 986 ===
Loss reward (iter 986): 5.934122085571289
=== Iterazione IRL 987 ===
Loss reward (iter 987): 5.8421430587768555
=== Iterazione IRL 988 ===
Loss reward (iter 988): 5.1563239097595215
=== Iterazione IRL 989 ===
Loss reward (iter 989): 5.448763847351074
=== Iterazione IRL 990 ===
Loss reward (iter 990): 5.518781661987305
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 141      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 403      |
|    critic_loss     | 584      |
|    ent_coef        | 1.44     |
|    ent_coef_loss   | -0.00717 |
|    learning_rate   | 0.0003   |
|    n_updates       | 277499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 122      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 410      |
|    critic_loss     | 248      |
|    ent_coef        | 1.44     |
|    ent_coef_loss   | 0.0353   |
|    learning_rate   | 0.0003   |
|    n_updates       | 277899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 117      |
|    time_elapsed    | 10       |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 397      |
|    critic_loss     | 140      |
|    ent_coef        | 1.42     |
|    ent_coef_loss   | -0.0227  |
|    learning_rate   | 0.0003   |
|    n_updates       | 278299   |
---------------------------------
=== Iterazione IRL 991 ===
Loss reward (iter 991): 5.9700093269348145
=== Iterazione IRL 992 ===
Loss reward (iter 992): 5.266644477844238
=== Iterazione IRL 993 ===
Loss reward (iter 993): 5.964670181274414
=== Iterazione IRL 994 ===
Loss reward (iter 994): 5.279567718505859
=== Iterazione IRL 995 ===
Loss reward (iter 995): 6.156829833984375
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 458      |
|    critic_loss     | 536      |
|    ent_coef        | 1.45     |
|    ent_coef_loss   | -0.0572  |
|    learning_rate   | 0.0003   |
|    n_updates       | 278899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 404      |
|    critic_loss     | 299      |
|    ent_coef        | 1.46     |
|    ent_coef_loss   | -0.0236  |
|    learning_rate   | 0.0003   |
|    n_updates       | 279299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 412      |
|    critic_loss     | 297      |
|    ent_coef        | 1.44     |
|    ent_coef_loss   | -0.0162  |
|    learning_rate   | 0.0003   |
|    n_updates       | 279699   |
---------------------------------
=== Iterazione IRL 996 ===
Loss reward (iter 996): 5.73314094543457
=== Iterazione IRL 997 ===
Loss reward (iter 997): 5.377113342285156
=== Iterazione IRL 998 ===
Loss reward (iter 998): 5.012918472290039
=== Iterazione IRL 999 ===
Loss reward (iter 999): 5.284688949584961
Modello SAC salvato.
