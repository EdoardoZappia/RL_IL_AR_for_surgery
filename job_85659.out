Inizio training IRL
Using device: cuda
Using cuda device
=== Iterazione IRL 0 ===
Loss reward (iter 0): 6.99708366394043
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 262      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -0.042   |
|    critic_loss     | 5.89e-05 |
|    learning_rate   | 0.001    |
|    n_updates       | 299      |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -0.121   |
|    critic_loss     | 7.59e-05 |
|    learning_rate   | 0.001    |
|    n_updates       | 699      |
---------------------------------
=== Iterazione IRL 1 ===
Loss reward (iter 1): 7.003176689147949
=== Iterazione IRL 2 ===
Loss reward (iter 2): 7.003252029418945
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -0.18    |
|    critic_loss     | 0.000103 |
|    learning_rate   | 0.001    |
|    n_updates       | 1199     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -0.257   |
|    critic_loss     | 0.000183 |
|    learning_rate   | 0.001    |
|    n_updates       | 1599     |
---------------------------------
=== Iterazione IRL 3 ===
Loss reward (iter 3): 6.964842796325684
=== Iterazione IRL 4 ===
Loss reward (iter 4): 7.000065803527832
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -0.304   |
|    critic_loss     | 0.000212 |
|    learning_rate   | 0.001    |
|    n_updates       | 2099     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -0.375   |
|    critic_loss     | 0.0002   |
|    learning_rate   | 0.001    |
|    n_updates       | 2499     |
---------------------------------
=== Iterazione IRL 5 ===
Loss reward (iter 5): 7.006892681121826
=== Iterazione IRL 6 ===
Loss reward (iter 6): 7.005131244659424
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -0.411   |
|    critic_loss     | 0.000281 |
|    learning_rate   | 0.001    |
|    n_updates       | 2999     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -0.424   |
|    critic_loss     | 0.000345 |
|    learning_rate   | 0.001    |
|    n_updates       | 3399     |
---------------------------------
=== Iterazione IRL 7 ===
Loss reward (iter 7): 6.976781368255615
=== Iterazione IRL 8 ===
Loss reward (iter 8): 6.967170238494873
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -0.49    |
|    critic_loss     | 0.000283 |
|    learning_rate   | 0.001    |
|    n_updates       | 3899     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -0.523   |
|    critic_loss     | 0.000369 |
|    learning_rate   | 0.001    |
|    n_updates       | 4299     |
---------------------------------
=== Iterazione IRL 9 ===
Loss reward (iter 9): 6.96579122543335
=== Iterazione IRL 10 ===
Loss reward (iter 10): 6.979640960693359
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -0.59    |
|    critic_loss     | 0.000322 |
|    learning_rate   | 0.001    |
|    n_updates       | 4799     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -0.64    |
|    critic_loss     | 0.00035  |
|    learning_rate   | 0.001    |
|    n_updates       | 5199     |
---------------------------------
=== Iterazione IRL 11 ===
Loss reward (iter 11): 6.980583190917969
=== Iterazione IRL 12 ===
Loss reward (iter 12): 6.979587554931641
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -0.642   |
|    critic_loss     | 0.000289 |
|    learning_rate   | 0.001    |
|    n_updates       | 5699     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -0.684   |
|    critic_loss     | 0.000271 |
|    learning_rate   | 0.001    |
|    n_updates       | 6099     |
---------------------------------
=== Iterazione IRL 13 ===
Loss reward (iter 13): 6.976415157318115
=== Iterazione IRL 14 ===
Loss reward (iter 14): 6.995006561279297
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -0.746   |
|    critic_loss     | 0.000325 |
|    learning_rate   | 0.001    |
|    n_updates       | 6599     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -0.77    |
|    critic_loss     | 0.000238 |
|    learning_rate   | 0.001    |
|    n_updates       | 6999     |
---------------------------------
=== Iterazione IRL 15 ===
Loss reward (iter 15): 6.984614849090576
=== Iterazione IRL 16 ===
Loss reward (iter 16): 6.985239505767822
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -0.814   |
|    critic_loss     | 0.000263 |
|    learning_rate   | 0.001    |
|    n_updates       | 7499     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -0.864   |
|    critic_loss     | 0.0002   |
|    learning_rate   | 0.001    |
|    n_updates       | 7899     |
---------------------------------
=== Iterazione IRL 17 ===
Loss reward (iter 17): 6.988347053527832
=== Iterazione IRL 18 ===
Loss reward (iter 18): 6.9744648933410645
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -0.904   |
|    critic_loss     | 0.000265 |
|    learning_rate   | 0.001    |
|    n_updates       | 8399     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -0.899   |
|    critic_loss     | 0.000256 |
|    learning_rate   | 0.001    |
|    n_updates       | 8799     |
---------------------------------
=== Iterazione IRL 19 ===
Loss reward (iter 19): 6.976348400115967
=== Iterazione IRL 20 ===
Loss reward (iter 20): 6.971005916595459
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -1.02    |
|    critic_loss     | 0.000303 |
|    learning_rate   | 0.001    |
|    n_updates       | 9299     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -1.02    |
|    critic_loss     | 0.000478 |
|    learning_rate   | 0.001    |
|    n_updates       | 9699     |
---------------------------------
=== Iterazione IRL 21 ===
Loss reward (iter 21): 6.997784614562988
=== Iterazione IRL 22 ===
Loss reward (iter 22): 6.9815449714660645
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -1.07    |
|    critic_loss     | 0.000289 |
|    learning_rate   | 0.001    |
|    n_updates       | 10199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -1.07    |
|    critic_loss     | 0.000374 |
|    learning_rate   | 0.001    |
|    n_updates       | 10599    |
---------------------------------
=== Iterazione IRL 23 ===
Loss reward (iter 23): 6.9777092933654785
=== Iterazione IRL 24 ===
Loss reward (iter 24): 6.949437618255615
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -1.14    |
|    critic_loss     | 0.000508 |
|    learning_rate   | 0.001    |
|    n_updates       | 11099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -1.12    |
|    critic_loss     | 0.000463 |
|    learning_rate   | 0.001    |
|    n_updates       | 11499    |
---------------------------------
=== Iterazione IRL 25 ===
Loss reward (iter 25): 6.979334354400635
=== Iterazione IRL 26 ===
Loss reward (iter 26): 6.969232559204102
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -1.24    |
|    critic_loss     | 0.00044  |
|    learning_rate   | 0.001    |
|    n_updates       | 11999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -1.23    |
|    critic_loss     | 0.000447 |
|    learning_rate   | 0.001    |
|    n_updates       | 12399    |
---------------------------------
=== Iterazione IRL 27 ===
Loss reward (iter 27): 6.979643821716309
=== Iterazione IRL 28 ===
Loss reward (iter 28): 6.979723930358887
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -1.31    |
|    critic_loss     | 0.000725 |
|    learning_rate   | 0.001    |
|    n_updates       | 12899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -1.35    |
|    critic_loss     | 0.000823 |
|    learning_rate   | 0.001    |
|    n_updates       | 13299    |
---------------------------------
=== Iterazione IRL 29 ===
Loss reward (iter 29): 6.966166973114014
=== Iterazione IRL 30 ===
Loss reward (iter 30): 6.972107887268066
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -1.37    |
|    critic_loss     | 0.000624 |
|    learning_rate   | 0.001    |
|    n_updates       | 13799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -1.38    |
|    critic_loss     | 0.000781 |
|    learning_rate   | 0.001    |
|    n_updates       | 14199    |
---------------------------------
=== Iterazione IRL 31 ===
Loss reward (iter 31): 6.978779315948486
=== Iterazione IRL 32 ===
Loss reward (iter 32): 6.97271728515625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -1.44    |
|    critic_loss     | 0.000662 |
|    learning_rate   | 0.001    |
|    n_updates       | 14699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -1.43    |
|    critic_loss     | 0.00126  |
|    learning_rate   | 0.001    |
|    n_updates       | 15099    |
---------------------------------
=== Iterazione IRL 33 ===
Loss reward (iter 33): 6.974604606628418
=== Iterazione IRL 34 ===
Loss reward (iter 34): 6.987839221954346
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -1.43    |
|    critic_loss     | 0.00076  |
|    learning_rate   | 0.001    |
|    n_updates       | 15599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -1.46    |
|    critic_loss     | 0.000929 |
|    learning_rate   | 0.001    |
|    n_updates       | 15999    |
---------------------------------
=== Iterazione IRL 35 ===
Loss reward (iter 35): 6.9581522941589355
=== Iterazione IRL 36 ===
Loss reward (iter 36): 6.954222202301025
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -1.4     |
|    critic_loss     | 0.000775 |
|    learning_rate   | 0.001    |
|    n_updates       | 16499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -1.53    |
|    critic_loss     | 0.00088  |
|    learning_rate   | 0.001    |
|    n_updates       | 16899    |
---------------------------------
=== Iterazione IRL 37 ===
Loss reward (iter 37): 6.976047515869141
=== Iterazione IRL 38 ===
Loss reward (iter 38): 6.973844528198242
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -1.49    |
|    critic_loss     | 0.00109  |
|    learning_rate   | 0.001    |
|    n_updates       | 17399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -1.58    |
|    critic_loss     | 0.000886 |
|    learning_rate   | 0.001    |
|    n_updates       | 17799    |
---------------------------------
=== Iterazione IRL 39 ===
Loss reward (iter 39): 6.963645935058594
=== Iterazione IRL 40 ===
Loss reward (iter 40): 6.962555408477783
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -1.54    |
|    critic_loss     | 0.000959 |
|    learning_rate   | 0.001    |
|    n_updates       | 18299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -1.56    |
|    critic_loss     | 0.00103  |
|    learning_rate   | 0.001    |
|    n_updates       | 18699    |
---------------------------------
=== Iterazione IRL 41 ===
Loss reward (iter 41): 6.952126502990723
=== Iterazione IRL 42 ===
Loss reward (iter 42): 6.9514055252075195
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -1.61    |
|    critic_loss     | 0.00128  |
|    learning_rate   | 0.001    |
|    n_updates       | 19199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -1.61    |
|    critic_loss     | 0.00123  |
|    learning_rate   | 0.001    |
|    n_updates       | 19599    |
---------------------------------
=== Iterazione IRL 43 ===
Loss reward (iter 43): 6.966501712799072
=== Iterazione IRL 44 ===
Loss reward (iter 44): 6.972337245941162
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -1.57    |
|    critic_loss     | 0.000808 |
|    learning_rate   | 0.001    |
|    n_updates       | 20099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -1.64    |
|    critic_loss     | 0.00112  |
|    learning_rate   | 0.001    |
|    n_updates       | 20499    |
---------------------------------
=== Iterazione IRL 45 ===
Loss reward (iter 45): 6.9618682861328125
=== Iterazione IRL 46 ===
Loss reward (iter 46): 6.949312210083008
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -1.67    |
|    critic_loss     | 0.000809 |
|    learning_rate   | 0.001    |
|    n_updates       | 20999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -1.72    |
|    critic_loss     | 0.000934 |
|    learning_rate   | 0.001    |
|    n_updates       | 21399    |
---------------------------------
=== Iterazione IRL 47 ===
Loss reward (iter 47): 6.95989990234375
=== Iterazione IRL 48 ===
Loss reward (iter 48): 6.96905517578125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -1.72    |
|    critic_loss     | 0.000942 |
|    learning_rate   | 0.001    |
|    n_updates       | 21899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -1.69    |
|    critic_loss     | 0.000968 |
|    learning_rate   | 0.001    |
|    n_updates       | 22299    |
---------------------------------
=== Iterazione IRL 49 ===
Loss reward (iter 49): 6.950112819671631
=== Iterazione IRL 50 ===
Loss reward (iter 50): 6.956324577331543
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -1.66    |
|    critic_loss     | 0.00138  |
|    learning_rate   | 0.001    |
|    n_updates       | 22799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -1.69    |
|    critic_loss     | 0.00106  |
|    learning_rate   | 0.001    |
|    n_updates       | 23199    |
---------------------------------
=== Iterazione IRL 51 ===
Loss reward (iter 51): 6.949042797088623
=== Iterazione IRL 52 ===
Loss reward (iter 52): 6.947515964508057
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -1.72    |
|    critic_loss     | 0.00108  |
|    learning_rate   | 0.001    |
|    n_updates       | 23699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -1.77    |
|    critic_loss     | 0.000999 |
|    learning_rate   | 0.001    |
|    n_updates       | 24099    |
---------------------------------
=== Iterazione IRL 53 ===
Loss reward (iter 53): 6.950919151306152
=== Iterazione IRL 54 ===
Loss reward (iter 54): 6.945108413696289
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -1.78    |
|    critic_loss     | 0.00124  |
|    learning_rate   | 0.001    |
|    n_updates       | 24599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -1.76    |
|    critic_loss     | 0.00124  |
|    learning_rate   | 0.001    |
|    n_updates       | 24999    |
---------------------------------
=== Iterazione IRL 55 ===
Loss reward (iter 55): 6.960536956787109
=== Iterazione IRL 56 ===
Loss reward (iter 56): 6.952792167663574
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -1.81    |
|    critic_loss     | 0.000889 |
|    learning_rate   | 0.001    |
|    n_updates       | 25499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -1.74    |
|    critic_loss     | 0.00123  |
|    learning_rate   | 0.001    |
|    n_updates       | 25899    |
---------------------------------
=== Iterazione IRL 57 ===
Loss reward (iter 57): 6.960134506225586
=== Iterazione IRL 58 ===
Loss reward (iter 58): 6.943642616271973
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -1.81    |
|    critic_loss     | 0.00133  |
|    learning_rate   | 0.001    |
|    n_updates       | 26399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -1.76    |
|    critic_loss     | 0.00131  |
|    learning_rate   | 0.001    |
|    n_updates       | 26799    |
---------------------------------
=== Iterazione IRL 59 ===
Loss reward (iter 59): 6.949519634246826
=== Iterazione IRL 60 ===
Loss reward (iter 60): 6.937978744506836
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -1.85    |
|    critic_loss     | 0.000875 |
|    learning_rate   | 0.001    |
|    n_updates       | 27299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -1.84    |
|    critic_loss     | 0.000982 |
|    learning_rate   | 0.001    |
|    n_updates       | 27699    |
---------------------------------
=== Iterazione IRL 61 ===
Loss reward (iter 61): 6.946598529815674
=== Iterazione IRL 62 ===
Loss reward (iter 62): 6.950788974761963
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -1.78    |
|    critic_loss     | 0.000954 |
|    learning_rate   | 0.001    |
|    n_updates       | 28199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -1.79    |
|    critic_loss     | 0.00138  |
|    learning_rate   | 0.001    |
|    n_updates       | 28599    |
---------------------------------
=== Iterazione IRL 63 ===
Loss reward (iter 63): 6.945613861083984
=== Iterazione IRL 64 ===
Loss reward (iter 64): 6.938782215118408
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -1.91    |
|    critic_loss     | 0.00112  |
|    learning_rate   | 0.001    |
|    n_updates       | 29099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -1.92    |
|    critic_loss     | 0.00173  |
|    learning_rate   | 0.001    |
|    n_updates       | 29499    |
---------------------------------
=== Iterazione IRL 65 ===
Loss reward (iter 65): 6.936330318450928
=== Iterazione IRL 66 ===
Loss reward (iter 66): 6.938759803771973
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -1.85    |
|    critic_loss     | 0.00134  |
|    learning_rate   | 0.001    |
|    n_updates       | 29999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -1.83    |
|    critic_loss     | 0.00127  |
|    learning_rate   | 0.001    |
|    n_updates       | 30399    |
---------------------------------
=== Iterazione IRL 67 ===
Loss reward (iter 67): 6.924835681915283
=== Iterazione IRL 68 ===
Loss reward (iter 68): 6.929506301879883
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -1.9     |
|    critic_loss     | 0.0014   |
|    learning_rate   | 0.001    |
|    n_updates       | 30899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -1.93    |
|    critic_loss     | 0.00119  |
|    learning_rate   | 0.001    |
|    n_updates       | 31299    |
---------------------------------
=== Iterazione IRL 69 ===
Loss reward (iter 69): 6.936577320098877
=== Iterazione IRL 70 ===
Loss reward (iter 70): 6.943019390106201
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -1.85    |
|    critic_loss     | 0.0013   |
|    learning_rate   | 0.001    |
|    n_updates       | 31799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.02    |
|    critic_loss     | 0.000951 |
|    learning_rate   | 0.001    |
|    n_updates       | 32199    |
---------------------------------
=== Iterazione IRL 71 ===
Loss reward (iter 71): 6.930679798126221
=== Iterazione IRL 72 ===
Loss reward (iter 72): 6.933338165283203
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2       |
|    critic_loss     | 0.00164  |
|    learning_rate   | 0.001    |
|    n_updates       | 32699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -1.82    |
|    critic_loss     | 0.00132  |
|    learning_rate   | 0.001    |
|    n_updates       | 33099    |
---------------------------------
=== Iterazione IRL 73 ===
Loss reward (iter 73): 6.936098575592041
=== Iterazione IRL 74 ===
Loss reward (iter 74): 6.942998886108398
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2       |
|    critic_loss     | 0.00113  |
|    learning_rate   | 0.001    |
|    n_updates       | 33599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -1.94    |
|    critic_loss     | 0.00162  |
|    learning_rate   | 0.001    |
|    n_updates       | 33999    |
---------------------------------
=== Iterazione IRL 75 ===
Loss reward (iter 75): 6.926924705505371
=== Iterazione IRL 76 ===
Loss reward (iter 76): 6.9459710121154785
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2       |
|    critic_loss     | 0.00146  |
|    learning_rate   | 0.001    |
|    n_updates       | 34499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.06    |
|    critic_loss     | 0.00155  |
|    learning_rate   | 0.001    |
|    n_updates       | 34899    |
---------------------------------
=== Iterazione IRL 77 ===
Loss reward (iter 77): 6.925021171569824
=== Iterazione IRL 78 ===
Loss reward (iter 78): 6.923327445983887
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -1.98    |
|    critic_loss     | 0.00136  |
|    learning_rate   | 0.001    |
|    n_updates       | 35399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.17    |
|    critic_loss     | 0.00154  |
|    learning_rate   | 0.001    |
|    n_updates       | 35799    |
---------------------------------
=== Iterazione IRL 79 ===
Loss reward (iter 79): 6.925600528717041
=== Iterazione IRL 80 ===
Loss reward (iter 80): 6.94331169128418
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -1.89    |
|    critic_loss     | 0.00146  |
|    learning_rate   | 0.001    |
|    n_updates       | 36299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.14    |
|    critic_loss     | 0.00124  |
|    learning_rate   | 0.001    |
|    n_updates       | 36699    |
---------------------------------
=== Iterazione IRL 81 ===
Loss reward (iter 81): 6.920241355895996
=== Iterazione IRL 82 ===
Loss reward (iter 82): 6.9129180908203125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.11    |
|    critic_loss     | 0.00139  |
|    learning_rate   | 0.001    |
|    n_updates       | 37199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.17    |
|    critic_loss     | 0.00161  |
|    learning_rate   | 0.001    |
|    n_updates       | 37599    |
---------------------------------
=== Iterazione IRL 83 ===
Loss reward (iter 83): 6.9218597412109375
=== Iterazione IRL 84 ===
Loss reward (iter 84): 6.947350025177002
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.2     |
|    critic_loss     | 0.00138  |
|    learning_rate   | 0.001    |
|    n_updates       | 38099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.01    |
|    critic_loss     | 0.00131  |
|    learning_rate   | 0.001    |
|    n_updates       | 38499    |
---------------------------------
=== Iterazione IRL 85 ===
Loss reward (iter 85): 6.966081142425537
=== Iterazione IRL 86 ===
Loss reward (iter 86): 6.924109935760498
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.12    |
|    critic_loss     | 0.00102  |
|    learning_rate   | 0.001    |
|    n_updates       | 38999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.15    |
|    critic_loss     | 0.00273  |
|    learning_rate   | 0.001    |
|    n_updates       | 39399    |
---------------------------------
=== Iterazione IRL 87 ===
Loss reward (iter 87): 6.968048572540283
=== Iterazione IRL 88 ===
Loss reward (iter 88): 6.946656703948975
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.12    |
|    critic_loss     | 0.00188  |
|    learning_rate   | 0.001    |
|    n_updates       | 39899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.15    |
|    critic_loss     | 0.00187  |
|    learning_rate   | 0.001    |
|    n_updates       | 40299    |
---------------------------------
=== Iterazione IRL 89 ===
Loss reward (iter 89): 6.940238952636719
=== Iterazione IRL 90 ===
Loss reward (iter 90): 6.929155349731445
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.14    |
|    critic_loss     | 0.00215  |
|    learning_rate   | 0.001    |
|    n_updates       | 40799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.18    |
|    critic_loss     | 0.00202  |
|    learning_rate   | 0.001    |
|    n_updates       | 41199    |
---------------------------------
=== Iterazione IRL 91 ===
Loss reward (iter 91): 6.941029071807861
=== Iterazione IRL 92 ===
Loss reward (iter 92): 6.939908981323242
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.3     |
|    critic_loss     | 0.0021   |
|    learning_rate   | 0.001    |
|    n_updates       | 41699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.28    |
|    critic_loss     | 0.00187  |
|    learning_rate   | 0.001    |
|    n_updates       | 42099    |
---------------------------------
=== Iterazione IRL 93 ===
Loss reward (iter 93): 6.92472505569458
=== Iterazione IRL 94 ===
Loss reward (iter 94): 6.940273761749268
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.29    |
|    critic_loss     | 0.00294  |
|    learning_rate   | 0.001    |
|    n_updates       | 42599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.29    |
|    critic_loss     | 0.00285  |
|    learning_rate   | 0.001    |
|    n_updates       | 42999    |
---------------------------------
=== Iterazione IRL 95 ===
Loss reward (iter 95): 6.918210029602051
=== Iterazione IRL 96 ===
Loss reward (iter 96): 6.950785160064697
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.28    |
|    critic_loss     | 0.00237  |
|    learning_rate   | 0.001    |
|    n_updates       | 43499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.24    |
|    critic_loss     | 0.00322  |
|    learning_rate   | 0.001    |
|    n_updates       | 43899    |
---------------------------------
=== Iterazione IRL 97 ===
Loss reward (iter 97): 6.949914455413818
=== Iterazione IRL 98 ===
Loss reward (iter 98): 6.929277420043945
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.45    |
|    critic_loss     | 0.00344  |
|    learning_rate   | 0.001    |
|    n_updates       | 44399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.43    |
|    critic_loss     | 0.00308  |
|    learning_rate   | 0.001    |
|    n_updates       | 44799    |
---------------------------------
=== Iterazione IRL 99 ===
Loss reward (iter 99): 6.92938232421875
=== Iterazione IRL 100 ===
Loss reward (iter 100): 6.948660850524902
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.57    |
|    critic_loss     | 0.0041   |
|    learning_rate   | 0.001    |
|    n_updates       | 45299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.57    |
|    critic_loss     | 0.00403  |
|    learning_rate   | 0.001    |
|    n_updates       | 45699    |
---------------------------------
=== Iterazione IRL 101 ===
Loss reward (iter 101): 6.9290995597839355
=== Iterazione IRL 102 ===
Loss reward (iter 102): 6.931011199951172
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.58    |
|    critic_loss     | 0.00386  |
|    learning_rate   | 0.001    |
|    n_updates       | 46199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.52    |
|    critic_loss     | 0.00408  |
|    learning_rate   | 0.001    |
|    n_updates       | 46599    |
---------------------------------
=== Iterazione IRL 103 ===
Loss reward (iter 103): 6.925881862640381
=== Iterazione IRL 104 ===
Loss reward (iter 104): 6.923085689544678
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.59    |
|    critic_loss     | 0.00421  |
|    learning_rate   | 0.001    |
|    n_updates       | 47099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.67    |
|    critic_loss     | 0.00516  |
|    learning_rate   | 0.001    |
|    n_updates       | 47499    |
---------------------------------
=== Iterazione IRL 105 ===
Loss reward (iter 105): 6.916716575622559
=== Iterazione IRL 106 ===
Loss reward (iter 106): 6.919062614440918
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.88    |
|    critic_loss     | 0.00425  |
|    learning_rate   | 0.001    |
|    n_updates       | 47999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.01    |
|    critic_loss     | 0.00609  |
|    learning_rate   | 0.001    |
|    n_updates       | 48399    |
---------------------------------
=== Iterazione IRL 107 ===
Loss reward (iter 107): 6.923028469085693
=== Iterazione IRL 108 ===
Loss reward (iter 108): 6.915517807006836
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.99    |
|    critic_loss     | 0.00586  |
|    learning_rate   | 0.001    |
|    n_updates       | 48899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.94    |
|    critic_loss     | 0.00615  |
|    learning_rate   | 0.001    |
|    n_updates       | 49299    |
---------------------------------
=== Iterazione IRL 109 ===
Loss reward (iter 109): 6.934518814086914
=== Iterazione IRL 110 ===
Loss reward (iter 110): 6.933380126953125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.22    |
|    critic_loss     | 0.00637  |
|    learning_rate   | 0.001    |
|    n_updates       | 49799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.21    |
|    critic_loss     | 0.00657  |
|    learning_rate   | 0.001    |
|    n_updates       | 50199    |
---------------------------------
=== Iterazione IRL 111 ===
Loss reward (iter 111): 6.913949966430664
=== Iterazione IRL 112 ===
Loss reward (iter 112): 6.875700950622559
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.5     |
|    critic_loss     | 0.00724  |
|    learning_rate   | 0.001    |
|    n_updates       | 50699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.67    |
|    critic_loss     | 0.0071   |
|    learning_rate   | 0.001    |
|    n_updates       | 51099    |
---------------------------------
=== Iterazione IRL 113 ===
Loss reward (iter 113): 6.902958393096924
=== Iterazione IRL 114 ===
Loss reward (iter 114): 6.890769958496094
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.97    |
|    critic_loss     | 0.00696  |
|    learning_rate   | 0.001    |
|    n_updates       | 51599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.11    |
|    critic_loss     | 0.00695  |
|    learning_rate   | 0.001    |
|    n_updates       | 51999    |
---------------------------------
=== Iterazione IRL 115 ===
Loss reward (iter 115): 6.791016578674316
=== Iterazione IRL 116 ===
Loss reward (iter 116): 6.871065616607666
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.09    |
|    critic_loss     | 0.00792  |
|    learning_rate   | 0.001    |
|    n_updates       | 52499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.41    |
|    critic_loss     | 0.00864  |
|    learning_rate   | 0.001    |
|    n_updates       | 52899    |
---------------------------------
=== Iterazione IRL 117 ===
Loss reward (iter 117): 6.892477989196777
=== Iterazione IRL 118 ===
Loss reward (iter 118): 6.903632640838623
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.5     |
|    critic_loss     | 0.00786  |
|    learning_rate   | 0.001    |
|    n_updates       | 53399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.56    |
|    critic_loss     | 0.00664  |
|    learning_rate   | 0.001    |
|    n_updates       | 53799    |
---------------------------------
=== Iterazione IRL 119 ===
Loss reward (iter 119): 6.765401840209961
=== Iterazione IRL 120 ===
Loss reward (iter 120): 6.797179222106934
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.48    |
|    critic_loss     | 0.00718  |
|    learning_rate   | 0.001    |
|    n_updates       | 54299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.59    |
|    critic_loss     | 0.00909  |
|    learning_rate   | 0.001    |
|    n_updates       | 54699    |
---------------------------------
=== Iterazione IRL 121 ===
Loss reward (iter 121): 6.880489349365234
=== Iterazione IRL 122 ===
Loss reward (iter 122): 6.860595703125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.7     |
|    critic_loss     | 0.00761  |
|    learning_rate   | 0.001    |
|    n_updates       | 55199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.61    |
|    critic_loss     | 0.00699  |
|    learning_rate   | 0.001    |
|    n_updates       | 55599    |
---------------------------------
=== Iterazione IRL 123 ===
Loss reward (iter 123): 6.850574970245361
=== Iterazione IRL 124 ===
Loss reward (iter 124): 6.843997478485107
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.68    |
|    critic_loss     | 0.00671  |
|    learning_rate   | 0.001    |
|    n_updates       | 56099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.01    |
|    critic_loss     | 0.00662  |
|    learning_rate   | 0.001    |
|    n_updates       | 56499    |
---------------------------------
=== Iterazione IRL 125 ===
Loss reward (iter 125): 6.8759379386901855
=== Iterazione IRL 126 ===
Loss reward (iter 126): 6.868479251861572
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.84    |
|    critic_loss     | 0.00812  |
|    learning_rate   | 0.001    |
|    n_updates       | 56999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.38    |
|    critic_loss     | 0.00689  |
|    learning_rate   | 0.001    |
|    n_updates       | 57399    |
---------------------------------
=== Iterazione IRL 127 ===
Loss reward (iter 127): 6.877923011779785
=== Iterazione IRL 128 ===
Loss reward (iter 128): 6.893250465393066
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.38    |
|    critic_loss     | 0.00732  |
|    learning_rate   | 0.001    |
|    n_updates       | 57899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.05    |
|    critic_loss     | 0.0079   |
|    learning_rate   | 0.001    |
|    n_updates       | 58299    |
---------------------------------
=== Iterazione IRL 129 ===
Loss reward (iter 129): 6.898760795593262
=== Iterazione IRL 130 ===
Loss reward (iter 130): 6.894195079803467
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.92    |
|    critic_loss     | 0.00965  |
|    learning_rate   | 0.001    |
|    n_updates       | 58799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.09    |
|    critic_loss     | 0.00957  |
|    learning_rate   | 0.001    |
|    n_updates       | 59199    |
---------------------------------
=== Iterazione IRL 131 ===
Loss reward (iter 131): 6.88759708404541
=== Iterazione IRL 132 ===
Loss reward (iter 132): 6.907425403594971
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.1     |
|    critic_loss     | 0.0107   |
|    learning_rate   | 0.001    |
|    n_updates       | 59699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.15    |
|    critic_loss     | 0.00839  |
|    learning_rate   | 0.001    |
|    n_updates       | 60099    |
---------------------------------
=== Iterazione IRL 133 ===
Loss reward (iter 133): 6.8956828117370605
=== Iterazione IRL 134 ===
Loss reward (iter 134): 6.880786895751953
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.22    |
|    critic_loss     | 0.00702  |
|    learning_rate   | 0.001    |
|    n_updates       | 60599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.98    |
|    critic_loss     | 0.00835  |
|    learning_rate   | 0.001    |
|    n_updates       | 60999    |
---------------------------------
=== Iterazione IRL 135 ===
Loss reward (iter 135): 6.897414207458496
=== Iterazione IRL 136 ===
Loss reward (iter 136): 6.88459587097168
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.97    |
|    critic_loss     | 0.0105   |
|    learning_rate   | 0.001    |
|    n_updates       | 61499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.09    |
|    critic_loss     | 0.0103   |
|    learning_rate   | 0.001    |
|    n_updates       | 61899    |
---------------------------------
=== Iterazione IRL 137 ===
Loss reward (iter 137): 6.917590618133545
=== Iterazione IRL 138 ===
Loss reward (iter 138): 6.888641834259033
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.29    |
|    critic_loss     | 0.00816  |
|    learning_rate   | 0.001    |
|    n_updates       | 62399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.08    |
|    critic_loss     | 0.00897  |
|    learning_rate   | 0.001    |
|    n_updates       | 62799    |
---------------------------------
=== Iterazione IRL 139 ===
Loss reward (iter 139): 6.908614158630371
=== Iterazione IRL 140 ===
Loss reward (iter 140): 6.894591331481934
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.04    |
|    critic_loss     | 0.00997  |
|    learning_rate   | 0.001    |
|    n_updates       | 63299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.96    |
|    critic_loss     | 0.0076   |
|    learning_rate   | 0.001    |
|    n_updates       | 63699    |
---------------------------------
=== Iterazione IRL 141 ===
Loss reward (iter 141): 6.89721155166626
=== Iterazione IRL 142 ===
Loss reward (iter 142): 6.916522979736328
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.86    |
|    critic_loss     | 0.00968  |
|    learning_rate   | 0.001    |
|    n_updates       | 64199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.16    |
|    critic_loss     | 0.0125   |
|    learning_rate   | 0.001    |
|    n_updates       | 64599    |
---------------------------------
=== Iterazione IRL 143 ===
Loss reward (iter 143): 6.917886257171631
=== Iterazione IRL 144 ===
Loss reward (iter 144): 6.900384902954102
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.88    |
|    critic_loss     | 0.00794  |
|    learning_rate   | 0.001    |
|    n_updates       | 65099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.89    |
|    critic_loss     | 0.00752  |
|    learning_rate   | 0.001    |
|    n_updates       | 65499    |
---------------------------------
=== Iterazione IRL 145 ===
Loss reward (iter 145): 6.849696636199951
=== Iterazione IRL 146 ===
Loss reward (iter 146): 6.868552207946777
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.83    |
|    critic_loss     | 0.00778  |
|    learning_rate   | 0.001    |
|    n_updates       | 65999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.69    |
|    critic_loss     | 0.00839  |
|    learning_rate   | 0.001    |
|    n_updates       | 66399    |
---------------------------------
=== Iterazione IRL 147 ===
Loss reward (iter 147): 6.896660327911377
=== Iterazione IRL 148 ===
Loss reward (iter 148): 6.865922451019287
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.67    |
|    critic_loss     | 0.00636  |
|    learning_rate   | 0.001    |
|    n_updates       | 66899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.83    |
|    critic_loss     | 0.00781  |
|    learning_rate   | 0.001    |
|    n_updates       | 67299    |
---------------------------------
=== Iterazione IRL 149 ===
Loss reward (iter 149): 6.911093235015869
=== Iterazione IRL 150 ===
Loss reward (iter 150): 6.89094877243042
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.6     |
|    critic_loss     | 0.00733  |
|    learning_rate   | 0.001    |
|    n_updates       | 67799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.57    |
|    critic_loss     | 0.00926  |
|    learning_rate   | 0.001    |
|    n_updates       | 68199    |
---------------------------------
=== Iterazione IRL 151 ===
Loss reward (iter 151): 6.91335916519165
=== Iterazione IRL 152 ===
Loss reward (iter 152): 6.896927356719971
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.63    |
|    critic_loss     | 0.00972  |
|    learning_rate   | 0.001    |
|    n_updates       | 68699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.71    |
|    critic_loss     | 0.00748  |
|    learning_rate   | 0.001    |
|    n_updates       | 69099    |
---------------------------------
=== Iterazione IRL 153 ===
Loss reward (iter 153): 6.9051361083984375
=== Iterazione IRL 154 ===
Loss reward (iter 154): 6.891354084014893
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.8     |
|    critic_loss     | 0.0056   |
|    learning_rate   | 0.001    |
|    n_updates       | 69599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.74    |
|    critic_loss     | 0.00912  |
|    learning_rate   | 0.001    |
|    n_updates       | 69999    |
---------------------------------
=== Iterazione IRL 155 ===
Loss reward (iter 155): 6.900606155395508
=== Iterazione IRL 156 ===
Loss reward (iter 156): 6.89425802230835
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.58    |
|    critic_loss     | 0.00762  |
|    learning_rate   | 0.001    |
|    n_updates       | 70499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.5     |
|    critic_loss     | 0.00773  |
|    learning_rate   | 0.001    |
|    n_updates       | 70899    |
---------------------------------
=== Iterazione IRL 157 ===
Loss reward (iter 157): 6.893815994262695
=== Iterazione IRL 158 ===
Loss reward (iter 158): 6.916317939758301
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.64    |
|    critic_loss     | 0.00739  |
|    learning_rate   | 0.001    |
|    n_updates       | 71399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.7     |
|    critic_loss     | 0.00663  |
|    learning_rate   | 0.001    |
|    n_updates       | 71799    |
---------------------------------
=== Iterazione IRL 159 ===
Loss reward (iter 159): 6.902863502502441
=== Iterazione IRL 160 ===
Loss reward (iter 160): 6.896111488342285
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.4     |
|    critic_loss     | 0.00786  |
|    learning_rate   | 0.001    |
|    n_updates       | 72299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.42    |
|    critic_loss     | 0.00762  |
|    learning_rate   | 0.001    |
|    n_updates       | 72699    |
---------------------------------
=== Iterazione IRL 161 ===
Loss reward (iter 161): 6.90252161026001
=== Iterazione IRL 162 ===
Loss reward (iter 162): 6.908109188079834
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.55    |
|    critic_loss     | 0.0077   |
|    learning_rate   | 0.001    |
|    n_updates       | 73199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.52    |
|    critic_loss     | 0.00623  |
|    learning_rate   | 0.001    |
|    n_updates       | 73599    |
---------------------------------
=== Iterazione IRL 163 ===
Loss reward (iter 163): 6.9006242752075195
=== Iterazione IRL 164 ===
Loss reward (iter 164): 6.909640312194824
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.73    |
|    critic_loss     | 0.00771  |
|    learning_rate   | 0.001    |
|    n_updates       | 74099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.59    |
|    critic_loss     | 0.00781  |
|    learning_rate   | 0.001    |
|    n_updates       | 74499    |
---------------------------------
=== Iterazione IRL 165 ===
Loss reward (iter 165): 6.900687217712402
=== Iterazione IRL 166 ===
Loss reward (iter 166): 6.89333438873291
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.48    |
|    critic_loss     | 0.00704  |
|    learning_rate   | 0.001    |
|    n_updates       | 74999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.24    |
|    critic_loss     | 0.00638  |
|    learning_rate   | 0.001    |
|    n_updates       | 75399    |
---------------------------------
=== Iterazione IRL 167 ===
Loss reward (iter 167): 6.907697677612305
=== Iterazione IRL 168 ===
Loss reward (iter 168): 6.899352073669434
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.2     |
|    critic_loss     | 0.00659  |
|    learning_rate   | 0.001    |
|    n_updates       | 75899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.56    |
|    critic_loss     | 0.00975  |
|    learning_rate   | 0.001    |
|    n_updates       | 76299    |
---------------------------------
=== Iterazione IRL 169 ===
Loss reward (iter 169): 6.891950607299805
=== Iterazione IRL 170 ===
Loss reward (iter 170): 6.909243106842041
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.41    |
|    critic_loss     | 0.01     |
|    learning_rate   | 0.001    |
|    n_updates       | 76799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.53    |
|    critic_loss     | 0.00672  |
|    learning_rate   | 0.001    |
|    n_updates       | 77199    |
---------------------------------
=== Iterazione IRL 171 ===
Loss reward (iter 171): 6.886942386627197
=== Iterazione IRL 172 ===
Loss reward (iter 172): 6.879993438720703
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.14    |
|    critic_loss     | 0.00834  |
|    learning_rate   | 0.001    |
|    n_updates       | 77699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.16    |
|    critic_loss     | 0.00704  |
|    learning_rate   | 0.001    |
|    n_updates       | 78099    |
---------------------------------
=== Iterazione IRL 173 ===
Loss reward (iter 173): 6.891772747039795
=== Iterazione IRL 174 ===
Loss reward (iter 174): 6.896514892578125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.21    |
|    critic_loss     | 0.0108   |
|    learning_rate   | 0.001    |
|    n_updates       | 78599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.16    |
|    critic_loss     | 0.00714  |
|    learning_rate   | 0.001    |
|    n_updates       | 78999    |
---------------------------------
=== Iterazione IRL 175 ===
Loss reward (iter 175): 6.88795280456543
=== Iterazione IRL 176 ===
Loss reward (iter 176): 6.892149448394775
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.31    |
|    critic_loss     | 0.00793  |
|    learning_rate   | 0.001    |
|    n_updates       | 79499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.31    |
|    critic_loss     | 0.0111   |
|    learning_rate   | 0.001    |
|    n_updates       | 79899    |
---------------------------------
=== Iterazione IRL 177 ===
Loss reward (iter 177): 6.865128517150879
=== Iterazione IRL 178 ===
Loss reward (iter 178): 6.875026702880859
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.06    |
|    critic_loss     | 0.00823  |
|    learning_rate   | 0.001    |
|    n_updates       | 80399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.1     |
|    critic_loss     | 0.00947  |
|    learning_rate   | 0.001    |
|    n_updates       | 80799    |
---------------------------------
=== Iterazione IRL 179 ===
Loss reward (iter 179): 6.896962642669678
=== Iterazione IRL 180 ===
Loss reward (iter 180): 6.879953384399414
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.96    |
|    critic_loss     | 0.0104   |
|    learning_rate   | 0.001    |
|    n_updates       | 81299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.31    |
|    critic_loss     | 0.00901  |
|    learning_rate   | 0.001    |
|    n_updates       | 81699    |
---------------------------------
=== Iterazione IRL 181 ===
Loss reward (iter 181): 6.880216598510742
=== Iterazione IRL 182 ===
Loss reward (iter 182): 6.8743767738342285
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.33    |
|    critic_loss     | 0.00956  |
|    learning_rate   | 0.001    |
|    n_updates       | 82199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 241      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.06    |
|    critic_loss     | 0.00718  |
|    learning_rate   | 0.001    |
|    n_updates       | 82599    |
---------------------------------
=== Iterazione IRL 183 ===
Loss reward (iter 183): 6.85855770111084
=== Iterazione IRL 184 ===
Loss reward (iter 184): 6.892577648162842
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.12    |
|    critic_loss     | 0.00835  |
|    learning_rate   | 0.001    |
|    n_updates       | 83099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 241      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.05    |
|    critic_loss     | 0.0113   |
|    learning_rate   | 0.001    |
|    n_updates       | 83499    |
---------------------------------
=== Iterazione IRL 185 ===
Loss reward (iter 185): 6.893207550048828
=== Iterazione IRL 186 ===
Loss reward (iter 186): 6.881661415100098
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 275      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.89    |
|    critic_loss     | 0.00467  |
|    learning_rate   | 0.001    |
|    n_updates       | 83999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 241      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.15    |
|    critic_loss     | 0.0074   |
|    learning_rate   | 0.001    |
|    n_updates       | 84399    |
---------------------------------
=== Iterazione IRL 187 ===
Loss reward (iter 187): 6.884829044342041
=== Iterazione IRL 188 ===
Loss reward (iter 188): 6.884199619293213
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.05    |
|    critic_loss     | 0.00853  |
|    learning_rate   | 0.001    |
|    n_updates       | 84899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.99    |
|    critic_loss     | 0.00988  |
|    learning_rate   | 0.001    |
|    n_updates       | 85299    |
---------------------------------
=== Iterazione IRL 189 ===
Loss reward (iter 189): 6.866981029510498
=== Iterazione IRL 190 ===
Loss reward (iter 190): 6.89039945602417
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 275      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.08    |
|    critic_loss     | 0.00834  |
|    learning_rate   | 0.001    |
|    n_updates       | 85799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 241      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.08    |
|    critic_loss     | 0.0127   |
|    learning_rate   | 0.001    |
|    n_updates       | 86199    |
---------------------------------
=== Iterazione IRL 191 ===
Loss reward (iter 191): 6.877665996551514
=== Iterazione IRL 192 ===
Loss reward (iter 192): 6.881410121917725
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.88    |
|    critic_loss     | 0.00672  |
|    learning_rate   | 0.001    |
|    n_updates       | 86699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 241      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.77    |
|    critic_loss     | 0.0112   |
|    learning_rate   | 0.001    |
|    n_updates       | 87099    |
---------------------------------
=== Iterazione IRL 193 ===
Loss reward (iter 193): 6.8601837158203125
=== Iterazione IRL 194 ===
Loss reward (iter 194): 6.828099250793457
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.86    |
|    critic_loss     | 0.0102   |
|    learning_rate   | 0.001    |
|    n_updates       | 87599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 241      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.86    |
|    critic_loss     | 0.0117   |
|    learning_rate   | 0.001    |
|    n_updates       | 87999    |
---------------------------------
=== Iterazione IRL 195 ===
Loss reward (iter 195): 6.879062652587891
=== Iterazione IRL 196 ===
Loss reward (iter 196): 6.851848602294922
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 275      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.95    |
|    critic_loss     | 0.00675  |
|    learning_rate   | 0.001    |
|    n_updates       | 88499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 241      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4       |
|    critic_loss     | 0.00722  |
|    learning_rate   | 0.001    |
|    n_updates       | 88899    |
---------------------------------
=== Iterazione IRL 197 ===
Loss reward (iter 197): 6.82115364074707
=== Iterazione IRL 198 ===
Loss reward (iter 198): 6.830323219299316
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 275      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.78    |
|    critic_loss     | 0.00786  |
|    learning_rate   | 0.001    |
|    n_updates       | 89399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 241      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.68    |
|    critic_loss     | 0.00855  |
|    learning_rate   | 0.001    |
|    n_updates       | 89799    |
---------------------------------
=== Iterazione IRL 199 ===
Loss reward (iter 199): 6.8564677238464355
=== Iterazione IRL 200 ===
Loss reward (iter 200): 6.866716384887695
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.76    |
|    critic_loss     | 0.00968  |
|    learning_rate   | 0.001    |
|    n_updates       | 90299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 241      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.73    |
|    critic_loss     | 0.0097   |
|    learning_rate   | 0.001    |
|    n_updates       | 90699    |
---------------------------------
=== Iterazione IRL 201 ===
Loss reward (iter 201): 6.866307258605957
=== Iterazione IRL 202 ===
Loss reward (iter 202): 6.8625993728637695
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.77    |
|    critic_loss     | 0.00968  |
|    learning_rate   | 0.001    |
|    n_updates       | 91199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.9     |
|    critic_loss     | 0.00924  |
|    learning_rate   | 0.001    |
|    n_updates       | 91599    |
---------------------------------
=== Iterazione IRL 203 ===
Loss reward (iter 203): 6.882940769195557
=== Iterazione IRL 204 ===
Loss reward (iter 204): 6.855251312255859
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.58    |
|    critic_loss     | 0.00792  |
|    learning_rate   | 0.001    |
|    n_updates       | 92099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 241      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.79    |
|    critic_loss     | 0.0102   |
|    learning_rate   | 0.001    |
|    n_updates       | 92499    |
---------------------------------
=== Iterazione IRL 205 ===
Loss reward (iter 205): 6.882866859436035
=== Iterazione IRL 206 ===
Loss reward (iter 206): 6.871435642242432
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.78    |
|    critic_loss     | 0.00735  |
|    learning_rate   | 0.001    |
|    n_updates       | 92999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 241      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.51    |
|    critic_loss     | 0.00627  |
|    learning_rate   | 0.001    |
|    n_updates       | 93399    |
---------------------------------
=== Iterazione IRL 207 ===
Loss reward (iter 207): 6.8789191246032715
=== Iterazione IRL 208 ===
Loss reward (iter 208): 6.827950477600098
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 275      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.66    |
|    critic_loss     | 0.0112   |
|    learning_rate   | 0.001    |
|    n_updates       | 93899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 241      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.66    |
|    critic_loss     | 0.0072   |
|    learning_rate   | 0.001    |
|    n_updates       | 94299    |
---------------------------------
=== Iterazione IRL 209 ===
Loss reward (iter 209): 6.880762100219727
=== Iterazione IRL 210 ===
Loss reward (iter 210): 6.874032974243164
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.61    |
|    critic_loss     | 0.00589  |
|    learning_rate   | 0.001    |
|    n_updates       | 94799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.76    |
|    critic_loss     | 0.00698  |
|    learning_rate   | 0.001    |
|    n_updates       | 95199    |
---------------------------------
=== Iterazione IRL 211 ===
Loss reward (iter 211): 6.842154026031494
=== Iterazione IRL 212 ===
Loss reward (iter 212): 6.84827184677124
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.63    |
|    critic_loss     | 0.0072   |
|    learning_rate   | 0.001    |
|    n_updates       | 95699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 241      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.42    |
|    critic_loss     | 0.00994  |
|    learning_rate   | 0.001    |
|    n_updates       | 96099    |
---------------------------------
=== Iterazione IRL 213 ===
Loss reward (iter 213): 6.860758304595947
=== Iterazione IRL 214 ===
Loss reward (iter 214): 6.867680072784424
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.8     |
|    critic_loss     | 0.00786  |
|    learning_rate   | 0.001    |
|    n_updates       | 96599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.52    |
|    critic_loss     | 0.00882  |
|    learning_rate   | 0.001    |
|    n_updates       | 96999    |
---------------------------------
=== Iterazione IRL 215 ===
Loss reward (iter 215): 6.861519813537598
=== Iterazione IRL 216 ===
Loss reward (iter 216): 6.861258506774902
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.69    |
|    critic_loss     | 0.00647  |
|    learning_rate   | 0.001    |
|    n_updates       | 97499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.44    |
|    critic_loss     | 0.00944  |
|    learning_rate   | 0.001    |
|    n_updates       | 97899    |
---------------------------------
=== Iterazione IRL 217 ===
Loss reward (iter 217): 6.868330478668213
=== Iterazione IRL 218 ===
Loss reward (iter 218): 6.87628173828125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.58    |
|    critic_loss     | 0.00892  |
|    learning_rate   | 0.001    |
|    n_updates       | 98399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.48    |
|    critic_loss     | 0.00825  |
|    learning_rate   | 0.001    |
|    n_updates       | 98799    |
---------------------------------
=== Iterazione IRL 219 ===
Loss reward (iter 219): 6.857577800750732
=== Iterazione IRL 220 ===
Loss reward (iter 220): 6.851090431213379
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.39    |
|    critic_loss     | 0.00662  |
|    learning_rate   | 0.001    |
|    n_updates       | 99299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.31    |
|    critic_loss     | 0.00788  |
|    learning_rate   | 0.001    |
|    n_updates       | 99699    |
---------------------------------
=== Iterazione IRL 221 ===
Loss reward (iter 221): 6.8550519943237305
=== Iterazione IRL 222 ===
Loss reward (iter 222): 6.883453369140625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.53    |
|    critic_loss     | 0.00898  |
|    learning_rate   | 0.001    |
|    n_updates       | 100199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.5     |
|    critic_loss     | 0.00757  |
|    learning_rate   | 0.001    |
|    n_updates       | 100599   |
---------------------------------
=== Iterazione IRL 223 ===
Loss reward (iter 223): 6.884159088134766
=== Iterazione IRL 224 ===
Loss reward (iter 224): 6.887254238128662
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 275      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.36    |
|    critic_loss     | 0.00624  |
|    learning_rate   | 0.001    |
|    n_updates       | 101099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 241      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.53    |
|    critic_loss     | 0.00668  |
|    learning_rate   | 0.001    |
|    n_updates       | 101499   |
---------------------------------
=== Iterazione IRL 225 ===
Loss reward (iter 225): 6.875593662261963
=== Iterazione IRL 226 ===
Loss reward (iter 226): 6.853824615478516
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.31    |
|    critic_loss     | 0.00869  |
|    learning_rate   | 0.001    |
|    n_updates       | 101999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.51    |
|    critic_loss     | 0.00872  |
|    learning_rate   | 0.001    |
|    n_updates       | 102399   |
---------------------------------
=== Iterazione IRL 227 ===
Loss reward (iter 227): 6.840728282928467
=== Iterazione IRL 228 ===
Loss reward (iter 228): 6.84755802154541
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.25    |
|    critic_loss     | 0.00638  |
|    learning_rate   | 0.001    |
|    n_updates       | 102899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.38    |
|    critic_loss     | 0.00747  |
|    learning_rate   | 0.001    |
|    n_updates       | 103299   |
---------------------------------
=== Iterazione IRL 229 ===
Loss reward (iter 229): 6.85728645324707
=== Iterazione IRL 230 ===
Loss reward (iter 230): 6.8825788497924805
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.4     |
|    critic_loss     | 0.0103   |
|    learning_rate   | 0.001    |
|    n_updates       | 103799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.39    |
|    critic_loss     | 0.00682  |
|    learning_rate   | 0.001    |
|    n_updates       | 104199   |
---------------------------------
=== Iterazione IRL 231 ===
Loss reward (iter 231): 6.868410587310791
=== Iterazione IRL 232 ===
Loss reward (iter 232): 6.855937957763672
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.21    |
|    critic_loss     | 0.00603  |
|    learning_rate   | 0.001    |
|    n_updates       | 104699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.2     |
|    critic_loss     | 0.00737  |
|    learning_rate   | 0.001    |
|    n_updates       | 105099   |
---------------------------------
=== Iterazione IRL 233 ===
Loss reward (iter 233): 6.865913391113281
=== Iterazione IRL 234 ===
Loss reward (iter 234): 6.827707290649414
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.27    |
|    critic_loss     | 0.00658  |
|    learning_rate   | 0.001    |
|    n_updates       | 105599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.2     |
|    critic_loss     | 0.0103   |
|    learning_rate   | 0.001    |
|    n_updates       | 105999   |
---------------------------------
=== Iterazione IRL 235 ===
Loss reward (iter 235): 6.85545539855957
=== Iterazione IRL 236 ===
Loss reward (iter 236): 6.846652507781982
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.11    |
|    critic_loss     | 0.00967  |
|    learning_rate   | 0.001    |
|    n_updates       | 106499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.43    |
|    critic_loss     | 0.00771  |
|    learning_rate   | 0.001    |
|    n_updates       | 106899   |
---------------------------------
=== Iterazione IRL 237 ===
Loss reward (iter 237): 6.863987445831299
=== Iterazione IRL 238 ===
Loss reward (iter 238): 6.854306221008301
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.31    |
|    critic_loss     | 0.0135   |
|    learning_rate   | 0.001    |
|    n_updates       | 107399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.26    |
|    critic_loss     | 0.00625  |
|    learning_rate   | 0.001    |
|    n_updates       | 107799   |
---------------------------------
=== Iterazione IRL 239 ===
Loss reward (iter 239): 6.854479789733887
=== Iterazione IRL 240 ===
Loss reward (iter 240): 6.85761022567749
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.15    |
|    critic_loss     | 0.00692  |
|    learning_rate   | 0.001    |
|    n_updates       | 108299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.23    |
|    critic_loss     | 0.00929  |
|    learning_rate   | 0.001    |
|    n_updates       | 108699   |
---------------------------------
=== Iterazione IRL 241 ===
Loss reward (iter 241): 6.844663143157959
=== Iterazione IRL 242 ===
Loss reward (iter 242): 6.855841636657715
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 275      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.2     |
|    critic_loss     | 0.00654  |
|    learning_rate   | 0.001    |
|    n_updates       | 109199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 241      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.18    |
|    critic_loss     | 0.00779  |
|    learning_rate   | 0.001    |
|    n_updates       | 109599   |
---------------------------------
=== Iterazione IRL 243 ===
Loss reward (iter 243): 6.849551677703857
=== Iterazione IRL 244 ===
Loss reward (iter 244): 6.852787017822266
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.18    |
|    critic_loss     | 0.0065   |
|    learning_rate   | 0.001    |
|    n_updates       | 110099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 241      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.09    |
|    critic_loss     | 0.00548  |
|    learning_rate   | 0.001    |
|    n_updates       | 110499   |
---------------------------------
=== Iterazione IRL 245 ===
Loss reward (iter 245): 6.846611976623535
=== Iterazione IRL 246 ===
Loss reward (iter 246): 6.859802722930908
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.04    |
|    critic_loss     | 0.00588  |
|    learning_rate   | 0.001    |
|    n_updates       | 110999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 241      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.04    |
|    critic_loss     | 0.00711  |
|    learning_rate   | 0.001    |
|    n_updates       | 111399   |
---------------------------------
=== Iterazione IRL 247 ===
Loss reward (iter 247): 6.860072612762451
=== Iterazione IRL 248 ===
Loss reward (iter 248): 6.86552095413208
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.97    |
|    critic_loss     | 0.00757  |
|    learning_rate   | 0.001    |
|    n_updates       | 111899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 241      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.92    |
|    critic_loss     | 0.0055   |
|    learning_rate   | 0.001    |
|    n_updates       | 112299   |
---------------------------------
=== Iterazione IRL 249 ===
Loss reward (iter 249): 6.821954250335693
=== Iterazione IRL 250 ===
Loss reward (iter 250): 6.808267593383789
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 275      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.11    |
|    critic_loss     | 0.0069   |
|    learning_rate   | 0.001    |
|    n_updates       | 112799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 241      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.24    |
|    critic_loss     | 0.00705  |
|    learning_rate   | 0.001    |
|    n_updates       | 113199   |
---------------------------------
=== Iterazione IRL 251 ===
Loss reward (iter 251): 6.814053058624268
=== Iterazione IRL 252 ===
Loss reward (iter 252): 6.838384628295898
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 275      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.97    |
|    critic_loss     | 0.00834  |
|    learning_rate   | 0.001    |
|    n_updates       | 113699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 241      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.16    |
|    critic_loss     | 0.00621  |
|    learning_rate   | 0.001    |
|    n_updates       | 114099   |
---------------------------------
=== Iterazione IRL 253 ===
Loss reward (iter 253): 6.798297882080078
=== Iterazione IRL 254 ===
Loss reward (iter 254): 6.8603515625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 276      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.06    |
|    critic_loss     | 0.00694  |
|    learning_rate   | 0.001    |
|    n_updates       | 114599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 241      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.12    |
|    critic_loss     | 0.00882  |
|    learning_rate   | 0.001    |
|    n_updates       | 114999   |
---------------------------------
=== Iterazione IRL 255 ===
Loss reward (iter 255): 6.842837810516357
=== Iterazione IRL 256 ===
Loss reward (iter 256): 6.829380035400391
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.41    |
|    critic_loss     | 0.00651  |
|    learning_rate   | 0.001    |
|    n_updates       | 115499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 241      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.01    |
|    critic_loss     | 0.00881  |
|    learning_rate   | 0.001    |
|    n_updates       | 115899   |
---------------------------------
=== Iterazione IRL 257 ===
Loss reward (iter 257): 6.812300205230713
=== Iterazione IRL 258 ===
Loss reward (iter 258): 6.814403533935547
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 275      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.29    |
|    critic_loss     | 0.00924  |
|    learning_rate   | 0.001    |
|    n_updates       | 116399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 241      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.57    |
|    critic_loss     | 0.00888  |
|    learning_rate   | 0.001    |
|    n_updates       | 116799   |
---------------------------------
=== Iterazione IRL 259 ===
Loss reward (iter 259): 6.788148880004883
=== Iterazione IRL 260 ===
Loss reward (iter 260): 6.843643665313721
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 275      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.45    |
|    critic_loss     | 0.0067   |
|    learning_rate   | 0.001    |
|    n_updates       | 117299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 241      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.3     |
|    critic_loss     | 0.0089   |
|    learning_rate   | 0.001    |
|    n_updates       | 117699   |
---------------------------------
=== Iterazione IRL 261 ===
Loss reward (iter 261): 6.804459095001221
=== Iterazione IRL 262 ===
Loss reward (iter 262): 6.790156841278076
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 275      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.52    |
|    critic_loss     | 0.00991  |
|    learning_rate   | 0.001    |
|    n_updates       | 118199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 241      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.5     |
|    critic_loss     | 0.00797  |
|    learning_rate   | 0.001    |
|    n_updates       | 118599   |
---------------------------------
=== Iterazione IRL 263 ===
Loss reward (iter 263): 6.844172477722168
=== Iterazione IRL 264 ===
Loss reward (iter 264): 6.819601058959961
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 275      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.34    |
|    critic_loss     | 0.00979  |
|    learning_rate   | 0.001    |
|    n_updates       | 119099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 241      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.23    |
|    critic_loss     | 0.00895  |
|    learning_rate   | 0.001    |
|    n_updates       | 119499   |
---------------------------------
=== Iterazione IRL 265 ===
Loss reward (iter 265): 6.79881477355957
=== Iterazione IRL 266 ===
Loss reward (iter 266): 6.764669418334961
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 275      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.32    |
|    critic_loss     | 0.00849  |
|    learning_rate   | 0.001    |
|    n_updates       | 119999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 241      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.16    |
|    critic_loss     | 0.00842  |
|    learning_rate   | 0.001    |
|    n_updates       | 120399   |
---------------------------------
=== Iterazione IRL 267 ===
Loss reward (iter 267): 6.796417236328125
=== Iterazione IRL 268 ===
Loss reward (iter 268): 6.7872538566589355
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 275      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.14    |
|    critic_loss     | 0.00753  |
|    learning_rate   | 0.001    |
|    n_updates       | 120899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 241      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.33    |
|    critic_loss     | 0.00745  |
|    learning_rate   | 0.001    |
|    n_updates       | 121299   |
---------------------------------
=== Iterazione IRL 269 ===
Loss reward (iter 269): 6.764659404754639
=== Iterazione IRL 270 ===
Loss reward (iter 270): 6.866604328155518
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.4     |
|    critic_loss     | 0.00976  |
|    learning_rate   | 0.001    |
|    n_updates       | 121799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.46    |
|    critic_loss     | 0.01     |
|    learning_rate   | 0.001    |
|    n_updates       | 122199   |
---------------------------------
=== Iterazione IRL 271 ===
Loss reward (iter 271): 6.807043075561523
=== Iterazione IRL 272 ===
Loss reward (iter 272): 6.830574989318848
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 275      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.3     |
|    critic_loss     | 0.012    |
|    learning_rate   | 0.001    |
|    n_updates       | 122699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 241      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.52    |
|    critic_loss     | 0.0108   |
|    learning_rate   | 0.001    |
|    n_updates       | 123099   |
---------------------------------
=== Iterazione IRL 273 ===
Loss reward (iter 273): 6.751147270202637
=== Iterazione IRL 274 ===
Loss reward (iter 274): 6.728748798370361
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.06    |
|    critic_loss     | 0.0075   |
|    learning_rate   | 0.001    |
|    n_updates       | 123599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 241      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.25    |
|    critic_loss     | 0.00761  |
|    learning_rate   | 0.001    |
|    n_updates       | 123999   |
---------------------------------
=== Iterazione IRL 275 ===
Loss reward (iter 275): 6.817255973815918
=== Iterazione IRL 276 ===
Loss reward (iter 276): 6.845000743865967
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.38    |
|    critic_loss     | 0.00987  |
|    learning_rate   | 0.001    |
|    n_updates       | 124499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 241      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.08    |
|    critic_loss     | 0.0118   |
|    learning_rate   | 0.001    |
|    n_updates       | 124899   |
---------------------------------
=== Iterazione IRL 277 ===
Loss reward (iter 277): 6.807037353515625
=== Iterazione IRL 278 ===
Loss reward (iter 278): 6.771520137786865
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.3     |
|    critic_loss     | 0.0105   |
|    learning_rate   | 0.001    |
|    n_updates       | 125399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.13    |
|    critic_loss     | 0.00927  |
|    learning_rate   | 0.001    |
|    n_updates       | 125799   |
---------------------------------
=== Iterazione IRL 279 ===
Loss reward (iter 279): 6.819731712341309
=== Iterazione IRL 280 ===
Loss reward (iter 280): 6.822721481323242
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 275      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.06    |
|    critic_loss     | 0.0101   |
|    learning_rate   | 0.001    |
|    n_updates       | 126299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.28    |
|    critic_loss     | 0.01     |
|    learning_rate   | 0.001    |
|    n_updates       | 126699   |
---------------------------------
=== Iterazione IRL 281 ===
Loss reward (iter 281): 6.837390899658203
=== Iterazione IRL 282 ===
Loss reward (iter 282): 6.83321475982666
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 275      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.99    |
|    critic_loss     | 0.0109   |
|    learning_rate   | 0.001    |
|    n_updates       | 127199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 241      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.03    |
|    critic_loss     | 0.00665  |
|    learning_rate   | 0.001    |
|    n_updates       | 127599   |
---------------------------------
=== Iterazione IRL 283 ===
Loss reward (iter 283): 6.812580108642578
=== Iterazione IRL 284 ===
Loss reward (iter 284): 6.823401927947998
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 275      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.92    |
|    critic_loss     | 0.00944  |
|    learning_rate   | 0.001    |
|    n_updates       | 128099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 241      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.13    |
|    critic_loss     | 0.0105   |
|    learning_rate   | 0.001    |
|    n_updates       | 128499   |
---------------------------------
=== Iterazione IRL 285 ===
Loss reward (iter 285): 6.830770015716553
=== Iterazione IRL 286 ===
Loss reward (iter 286): 6.84134578704834
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.04    |
|    critic_loss     | 0.00595  |
|    learning_rate   | 0.001    |
|    n_updates       | 128999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.22    |
|    critic_loss     | 0.00707  |
|    learning_rate   | 0.001    |
|    n_updates       | 129399   |
---------------------------------
=== Iterazione IRL 287 ===
Loss reward (iter 287): 6.821490287780762
=== Iterazione IRL 288 ===
Loss reward (iter 288): 6.821307182312012
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 275      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.19    |
|    critic_loss     | 0.00732  |
|    learning_rate   | 0.001    |
|    n_updates       | 129899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 241      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.04    |
|    critic_loss     | 0.00701  |
|    learning_rate   | 0.001    |
|    n_updates       | 130299   |
---------------------------------
=== Iterazione IRL 289 ===
Loss reward (iter 289): 6.839910507202148
=== Iterazione IRL 290 ===
Loss reward (iter 290): 6.8527374267578125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 275      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.32    |
|    critic_loss     | 0.00908  |
|    learning_rate   | 0.001    |
|    n_updates       | 130799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 241      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.34    |
|    critic_loss     | 0.0102   |
|    learning_rate   | 0.001    |
|    n_updates       | 131199   |
---------------------------------
=== Iterazione IRL 291 ===
Loss reward (iter 291): 6.817470550537109
=== Iterazione IRL 292 ===
Loss reward (iter 292): 6.830259323120117
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 276      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.07    |
|    critic_loss     | 0.00856  |
|    learning_rate   | 0.001    |
|    n_updates       | 131699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 241      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.11    |
|    critic_loss     | 0.00707  |
|    learning_rate   | 0.001    |
|    n_updates       | 132099   |
---------------------------------
=== Iterazione IRL 293 ===
Loss reward (iter 293): 6.855744361877441
=== Iterazione IRL 294 ===
Loss reward (iter 294): 6.854827880859375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 275      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.01    |
|    critic_loss     | 0.00778  |
|    learning_rate   | 0.001    |
|    n_updates       | 132599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 241      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.13    |
|    critic_loss     | 0.00719  |
|    learning_rate   | 0.001    |
|    n_updates       | 132999   |
---------------------------------
=== Iterazione IRL 295 ===
Loss reward (iter 295): 6.833617210388184
=== Iterazione IRL 296 ===
Loss reward (iter 296): 6.8378753662109375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 276      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3       |
|    critic_loss     | 0.00638  |
|    learning_rate   | 0.001    |
|    n_updates       | 133499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 241      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.9     |
|    critic_loss     | 0.00594  |
|    learning_rate   | 0.001    |
|    n_updates       | 133899   |
---------------------------------
=== Iterazione IRL 297 ===
Loss reward (iter 297): 6.855155944824219
=== Iterazione IRL 298 ===
Loss reward (iter 298): 6.852783203125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 275      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.17    |
|    critic_loss     | 0.00672  |
|    learning_rate   | 0.001    |
|    n_updates       | 134399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 241      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.91    |
|    critic_loss     | 0.00852  |
|    learning_rate   | 0.001    |
|    n_updates       | 134799   |
---------------------------------
=== Iterazione IRL 299 ===
Loss reward (iter 299): 6.831264972686768
=== Iterazione IRL 300 ===
Loss reward (iter 300): 6.849115371704102
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 275      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.98    |
|    critic_loss     | 0.00836  |
|    learning_rate   | 0.001    |
|    n_updates       | 135299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 241      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.91    |
|    critic_loss     | 0.00838  |
|    learning_rate   | 0.001    |
|    n_updates       | 135699   |
---------------------------------
=== Iterazione IRL 301 ===
Loss reward (iter 301): 6.864991188049316
=== Iterazione IRL 302 ===
Loss reward (iter 302): 6.812661647796631
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 275      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.99    |
|    critic_loss     | 0.00865  |
|    learning_rate   | 0.001    |
|    n_updates       | 136199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 241      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.8     |
|    critic_loss     | 0.0059   |
|    learning_rate   | 0.001    |
|    n_updates       | 136599   |
---------------------------------
=== Iterazione IRL 303 ===
Loss reward (iter 303): 6.83404541015625
=== Iterazione IRL 304 ===
Loss reward (iter 304): 6.825163841247559
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 275      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.86    |
|    critic_loss     | 0.006    |
|    learning_rate   | 0.001    |
|    n_updates       | 137099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 241      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.83    |
|    critic_loss     | 0.00806  |
|    learning_rate   | 0.001    |
|    n_updates       | 137499   |
---------------------------------
=== Iterazione IRL 305 ===
Loss reward (iter 305): 6.8314642906188965
=== Iterazione IRL 306 ===
Loss reward (iter 306): 6.822921276092529
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 275      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.75    |
|    critic_loss     | 0.00606  |
|    learning_rate   | 0.001    |
|    n_updates       | 137999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 241      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.73    |
|    critic_loss     | 0.00644  |
|    learning_rate   | 0.001    |
|    n_updates       | 138399   |
---------------------------------
=== Iterazione IRL 307 ===
Loss reward (iter 307): 6.853550910949707
=== Iterazione IRL 308 ===
Loss reward (iter 308): 6.830798625946045
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.83    |
|    critic_loss     | 0.00817  |
|    learning_rate   | 0.001    |
|    n_updates       | 138899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.86    |
|    critic_loss     | 0.00656  |
|    learning_rate   | 0.001    |
|    n_updates       | 139299   |
---------------------------------
=== Iterazione IRL 309 ===
Loss reward (iter 309): 6.830452919006348
=== Iterazione IRL 310 ===
Loss reward (iter 310): 6.829428195953369
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 275      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.83    |
|    critic_loss     | 0.0095   |
|    learning_rate   | 0.001    |
|    n_updates       | 139799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 241      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.83    |
|    critic_loss     | 0.00666  |
|    learning_rate   | 0.001    |
|    n_updates       | 140199   |
---------------------------------
=== Iterazione IRL 311 ===
Loss reward (iter 311): 6.8069844245910645
=== Iterazione IRL 312 ===
Loss reward (iter 312): 6.8345417976379395
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 275      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.58    |
|    critic_loss     | 0.0051   |
|    learning_rate   | 0.001    |
|    n_updates       | 140699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 241      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.49    |
|    critic_loss     | 0.00819  |
|    learning_rate   | 0.001    |
|    n_updates       | 141099   |
---------------------------------
=== Iterazione IRL 313 ===
Loss reward (iter 313): 6.810724258422852
=== Iterazione IRL 314 ===
Loss reward (iter 314): 6.835682392120361
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 275      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.95    |
|    critic_loss     | 0.00672  |
|    learning_rate   | 0.001    |
|    n_updates       | 141599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 241      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.83    |
|    critic_loss     | 0.00632  |
|    learning_rate   | 0.001    |
|    n_updates       | 141999   |
---------------------------------
=== Iterazione IRL 315 ===
Loss reward (iter 315): 6.8492231369018555
=== Iterazione IRL 316 ===
Loss reward (iter 316): 6.816471576690674
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.63    |
|    critic_loss     | 0.00676  |
|    learning_rate   | 0.001    |
|    n_updates       | 142499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.82    |
|    critic_loss     | 0.00751  |
|    learning_rate   | 0.001    |
|    n_updates       | 142899   |
---------------------------------
=== Iterazione IRL 317 ===
Loss reward (iter 317): 6.800572395324707
=== Iterazione IRL 318 ===
Loss reward (iter 318): 6.814550876617432
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 276      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.69    |
|    critic_loss     | 0.0061   |
|    learning_rate   | 0.001    |
|    n_updates       | 143399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 241      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.95    |
|    critic_loss     | 0.00789  |
|    learning_rate   | 0.001    |
|    n_updates       | 143799   |
---------------------------------
=== Iterazione IRL 319 ===
Loss reward (iter 319): 6.820078372955322
=== Iterazione IRL 320 ===
Loss reward (iter 320): 6.83156681060791
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 276      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.68    |
|    critic_loss     | 0.00745  |
|    learning_rate   | 0.001    |
|    n_updates       | 144299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 241      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.67    |
|    critic_loss     | 0.0062   |
|    learning_rate   | 0.001    |
|    n_updates       | 144699   |
---------------------------------
=== Iterazione IRL 321 ===
Loss reward (iter 321): 6.830887794494629
=== Iterazione IRL 322 ===
Loss reward (iter 322): 6.81876802444458
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 277      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.56    |
|    critic_loss     | 0.00694  |
|    learning_rate   | 0.001    |
|    n_updates       | 145199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 243      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.68    |
|    critic_loss     | 0.00756  |
|    learning_rate   | 0.001    |
|    n_updates       | 145599   |
---------------------------------
=== Iterazione IRL 323 ===
Loss reward (iter 323): 6.817153453826904
=== Iterazione IRL 324 ===
Loss reward (iter 324): 6.829632759094238
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 276      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.65    |
|    critic_loss     | 0.00693  |
|    learning_rate   | 0.001    |
|    n_updates       | 146099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 242      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.57    |
|    critic_loss     | 0.00499  |
|    learning_rate   | 0.001    |
|    n_updates       | 146499   |
---------------------------------
=== Iterazione IRL 325 ===
Loss reward (iter 325): 6.808684349060059
=== Iterazione IRL 326 ===
Loss reward (iter 326): 6.803602695465088
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 276      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.76    |
|    critic_loss     | 0.00892  |
|    learning_rate   | 0.001    |
|    n_updates       | 146999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 242      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.71    |
|    critic_loss     | 0.00733  |
|    learning_rate   | 0.001    |
|    n_updates       | 147399   |
---------------------------------
=== Iterazione IRL 327 ===
Loss reward (iter 327): 6.819492340087891
=== Iterazione IRL 328 ===
Loss reward (iter 328): 6.83230447769165
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 276      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.59    |
|    critic_loss     | 0.00784  |
|    learning_rate   | 0.001    |
|    n_updates       | 147899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 242      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.7     |
|    critic_loss     | 0.00592  |
|    learning_rate   | 0.001    |
|    n_updates       | 148299   |
---------------------------------
=== Iterazione IRL 329 ===
Loss reward (iter 329): 6.805307388305664
=== Iterazione IRL 330 ===
Loss reward (iter 330): 6.826787948608398
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 276      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.66    |
|    critic_loss     | 0.00615  |
|    learning_rate   | 0.001    |
|    n_updates       | 148799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 241      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.64    |
|    critic_loss     | 0.00775  |
|    learning_rate   | 0.001    |
|    n_updates       | 149199   |
---------------------------------
=== Iterazione IRL 331 ===
Loss reward (iter 331): 6.810654640197754
=== Iterazione IRL 332 ===
Loss reward (iter 332): 6.805238246917725
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 275      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.54    |
|    critic_loss     | 0.0089   |
|    learning_rate   | 0.001    |
|    n_updates       | 149699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 241      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.56    |
|    critic_loss     | 0.00723  |
|    learning_rate   | 0.001    |
|    n_updates       | 150099   |
---------------------------------
=== Iterazione IRL 333 ===
Loss reward (iter 333): 6.816091060638428
=== Iterazione IRL 334 ===
Loss reward (iter 334): 6.818713188171387
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 276      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.35    |
|    critic_loss     | 0.00819  |
|    learning_rate   | 0.001    |
|    n_updates       | 150599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 242      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.56    |
|    critic_loss     | 0.00687  |
|    learning_rate   | 0.001    |
|    n_updates       | 150999   |
---------------------------------
=== Iterazione IRL 335 ===
Loss reward (iter 335): 6.821427822113037
=== Iterazione IRL 336 ===
Loss reward (iter 336): 6.791467189788818
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 276      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.73    |
|    critic_loss     | 0.00681  |
|    learning_rate   | 0.001    |
|    n_updates       | 151499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 242      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.59    |
|    critic_loss     | 0.00609  |
|    learning_rate   | 0.001    |
|    n_updates       | 151899   |
---------------------------------
=== Iterazione IRL 337 ===
Loss reward (iter 337): 6.804349422454834
=== Iterazione IRL 338 ===
Loss reward (iter 338): 6.822809219360352
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 275      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.77    |
|    critic_loss     | 0.00793  |
|    learning_rate   | 0.001    |
|    n_updates       | 152399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 241      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.35    |
|    critic_loss     | 0.00815  |
|    learning_rate   | 0.001    |
|    n_updates       | 152799   |
---------------------------------
=== Iterazione IRL 339 ===
Loss reward (iter 339): 6.800901889801025
=== Iterazione IRL 340 ===
Loss reward (iter 340): 6.832023620605469
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 277      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.62    |
|    critic_loss     | 0.00813  |
|    learning_rate   | 0.001    |
|    n_updates       | 153299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 243      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.42    |
|    critic_loss     | 0.00729  |
|    learning_rate   | 0.001    |
|    n_updates       | 153699   |
---------------------------------
=== Iterazione IRL 341 ===
Loss reward (iter 341): 6.801035404205322
=== Iterazione IRL 342 ===
Loss reward (iter 342): 6.808371543884277
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 277      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.62    |
|    critic_loss     | 0.00721  |
|    learning_rate   | 0.001    |
|    n_updates       | 154199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 242      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.33    |
|    critic_loss     | 0.00594  |
|    learning_rate   | 0.001    |
|    n_updates       | 154599   |
---------------------------------
=== Iterazione IRL 343 ===
Loss reward (iter 343): 6.805854320526123
=== Iterazione IRL 344 ===
Loss reward (iter 344): 6.806181907653809
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 277      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.57    |
|    critic_loss     | 0.00712  |
|    learning_rate   | 0.001    |
|    n_updates       | 155099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 242      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.44    |
|    critic_loss     | 0.00682  |
|    learning_rate   | 0.001    |
|    n_updates       | 155499   |
---------------------------------
=== Iterazione IRL 345 ===
Loss reward (iter 345): 6.822502136230469
=== Iterazione IRL 346 ===
Loss reward (iter 346): 6.812437057495117
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 277      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.49    |
|    critic_loss     | 0.00613  |
|    learning_rate   | 0.001    |
|    n_updates       | 155999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 243      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.55    |
|    critic_loss     | 0.00737  |
|    learning_rate   | 0.001    |
|    n_updates       | 156399   |
---------------------------------
=== Iterazione IRL 347 ===
Loss reward (iter 347): 6.8174614906311035
=== Iterazione IRL 348 ===
Loss reward (iter 348): 6.810729026794434
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 278      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.48    |
|    critic_loss     | 0.00743  |
|    learning_rate   | 0.001    |
|    n_updates       | 156899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 243      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.73    |
|    critic_loss     | 0.00635  |
|    learning_rate   | 0.001    |
|    n_updates       | 157299   |
---------------------------------
=== Iterazione IRL 349 ===
Loss reward (iter 349): 6.798739433288574
=== Iterazione IRL 350 ===
Loss reward (iter 350): 6.791675090789795
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 278      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.56    |
|    critic_loss     | 0.00614  |
|    learning_rate   | 0.001    |
|    n_updates       | 157799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 244      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.36    |
|    critic_loss     | 0.00654  |
|    learning_rate   | 0.001    |
|    n_updates       | 158199   |
---------------------------------
=== Iterazione IRL 351 ===
Loss reward (iter 351): 6.780662536621094
=== Iterazione IRL 352 ===
Loss reward (iter 352): 6.7980637550354
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 278      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.45    |
|    critic_loss     | 0.00742  |
|    learning_rate   | 0.001    |
|    n_updates       | 158699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 244      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.48    |
|    critic_loss     | 0.00779  |
|    learning_rate   | 0.001    |
|    n_updates       | 159099   |
---------------------------------
=== Iterazione IRL 353 ===
Loss reward (iter 353): 6.81260871887207
=== Iterazione IRL 354 ===
Loss reward (iter 354): 6.798460006713867
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 277      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.37    |
|    critic_loss     | 0.00645  |
|    learning_rate   | 0.001    |
|    n_updates       | 159599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 243      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.43    |
|    critic_loss     | 0.0047   |
|    learning_rate   | 0.001    |
|    n_updates       | 159999   |
---------------------------------
