Inizio training IRL
Using device: cuda
Using cuda device
=== Iterazione IRL 0 ===
Loss reward (iter 0): 6.674534320831299
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 258      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -0.355   |
|    critic_loss     | 0.000164 |
|    learning_rate   | 0.001    |
|    n_updates       | 299      |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 229      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -0.456   |
|    critic_loss     | 0.000218 |
|    learning_rate   | 0.001    |
|    n_updates       | 699      |
---------------------------------
=== Iterazione IRL 1 ===
Loss reward (iter 1): 6.789131164550781
=== Iterazione IRL 2 ===
Loss reward (iter 2): 6.774746417999268
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -0.598   |
|    critic_loss     | 0.000383 |
|    learning_rate   | 0.001    |
|    n_updates       | 1199     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -0.724   |
|    critic_loss     | 0.00035  |
|    learning_rate   | 0.001    |
|    n_updates       | 1599     |
---------------------------------
=== Iterazione IRL 3 ===
Loss reward (iter 3): 6.861733913421631
=== Iterazione IRL 4 ===
Loss reward (iter 4): 6.814388275146484
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -0.864   |
|    critic_loss     | 0.000694 |
|    learning_rate   | 0.001    |
|    n_updates       | 2099     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -0.98    |
|    critic_loss     | 0.000835 |
|    learning_rate   | 0.001    |
|    n_updates       | 2499     |
---------------------------------
=== Iterazione IRL 5 ===
Loss reward (iter 5): 6.9460768699646
=== Iterazione IRL 6 ===
Loss reward (iter 6): 6.747503280639648
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -1.13    |
|    critic_loss     | 0.00132  |
|    learning_rate   | 0.001    |
|    n_updates       | 2999     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -1.23    |
|    critic_loss     | 0.00132  |
|    learning_rate   | 0.001    |
|    n_updates       | 3399     |
---------------------------------
=== Iterazione IRL 7 ===
Loss reward (iter 7): 6.804495811462402
=== Iterazione IRL 8 ===
Loss reward (iter 8): 6.85319709777832
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -1.34    |
|    critic_loss     | 0.00152  |
|    learning_rate   | 0.001    |
|    n_updates       | 3899     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 234      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -1.44    |
|    critic_loss     | 0.00165  |
|    learning_rate   | 0.001    |
|    n_updates       | 4299     |
---------------------------------
=== Iterazione IRL 9 ===
Loss reward (iter 9): 6.822510719299316
=== Iterazione IRL 10 ===
Loss reward (iter 10): 6.743918418884277
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -1.56    |
|    critic_loss     | 0.00159  |
|    learning_rate   | 0.001    |
|    n_updates       | 4799     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -1.69    |
|    critic_loss     | 0.0019   |
|    learning_rate   | 0.001    |
|    n_updates       | 5199     |
---------------------------------
=== Iterazione IRL 11 ===
Loss reward (iter 11): 6.707245826721191
=== Iterazione IRL 12 ===
Loss reward (iter 12): 6.824212074279785
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -1.84    |
|    critic_loss     | 0.00199  |
|    learning_rate   | 0.001    |
|    n_updates       | 5699     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -1.92    |
|    critic_loss     | 0.00209  |
|    learning_rate   | 0.001    |
|    n_updates       | 6099     |
---------------------------------
=== Iterazione IRL 13 ===
Loss reward (iter 13): 6.725762367248535
=== Iterazione IRL 14 ===
Loss reward (iter 14): 6.759249687194824
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.08    |
|    critic_loss     | 0.00157  |
|    learning_rate   | 0.001    |
|    n_updates       | 6599     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.19    |
|    critic_loss     | 0.00169  |
|    learning_rate   | 0.001    |
|    n_updates       | 6999     |
---------------------------------
=== Iterazione IRL 15 ===
Loss reward (iter 15): 6.668286323547363
=== Iterazione IRL 16 ===
Loss reward (iter 16): 6.782713413238525
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.32    |
|    critic_loss     | 0.00202  |
|    learning_rate   | 0.001    |
|    n_updates       | 7499     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.37    |
|    critic_loss     | 0.00148  |
|    learning_rate   | 0.001    |
|    n_updates       | 7899     |
---------------------------------
=== Iterazione IRL 17 ===
Loss reward (iter 17): 6.795709133148193
=== Iterazione IRL 18 ===
Loss reward (iter 18): 6.771997451782227
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.5     |
|    critic_loss     | 0.00149  |
|    learning_rate   | 0.001    |
|    n_updates       | 8399     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.6     |
|    critic_loss     | 0.00212  |
|    learning_rate   | 0.001    |
|    n_updates       | 8799     |
---------------------------------
=== Iterazione IRL 19 ===
Loss reward (iter 19): 6.717875003814697
=== Iterazione IRL 20 ===
Loss reward (iter 20): 6.80202579498291
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.69    |
|    critic_loss     | 0.00164  |
|    learning_rate   | 0.001    |
|    n_updates       | 9299     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.73    |
|    critic_loss     | 0.00151  |
|    learning_rate   | 0.001    |
|    n_updates       | 9699     |
---------------------------------
=== Iterazione IRL 21 ===
Loss reward (iter 21): 6.698307037353516
=== Iterazione IRL 22 ===
Loss reward (iter 22): 6.871032238006592
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.88    |
|    critic_loss     | 0.00169  |
|    learning_rate   | 0.001    |
|    n_updates       | 10199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.96    |
|    critic_loss     | 0.00173  |
|    learning_rate   | 0.001    |
|    n_updates       | 10599    |
---------------------------------
=== Iterazione IRL 23 ===
Loss reward (iter 23): 6.7465715408325195
=== Iterazione IRL 24 ===
Loss reward (iter 24): 6.708823204040527
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.05    |
|    critic_loss     | 0.00185  |
|    learning_rate   | 0.001    |
|    n_updates       | 11099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.12    |
|    critic_loss     | 0.00211  |
|    learning_rate   | 0.001    |
|    n_updates       | 11499    |
---------------------------------
=== Iterazione IRL 25 ===
Loss reward (iter 25): 6.874385833740234
=== Iterazione IRL 26 ===
Loss reward (iter 26): 6.619859218597412
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.2     |
|    critic_loss     | 0.00191  |
|    learning_rate   | 0.001    |
|    n_updates       | 11999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.33    |
|    critic_loss     | 0.00221  |
|    learning_rate   | 0.001    |
|    n_updates       | 12399    |
---------------------------------
=== Iterazione IRL 27 ===
Loss reward (iter 27): 6.73030948638916
=== Iterazione IRL 28 ===
Loss reward (iter 28): 6.778017997741699
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.38    |
|    critic_loss     | 0.00182  |
|    learning_rate   | 0.001    |
|    n_updates       | 12899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.51    |
|    critic_loss     | 0.00189  |
|    learning_rate   | 0.001    |
|    n_updates       | 13299    |
---------------------------------
=== Iterazione IRL 29 ===
Loss reward (iter 29): 6.766974449157715
=== Iterazione IRL 30 ===
Loss reward (iter 30): 6.77345609664917
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.58    |
|    critic_loss     | 0.00183  |
|    learning_rate   | 0.001    |
|    n_updates       | 13799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.68    |
|    critic_loss     | 0.00257  |
|    learning_rate   | 0.001    |
|    n_updates       | 14199    |
---------------------------------
=== Iterazione IRL 31 ===
Loss reward (iter 31): 6.712322235107422
=== Iterazione IRL 32 ===
Loss reward (iter 32): 6.6604485511779785
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.73    |
|    critic_loss     | 0.00188  |
|    learning_rate   | 0.001    |
|    n_updates       | 14699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.8     |
|    critic_loss     | 0.00168  |
|    learning_rate   | 0.001    |
|    n_updates       | 15099    |
---------------------------------
=== Iterazione IRL 33 ===
Loss reward (iter 33): 6.75012731552124
=== Iterazione IRL 34 ===
Loss reward (iter 34): 6.670502662658691
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.88    |
|    critic_loss     | 0.00229  |
|    learning_rate   | 0.001    |
|    n_updates       | 15599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.01    |
|    critic_loss     | 0.00184  |
|    learning_rate   | 0.001    |
|    n_updates       | 15999    |
---------------------------------
=== Iterazione IRL 35 ===
Loss reward (iter 35): 6.711975574493408
=== Iterazione IRL 36 ===
Loss reward (iter 36): 6.667267322540283
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 263      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.04    |
|    critic_loss     | 0.00174  |
|    learning_rate   | 0.001    |
|    n_updates       | 16499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 230      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.13    |
|    critic_loss     | 0.00193  |
|    learning_rate   | 0.001    |
|    n_updates       | 16899    |
---------------------------------
=== Iterazione IRL 37 ===
Loss reward (iter 37): 6.770251274108887
=== Iterazione IRL 38 ===
Loss reward (iter 38): 6.615974426269531
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.13    |
|    critic_loss     | 0.00237  |
|    learning_rate   | 0.001    |
|    n_updates       | 17399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.21    |
|    critic_loss     | 0.00243  |
|    learning_rate   | 0.001    |
|    n_updates       | 17799    |
---------------------------------
=== Iterazione IRL 39 ===
Loss reward (iter 39): 6.714644432067871
=== Iterazione IRL 40 ===
Loss reward (iter 40): 6.684802532196045
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.26    |
|    critic_loss     | 0.00258  |
|    learning_rate   | 0.001    |
|    n_updates       | 18299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.37    |
|    critic_loss     | 0.00206  |
|    learning_rate   | 0.001    |
|    n_updates       | 18699    |
---------------------------------
=== Iterazione IRL 41 ===
Loss reward (iter 41): 6.694029331207275
=== Iterazione IRL 42 ===
Loss reward (iter 42): 6.719085693359375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 263      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.42    |
|    critic_loss     | 0.00275  |
|    learning_rate   | 0.001    |
|    n_updates       | 19199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.49    |
|    critic_loss     | 0.00176  |
|    learning_rate   | 0.001    |
|    n_updates       | 19599    |
---------------------------------
=== Iterazione IRL 43 ===
Loss reward (iter 43): 6.699143409729004
=== Iterazione IRL 44 ===
Loss reward (iter 44): 6.660458564758301
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 263      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.58    |
|    critic_loss     | 0.00213  |
|    learning_rate   | 0.001    |
|    n_updates       | 20099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.74    |
|    critic_loss     | 0.00167  |
|    learning_rate   | 0.001    |
|    n_updates       | 20499    |
---------------------------------
=== Iterazione IRL 45 ===
Loss reward (iter 45): 6.737843036651611
=== Iterazione IRL 46 ===
Loss reward (iter 46): 6.756733417510986
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 263      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.74    |
|    critic_loss     | 0.00195  |
|    learning_rate   | 0.001    |
|    n_updates       | 20999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 230      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.83    |
|    critic_loss     | 0.00326  |
|    learning_rate   | 0.001    |
|    n_updates       | 21399    |
---------------------------------
=== Iterazione IRL 47 ===
Loss reward (iter 47): 6.588129043579102
=== Iterazione IRL 48 ===
Loss reward (iter 48): 6.746206283569336
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 263      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.89    |
|    critic_loss     | 0.0028   |
|    learning_rate   | 0.001    |
|    n_updates       | 21899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.81    |
|    critic_loss     | 0.00198  |
|    learning_rate   | 0.001    |
|    n_updates       | 22299    |
---------------------------------
=== Iterazione IRL 49 ===
Loss reward (iter 49): 6.655311584472656
=== Iterazione IRL 50 ===
Loss reward (iter 50): 6.655512809753418
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 263      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.04    |
|    critic_loss     | 0.00214  |
|    learning_rate   | 0.001    |
|    n_updates       | 22799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.09    |
|    critic_loss     | 0.00253  |
|    learning_rate   | 0.001    |
|    n_updates       | 23199    |
---------------------------------
=== Iterazione IRL 51 ===
Loss reward (iter 51): 6.6771345138549805
=== Iterazione IRL 52 ===
Loss reward (iter 52): 6.633146286010742
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 263      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.11    |
|    critic_loss     | 0.00263  |
|    learning_rate   | 0.001    |
|    n_updates       | 23699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.15    |
|    critic_loss     | 0.00305  |
|    learning_rate   | 0.001    |
|    n_updates       | 24099    |
---------------------------------
=== Iterazione IRL 53 ===
Loss reward (iter 53): 6.767908096313477
=== Iterazione IRL 54 ===
Loss reward (iter 54): 6.721584796905518
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 263      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.2     |
|    critic_loss     | 0.00239  |
|    learning_rate   | 0.001    |
|    n_updates       | 24599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.31    |
|    critic_loss     | 0.00293  |
|    learning_rate   | 0.001    |
|    n_updates       | 24999    |
---------------------------------
=== Iterazione IRL 55 ===
Loss reward (iter 55): 6.726120471954346
=== Iterazione IRL 56 ===
Loss reward (iter 56): 6.683620929718018
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.27    |
|    critic_loss     | 0.00181  |
|    learning_rate   | 0.001    |
|    n_updates       | 25499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.31    |
|    critic_loss     | 0.00219  |
|    learning_rate   | 0.001    |
|    n_updates       | 25899    |
---------------------------------
=== Iterazione IRL 57 ===
Loss reward (iter 57): 6.7123212814331055
=== Iterazione IRL 58 ===
Loss reward (iter 58): 6.668671607971191
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 267      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.39    |
|    critic_loss     | 0.00329  |
|    learning_rate   | 0.001    |
|    n_updates       | 26399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 234      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.45    |
|    critic_loss     | 0.00255  |
|    learning_rate   | 0.001    |
|    n_updates       | 26799    |
---------------------------------
=== Iterazione IRL 59 ===
Loss reward (iter 59): 6.621614456176758
=== Iterazione IRL 60 ===
Loss reward (iter 60): 6.6119465827941895
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.64    |
|    critic_loss     | 0.00268  |
|    learning_rate   | 0.001    |
|    n_updates       | 27299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.54    |
|    critic_loss     | 0.00313  |
|    learning_rate   | 0.001    |
|    n_updates       | 27699    |
---------------------------------
=== Iterazione IRL 61 ===
Loss reward (iter 61): 6.621702194213867
=== Iterazione IRL 62 ===
Loss reward (iter 62): 6.750106334686279
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 267      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.61    |
|    critic_loss     | 0.00309  |
|    learning_rate   | 0.001    |
|    n_updates       | 28199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 234      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.68    |
|    critic_loss     | 0.00254  |
|    learning_rate   | 0.001    |
|    n_updates       | 28599    |
---------------------------------
=== Iterazione IRL 63 ===
Loss reward (iter 63): 6.655916690826416
=== Iterazione IRL 64 ===
Loss reward (iter 64): 6.668928623199463
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 267      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.78    |
|    critic_loss     | 0.00304  |
|    learning_rate   | 0.001    |
|    n_updates       | 29099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.88    |
|    critic_loss     | 0.00317  |
|    learning_rate   | 0.001    |
|    n_updates       | 29499    |
---------------------------------
=== Iterazione IRL 65 ===
Loss reward (iter 65): 6.659236907958984
=== Iterazione IRL 66 ===
Loss reward (iter 66): 6.652085781097412
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.92    |
|    critic_loss     | 0.00422  |
|    learning_rate   | 0.001    |
|    n_updates       | 29999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.88    |
|    critic_loss     | 0.00314  |
|    learning_rate   | 0.001    |
|    n_updates       | 30399    |
---------------------------------
=== Iterazione IRL 67 ===
Loss reward (iter 67): 6.591765403747559
=== Iterazione IRL 68 ===
Loss reward (iter 68): 6.613712787628174
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 267      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.9     |
|    critic_loss     | 0.00268  |
|    learning_rate   | 0.001    |
|    n_updates       | 30899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 234      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.14    |
|    critic_loss     | 0.00226  |
|    learning_rate   | 0.001    |
|    n_updates       | 31299    |
---------------------------------
=== Iterazione IRL 69 ===
Loss reward (iter 69): 6.631985187530518
=== Iterazione IRL 70 ===
Loss reward (iter 70): 6.722076416015625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 267      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.12    |
|    critic_loss     | 0.00275  |
|    learning_rate   | 0.001    |
|    n_updates       | 31799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.11    |
|    critic_loss     | 0.00279  |
|    learning_rate   | 0.001    |
|    n_updates       | 32199    |
---------------------------------
=== Iterazione IRL 71 ===
Loss reward (iter 71): 6.690045356750488
=== Iterazione IRL 72 ===
Loss reward (iter 72): 6.691458702087402
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.08    |
|    critic_loss     | 0.0027   |
|    learning_rate   | 0.001    |
|    n_updates       | 32699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 234      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.35    |
|    critic_loss     | 0.00496  |
|    learning_rate   | 0.001    |
|    n_updates       | 33099    |
---------------------------------
=== Iterazione IRL 73 ===
Loss reward (iter 73): 6.657784461975098
=== Iterazione IRL 74 ===
Loss reward (iter 74): 6.583985805511475
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.28    |
|    critic_loss     | 0.00277  |
|    learning_rate   | 0.001    |
|    n_updates       | 33599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.37    |
|    critic_loss     | 0.00273  |
|    learning_rate   | 0.001    |
|    n_updates       | 33999    |
---------------------------------
=== Iterazione IRL 75 ===
Loss reward (iter 75): 6.7914228439331055
=== Iterazione IRL 76 ===
Loss reward (iter 76): 6.747335910797119
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.33    |
|    critic_loss     | 0.00312  |
|    learning_rate   | 0.001    |
|    n_updates       | 34499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.32    |
|    critic_loss     | 0.00343  |
|    learning_rate   | 0.001    |
|    n_updates       | 34899    |
---------------------------------
=== Iterazione IRL 77 ===
Loss reward (iter 77): 6.6500020027160645
=== Iterazione IRL 78 ===
Loss reward (iter 78): 6.6275763511657715
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.39    |
|    critic_loss     | 0.00287  |
|    learning_rate   | 0.001    |
|    n_updates       | 35399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.45    |
|    critic_loss     | 0.0027   |
|    learning_rate   | 0.001    |
|    n_updates       | 35799    |
---------------------------------
=== Iterazione IRL 79 ===
Loss reward (iter 79): 6.679169178009033
=== Iterazione IRL 80 ===
Loss reward (iter 80): 6.75861930847168
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.6     |
|    critic_loss     | 0.00325  |
|    learning_rate   | 0.001    |
|    n_updates       | 36299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.51    |
|    critic_loss     | 0.00321  |
|    learning_rate   | 0.001    |
|    n_updates       | 36699    |
---------------------------------
=== Iterazione IRL 81 ===
Loss reward (iter 81): 6.6284003257751465
=== Iterazione IRL 82 ===
Loss reward (iter 82): 6.6299591064453125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.55    |
|    critic_loss     | 0.0034   |
|    learning_rate   | 0.001    |
|    n_updates       | 37199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.65    |
|    critic_loss     | 0.00435  |
|    learning_rate   | 0.001    |
|    n_updates       | 37599    |
---------------------------------
=== Iterazione IRL 83 ===
Loss reward (iter 83): 6.799663543701172
=== Iterazione IRL 84 ===
Loss reward (iter 84): 6.604873180389404
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.59    |
|    critic_loss     | 0.00304  |
|    learning_rate   | 0.001    |
|    n_updates       | 38099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.63    |
|    critic_loss     | 0.00285  |
|    learning_rate   | 0.001    |
|    n_updates       | 38499    |
---------------------------------
=== Iterazione IRL 85 ===
Loss reward (iter 85): 6.788330554962158
=== Iterazione IRL 86 ===
Loss reward (iter 86): 6.684756755828857
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.74    |
|    critic_loss     | 0.00381  |
|    learning_rate   | 0.001    |
|    n_updates       | 38999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.88    |
|    critic_loss     | 0.00283  |
|    learning_rate   | 0.001    |
|    n_updates       | 39399    |
---------------------------------
=== Iterazione IRL 87 ===
Loss reward (iter 87): 6.736889839172363
=== Iterazione IRL 88 ===
Loss reward (iter 88): 6.67963171005249
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.81    |
|    critic_loss     | 0.00468  |
|    learning_rate   | 0.001    |
|    n_updates       | 39899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.88    |
|    critic_loss     | 0.00268  |
|    learning_rate   | 0.001    |
|    n_updates       | 40299    |
---------------------------------
=== Iterazione IRL 89 ===
Loss reward (iter 89): 6.780127048492432
=== Iterazione IRL 90 ===
Loss reward (iter 90): 6.720122814178467
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.89    |
|    critic_loss     | 0.00414  |
|    learning_rate   | 0.001    |
|    n_updates       | 40799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.89    |
|    critic_loss     | 0.00292  |
|    learning_rate   | 0.001    |
|    n_updates       | 41199    |
---------------------------------
=== Iterazione IRL 91 ===
Loss reward (iter 91): 6.8032145500183105
=== Iterazione IRL 92 ===
Loss reward (iter 92): 6.811823844909668
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.95    |
|    critic_loss     | 0.0034   |
|    learning_rate   | 0.001    |
|    n_updates       | 41699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 230      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.13    |
|    critic_loss     | 0.00338  |
|    learning_rate   | 0.001    |
|    n_updates       | 42099    |
---------------------------------
=== Iterazione IRL 93 ===
Loss reward (iter 93): 6.711482524871826
=== Iterazione IRL 94 ===
Loss reward (iter 94): 6.676492214202881
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7       |
|    critic_loss     | 0.00357  |
|    learning_rate   | 0.001    |
|    n_updates       | 42599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.07    |
|    critic_loss     | 0.0028   |
|    learning_rate   | 0.001    |
|    n_updates       | 42999    |
---------------------------------
=== Iterazione IRL 95 ===
Loss reward (iter 95): 6.6654767990112305
=== Iterazione IRL 96 ===
Loss reward (iter 96): 6.649798393249512
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.96    |
|    critic_loss     | 0.00364  |
|    learning_rate   | 0.001    |
|    n_updates       | 43499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.11    |
|    critic_loss     | 0.00439  |
|    learning_rate   | 0.001    |
|    n_updates       | 43899    |
---------------------------------
=== Iterazione IRL 97 ===
Loss reward (iter 97): 6.779333591461182
=== Iterazione IRL 98 ===
Loss reward (iter 98): 6.693629264831543
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.22    |
|    critic_loss     | 0.00475  |
|    learning_rate   | 0.001    |
|    n_updates       | 44399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.25    |
|    critic_loss     | 0.00297  |
|    learning_rate   | 0.001    |
|    n_updates       | 44799    |
---------------------------------
=== Iterazione IRL 99 ===
Loss reward (iter 99): 6.744717597961426
=== Iterazione IRL 100 ===
Loss reward (iter 100): 6.793100357055664
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 267      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.19    |
|    critic_loss     | 0.00382  |
|    learning_rate   | 0.001    |
|    n_updates       | 45299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 234      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.23    |
|    critic_loss     | 0.00375  |
|    learning_rate   | 0.001    |
|    n_updates       | 45699    |
---------------------------------
=== Iterazione IRL 101 ===
Loss reward (iter 101): 6.650390625
=== Iterazione IRL 102 ===
Loss reward (iter 102): 6.645553112030029
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.35    |
|    critic_loss     | 0.0041   |
|    learning_rate   | 0.001    |
|    n_updates       | 46199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.32    |
|    critic_loss     | 0.00444  |
|    learning_rate   | 0.001    |
|    n_updates       | 46599    |
---------------------------------
=== Iterazione IRL 103 ===
Loss reward (iter 103): 6.813318729400635
=== Iterazione IRL 104 ===
Loss reward (iter 104): 6.764707565307617
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.31    |
|    critic_loss     | 0.00453  |
|    learning_rate   | 0.001    |
|    n_updates       | 47099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.49    |
|    critic_loss     | 0.00444  |
|    learning_rate   | 0.001    |
|    n_updates       | 47499    |
---------------------------------
=== Iterazione IRL 105 ===
Loss reward (iter 105): 6.664946556091309
=== Iterazione IRL 106 ===
Loss reward (iter 106): 6.725405693054199
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 267      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.42    |
|    critic_loss     | 0.00353  |
|    learning_rate   | 0.001    |
|    n_updates       | 47999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 234      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.43    |
|    critic_loss     | 0.00358  |
|    learning_rate   | 0.001    |
|    n_updates       | 48399    |
---------------------------------
=== Iterazione IRL 107 ===
Loss reward (iter 107): 6.689401626586914
=== Iterazione IRL 108 ===
Loss reward (iter 108): 6.649700164794922
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.44    |
|    critic_loss     | 0.0041   |
|    learning_rate   | 0.001    |
|    n_updates       | 48899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.49    |
|    critic_loss     | 0.00316  |
|    learning_rate   | 0.001    |
|    n_updates       | 49299    |
---------------------------------
=== Iterazione IRL 109 ===
Loss reward (iter 109): 6.634256362915039
=== Iterazione IRL 110 ===
Loss reward (iter 110): 6.688819408416748
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 267      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.62    |
|    critic_loss     | 0.0054   |
|    learning_rate   | 0.001    |
|    n_updates       | 49799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.45    |
|    critic_loss     | 0.00348  |
|    learning_rate   | 0.001    |
|    n_updates       | 50199    |
---------------------------------
=== Iterazione IRL 111 ===
Loss reward (iter 111): 6.604481220245361
=== Iterazione IRL 112 ===
Loss reward (iter 112): 6.700902462005615
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 267      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.58    |
|    critic_loss     | 0.00376  |
|    learning_rate   | 0.001    |
|    n_updates       | 50699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 234      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.64    |
|    critic_loss     | 0.00433  |
|    learning_rate   | 0.001    |
|    n_updates       | 51099    |
---------------------------------
=== Iterazione IRL 113 ===
Loss reward (iter 113): 6.756283283233643
=== Iterazione IRL 114 ===
Loss reward (iter 114): 6.658892631530762
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.73    |
|    critic_loss     | 0.00446  |
|    learning_rate   | 0.001    |
|    n_updates       | 51599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.59    |
|    critic_loss     | 0.0038   |
|    learning_rate   | 0.001    |
|    n_updates       | 51999    |
---------------------------------
=== Iterazione IRL 115 ===
Loss reward (iter 115): 6.6831955909729
=== Iterazione IRL 116 ===
Loss reward (iter 116): 6.715569496154785
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.58    |
|    critic_loss     | 0.00525  |
|    learning_rate   | 0.001    |
|    n_updates       | 52499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.74    |
|    critic_loss     | 0.00378  |
|    learning_rate   | 0.001    |
|    n_updates       | 52899    |
---------------------------------
=== Iterazione IRL 117 ===
Loss reward (iter 117): 6.677867412567139
=== Iterazione IRL 118 ===
Loss reward (iter 118): 6.756026268005371
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.66    |
|    critic_loss     | 0.00483  |
|    learning_rate   | 0.001    |
|    n_updates       | 53399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.78    |
|    critic_loss     | 0.00426  |
|    learning_rate   | 0.001    |
|    n_updates       | 53799    |
---------------------------------
=== Iterazione IRL 119 ===
Loss reward (iter 119): 6.5373148918151855
=== Iterazione IRL 120 ===
Loss reward (iter 120): 6.6239423751831055
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.75    |
|    critic_loss     | 0.00373  |
|    learning_rate   | 0.001    |
|    n_updates       | 54299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.71    |
|    critic_loss     | 0.0034   |
|    learning_rate   | 0.001    |
|    n_updates       | 54699    |
---------------------------------
=== Iterazione IRL 121 ===
Loss reward (iter 121): 6.672916412353516
=== Iterazione IRL 122 ===
Loss reward (iter 122): 6.642518997192383
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.81    |
|    critic_loss     | 0.00368  |
|    learning_rate   | 0.001    |
|    n_updates       | 55199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.95    |
|    critic_loss     | 0.00468  |
|    learning_rate   | 0.001    |
|    n_updates       | 55599    |
---------------------------------
=== Iterazione IRL 123 ===
Loss reward (iter 123): 6.549978733062744
=== Iterazione IRL 124 ===
Loss reward (iter 124): 6.732961177825928
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.84    |
|    critic_loss     | 0.00365  |
|    learning_rate   | 0.001    |
|    n_updates       | 56099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.76    |
|    critic_loss     | 0.00292  |
|    learning_rate   | 0.001    |
|    n_updates       | 56499    |
---------------------------------
=== Iterazione IRL 125 ===
Loss reward (iter 125): 6.652641773223877
=== Iterazione IRL 126 ===
Loss reward (iter 126): 6.568649768829346
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.88    |
|    critic_loss     | 0.00359  |
|    learning_rate   | 0.001    |
|    n_updates       | 56999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.92    |
|    critic_loss     | 0.00372  |
|    learning_rate   | 0.001    |
|    n_updates       | 57399    |
---------------------------------
=== Iterazione IRL 127 ===
Loss reward (iter 127): 6.597358226776123
=== Iterazione IRL 128 ===
Loss reward (iter 128): 6.6639275550842285
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.86    |
|    critic_loss     | 0.00362  |
|    learning_rate   | 0.001    |
|    n_updates       | 57899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.99    |
|    critic_loss     | 0.00581  |
|    learning_rate   | 0.001    |
|    n_updates       | 58299    |
---------------------------------
=== Iterazione IRL 129 ===
Loss reward (iter 129): 6.605343818664551
=== Iterazione IRL 130 ===
Loss reward (iter 130): 6.612024784088135
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8       |
|    critic_loss     | 0.00462  |
|    learning_rate   | 0.001    |
|    n_updates       | 58799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.94    |
|    critic_loss     | 0.00348  |
|    learning_rate   | 0.001    |
|    n_updates       | 59199    |
---------------------------------
=== Iterazione IRL 131 ===
Loss reward (iter 131): 6.631964206695557
=== Iterazione IRL 132 ===
Loss reward (iter 132): 6.633214950561523
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.79    |
|    critic_loss     | 0.00768  |
|    learning_rate   | 0.001    |
|    n_updates       | 59699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.03    |
|    critic_loss     | 0.00346  |
|    learning_rate   | 0.001    |
|    n_updates       | 60099    |
---------------------------------
=== Iterazione IRL 133 ===
Loss reward (iter 133): 6.612290382385254
=== Iterazione IRL 134 ===
Loss reward (iter 134): 6.62429141998291
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.92    |
|    critic_loss     | 0.00382  |
|    learning_rate   | 0.001    |
|    n_updates       | 60599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.03    |
|    critic_loss     | 0.00459  |
|    learning_rate   | 0.001    |
|    n_updates       | 60999    |
---------------------------------
=== Iterazione IRL 135 ===
Loss reward (iter 135): 6.601029872894287
=== Iterazione IRL 136 ===
Loss reward (iter 136): 6.669474124908447
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.08    |
|    critic_loss     | 0.00458  |
|    learning_rate   | 0.001    |
|    n_updates       | 61499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.09    |
|    critic_loss     | 0.00386  |
|    learning_rate   | 0.001    |
|    n_updates       | 61899    |
---------------------------------
=== Iterazione IRL 137 ===
Loss reward (iter 137): 6.585384845733643
=== Iterazione IRL 138 ===
Loss reward (iter 138): 6.654125690460205
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.13    |
|    critic_loss     | 0.00442  |
|    learning_rate   | 0.001    |
|    n_updates       | 62399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.06    |
|    critic_loss     | 0.00381  |
|    learning_rate   | 0.001    |
|    n_updates       | 62799    |
---------------------------------
=== Iterazione IRL 139 ===
Loss reward (iter 139): 6.663109302520752
=== Iterazione IRL 140 ===
Loss reward (iter 140): 6.692243576049805
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.13    |
|    critic_loss     | 0.00391  |
|    learning_rate   | 0.001    |
|    n_updates       | 63299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.04    |
|    critic_loss     | 0.00449  |
|    learning_rate   | 0.001    |
|    n_updates       | 63699    |
---------------------------------
=== Iterazione IRL 141 ===
Loss reward (iter 141): 6.630087852478027
=== Iterazione IRL 142 ===
Loss reward (iter 142): 6.672776222229004
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.07    |
|    critic_loss     | 0.00427  |
|    learning_rate   | 0.001    |
|    n_updates       | 64199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.1     |
|    critic_loss     | 0.00459  |
|    learning_rate   | 0.001    |
|    n_updates       | 64599    |
---------------------------------
=== Iterazione IRL 143 ===
Loss reward (iter 143): 6.624301910400391
=== Iterazione IRL 144 ===
Loss reward (iter 144): 6.642213344573975
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.26    |
|    critic_loss     | 0.00415  |
|    learning_rate   | 0.001    |
|    n_updates       | 65099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.25    |
|    critic_loss     | 0.00372  |
|    learning_rate   | 0.001    |
|    n_updates       | 65499    |
---------------------------------
=== Iterazione IRL 145 ===
Loss reward (iter 145): 6.64691686630249
=== Iterazione IRL 146 ===
Loss reward (iter 146): 6.575169563293457
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.07    |
|    critic_loss     | 0.00444  |
|    learning_rate   | 0.001    |
|    n_updates       | 65999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.05    |
|    critic_loss     | 0.00414  |
|    learning_rate   | 0.001    |
|    n_updates       | 66399    |
---------------------------------
=== Iterazione IRL 147 ===
Loss reward (iter 147): 6.560751914978027
=== Iterazione IRL 148 ===
Loss reward (iter 148): 6.560862064361572
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.1     |
|    critic_loss     | 0.0037   |
|    learning_rate   | 0.001    |
|    n_updates       | 66899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.23    |
|    critic_loss     | 0.00434  |
|    learning_rate   | 0.001    |
|    n_updates       | 67299    |
---------------------------------
=== Iterazione IRL 149 ===
Loss reward (iter 149): 6.654170513153076
=== Iterazione IRL 150 ===
Loss reward (iter 150): 6.598522663116455
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.23    |
|    critic_loss     | 0.00491  |
|    learning_rate   | 0.001    |
|    n_updates       | 67799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.37    |
|    critic_loss     | 0.00511  |
|    learning_rate   | 0.001    |
|    n_updates       | 68199    |
---------------------------------
=== Iterazione IRL 151 ===
Loss reward (iter 151): 6.561445713043213
=== Iterazione IRL 152 ===
Loss reward (iter 152): 6.640370845794678
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.35    |
|    critic_loss     | 0.0044   |
|    learning_rate   | 0.001    |
|    n_updates       | 68699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.35    |
|    critic_loss     | 0.0042   |
|    learning_rate   | 0.001    |
|    n_updates       | 69099    |
---------------------------------
=== Iterazione IRL 153 ===
Loss reward (iter 153): 6.604403972625732
=== Iterazione IRL 154 ===
Loss reward (iter 154): 6.560225963592529
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.24    |
|    critic_loss     | 0.00405  |
|    learning_rate   | 0.001    |
|    n_updates       | 69599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.29    |
|    critic_loss     | 0.00418  |
|    learning_rate   | 0.001    |
|    n_updates       | 69999    |
---------------------------------
=== Iterazione IRL 155 ===
Loss reward (iter 155): 6.711299896240234
=== Iterazione IRL 156 ===
Loss reward (iter 156): 6.6604533195495605
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.33    |
|    critic_loss     | 0.00595  |
|    learning_rate   | 0.001    |
|    n_updates       | 70499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.4     |
|    critic_loss     | 0.00458  |
|    learning_rate   | 0.001    |
|    n_updates       | 70899    |
---------------------------------
=== Iterazione IRL 157 ===
Loss reward (iter 157): 6.6407389640808105
=== Iterazione IRL 158 ===
Loss reward (iter 158): 6.587136268615723
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.37    |
|    critic_loss     | 0.00547  |
|    learning_rate   | 0.001    |
|    n_updates       | 71399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.18    |
|    critic_loss     | 0.00556  |
|    learning_rate   | 0.001    |
|    n_updates       | 71799    |
---------------------------------
=== Iterazione IRL 159 ===
Loss reward (iter 159): 6.570069789886475
=== Iterazione IRL 160 ===
Loss reward (iter 160): 6.618669509887695
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 263      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.33    |
|    critic_loss     | 0.00408  |
|    learning_rate   | 0.001    |
|    n_updates       | 72299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.28    |
|    critic_loss     | 0.00564  |
|    learning_rate   | 0.001    |
|    n_updates       | 72699    |
---------------------------------
=== Iterazione IRL 161 ===
Loss reward (iter 161): 6.714260101318359
=== Iterazione IRL 162 ===
Loss reward (iter 162): 6.661895275115967
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.27    |
|    critic_loss     | 0.00516  |
|    learning_rate   | 0.001    |
|    n_updates       | 73199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.13    |
|    critic_loss     | 0.00538  |
|    learning_rate   | 0.001    |
|    n_updates       | 73599    |
---------------------------------
=== Iterazione IRL 163 ===
Loss reward (iter 163): 6.585057258605957
=== Iterazione IRL 164 ===
Loss reward (iter 164): 6.606006145477295
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.42    |
|    critic_loss     | 0.006    |
|    learning_rate   | 0.001    |
|    n_updates       | 74099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.36    |
|    critic_loss     | 0.00436  |
|    learning_rate   | 0.001    |
|    n_updates       | 74499    |
---------------------------------
=== Iterazione IRL 165 ===
Loss reward (iter 165): 6.5580596923828125
=== Iterazione IRL 166 ===
Loss reward (iter 166): 6.69650411605835
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.33    |
|    critic_loss     | 0.00581  |
|    learning_rate   | 0.001    |
|    n_updates       | 74999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.48    |
|    critic_loss     | 0.00537  |
|    learning_rate   | 0.001    |
|    n_updates       | 75399    |
---------------------------------
=== Iterazione IRL 167 ===
Loss reward (iter 167): 6.721319198608398
=== Iterazione IRL 168 ===
Loss reward (iter 168): 6.670949459075928
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 267      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.36    |
|    critic_loss     | 0.00614  |
|    learning_rate   | 0.001    |
|    n_updates       | 75899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 234      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.28    |
|    critic_loss     | 0.00514  |
|    learning_rate   | 0.001    |
|    n_updates       | 76299    |
---------------------------------
=== Iterazione IRL 169 ===
Loss reward (iter 169): 6.880208492279053
=== Iterazione IRL 170 ===
Loss reward (iter 170): 6.63012170791626
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.34    |
|    critic_loss     | 0.00424  |
|    learning_rate   | 0.001    |
|    n_updates       | 76799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.32    |
|    critic_loss     | 0.00518  |
|    learning_rate   | 0.001    |
|    n_updates       | 77199    |
---------------------------------
=== Iterazione IRL 171 ===
Loss reward (iter 171): 6.642111778259277
=== Iterazione IRL 172 ===
Loss reward (iter 172): 6.584334850311279
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.51    |
|    critic_loss     | 0.0062   |
|    learning_rate   | 0.001    |
|    n_updates       | 77699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.37    |
|    critic_loss     | 0.0055   |
|    learning_rate   | 0.001    |
|    n_updates       | 78099    |
---------------------------------
=== Iterazione IRL 173 ===
Loss reward (iter 173): 6.614224433898926
=== Iterazione IRL 174 ===
Loss reward (iter 174): 6.6812005043029785
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 263      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.37    |
|    critic_loss     | 0.00439  |
|    learning_rate   | 0.001    |
|    n_updates       | 78599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 230      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.56    |
|    critic_loss     | 0.00537  |
|    learning_rate   | 0.001    |
|    n_updates       | 78999    |
---------------------------------
=== Iterazione IRL 175 ===
Loss reward (iter 175): 6.732240676879883
=== Iterazione IRL 176 ===
Loss reward (iter 176): 6.557297229766846
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 263      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.39    |
|    critic_loss     | 0.00642  |
|    learning_rate   | 0.001    |
|    n_updates       | 79499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 230      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.33    |
|    critic_loss     | 0.00553  |
|    learning_rate   | 0.001    |
|    n_updates       | 79899    |
---------------------------------
=== Iterazione IRL 177 ===
Loss reward (iter 177): 6.617624759674072
=== Iterazione IRL 178 ===
Loss reward (iter 178): 6.739651203155518
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 263      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.36    |
|    critic_loss     | 0.00499  |
|    learning_rate   | 0.001    |
|    n_updates       | 80399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 230      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.45    |
|    critic_loss     | 0.00458  |
|    learning_rate   | 0.001    |
|    n_updates       | 80799    |
---------------------------------
=== Iterazione IRL 179 ===
Loss reward (iter 179): 6.590405464172363
=== Iterazione IRL 180 ===
Loss reward (iter 180): 6.581725597381592
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 263      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.41    |
|    critic_loss     | 0.00504  |
|    learning_rate   | 0.001    |
|    n_updates       | 81299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 230      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.42    |
|    critic_loss     | 0.00615  |
|    learning_rate   | 0.001    |
|    n_updates       | 81699    |
---------------------------------
=== Iterazione IRL 181 ===
Loss reward (iter 181): 6.612090110778809
=== Iterazione IRL 182 ===
Loss reward (iter 182): 6.641953945159912
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 262      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.57    |
|    critic_loss     | 0.00604  |
|    learning_rate   | 0.001    |
|    n_updates       | 82199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 230      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.52    |
|    critic_loss     | 0.00468  |
|    learning_rate   | 0.001    |
|    n_updates       | 82599    |
---------------------------------
=== Iterazione IRL 183 ===
Loss reward (iter 183): 6.669220447540283
=== Iterazione IRL 184 ===
Loss reward (iter 184): 6.65562105178833
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.5     |
|    critic_loss     | 0.00543  |
|    learning_rate   | 0.001    |
|    n_updates       | 83099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.53    |
|    critic_loss     | 0.0067   |
|    learning_rate   | 0.001    |
|    n_updates       | 83499    |
---------------------------------
=== Iterazione IRL 185 ===
Loss reward (iter 185): 6.6713972091674805
=== Iterazione IRL 186 ===
Loss reward (iter 186): 6.579626560211182
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.49    |
|    critic_loss     | 0.00576  |
|    learning_rate   | 0.001    |
|    n_updates       | 83999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.38    |
|    critic_loss     | 0.0051   |
|    learning_rate   | 0.001    |
|    n_updates       | 84399    |
---------------------------------
=== Iterazione IRL 187 ===
Loss reward (iter 187): 6.645954132080078
=== Iterazione IRL 188 ===
Loss reward (iter 188): 6.5982184410095215
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.44    |
|    critic_loss     | 0.00574  |
|    learning_rate   | 0.001    |
|    n_updates       | 84899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.5     |
|    critic_loss     | 0.00619  |
|    learning_rate   | 0.001    |
|    n_updates       | 85299    |
---------------------------------
=== Iterazione IRL 189 ===
Loss reward (iter 189): 6.590388298034668
=== Iterazione IRL 190 ===
Loss reward (iter 190): 6.640914440155029
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.5     |
|    critic_loss     | 0.0063   |
|    learning_rate   | 0.001    |
|    n_updates       | 85799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.54    |
|    critic_loss     | 0.00484  |
|    learning_rate   | 0.001    |
|    n_updates       | 86199    |
---------------------------------
=== Iterazione IRL 191 ===
Loss reward (iter 191): 6.589016437530518
=== Iterazione IRL 192 ===
Loss reward (iter 192): 6.612483978271484
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.57    |
|    critic_loss     | 0.00507  |
|    learning_rate   | 0.001    |
|    n_updates       | 86699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.48    |
|    critic_loss     | 0.00454  |
|    learning_rate   | 0.001    |
|    n_updates       | 87099    |
---------------------------------
=== Iterazione IRL 193 ===
Loss reward (iter 193): 6.58917760848999
=== Iterazione IRL 194 ===
Loss reward (iter 194): 6.6666717529296875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.51    |
|    critic_loss     | 0.00592  |
|    learning_rate   | 0.001    |
|    n_updates       | 87599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.57    |
|    critic_loss     | 0.00609  |
|    learning_rate   | 0.001    |
|    n_updates       | 87999    |
---------------------------------
=== Iterazione IRL 195 ===
Loss reward (iter 195): 6.624330520629883
=== Iterazione IRL 196 ===
Loss reward (iter 196): 6.552779674530029
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.49    |
|    critic_loss     | 0.00597  |
|    learning_rate   | 0.001    |
|    n_updates       | 88499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.63    |
|    critic_loss     | 0.00644  |
|    learning_rate   | 0.001    |
|    n_updates       | 88899    |
---------------------------------
=== Iterazione IRL 197 ===
Loss reward (iter 197): 6.587976455688477
=== Iterazione IRL 198 ===
Loss reward (iter 198): 6.573156833648682
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.62    |
|    critic_loss     | 0.0059   |
|    learning_rate   | 0.001    |
|    n_updates       | 89399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.45    |
|    critic_loss     | 0.00562  |
|    learning_rate   | 0.001    |
|    n_updates       | 89799    |
---------------------------------
=== Iterazione IRL 199 ===
Loss reward (iter 199): 6.689402103424072
=== Iterazione IRL 200 ===
Loss reward (iter 200): 6.543007850646973
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.56    |
|    critic_loss     | 0.00717  |
|    learning_rate   | 0.001    |
|    n_updates       | 90299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.67    |
|    critic_loss     | 0.00578  |
|    learning_rate   | 0.001    |
|    n_updates       | 90699    |
---------------------------------
=== Iterazione IRL 201 ===
Loss reward (iter 201): 6.586193561553955
=== Iterazione IRL 202 ===
Loss reward (iter 202): 6.6213555335998535
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 263      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.48    |
|    critic_loss     | 0.0048   |
|    learning_rate   | 0.001    |
|    n_updates       | 91199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.6     |
|    critic_loss     | 0.00541  |
|    learning_rate   | 0.001    |
|    n_updates       | 91599    |
---------------------------------
=== Iterazione IRL 203 ===
Loss reward (iter 203): 6.677408695220947
=== Iterazione IRL 204 ===
Loss reward (iter 204): 6.570041179656982
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 263      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.42    |
|    critic_loss     | 0.00657  |
|    learning_rate   | 0.001    |
|    n_updates       | 92099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.62    |
|    critic_loss     | 0.00482  |
|    learning_rate   | 0.001    |
|    n_updates       | 92499    |
---------------------------------
=== Iterazione IRL 205 ===
Loss reward (iter 205): 6.648526191711426
=== Iterazione IRL 206 ===
Loss reward (iter 206): 6.55750846862793
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 263      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.54    |
|    critic_loss     | 0.00614  |
|    learning_rate   | 0.001    |
|    n_updates       | 92999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.6     |
|    critic_loss     | 0.0062   |
|    learning_rate   | 0.001    |
|    n_updates       | 93399    |
---------------------------------
=== Iterazione IRL 207 ===
Loss reward (iter 207): 6.549792289733887
=== Iterazione IRL 208 ===
Loss reward (iter 208): 6.752331256866455
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.66    |
|    critic_loss     | 0.00584  |
|    learning_rate   | 0.001    |
|    n_updates       | 93899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.63    |
|    critic_loss     | 0.00638  |
|    learning_rate   | 0.001    |
|    n_updates       | 94299    |
---------------------------------
=== Iterazione IRL 209 ===
Loss reward (iter 209): 6.528954029083252
=== Iterazione IRL 210 ===
Loss reward (iter 210): 6.63726806640625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.57    |
|    critic_loss     | 0.00722  |
|    learning_rate   | 0.001    |
|    n_updates       | 94799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.63    |
|    critic_loss     | 0.00645  |
|    learning_rate   | 0.001    |
|    n_updates       | 95199    |
---------------------------------
=== Iterazione IRL 211 ===
Loss reward (iter 211): 6.622478485107422
=== Iterazione IRL 212 ===
Loss reward (iter 212): 6.599563121795654
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.55    |
|    critic_loss     | 0.0102   |
|    learning_rate   | 0.001    |
|    n_updates       | 95699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.52    |
|    critic_loss     | 0.0065   |
|    learning_rate   | 0.001    |
|    n_updates       | 96099    |
---------------------------------
=== Iterazione IRL 213 ===
Loss reward (iter 213): 6.604040145874023
=== Iterazione IRL 214 ===
Loss reward (iter 214): 6.540285110473633
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.66    |
|    critic_loss     | 0.00603  |
|    learning_rate   | 0.001    |
|    n_updates       | 96599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.59    |
|    critic_loss     | 0.00433  |
|    learning_rate   | 0.001    |
|    n_updates       | 96999    |
---------------------------------
=== Iterazione IRL 215 ===
Loss reward (iter 215): 6.548576831817627
=== Iterazione IRL 216 ===
Loss reward (iter 216): 6.650439262390137
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.56    |
|    critic_loss     | 0.00592  |
|    learning_rate   | 0.001    |
|    n_updates       | 97499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.66    |
|    critic_loss     | 0.00574  |
|    learning_rate   | 0.001    |
|    n_updates       | 97899    |
---------------------------------
=== Iterazione IRL 217 ===
Loss reward (iter 217): 6.637047290802002
=== Iterazione IRL 218 ===
Loss reward (iter 218): 6.646677017211914
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.59    |
|    critic_loss     | 0.00561  |
|    learning_rate   | 0.001    |
|    n_updates       | 98399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.52    |
|    critic_loss     | 0.00617  |
|    learning_rate   | 0.001    |
|    n_updates       | 98799    |
---------------------------------
=== Iterazione IRL 219 ===
Loss reward (iter 219): 6.5154948234558105
=== Iterazione IRL 220 ===
Loss reward (iter 220): 6.595755577087402
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.61    |
|    critic_loss     | 0.00656  |
|    learning_rate   | 0.001    |
|    n_updates       | 99299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.66    |
|    critic_loss     | 0.00579  |
|    learning_rate   | 0.001    |
|    n_updates       | 99699    |
---------------------------------
=== Iterazione IRL 221 ===
Loss reward (iter 221): 6.593829154968262
=== Iterazione IRL 222 ===
Loss reward (iter 222): 6.656705379486084
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.53    |
|    critic_loss     | 0.00685  |
|    learning_rate   | 0.001    |
|    n_updates       | 100199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.45    |
|    critic_loss     | 0.00664  |
|    learning_rate   | 0.001    |
|    n_updates       | 100599   |
---------------------------------
=== Iterazione IRL 223 ===
Loss reward (iter 223): 6.545298099517822
=== Iterazione IRL 224 ===
Loss reward (iter 224): 6.575043678283691
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.63    |
|    critic_loss     | 0.00559  |
|    learning_rate   | 0.001    |
|    n_updates       | 101099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.69    |
|    critic_loss     | 0.00534  |
|    learning_rate   | 0.001    |
|    n_updates       | 101499   |
---------------------------------
=== Iterazione IRL 225 ===
Loss reward (iter 225): 6.550470352172852
=== Iterazione IRL 226 ===
Loss reward (iter 226): 6.687947750091553
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 267      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.49    |
|    critic_loss     | 0.00789  |
|    learning_rate   | 0.001    |
|    n_updates       | 101999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 234      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.5     |
|    critic_loss     | 0.0065   |
|    learning_rate   | 0.001    |
|    n_updates       | 102399   |
---------------------------------
=== Iterazione IRL 227 ===
Loss reward (iter 227): 6.673771858215332
=== Iterazione IRL 228 ===
Loss reward (iter 228): 6.65116024017334
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.46    |
|    critic_loss     | 0.00567  |
|    learning_rate   | 0.001    |
|    n_updates       | 102899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.46    |
|    critic_loss     | 0.00712  |
|    learning_rate   | 0.001    |
|    n_updates       | 103299   |
---------------------------------
=== Iterazione IRL 229 ===
Loss reward (iter 229): 6.598568439483643
=== Iterazione IRL 230 ===
Loss reward (iter 230): 6.542891502380371
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.63    |
|    critic_loss     | 0.00716  |
|    learning_rate   | 0.001    |
|    n_updates       | 103799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.54    |
|    critic_loss     | 0.00556  |
|    learning_rate   | 0.001    |
|    n_updates       | 104199   |
---------------------------------
=== Iterazione IRL 231 ===
Loss reward (iter 231): 6.6645731925964355
=== Iterazione IRL 232 ===
Loss reward (iter 232): 6.585148811340332
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.58    |
|    critic_loss     | 0.00518  |
|    learning_rate   | 0.001    |
|    n_updates       | 104699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.62    |
|    critic_loss     | 0.00794  |
|    learning_rate   | 0.001    |
|    n_updates       | 105099   |
---------------------------------
=== Iterazione IRL 233 ===
Loss reward (iter 233): 6.578928470611572
=== Iterazione IRL 234 ===
Loss reward (iter 234): 6.589264392852783
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.56    |
|    critic_loss     | 0.00857  |
|    learning_rate   | 0.001    |
|    n_updates       | 105599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.68    |
|    critic_loss     | 0.00657  |
|    learning_rate   | 0.001    |
|    n_updates       | 105999   |
---------------------------------
=== Iterazione IRL 235 ===
Loss reward (iter 235): 6.77774715423584
=== Iterazione IRL 236 ===
Loss reward (iter 236): 6.582052707672119
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.58    |
|    critic_loss     | 0.00747  |
|    learning_rate   | 0.001    |
|    n_updates       | 106499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.62    |
|    critic_loss     | 0.00727  |
|    learning_rate   | 0.001    |
|    n_updates       | 106899   |
---------------------------------
=== Iterazione IRL 237 ===
Loss reward (iter 237): 6.604628086090088
=== Iterazione IRL 238 ===
Loss reward (iter 238): 6.615787506103516
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 267      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.61    |
|    critic_loss     | 0.00615  |
|    learning_rate   | 0.001    |
|    n_updates       | 107399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 234      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.49    |
|    critic_loss     | 0.00703  |
|    learning_rate   | 0.001    |
|    n_updates       | 107799   |
---------------------------------
=== Iterazione IRL 239 ===
Loss reward (iter 239): 6.649383544921875
=== Iterazione IRL 240 ===
Loss reward (iter 240): 6.573822021484375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 267      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.48    |
|    critic_loss     | 0.00595  |
|    learning_rate   | 0.001    |
|    n_updates       | 108299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 234      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.53    |
|    critic_loss     | 0.00719  |
|    learning_rate   | 0.001    |
|    n_updates       | 108699   |
---------------------------------
=== Iterazione IRL 241 ===
Loss reward (iter 241): 6.61725378036499
=== Iterazione IRL 242 ===
Loss reward (iter 242): 6.534019947052002
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 267      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.59    |
|    critic_loss     | 0.00767  |
|    learning_rate   | 0.001    |
|    n_updates       | 109199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 234      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.5     |
|    critic_loss     | 0.00656  |
|    learning_rate   | 0.001    |
|    n_updates       | 109599   |
---------------------------------
=== Iterazione IRL 243 ===
Loss reward (iter 243): 6.604931354522705
=== Iterazione IRL 244 ===
Loss reward (iter 244): 6.61023473739624
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 267      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.57    |
|    critic_loss     | 0.00499  |
|    learning_rate   | 0.001    |
|    n_updates       | 110099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 234      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.38    |
|    critic_loss     | 0.00647  |
|    learning_rate   | 0.001    |
|    n_updates       | 110499   |
---------------------------------
=== Iterazione IRL 245 ===
Loss reward (iter 245): 6.589020729064941
=== Iterazione IRL 246 ===
Loss reward (iter 246): 6.627262592315674
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 267      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.46    |
|    critic_loss     | 0.00567  |
|    learning_rate   | 0.001    |
|    n_updates       | 110999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 234      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.35    |
|    critic_loss     | 0.00726  |
|    learning_rate   | 0.001    |
|    n_updates       | 111399   |
---------------------------------
=== Iterazione IRL 247 ===
Loss reward (iter 247): 6.5808916091918945
=== Iterazione IRL 248 ===
Loss reward (iter 248): 6.598377227783203
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.48    |
|    critic_loss     | 0.00626  |
|    learning_rate   | 0.001    |
|    n_updates       | 111899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 234      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.5     |
|    critic_loss     | 0.00809  |
|    learning_rate   | 0.001    |
|    n_updates       | 112299   |
---------------------------------
=== Iterazione IRL 249 ===
Loss reward (iter 249): 6.5776543617248535
=== Iterazione IRL 250 ===
Loss reward (iter 250): 6.548652648925781
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 267      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.58    |
|    critic_loss     | 0.00629  |
|    learning_rate   | 0.001    |
|    n_updates       | 112799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 234      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.51    |
|    critic_loss     | 0.00663  |
|    learning_rate   | 0.001    |
|    n_updates       | 113199   |
---------------------------------
=== Iterazione IRL 251 ===
Loss reward (iter 251): 6.582003593444824
=== Iterazione IRL 252 ===
Loss reward (iter 252): 6.572112560272217
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 267      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.42    |
|    critic_loss     | 0.00815  |
|    learning_rate   | 0.001    |
|    n_updates       | 113699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 234      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.52    |
|    critic_loss     | 0.00741  |
|    learning_rate   | 0.001    |
|    n_updates       | 114099   |
---------------------------------
=== Iterazione IRL 253 ===
Loss reward (iter 253): 6.581985950469971
=== Iterazione IRL 254 ===
Loss reward (iter 254): 6.537787437438965
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 267      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.45    |
|    critic_loss     | 0.00515  |
|    learning_rate   | 0.001    |
|    n_updates       | 114599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 234      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.39    |
|    critic_loss     | 0.00703  |
|    learning_rate   | 0.001    |
|    n_updates       | 114999   |
---------------------------------
=== Iterazione IRL 255 ===
Loss reward (iter 255): 6.568848609924316
=== Iterazione IRL 256 ===
Loss reward (iter 256): 6.609182357788086
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.63    |
|    critic_loss     | 0.00829  |
|    learning_rate   | 0.001    |
|    n_updates       | 115499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.45    |
|    critic_loss     | 0.00682  |
|    learning_rate   | 0.001    |
|    n_updates       | 115899   |
---------------------------------
=== Iterazione IRL 257 ===
Loss reward (iter 257): 6.637815952301025
=== Iterazione IRL 258 ===
Loss reward (iter 258): 6.544266223907471
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 267      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.62    |
|    critic_loss     | 0.00709  |
|    learning_rate   | 0.001    |
|    n_updates       | 116399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.33    |
|    critic_loss     | 0.00755  |
|    learning_rate   | 0.001    |
|    n_updates       | 116799   |
---------------------------------
=== Iterazione IRL 259 ===
Loss reward (iter 259): 6.695903301239014
=== Iterazione IRL 260 ===
Loss reward (iter 260): 6.563925266265869
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.45    |
|    critic_loss     | 0.00665  |
|    learning_rate   | 0.001    |
|    n_updates       | 117299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.47    |
|    critic_loss     | 0.00699  |
|    learning_rate   | 0.001    |
|    n_updates       | 117699   |
---------------------------------
=== Iterazione IRL 261 ===
Loss reward (iter 261): 6.599254608154297
=== Iterazione IRL 262 ===
Loss reward (iter 262): 6.6125898361206055
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 267      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.4     |
|    critic_loss     | 0.00727  |
|    learning_rate   | 0.001    |
|    n_updates       | 118199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 234      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.43    |
|    critic_loss     | 0.00759  |
|    learning_rate   | 0.001    |
|    n_updates       | 118599   |
---------------------------------
=== Iterazione IRL 263 ===
Loss reward (iter 263): 6.567063808441162
=== Iterazione IRL 264 ===
Loss reward (iter 264): 6.605337142944336
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 267      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.52    |
|    critic_loss     | 0.00674  |
|    learning_rate   | 0.001    |
|    n_updates       | 119099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 234      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.61    |
|    critic_loss     | 0.00745  |
|    learning_rate   | 0.001    |
|    n_updates       | 119499   |
---------------------------------
=== Iterazione IRL 265 ===
Loss reward (iter 265): 6.625833034515381
=== Iterazione IRL 266 ===
Loss reward (iter 266): 6.557857990264893
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 267      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.52    |
|    critic_loss     | 0.00585  |
|    learning_rate   | 0.001    |
|    n_updates       | 119999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.51    |
|    critic_loss     | 0.00687  |
|    learning_rate   | 0.001    |
|    n_updates       | 120399   |
---------------------------------
=== Iterazione IRL 267 ===
Loss reward (iter 267): 6.731648921966553
=== Iterazione IRL 268 ===
Loss reward (iter 268): 6.608397483825684
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 267      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.36    |
|    critic_loss     | 0.00589  |
|    learning_rate   | 0.001    |
|    n_updates       | 120899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.45    |
|    critic_loss     | 0.00645  |
|    learning_rate   | 0.001    |
|    n_updates       | 121299   |
---------------------------------
=== Iterazione IRL 269 ===
Loss reward (iter 269): 6.5726776123046875
=== Iterazione IRL 270 ===
Loss reward (iter 270): 6.530141830444336
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.41    |
|    critic_loss     | 0.00682  |
|    learning_rate   | 0.001    |
|    n_updates       | 121799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 234      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.44    |
|    critic_loss     | 0.00719  |
|    learning_rate   | 0.001    |
|    n_updates       | 122199   |
---------------------------------
=== Iterazione IRL 271 ===
Loss reward (iter 271): 6.54988431930542
=== Iterazione IRL 272 ===
Loss reward (iter 272): 6.557737350463867
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.37    |
|    critic_loss     | 0.0115   |
|    learning_rate   | 0.001    |
|    n_updates       | 122699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 234      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.52    |
|    critic_loss     | 0.00669  |
|    learning_rate   | 0.001    |
|    n_updates       | 123099   |
---------------------------------
=== Iterazione IRL 273 ===
Loss reward (iter 273): 6.564109802246094
=== Iterazione IRL 274 ===
Loss reward (iter 274): 6.611057758331299
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 267      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.36    |
|    critic_loss     | 0.00759  |
|    learning_rate   | 0.001    |
|    n_updates       | 123599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 234      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.42    |
|    critic_loss     | 0.0072   |
|    learning_rate   | 0.001    |
|    n_updates       | 123999   |
---------------------------------
=== Iterazione IRL 275 ===
Loss reward (iter 275): 6.592609405517578
=== Iterazione IRL 276 ===
Loss reward (iter 276): 6.599869728088379
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 267      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.26    |
|    critic_loss     | 0.00719  |
|    learning_rate   | 0.001    |
|    n_updates       | 124499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.47    |
|    critic_loss     | 0.008    |
|    learning_rate   | 0.001    |
|    n_updates       | 124899   |
---------------------------------
=== Iterazione IRL 277 ===
Loss reward (iter 277): 6.554594039916992
=== Iterazione IRL 278 ===
Loss reward (iter 278): 6.566775321960449
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.28    |
|    critic_loss     | 0.00716  |
|    learning_rate   | 0.001    |
|    n_updates       | 125399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.25    |
|    critic_loss     | 0.00644  |
|    learning_rate   | 0.001    |
|    n_updates       | 125799   |
---------------------------------
=== Iterazione IRL 279 ===
Loss reward (iter 279): 6.637238502502441
=== Iterazione IRL 280 ===
Loss reward (iter 280): 6.6542744636535645
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.47    |
|    critic_loss     | 0.00712  |
|    learning_rate   | 0.001    |
|    n_updates       | 126299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.5     |
|    critic_loss     | 0.00739  |
|    learning_rate   | 0.001    |
|    n_updates       | 126699   |
---------------------------------
=== Iterazione IRL 281 ===
Loss reward (iter 281): 6.650389671325684
=== Iterazione IRL 282 ===
Loss reward (iter 282): 6.67527437210083
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.46    |
|    critic_loss     | 0.00635  |
|    learning_rate   | 0.001    |
|    n_updates       | 127199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.47    |
|    critic_loss     | 0.00771  |
|    learning_rate   | 0.001    |
|    n_updates       | 127599   |
---------------------------------
=== Iterazione IRL 283 ===
Loss reward (iter 283): 6.585252285003662
=== Iterazione IRL 284 ===
Loss reward (iter 284): 6.611724853515625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.43    |
|    critic_loss     | 0.0089   |
|    learning_rate   | 0.001    |
|    n_updates       | 128099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.44    |
|    critic_loss     | 0.00575  |
|    learning_rate   | 0.001    |
|    n_updates       | 128499   |
---------------------------------
=== Iterazione IRL 285 ===
Loss reward (iter 285): 6.6166276931762695
=== Iterazione IRL 286 ===
Loss reward (iter 286): 6.612219333648682
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.32    |
|    critic_loss     | 0.00753  |
|    learning_rate   | 0.001    |
|    n_updates       | 128999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.35    |
|    critic_loss     | 0.00561  |
|    learning_rate   | 0.001    |
|    n_updates       | 129399   |
---------------------------------
=== Iterazione IRL 287 ===
Loss reward (iter 287): 6.672269821166992
=== Iterazione IRL 288 ===
Loss reward (iter 288): 6.665129661560059
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.35    |
|    critic_loss     | 0.00634  |
|    learning_rate   | 0.001    |
|    n_updates       | 129899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.37    |
|    critic_loss     | 0.00721  |
|    learning_rate   | 0.001    |
|    n_updates       | 130299   |
---------------------------------
=== Iterazione IRL 289 ===
Loss reward (iter 289): 6.683639049530029
=== Iterazione IRL 290 ===
Loss reward (iter 290): 6.651717185974121
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.36    |
|    critic_loss     | 0.0068   |
|    learning_rate   | 0.001    |
|    n_updates       | 130799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.32    |
|    critic_loss     | 0.00722  |
|    learning_rate   | 0.001    |
|    n_updates       | 131199   |
---------------------------------
=== Iterazione IRL 291 ===
Loss reward (iter 291): 6.768462657928467
=== Iterazione IRL 292 ===
Loss reward (iter 292): 6.650735855102539
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.43    |
|    critic_loss     | 0.00623  |
|    learning_rate   | 0.001    |
|    n_updates       | 131699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.36    |
|    critic_loss     | 0.00769  |
|    learning_rate   | 0.001    |
|    n_updates       | 132099   |
---------------------------------
=== Iterazione IRL 293 ===
Loss reward (iter 293): 6.608356475830078
=== Iterazione IRL 294 ===
Loss reward (iter 294): 6.625510215759277
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.27    |
|    critic_loss     | 0.00747  |
|    learning_rate   | 0.001    |
|    n_updates       | 132599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.3     |
|    critic_loss     | 0.0073   |
|    learning_rate   | 0.001    |
|    n_updates       | 132999   |
---------------------------------
=== Iterazione IRL 295 ===
Loss reward (iter 295): 6.607409954071045
=== Iterazione IRL 296 ===
Loss reward (iter 296): 6.6188225746154785
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.32    |
|    critic_loss     | 0.00632  |
|    learning_rate   | 0.001    |
|    n_updates       | 133499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.33    |
|    critic_loss     | 0.00851  |
|    learning_rate   | 0.001    |
|    n_updates       | 133899   |
---------------------------------
=== Iterazione IRL 297 ===
Loss reward (iter 297): 6.689265727996826
=== Iterazione IRL 298 ===
Loss reward (iter 298): 6.583399295806885
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.28    |
|    critic_loss     | 0.00683  |
|    learning_rate   | 0.001    |
|    n_updates       | 134399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.15    |
|    critic_loss     | 0.00834  |
|    learning_rate   | 0.001    |
|    n_updates       | 134799   |
---------------------------------
=== Iterazione IRL 299 ===
Loss reward (iter 299): 6.653205394744873
=== Iterazione IRL 300 ===
Loss reward (iter 300): 6.654260158538818
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.2     |
|    critic_loss     | 0.00651  |
|    learning_rate   | 0.001    |
|    n_updates       | 135299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.28    |
|    critic_loss     | 0.00605  |
|    learning_rate   | 0.001    |
|    n_updates       | 135699   |
---------------------------------
=== Iterazione IRL 301 ===
Loss reward (iter 301): 6.681632995605469
=== Iterazione IRL 302 ===
Loss reward (iter 302): 6.57357120513916
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.36    |
|    critic_loss     | 0.00662  |
|    learning_rate   | 0.001    |
|    n_updates       | 136199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.23    |
|    critic_loss     | 0.00648  |
|    learning_rate   | 0.001    |
|    n_updates       | 136599   |
---------------------------------
=== Iterazione IRL 303 ===
Loss reward (iter 303): 6.687662124633789
=== Iterazione IRL 304 ===
Loss reward (iter 304): 6.733643054962158
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.28    |
|    critic_loss     | 0.00679  |
|    learning_rate   | 0.001    |
|    n_updates       | 137099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.19    |
|    critic_loss     | 0.0101   |
|    learning_rate   | 0.001    |
|    n_updates       | 137499   |
---------------------------------
=== Iterazione IRL 305 ===
Loss reward (iter 305): 6.5814409255981445
=== Iterazione IRL 306 ===
Loss reward (iter 306): 6.5887627601623535
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.27    |
|    critic_loss     | 0.00701  |
|    learning_rate   | 0.001    |
|    n_updates       | 137999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.31    |
|    critic_loss     | 0.00617  |
|    learning_rate   | 0.001    |
|    n_updates       | 138399   |
---------------------------------
=== Iterazione IRL 307 ===
Loss reward (iter 307): 6.559203624725342
=== Iterazione IRL 308 ===
Loss reward (iter 308): 6.561752796173096
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.2     |
|    critic_loss     | 0.00756  |
|    learning_rate   | 0.001    |
|    n_updates       | 138899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.16    |
|    critic_loss     | 0.00672  |
|    learning_rate   | 0.001    |
|    n_updates       | 139299   |
---------------------------------
=== Iterazione IRL 309 ===
Loss reward (iter 309): 6.713897228240967
=== Iterazione IRL 310 ===
Loss reward (iter 310): 6.591505527496338
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.12    |
|    critic_loss     | 0.00914  |
|    learning_rate   | 0.001    |
|    n_updates       | 139799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.27    |
|    critic_loss     | 0.00801  |
|    learning_rate   | 0.001    |
|    n_updates       | 140199   |
---------------------------------
=== Iterazione IRL 311 ===
Loss reward (iter 311): 6.7358903884887695
=== Iterazione IRL 312 ===
Loss reward (iter 312): 6.641373634338379
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.25    |
|    critic_loss     | 0.00779  |
|    learning_rate   | 0.001    |
|    n_updates       | 140699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.01    |
|    critic_loss     | 0.01     |
|    learning_rate   | 0.001    |
|    n_updates       | 141099   |
---------------------------------
=== Iterazione IRL 313 ===
Loss reward (iter 313): 6.6292901039123535
=== Iterazione IRL 314 ===
Loss reward (iter 314): 6.634321689605713
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.29    |
|    critic_loss     | 0.0077   |
|    learning_rate   | 0.001    |
|    n_updates       | 141599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.11    |
|    critic_loss     | 0.00662  |
|    learning_rate   | 0.001    |
|    n_updates       | 141999   |
---------------------------------
=== Iterazione IRL 315 ===
Loss reward (iter 315): 6.781323432922363
=== Iterazione IRL 316 ===
Loss reward (iter 316): 6.642634391784668
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.18    |
|    critic_loss     | 0.0075   |
|    learning_rate   | 0.001    |
|    n_updates       | 142499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.08    |
|    critic_loss     | 0.00911  |
|    learning_rate   | 0.001    |
|    n_updates       | 142899   |
---------------------------------
=== Iterazione IRL 317 ===
Loss reward (iter 317): 6.630516529083252
=== Iterazione IRL 318 ===
Loss reward (iter 318): 6.727834701538086
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.04    |
|    critic_loss     | 0.00723  |
|    learning_rate   | 0.001    |
|    n_updates       | 143399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.14    |
|    critic_loss     | 0.0102   |
|    learning_rate   | 0.001    |
|    n_updates       | 143799   |
---------------------------------
=== Iterazione IRL 319 ===
Loss reward (iter 319): 6.614438533782959
=== Iterazione IRL 320 ===
Loss reward (iter 320): 6.61315393447876
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.09    |
|    critic_loss     | 0.0075   |
|    learning_rate   | 0.001    |
|    n_updates       | 144299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.08    |
|    critic_loss     | 0.00685  |
|    learning_rate   | 0.001    |
|    n_updates       | 144699   |
---------------------------------
=== Iterazione IRL 321 ===
Loss reward (iter 321): 6.593986988067627
=== Iterazione IRL 322 ===
Loss reward (iter 322): 6.592977046966553
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.17    |
|    critic_loss     | 0.00659  |
|    learning_rate   | 0.001    |
|    n_updates       | 145199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.04    |
|    critic_loss     | 0.00715  |
|    learning_rate   | 0.001    |
|    n_updates       | 145599   |
---------------------------------
=== Iterazione IRL 323 ===
Loss reward (iter 323): 6.636955261230469
=== Iterazione IRL 324 ===
Loss reward (iter 324): 6.7090229988098145
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.15    |
|    critic_loss     | 0.00838  |
|    learning_rate   | 0.001    |
|    n_updates       | 146099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.06    |
|    critic_loss     | 0.00825  |
|    learning_rate   | 0.001    |
|    n_updates       | 146499   |
---------------------------------
=== Iterazione IRL 325 ===
Loss reward (iter 325): 6.595578670501709
=== Iterazione IRL 326 ===
Loss reward (iter 326): 6.563732147216797
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.07    |
|    critic_loss     | 0.00667  |
|    learning_rate   | 0.001    |
|    n_updates       | 146999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.15    |
|    critic_loss     | 0.00709  |
|    learning_rate   | 0.001    |
|    n_updates       | 147399   |
---------------------------------
=== Iterazione IRL 327 ===
Loss reward (iter 327): 6.614953994750977
=== Iterazione IRL 328 ===
Loss reward (iter 328): 6.586787223815918
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.99    |
|    critic_loss     | 0.0101   |
|    learning_rate   | 0.001    |
|    n_updates       | 147899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.97    |
|    critic_loss     | 0.00855  |
|    learning_rate   | 0.001    |
|    n_updates       | 148299   |
---------------------------------
=== Iterazione IRL 329 ===
Loss reward (iter 329): 6.533376216888428
=== Iterazione IRL 330 ===
Loss reward (iter 330): 6.62001371383667
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.09    |
|    critic_loss     | 0.00731  |
|    learning_rate   | 0.001    |
|    n_updates       | 148799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.06    |
|    critic_loss     | 0.00783  |
|    learning_rate   | 0.001    |
|    n_updates       | 149199   |
---------------------------------
=== Iterazione IRL 331 ===
Loss reward (iter 331): 6.598211288452148
=== Iterazione IRL 332 ===
Loss reward (iter 332): 6.727444171905518
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.17    |
|    critic_loss     | 0.00684  |
|    learning_rate   | 0.001    |
|    n_updates       | 149699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.09    |
|    critic_loss     | 0.00879  |
|    learning_rate   | 0.001    |
|    n_updates       | 150099   |
---------------------------------
=== Iterazione IRL 333 ===
Loss reward (iter 333): 6.700897216796875
=== Iterazione IRL 334 ===
Loss reward (iter 334): 6.579479694366455
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8       |
|    critic_loss     | 0.00887  |
|    learning_rate   | 0.001    |
|    n_updates       | 150599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.1     |
|    critic_loss     | 0.00665  |
|    learning_rate   | 0.001    |
|    n_updates       | 150999   |
---------------------------------
=== Iterazione IRL 335 ===
Loss reward (iter 335): 6.657303810119629
=== Iterazione IRL 336 ===
Loss reward (iter 336): 6.628868579864502
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.02    |
|    critic_loss     | 0.008    |
|    learning_rate   | 0.001    |
|    n_updates       | 151499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.08    |
|    critic_loss     | 0.0098   |
|    learning_rate   | 0.001    |
|    n_updates       | 151899   |
---------------------------------
=== Iterazione IRL 337 ===
Loss reward (iter 337): 6.565298557281494
=== Iterazione IRL 338 ===
Loss reward (iter 338): 6.686684608459473
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.01    |
|    critic_loss     | 0.00716  |
|    learning_rate   | 0.001    |
|    n_updates       | 152399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.04    |
|    critic_loss     | 0.00941  |
|    learning_rate   | 0.001    |
|    n_updates       | 152799   |
---------------------------------
=== Iterazione IRL 339 ===
Loss reward (iter 339): 6.627493858337402
=== Iterazione IRL 340 ===
Loss reward (iter 340): 6.618597030639648
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.13    |
|    critic_loss     | 0.00804  |
|    learning_rate   | 0.001    |
|    n_updates       | 153299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.22    |
|    critic_loss     | 0.00703  |
|    learning_rate   | 0.001    |
|    n_updates       | 153699   |
---------------------------------
=== Iterazione IRL 341 ===
Loss reward (iter 341): 6.594661235809326
=== Iterazione IRL 342 ===
Loss reward (iter 342): 6.663377285003662
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.14    |
|    critic_loss     | 0.00962  |
|    learning_rate   | 0.001    |
|    n_updates       | 154199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.07    |
|    critic_loss     | 0.00782  |
|    learning_rate   | 0.001    |
|    n_updates       | 154599   |
---------------------------------
=== Iterazione IRL 343 ===
Loss reward (iter 343): 6.570838451385498
=== Iterazione IRL 344 ===
Loss reward (iter 344): 6.600096702575684
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.94    |
|    critic_loss     | 0.00667  |
|    learning_rate   | 0.001    |
|    n_updates       | 155099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.03    |
|    critic_loss     | 0.00699  |
|    learning_rate   | 0.001    |
|    n_updates       | 155499   |
---------------------------------
=== Iterazione IRL 345 ===
Loss reward (iter 345): 6.726532459259033
=== Iterazione IRL 346 ===
Loss reward (iter 346): 6.640228271484375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8       |
|    critic_loss     | 0.00729  |
|    learning_rate   | 0.001    |
|    n_updates       | 155999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.07    |
|    critic_loss     | 0.00802  |
|    learning_rate   | 0.001    |
|    n_updates       | 156399   |
---------------------------------
=== Iterazione IRL 347 ===
Loss reward (iter 347): 6.671816825866699
=== Iterazione IRL 348 ===
Loss reward (iter 348): 6.5623579025268555
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.79    |
|    critic_loss     | 0.00879  |
|    learning_rate   | 0.001    |
|    n_updates       | 156899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.01    |
|    critic_loss     | 0.00731  |
|    learning_rate   | 0.001    |
|    n_updates       | 157299   |
---------------------------------
=== Iterazione IRL 349 ===
Loss reward (iter 349): 6.535061836242676
=== Iterazione IRL 350 ===
Loss reward (iter 350): 6.742982864379883
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.97    |
|    critic_loss     | 0.00827  |
|    learning_rate   | 0.001    |
|    n_updates       | 157799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.04    |
|    critic_loss     | 0.00782  |
|    learning_rate   | 0.001    |
|    n_updates       | 158199   |
---------------------------------
=== Iterazione IRL 351 ===
Loss reward (iter 351): 6.598021030426025
=== Iterazione IRL 352 ===
Loss reward (iter 352): 6.634383678436279
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.03    |
|    critic_loss     | 0.00721  |
|    learning_rate   | 0.001    |
|    n_updates       | 158699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.98    |
|    critic_loss     | 0.0123   |
|    learning_rate   | 0.001    |
|    n_updates       | 159099   |
---------------------------------
=== Iterazione IRL 353 ===
Loss reward (iter 353): 6.733888626098633
=== Iterazione IRL 354 ===
Loss reward (iter 354): 6.5323100090026855
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.82    |
|    critic_loss     | 0.0106   |
|    learning_rate   | 0.001    |
|    n_updates       | 159599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8       |
|    critic_loss     | 0.00671  |
|    learning_rate   | 0.001    |
|    n_updates       | 159999   |
---------------------------------
=== Iterazione IRL 355 ===
Loss reward (iter 355): 6.697332382202148
=== Iterazione IRL 356 ===
Loss reward (iter 356): 6.561257362365723
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.95    |
|    critic_loss     | 0.00892  |
|    learning_rate   | 0.001    |
|    n_updates       | 160499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.86    |
|    critic_loss     | 0.00877  |
|    learning_rate   | 0.001    |
|    n_updates       | 160899   |
---------------------------------
=== Iterazione IRL 357 ===
Loss reward (iter 357): 6.588872909545898
=== Iterazione IRL 358 ===
Loss reward (iter 358): 6.609514236450195
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.93    |
|    critic_loss     | 0.0078   |
|    learning_rate   | 0.001    |
|    n_updates       | 161399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.85    |
|    critic_loss     | 0.00647  |
|    learning_rate   | 0.001    |
|    n_updates       | 161799   |
---------------------------------
=== Iterazione IRL 359 ===
Loss reward (iter 359): 6.559962749481201
=== Iterazione IRL 360 ===
Loss reward (iter 360): 6.641758918762207
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.87    |
|    critic_loss     | 0.00938  |
|    learning_rate   | 0.001    |
|    n_updates       | 162299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.87    |
|    critic_loss     | 0.01     |
|    learning_rate   | 0.001    |
|    n_updates       | 162699   |
---------------------------------
=== Iterazione IRL 361 ===
Loss reward (iter 361): 6.630598545074463
=== Iterazione IRL 362 ===
Loss reward (iter 362): 6.577822685241699
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.95    |
|    critic_loss     | 0.00869  |
|    learning_rate   | 0.001    |
|    n_updates       | 163199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.85    |
|    critic_loss     | 0.00799  |
|    learning_rate   | 0.001    |
|    n_updates       | 163599   |
---------------------------------
=== Iterazione IRL 363 ===
Loss reward (iter 363): 6.726717948913574
=== Iterazione IRL 364 ===
Loss reward (iter 364): 6.578115463256836
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.88    |
|    critic_loss     | 0.00897  |
|    learning_rate   | 0.001    |
|    n_updates       | 164099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.78    |
|    critic_loss     | 0.00682  |
|    learning_rate   | 0.001    |
|    n_updates       | 164499   |
---------------------------------
=== Iterazione IRL 365 ===
Loss reward (iter 365): 6.5709638595581055
=== Iterazione IRL 366 ===
Loss reward (iter 366): 6.6589837074279785
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.76    |
|    critic_loss     | 0.00896  |
|    learning_rate   | 0.001    |
|    n_updates       | 164999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.82    |
|    critic_loss     | 0.00867  |
|    learning_rate   | 0.001    |
|    n_updates       | 165399   |
---------------------------------
=== Iterazione IRL 367 ===
Loss reward (iter 367): 6.624273300170898
=== Iterazione IRL 368 ===
Loss reward (iter 368): 6.6929240226745605
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.61    |
|    critic_loss     | 0.0108   |
|    learning_rate   | 0.001    |
|    n_updates       | 165899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.85    |
|    critic_loss     | 0.0105   |
|    learning_rate   | 0.001    |
|    n_updates       | 166299   |
---------------------------------
=== Iterazione IRL 369 ===
Loss reward (iter 369): 6.864941596984863
=== Iterazione IRL 370 ===
Loss reward (iter 370): 6.588269233703613
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.81    |
|    critic_loss     | 0.00725  |
|    learning_rate   | 0.001    |
|    n_updates       | 166799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.82    |
|    critic_loss     | 0.00831  |
|    learning_rate   | 0.001    |
|    n_updates       | 167199   |
---------------------------------
=== Iterazione IRL 371 ===
Loss reward (iter 371): 6.668079853057861
=== Iterazione IRL 372 ===
Loss reward (iter 372): 6.661095142364502
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.54    |
|    critic_loss     | 0.00986  |
|    learning_rate   | 0.001    |
|    n_updates       | 167699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.71    |
|    critic_loss     | 0.00886  |
|    learning_rate   | 0.001    |
|    n_updates       | 168099   |
---------------------------------
=== Iterazione IRL 373 ===
Loss reward (iter 373): 6.568170070648193
=== Iterazione IRL 374 ===
Loss reward (iter 374): 6.795832633972168
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.6     |
|    critic_loss     | 0.00629  |
|    learning_rate   | 0.001    |
|    n_updates       | 168599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.8     |
|    critic_loss     | 0.0088   |
|    learning_rate   | 0.001    |
|    n_updates       | 168999   |
---------------------------------
=== Iterazione IRL 375 ===
Loss reward (iter 375): 6.580662250518799
=== Iterazione IRL 376 ===
Loss reward (iter 376): 6.623531818389893
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.66    |
|    critic_loss     | 0.00739  |
|    learning_rate   | 0.001    |
|    n_updates       | 169499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.76    |
|    critic_loss     | 0.0071   |
|    learning_rate   | 0.001    |
|    n_updates       | 169899   |
---------------------------------
=== Iterazione IRL 377 ===
Loss reward (iter 377): 6.689542770385742
=== Iterazione IRL 378 ===
Loss reward (iter 378): 6.674504280090332
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.82    |
|    critic_loss     | 0.00834  |
|    learning_rate   | 0.001    |
|    n_updates       | 170399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.8     |
|    critic_loss     | 0.00958  |
|    learning_rate   | 0.001    |
|    n_updates       | 170799   |
---------------------------------
=== Iterazione IRL 379 ===
Loss reward (iter 379): 6.594602584838867
=== Iterazione IRL 380 ===
Loss reward (iter 380): 6.594050884246826
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.57    |
|    critic_loss     | 0.00861  |
|    learning_rate   | 0.001    |
|    n_updates       | 171299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.82    |
|    critic_loss     | 0.00899  |
|    learning_rate   | 0.001    |
|    n_updates       | 171699   |
---------------------------------
=== Iterazione IRL 381 ===
Loss reward (iter 381): 6.562880992889404
=== Iterazione IRL 382 ===
Loss reward (iter 382): 6.730045318603516
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.69    |
|    critic_loss     | 0.00887  |
|    learning_rate   | 0.001    |
|    n_updates       | 172199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.78    |
|    critic_loss     | 0.00832  |
|    learning_rate   | 0.001    |
|    n_updates       | 172599   |
---------------------------------
=== Iterazione IRL 383 ===
Loss reward (iter 383): 6.627242088317871
=== Iterazione IRL 384 ===
Loss reward (iter 384): 6.568326950073242
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.72    |
|    critic_loss     | 0.00895  |
|    learning_rate   | 0.001    |
|    n_updates       | 173099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.62    |
|    critic_loss     | 0.0112   |
|    learning_rate   | 0.001    |
|    n_updates       | 173499   |
---------------------------------
=== Iterazione IRL 385 ===
Loss reward (iter 385): 6.606814861297607
=== Iterazione IRL 386 ===
Loss reward (iter 386): 6.641476154327393
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.57    |
|    critic_loss     | 0.0076   |
|    learning_rate   | 0.001    |
|    n_updates       | 173999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.65    |
|    critic_loss     | 0.00841  |
|    learning_rate   | 0.001    |
|    n_updates       | 174399   |
---------------------------------
=== Iterazione IRL 387 ===
Loss reward (iter 387): 6.689472198486328
=== Iterazione IRL 388 ===
Loss reward (iter 388): 6.68588399887085
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.55    |
|    critic_loss     | 0.0108   |
|    learning_rate   | 0.001    |
|    n_updates       | 174899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.71    |
|    critic_loss     | 0.00965  |
|    learning_rate   | 0.001    |
|    n_updates       | 175299   |
---------------------------------
=== Iterazione IRL 389 ===
Loss reward (iter 389): 6.630860805511475
=== Iterazione IRL 390 ===
Loss reward (iter 390): 6.632820129394531
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.45    |
|    critic_loss     | 0.0117   |
|    learning_rate   | 0.001    |
|    n_updates       | 175799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.63    |
|    critic_loss     | 0.0106   |
|    learning_rate   | 0.001    |
|    n_updates       | 176199   |
---------------------------------
=== Iterazione IRL 391 ===
Loss reward (iter 391): 6.577470302581787
=== Iterazione IRL 392 ===
Loss reward (iter 392): 6.586075782775879
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.68    |
|    critic_loss     | 0.0082   |
|    learning_rate   | 0.001    |
|    n_updates       | 176699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.5     |
|    critic_loss     | 0.0101   |
|    learning_rate   | 0.001    |
|    n_updates       | 177099   |
---------------------------------
=== Iterazione IRL 393 ===
Loss reward (iter 393): 6.62593936920166
=== Iterazione IRL 394 ===
Loss reward (iter 394): 6.611450672149658
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.64    |
|    critic_loss     | 0.0113   |
|    learning_rate   | 0.001    |
|    n_updates       | 177599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.6     |
|    critic_loss     | 0.0107   |
|    learning_rate   | 0.001    |
|    n_updates       | 177999   |
---------------------------------
=== Iterazione IRL 395 ===
Loss reward (iter 395): 6.6479644775390625
=== Iterazione IRL 396 ===
Loss reward (iter 396): 6.59986686706543
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.61    |
|    critic_loss     | 0.011    |
|    learning_rate   | 0.001    |
|    n_updates       | 178499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.51    |
|    critic_loss     | 0.00662  |
|    learning_rate   | 0.001    |
|    n_updates       | 178899   |
---------------------------------
=== Iterazione IRL 397 ===
Loss reward (iter 397): 6.55369758605957
=== Iterazione IRL 398 ===
Loss reward (iter 398): 6.545176982879639
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.57    |
|    critic_loss     | 0.00655  |
|    learning_rate   | 0.001    |
|    n_updates       | 179399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.61    |
|    critic_loss     | 0.00921  |
|    learning_rate   | 0.001    |
|    n_updates       | 179799   |
---------------------------------
=== Iterazione IRL 399 ===
Loss reward (iter 399): 6.664821147918701
=== Iterazione IRL 400 ===
Loss reward (iter 400): 6.62866735458374
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.41    |
|    critic_loss     | 0.00858  |
|    learning_rate   | 0.001    |
|    n_updates       | 180299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.49    |
|    critic_loss     | 0.00891  |
|    learning_rate   | 0.001    |
|    n_updates       | 180699   |
---------------------------------
=== Iterazione IRL 401 ===
Loss reward (iter 401): 6.552926063537598
=== Iterazione IRL 402 ===
Loss reward (iter 402): 6.661810874938965
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.39    |
|    critic_loss     | 0.00989  |
|    learning_rate   | 0.001    |
|    n_updates       | 181199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.44    |
|    critic_loss     | 0.00796  |
|    learning_rate   | 0.001    |
|    n_updates       | 181599   |
---------------------------------
=== Iterazione IRL 403 ===
Loss reward (iter 403): 6.5981221199035645
=== Iterazione IRL 404 ===
Loss reward (iter 404): 6.607894420623779
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.46    |
|    critic_loss     | 0.00652  |
|    learning_rate   | 0.001    |
|    n_updates       | 182099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.55    |
|    critic_loss     | 0.0105   |
|    learning_rate   | 0.001    |
|    n_updates       | 182499   |
---------------------------------
=== Iterazione IRL 405 ===
Loss reward (iter 405): 6.554107189178467
=== Iterazione IRL 406 ===
Loss reward (iter 406): 6.5523834228515625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.35    |
|    critic_loss     | 0.0086   |
|    learning_rate   | 0.001    |
|    n_updates       | 182999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.46    |
|    critic_loss     | 0.00963  |
|    learning_rate   | 0.001    |
|    n_updates       | 183399   |
---------------------------------
=== Iterazione IRL 407 ===
Loss reward (iter 407): 6.651688098907471
=== Iterazione IRL 408 ===
Loss reward (iter 408): 6.5290045738220215
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.48    |
|    critic_loss     | 0.0101   |
|    learning_rate   | 0.001    |
|    n_updates       | 183899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.61    |
|    critic_loss     | 0.00655  |
|    learning_rate   | 0.001    |
|    n_updates       | 184299   |
---------------------------------
=== Iterazione IRL 409 ===
Loss reward (iter 409): 6.557223796844482
=== Iterazione IRL 410 ===
Loss reward (iter 410): 6.600347995758057
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.55    |
|    critic_loss     | 0.00853  |
|    learning_rate   | 0.001    |
|    n_updates       | 184799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.41    |
|    critic_loss     | 0.0101   |
|    learning_rate   | 0.001    |
|    n_updates       | 185199   |
---------------------------------
=== Iterazione IRL 411 ===
Loss reward (iter 411): 6.563884258270264
=== Iterazione IRL 412 ===
Loss reward (iter 412): 6.596374988555908
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.51    |
|    critic_loss     | 0.00851  |
|    learning_rate   | 0.001    |
|    n_updates       | 185699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.4     |
|    critic_loss     | 0.0117   |
|    learning_rate   | 0.001    |
|    n_updates       | 186099   |
---------------------------------
=== Iterazione IRL 413 ===
Loss reward (iter 413): 6.579998970031738
=== Iterazione IRL 414 ===
Loss reward (iter 414): 6.700995445251465
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.55    |
|    critic_loss     | 0.0086   |
|    learning_rate   | 0.001    |
|    n_updates       | 186599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.48    |
|    critic_loss     | 0.00872  |
|    learning_rate   | 0.001    |
|    n_updates       | 186999   |
---------------------------------
=== Iterazione IRL 415 ===
Loss reward (iter 415): 6.6677141189575195
=== Iterazione IRL 416 ===
Loss reward (iter 416): 6.618045806884766
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.42    |
|    critic_loss     | 0.00897  |
|    learning_rate   | 0.001    |
|    n_updates       | 187499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.37    |
|    critic_loss     | 0.00812  |
|    learning_rate   | 0.001    |
|    n_updates       | 187899   |
---------------------------------
=== Iterazione IRL 417 ===
Loss reward (iter 417): 6.574118137359619
=== Iterazione IRL 418 ===
Loss reward (iter 418): 6.669419765472412
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.31    |
|    critic_loss     | 0.00891  |
|    learning_rate   | 0.001    |
|    n_updates       | 188399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.44    |
|    critic_loss     | 0.0106   |
|    learning_rate   | 0.001    |
|    n_updates       | 188799   |
---------------------------------
=== Iterazione IRL 419 ===
Loss reward (iter 419): 6.578446865081787
=== Iterazione IRL 420 ===
Loss reward (iter 420): 6.618232250213623
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.42    |
|    critic_loss     | 0.00794  |
|    learning_rate   | 0.001    |
|    n_updates       | 189299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.36    |
|    critic_loss     | 0.00758  |
|    learning_rate   | 0.001    |
|    n_updates       | 189699   |
---------------------------------
=== Iterazione IRL 421 ===
Loss reward (iter 421): 6.608286380767822
=== Iterazione IRL 422 ===
Loss reward (iter 422): 6.54646110534668
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.39    |
|    critic_loss     | 0.0111   |
|    learning_rate   | 0.001    |
|    n_updates       | 190199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.32    |
|    critic_loss     | 0.00889  |
|    learning_rate   | 0.001    |
|    n_updates       | 190599   |
---------------------------------
=== Iterazione IRL 423 ===
Loss reward (iter 423): 6.665332317352295
=== Iterazione IRL 424 ===
Loss reward (iter 424): 6.633089542388916
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.18    |
|    critic_loss     | 0.0064   |
|    learning_rate   | 0.001    |
|    n_updates       | 191099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.32    |
|    critic_loss     | 0.00931  |
|    learning_rate   | 0.001    |
|    n_updates       | 191499   |
---------------------------------
=== Iterazione IRL 425 ===
Loss reward (iter 425): 6.6420512199401855
=== Iterazione IRL 426 ===
Loss reward (iter 426): 6.672083854675293
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.29    |
|    critic_loss     | 0.0113   |
|    learning_rate   | 0.001    |
|    n_updates       | 191999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.36    |
|    critic_loss     | 0.00893  |
|    learning_rate   | 0.001    |
|    n_updates       | 192399   |
---------------------------------
=== Iterazione IRL 427 ===
Loss reward (iter 427): 6.585208892822266
=== Iterazione IRL 428 ===
Loss reward (iter 428): 6.5560173988342285
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.29    |
|    critic_loss     | 0.00945  |
|    learning_rate   | 0.001    |
|    n_updates       | 192899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.26    |
|    critic_loss     | 0.0105   |
|    learning_rate   | 0.001    |
|    n_updates       | 193299   |
---------------------------------
=== Iterazione IRL 429 ===
Loss reward (iter 429): 6.660987377166748
=== Iterazione IRL 430 ===
Loss reward (iter 430): 6.6499223709106445
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.38    |
|    critic_loss     | 0.00899  |
|    learning_rate   | 0.001    |
|    n_updates       | 193799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.44    |
|    critic_loss     | 0.0105   |
|    learning_rate   | 0.001    |
|    n_updates       | 194199   |
---------------------------------
=== Iterazione IRL 431 ===
Loss reward (iter 431): 6.727099895477295
=== Iterazione IRL 432 ===
Loss reward (iter 432): 6.632019519805908
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.47    |
|    critic_loss     | 0.00912  |
|    learning_rate   | 0.001    |
|    n_updates       | 194699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.07    |
|    critic_loss     | 0.00927  |
|    learning_rate   | 0.001    |
|    n_updates       | 195099   |
---------------------------------
=== Iterazione IRL 433 ===
Loss reward (iter 433): 6.657608985900879
=== Iterazione IRL 434 ===
Loss reward (iter 434): 6.627386569976807
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.2     |
|    critic_loss     | 0.00868  |
|    learning_rate   | 0.001    |
|    n_updates       | 195599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.22    |
|    critic_loss     | 0.0072   |
|    learning_rate   | 0.001    |
|    n_updates       | 195999   |
---------------------------------
=== Iterazione IRL 435 ===
Loss reward (iter 435): 6.637187957763672
=== Iterazione IRL 436 ===
Loss reward (iter 436): 6.594505310058594
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.4     |
|    critic_loss     | 0.00961  |
|    learning_rate   | 0.001    |
|    n_updates       | 196499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.31    |
|    critic_loss     | 0.0103   |
|    learning_rate   | 0.001    |
|    n_updates       | 196899   |
---------------------------------
=== Iterazione IRL 437 ===
Loss reward (iter 437): 6.575534343719482
=== Iterazione IRL 438 ===
Loss reward (iter 438): 6.77101993560791
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.27    |
|    critic_loss     | 0.00721  |
|    learning_rate   | 0.001    |
|    n_updates       | 197399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.22    |
|    critic_loss     | 0.00836  |
|    learning_rate   | 0.001    |
|    n_updates       | 197799   |
---------------------------------
=== Iterazione IRL 439 ===
Loss reward (iter 439): 6.85007381439209
=== Iterazione IRL 440 ===
Loss reward (iter 440): 6.664962291717529
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.3     |
|    critic_loss     | 0.00875  |
|    learning_rate   | 0.001    |
|    n_updates       | 198299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.46    |
|    critic_loss     | 0.00855  |
|    learning_rate   | 0.001    |
|    n_updates       | 198699   |
---------------------------------
=== Iterazione IRL 441 ===
Loss reward (iter 441): 6.62858772277832
=== Iterazione IRL 442 ===
Loss reward (iter 442): 6.877654075622559
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.14    |
|    critic_loss     | 0.00829  |
|    learning_rate   | 0.001    |
|    n_updates       | 199199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.15    |
|    critic_loss     | 0.00785  |
|    learning_rate   | 0.001    |
|    n_updates       | 199599   |
---------------------------------
=== Iterazione IRL 443 ===
Loss reward (iter 443): 6.61486291885376
=== Iterazione IRL 444 ===
Loss reward (iter 444): 6.582552909851074
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.17    |
|    critic_loss     | 0.00797  |
|    learning_rate   | 0.001    |
|    n_updates       | 200099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.28    |
|    critic_loss     | 0.00671  |
|    learning_rate   | 0.001    |
|    n_updates       | 200499   |
---------------------------------
=== Iterazione IRL 445 ===
Loss reward (iter 445): 6.666464328765869
=== Iterazione IRL 446 ===
Loss reward (iter 446): 6.668341159820557
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.23    |
|    critic_loss     | 0.00958  |
|    learning_rate   | 0.001    |
|    n_updates       | 200999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.11    |
|    critic_loss     | 0.0128   |
|    learning_rate   | 0.001    |
|    n_updates       | 201399   |
---------------------------------
=== Iterazione IRL 447 ===
Loss reward (iter 447): 6.6422119140625
=== Iterazione IRL 448 ===
Loss reward (iter 448): 6.6621856689453125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.16    |
|    critic_loss     | 0.0103   |
|    learning_rate   | 0.001    |
|    n_updates       | 201899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.16    |
|    critic_loss     | 0.0087   |
|    learning_rate   | 0.001    |
|    n_updates       | 202299   |
---------------------------------
=== Iterazione IRL 449 ===
Loss reward (iter 449): 6.663329601287842
=== Iterazione IRL 450 ===
Loss reward (iter 450): 6.651905536651611
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.23    |
|    critic_loss     | 0.0101   |
|    learning_rate   | 0.001    |
|    n_updates       | 202799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.15    |
|    critic_loss     | 0.0103   |
|    learning_rate   | 0.001    |
|    n_updates       | 203199   |
---------------------------------
=== Iterazione IRL 451 ===
Loss reward (iter 451): 6.623152256011963
=== Iterazione IRL 452 ===
Loss reward (iter 452): 6.59927225112915
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.17    |
|    critic_loss     | 0.0102   |
|    learning_rate   | 0.001    |
|    n_updates       | 203699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.03    |
|    critic_loss     | 0.00886  |
|    learning_rate   | 0.001    |
|    n_updates       | 204099   |
---------------------------------
=== Iterazione IRL 453 ===
Loss reward (iter 453): 6.574000358581543
=== Iterazione IRL 454 ===
Loss reward (iter 454): 6.612767219543457
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.07    |
|    critic_loss     | 0.0093   |
|    learning_rate   | 0.001    |
|    n_updates       | 204599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.26    |
|    critic_loss     | 0.00946  |
|    learning_rate   | 0.001    |
|    n_updates       | 204999   |
---------------------------------
=== Iterazione IRL 455 ===
Loss reward (iter 455): 6.53312873840332
=== Iterazione IRL 456 ===
Loss reward (iter 456): 6.5882487297058105
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.02    |
|    critic_loss     | 0.00755  |
|    learning_rate   | 0.001    |
|    n_updates       | 205499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.88    |
|    critic_loss     | 0.00869  |
|    learning_rate   | 0.001    |
|    n_updates       | 205899   |
---------------------------------
=== Iterazione IRL 457 ===
Loss reward (iter 457): 6.570483684539795
=== Iterazione IRL 458 ===
Loss reward (iter 458): 6.538469314575195
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.98    |
|    critic_loss     | 0.011    |
|    learning_rate   | 0.001    |
|    n_updates       | 206399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.16    |
|    critic_loss     | 0.00905  |
|    learning_rate   | 0.001    |
|    n_updates       | 206799   |
---------------------------------
=== Iterazione IRL 459 ===
Loss reward (iter 459): 6.68333101272583
=== Iterazione IRL 460 ===
Loss reward (iter 460): 6.578543663024902
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.15    |
|    critic_loss     | 0.0105   |
|    learning_rate   | 0.001    |
|    n_updates       | 207299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.1     |
|    critic_loss     | 0.0105   |
|    learning_rate   | 0.001    |
|    n_updates       | 207699   |
---------------------------------
=== Iterazione IRL 461 ===
Loss reward (iter 461): 6.589535713195801
=== Iterazione IRL 462 ===
Loss reward (iter 462): 6.696506500244141
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.18    |
|    critic_loss     | 0.00905  |
|    learning_rate   | 0.001    |
|    n_updates       | 208199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.07    |
|    critic_loss     | 0.0114   |
|    learning_rate   | 0.001    |
|    n_updates       | 208599   |
---------------------------------
=== Iterazione IRL 463 ===
Loss reward (iter 463): 6.58870267868042
=== Iterazione IRL 464 ===
Loss reward (iter 464): 6.604547500610352
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.88    |
|    critic_loss     | 0.0123   |
|    learning_rate   | 0.001    |
|    n_updates       | 209099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.21    |
|    critic_loss     | 0.00929  |
|    learning_rate   | 0.001    |
|    n_updates       | 209499   |
---------------------------------
=== Iterazione IRL 465 ===
Loss reward (iter 465): 6.546926498413086
=== Iterazione IRL 466 ===
Loss reward (iter 466): 6.551322937011719
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.02    |
|    critic_loss     | 0.0107   |
|    learning_rate   | 0.001    |
|    n_updates       | 209999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.93    |
|    critic_loss     | 0.0102   |
|    learning_rate   | 0.001    |
|    n_updates       | 210399   |
---------------------------------
=== Iterazione IRL 467 ===
Loss reward (iter 467): 6.591838359832764
=== Iterazione IRL 468 ===
Loss reward (iter 468): 6.550516605377197
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.95    |
|    critic_loss     | 0.0104   |
|    learning_rate   | 0.001    |
|    n_updates       | 210899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.08    |
|    critic_loss     | 0.0125   |
|    learning_rate   | 0.001    |
|    n_updates       | 211299   |
---------------------------------
=== Iterazione IRL 469 ===
Loss reward (iter 469): 6.63008975982666
=== Iterazione IRL 470 ===
Loss reward (iter 470): 6.554023742675781
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.09    |
|    critic_loss     | 0.0109   |
|    learning_rate   | 0.001    |
|    n_updates       | 211799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.89    |
|    critic_loss     | 0.0114   |
|    learning_rate   | 0.001    |
|    n_updates       | 212199   |
---------------------------------
=== Iterazione IRL 471 ===
Loss reward (iter 471): 6.563983917236328
=== Iterazione IRL 472 ===
Loss reward (iter 472): 6.605545520782471
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.91    |
|    critic_loss     | 0.0101   |
|    learning_rate   | 0.001    |
|    n_updates       | 212699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.95    |
|    critic_loss     | 0.00996  |
|    learning_rate   | 0.001    |
|    n_updates       | 213099   |
---------------------------------
=== Iterazione IRL 473 ===
Loss reward (iter 473): 6.587796211242676
=== Iterazione IRL 474 ===
Loss reward (iter 474): 6.708072185516357
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.16    |
|    critic_loss     | 0.012    |
|    learning_rate   | 0.001    |
|    n_updates       | 213599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.06    |
|    critic_loss     | 0.00957  |
|    learning_rate   | 0.001    |
|    n_updates       | 213999   |
---------------------------------
=== Iterazione IRL 475 ===
Loss reward (iter 475): 6.563268661499023
=== Iterazione IRL 476 ===
Loss reward (iter 476): 6.545629978179932
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.76    |
|    critic_loss     | 0.0112   |
|    learning_rate   | 0.001    |
|    n_updates       | 214499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.91    |
|    critic_loss     | 0.0107   |
|    learning_rate   | 0.001    |
|    n_updates       | 214899   |
---------------------------------
=== Iterazione IRL 477 ===
Loss reward (iter 477): 6.563737392425537
=== Iterazione IRL 478 ===
Loss reward (iter 478): 6.574675559997559
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.88    |
|    critic_loss     | 0.0111   |
|    learning_rate   | 0.001    |
|    n_updates       | 215399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.11    |
|    critic_loss     | 0.011    |
|    learning_rate   | 0.001    |
|    n_updates       | 215799   |
---------------------------------
=== Iterazione IRL 479 ===
Loss reward (iter 479): 6.668563365936279
=== Iterazione IRL 480 ===
Loss reward (iter 480): 6.636488437652588
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.96    |
|    critic_loss     | 0.00966  |
|    learning_rate   | 0.001    |
|    n_updates       | 216299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.84    |
|    critic_loss     | 0.01     |
|    learning_rate   | 0.001    |
|    n_updates       | 216699   |
---------------------------------
=== Iterazione IRL 481 ===
Loss reward (iter 481): 6.57518196105957
=== Iterazione IRL 482 ===
Loss reward (iter 482): 6.5872907638549805
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.77    |
|    critic_loss     | 0.0109   |
|    learning_rate   | 0.001    |
|    n_updates       | 217199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.82    |
|    critic_loss     | 0.00847  |
|    learning_rate   | 0.001    |
|    n_updates       | 217599   |
---------------------------------
=== Iterazione IRL 483 ===
Loss reward (iter 483): 6.611637592315674
=== Iterazione IRL 484 ===
Loss reward (iter 484): 6.673336029052734
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.79    |
|    critic_loss     | 0.0112   |
|    learning_rate   | 0.001    |
|    n_updates       | 218099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.7     |
|    critic_loss     | 0.0105   |
|    learning_rate   | 0.001    |
|    n_updates       | 218499   |
---------------------------------
=== Iterazione IRL 485 ===
Loss reward (iter 485): 6.629169940948486
=== Iterazione IRL 486 ===
Loss reward (iter 486): 6.605664253234863
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.94    |
|    critic_loss     | 0.011    |
|    learning_rate   | 0.001    |
|    n_updates       | 218999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.83    |
|    critic_loss     | 0.0113   |
|    learning_rate   | 0.001    |
|    n_updates       | 219399   |
---------------------------------
=== Iterazione IRL 487 ===
Loss reward (iter 487): 6.707594871520996
=== Iterazione IRL 488 ===
Loss reward (iter 488): 6.642087459564209
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.88    |
|    critic_loss     | 0.0114   |
|    learning_rate   | 0.001    |
|    n_updates       | 219899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.81    |
|    critic_loss     | 0.0114   |
|    learning_rate   | 0.001    |
|    n_updates       | 220299   |
---------------------------------
=== Iterazione IRL 489 ===
Loss reward (iter 489): 6.529955863952637
=== Iterazione IRL 490 ===
Loss reward (iter 490): 6.622109413146973
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.74    |
|    critic_loss     | 0.00946  |
|    learning_rate   | 0.001    |
|    n_updates       | 220799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.77    |
|    critic_loss     | 0.0105   |
|    learning_rate   | 0.001    |
|    n_updates       | 221199   |
---------------------------------
=== Iterazione IRL 491 ===
Loss reward (iter 491): 6.5682268142700195
=== Iterazione IRL 492 ===
Loss reward (iter 492): 6.622564315795898
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.87    |
|    critic_loss     | 0.00901  |
|    learning_rate   | 0.001    |
|    n_updates       | 221699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.78    |
|    critic_loss     | 0.01     |
|    learning_rate   | 0.001    |
|    n_updates       | 222099   |
---------------------------------
=== Iterazione IRL 493 ===
Loss reward (iter 493): 6.493757247924805
=== Iterazione IRL 494 ===
Loss reward (iter 494): 6.592752456665039
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.8     |
|    critic_loss     | 0.0115   |
|    learning_rate   | 0.001    |
|    n_updates       | 222599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.96    |
|    critic_loss     | 0.0101   |
|    learning_rate   | 0.001    |
|    n_updates       | 222999   |
---------------------------------
=== Iterazione IRL 495 ===
Loss reward (iter 495): 6.674992084503174
=== Iterazione IRL 496 ===
Loss reward (iter 496): 6.570281505584717
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.81    |
|    critic_loss     | 0.0102   |
|    learning_rate   | 0.001    |
|    n_updates       | 223499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.73    |
|    critic_loss     | 0.0105   |
|    learning_rate   | 0.001    |
|    n_updates       | 223899   |
---------------------------------
=== Iterazione IRL 497 ===
Loss reward (iter 497): 6.606197357177734
=== Iterazione IRL 498 ===
Loss reward (iter 498): 6.576406478881836
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.76    |
|    critic_loss     | 0.00963  |
|    learning_rate   | 0.001    |
|    n_updates       | 224399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.78    |
|    critic_loss     | 0.00977  |
|    learning_rate   | 0.001    |
|    n_updates       | 224799   |
---------------------------------
=== Iterazione IRL 499 ===
Loss reward (iter 499): 6.536426544189453
=== Iterazione IRL 500 ===
Loss reward (iter 500): 6.572785377502441
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.72    |
|    critic_loss     | 0.0108   |
|    learning_rate   | 0.001    |
|    n_updates       | 225299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 234      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.7     |
|    critic_loss     | 0.0102   |
|    learning_rate   | 0.001    |
|    n_updates       | 225699   |
---------------------------------
=== Iterazione IRL 501 ===
Loss reward (iter 501): 6.665132522583008
=== Iterazione IRL 502 ===
Loss reward (iter 502): 6.542757034301758
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.76    |
|    critic_loss     | 0.013    |
|    learning_rate   | 0.001    |
|    n_updates       | 226199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.69    |
|    critic_loss     | 0.011    |
|    learning_rate   | 0.001    |
|    n_updates       | 226599   |
---------------------------------
=== Iterazione IRL 503 ===
Loss reward (iter 503): 6.650416851043701
=== Iterazione IRL 504 ===
Loss reward (iter 504): 6.633080005645752
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.85    |
|    critic_loss     | 0.0117   |
|    learning_rate   | 0.001    |
|    n_updates       | 227099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.7     |
|    critic_loss     | 0.0101   |
|    learning_rate   | 0.001    |
|    n_updates       | 227499   |
---------------------------------
=== Iterazione IRL 505 ===
Loss reward (iter 505): 6.575418472290039
=== Iterazione IRL 506 ===
Loss reward (iter 506): 6.55519962310791
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.72    |
|    critic_loss     | 0.0102   |
|    learning_rate   | 0.001    |
|    n_updates       | 227999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.88    |
|    critic_loss     | 0.00945  |
|    learning_rate   | 0.001    |
|    n_updates       | 228399   |
---------------------------------
=== Iterazione IRL 507 ===
Loss reward (iter 507): 6.567477226257324
=== Iterazione IRL 508 ===
Loss reward (iter 508): 6.6081953048706055
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.76    |
|    critic_loss     | 0.00937  |
|    learning_rate   | 0.001    |
|    n_updates       | 228899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.68    |
|    critic_loss     | 0.00994  |
|    learning_rate   | 0.001    |
|    n_updates       | 229299   |
---------------------------------
=== Iterazione IRL 509 ===
Loss reward (iter 509): 6.583654880523682
=== Iterazione IRL 510 ===
Loss reward (iter 510): 6.6026763916015625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.79    |
|    critic_loss     | 0.00987  |
|    learning_rate   | 0.001    |
|    n_updates       | 229799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.58    |
|    critic_loss     | 0.00998  |
|    learning_rate   | 0.001    |
|    n_updates       | 230199   |
---------------------------------
=== Iterazione IRL 511 ===
Loss reward (iter 511): 6.531772136688232
=== Iterazione IRL 512 ===
Loss reward (iter 512): 6.660958766937256
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.56    |
|    critic_loss     | 0.00861  |
|    learning_rate   | 0.001    |
|    n_updates       | 230699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.67    |
|    critic_loss     | 0.0119   |
|    learning_rate   | 0.001    |
|    n_updates       | 231099   |
---------------------------------
=== Iterazione IRL 513 ===
Loss reward (iter 513): 6.569954872131348
=== Iterazione IRL 514 ===
Loss reward (iter 514): 6.62390661239624
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.41    |
|    critic_loss     | 0.0122   |
|    learning_rate   | 0.001    |
|    n_updates       | 231599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.7     |
|    critic_loss     | 0.00991  |
|    learning_rate   | 0.001    |
|    n_updates       | 231999   |
---------------------------------
=== Iterazione IRL 515 ===
Loss reward (iter 515): 6.585594177246094
=== Iterazione IRL 516 ===
Loss reward (iter 516): 6.6309919357299805
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.57    |
|    critic_loss     | 0.00914  |
|    learning_rate   | 0.001    |
|    n_updates       | 232499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.75    |
|    critic_loss     | 0.0109   |
|    learning_rate   | 0.001    |
|    n_updates       | 232899   |
---------------------------------
=== Iterazione IRL 517 ===
Loss reward (iter 517): 6.782895088195801
=== Iterazione IRL 518 ===
Loss reward (iter 518): 6.611837863922119
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.66    |
|    critic_loss     | 0.0124   |
|    learning_rate   | 0.001    |
|    n_updates       | 233399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.67    |
|    critic_loss     | 0.0102   |
|    learning_rate   | 0.001    |
|    n_updates       | 233799   |
---------------------------------
=== Iterazione IRL 519 ===
Loss reward (iter 519): 6.538980960845947
=== Iterazione IRL 520 ===
Loss reward (iter 520): 6.576516151428223
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.61    |
|    critic_loss     | 0.0126   |
|    learning_rate   | 0.001    |
|    n_updates       | 234299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.63    |
|    critic_loss     | 0.00856  |
|    learning_rate   | 0.001    |
|    n_updates       | 234699   |
---------------------------------
=== Iterazione IRL 521 ===
Loss reward (iter 521): 6.629758358001709
=== Iterazione IRL 522 ===
Loss reward (iter 522): 6.823760986328125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.53    |
|    critic_loss     | 0.0102   |
|    learning_rate   | 0.001    |
|    n_updates       | 235199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.51    |
|    critic_loss     | 0.00942  |
|    learning_rate   | 0.001    |
|    n_updates       | 235599   |
---------------------------------
=== Iterazione IRL 523 ===
Loss reward (iter 523): 6.617074966430664
=== Iterazione IRL 524 ===
Loss reward (iter 524): 6.583530426025391
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.85    |
|    critic_loss     | 0.0109   |
|    learning_rate   | 0.001    |
|    n_updates       | 236099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.54    |
|    critic_loss     | 0.00884  |
|    learning_rate   | 0.001    |
|    n_updates       | 236499   |
---------------------------------
=== Iterazione IRL 525 ===
Loss reward (iter 525): 6.660537242889404
=== Iterazione IRL 526 ===
Loss reward (iter 526): 6.579204559326172
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.74    |
|    critic_loss     | 0.0114   |
|    learning_rate   | 0.001    |
|    n_updates       | 236999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.45    |
|    critic_loss     | 0.0107   |
|    learning_rate   | 0.001    |
|    n_updates       | 237399   |
---------------------------------
=== Iterazione IRL 527 ===
Loss reward (iter 527): 6.580559253692627
=== Iterazione IRL 528 ===
Loss reward (iter 528): 6.579416275024414
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.51    |
|    critic_loss     | 0.00913  |
|    learning_rate   | 0.001    |
|    n_updates       | 237899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.63    |
|    critic_loss     | 0.00899  |
|    learning_rate   | 0.001    |
|    n_updates       | 238299   |
---------------------------------
=== Iterazione IRL 529 ===
Loss reward (iter 529): 6.604888916015625
=== Iterazione IRL 530 ===
Loss reward (iter 530): 6.579883098602295
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.56    |
|    critic_loss     | 0.0107   |
|    learning_rate   | 0.001    |
|    n_updates       | 238799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.55    |
|    critic_loss     | 0.0116   |
|    learning_rate   | 0.001    |
|    n_updates       | 239199   |
---------------------------------
=== Iterazione IRL 531 ===
Loss reward (iter 531): 6.579387187957764
=== Iterazione IRL 532 ===
Loss reward (iter 532): 6.663175582885742
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.48    |
|    critic_loss     | 0.00884  |
|    learning_rate   | 0.001    |
|    n_updates       | 239699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.68    |
|    critic_loss     | 0.00957  |
|    learning_rate   | 0.001    |
|    n_updates       | 240099   |
---------------------------------
=== Iterazione IRL 533 ===
Loss reward (iter 533): 6.643086910247803
=== Iterazione IRL 534 ===
Loss reward (iter 534): 6.5800461769104
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.47    |
|    critic_loss     | 0.0114   |
|    learning_rate   | 0.001    |
|    n_updates       | 240599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.33    |
|    critic_loss     | 0.00911  |
|    learning_rate   | 0.001    |
|    n_updates       | 240999   |
---------------------------------
=== Iterazione IRL 535 ===
Loss reward (iter 535): 6.5618977546691895
=== Iterazione IRL 536 ===
Loss reward (iter 536): 6.602940082550049
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.54    |
|    critic_loss     | 0.00823  |
|    learning_rate   | 0.001    |
|    n_updates       | 241499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.57    |
|    critic_loss     | 0.00917  |
|    learning_rate   | 0.001    |
|    n_updates       | 241899   |
---------------------------------
=== Iterazione IRL 537 ===
Loss reward (iter 537): 6.634433269500732
=== Iterazione IRL 538 ===
Loss reward (iter 538): 6.601342678070068
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.5     |
|    critic_loss     | 0.00927  |
|    learning_rate   | 0.001    |
|    n_updates       | 242399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.52    |
|    critic_loss     | 0.0116   |
|    learning_rate   | 0.001    |
|    n_updates       | 242799   |
---------------------------------
=== Iterazione IRL 539 ===
Loss reward (iter 539): 6.63465690612793
=== Iterazione IRL 540 ===
Loss reward (iter 540): 6.617592811584473
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.49    |
|    critic_loss     | 0.0085   |
|    learning_rate   | 0.001    |
|    n_updates       | 243299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.49    |
|    critic_loss     | 0.0111   |
|    learning_rate   | 0.001    |
|    n_updates       | 243699   |
---------------------------------
=== Iterazione IRL 541 ===
Loss reward (iter 541): 6.561251163482666
=== Iterazione IRL 542 ===
Loss reward (iter 542): 6.572448253631592
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.37    |
|    critic_loss     | 0.00988  |
|    learning_rate   | 0.001    |
|    n_updates       | 244199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.51    |
|    critic_loss     | 0.0129   |
|    learning_rate   | 0.001    |
|    n_updates       | 244599   |
---------------------------------
=== Iterazione IRL 543 ===
Loss reward (iter 543): 6.5710859298706055
=== Iterazione IRL 544 ===
Loss reward (iter 544): 6.577101230621338
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.51    |
|    critic_loss     | 0.0098   |
|    learning_rate   | 0.001    |
|    n_updates       | 245099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.64    |
|    critic_loss     | 0.0126   |
|    learning_rate   | 0.001    |
|    n_updates       | 245499   |
---------------------------------
=== Iterazione IRL 545 ===
Loss reward (iter 545): 6.6663618087768555
=== Iterazione IRL 546 ===
Loss reward (iter 546): 6.629659175872803
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.46    |
|    critic_loss     | 0.0107   |
|    learning_rate   | 0.001    |
|    n_updates       | 245999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.37    |
|    critic_loss     | 0.00914  |
|    learning_rate   | 0.001    |
|    n_updates       | 246399   |
---------------------------------
=== Iterazione IRL 547 ===
Loss reward (iter 547): 6.7257399559021
=== Iterazione IRL 548 ===
Loss reward (iter 548): 6.586218357086182
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.28    |
|    critic_loss     | 0.0108   |
|    learning_rate   | 0.001    |
|    n_updates       | 246899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.34    |
|    critic_loss     | 0.0109   |
|    learning_rate   | 0.001    |
|    n_updates       | 247299   |
---------------------------------
=== Iterazione IRL 549 ===
Loss reward (iter 549): 6.634494781494141
=== Iterazione IRL 550 ===
Loss reward (iter 550): 6.621826171875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.4     |
|    critic_loss     | 0.0105   |
|    learning_rate   | 0.001    |
|    n_updates       | 247799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.42    |
|    critic_loss     | 0.0109   |
|    learning_rate   | 0.001    |
|    n_updates       | 248199   |
---------------------------------
=== Iterazione IRL 551 ===
Loss reward (iter 551): 6.649621486663818
=== Iterazione IRL 552 ===
Loss reward (iter 552): 6.615890026092529
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.46    |
|    critic_loss     | 0.0128   |
|    learning_rate   | 0.001    |
|    n_updates       | 248699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.37    |
|    critic_loss     | 0.0102   |
|    learning_rate   | 0.001    |
|    n_updates       | 249099   |
---------------------------------
=== Iterazione IRL 553 ===
Loss reward (iter 553): 6.566725730895996
=== Iterazione IRL 554 ===
Loss reward (iter 554): 6.638574600219727
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.3     |
|    critic_loss     | 0.00932  |
|    learning_rate   | 0.001    |
|    n_updates       | 249599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.4     |
|    critic_loss     | 0.0123   |
|    learning_rate   | 0.001    |
|    n_updates       | 249999   |
---------------------------------
=== Iterazione IRL 555 ===
Loss reward (iter 555): 6.590326309204102
=== Iterazione IRL 556 ===
Loss reward (iter 556): 6.674229621887207
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.24    |
|    critic_loss     | 0.0105   |
|    learning_rate   | 0.001    |
|    n_updates       | 250499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.05    |
|    critic_loss     | 0.0124   |
|    learning_rate   | 0.001    |
|    n_updates       | 250899   |
---------------------------------
=== Iterazione IRL 557 ===
Loss reward (iter 557): 6.51075553894043
=== Iterazione IRL 558 ===
Loss reward (iter 558): 6.651996612548828
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.37    |
|    critic_loss     | 0.0107   |
|    learning_rate   | 0.001    |
|    n_updates       | 251399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.42    |
|    critic_loss     | 0.0123   |
|    learning_rate   | 0.001    |
|    n_updates       | 251799   |
---------------------------------
=== Iterazione IRL 559 ===
Loss reward (iter 559): 6.526116847991943
=== Iterazione IRL 560 ===
Loss reward (iter 560): 6.672122955322266
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.31    |
|    critic_loss     | 0.0102   |
|    learning_rate   | 0.001    |
|    n_updates       | 252299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.25    |
|    critic_loss     | 0.0113   |
|    learning_rate   | 0.001    |
|    n_updates       | 252699   |
---------------------------------
=== Iterazione IRL 561 ===
Loss reward (iter 561): 6.5430426597595215
=== Iterazione IRL 562 ===
Loss reward (iter 562): 6.631485939025879
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.15    |
|    critic_loss     | 0.00914  |
|    learning_rate   | 0.001    |
|    n_updates       | 253199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.38    |
|    critic_loss     | 0.00938  |
|    learning_rate   | 0.001    |
|    n_updates       | 253599   |
---------------------------------
=== Iterazione IRL 563 ===
Loss reward (iter 563): 6.58642053604126
=== Iterazione IRL 564 ===
Loss reward (iter 564): 6.558636665344238
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.26    |
|    critic_loss     | 0.0102   |
|    learning_rate   | 0.001    |
|    n_updates       | 254099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.14    |
|    critic_loss     | 0.0128   |
|    learning_rate   | 0.001    |
|    n_updates       | 254499   |
---------------------------------
=== Iterazione IRL 565 ===
Loss reward (iter 565): 6.476380348205566
=== Iterazione IRL 566 ===
Loss reward (iter 566): 6.577216148376465
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.26    |
|    critic_loss     | 0.00847  |
|    learning_rate   | 0.001    |
|    n_updates       | 254999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.32    |
|    critic_loss     | 0.0114   |
|    learning_rate   | 0.001    |
|    n_updates       | 255399   |
---------------------------------
=== Iterazione IRL 567 ===
Loss reward (iter 567): 6.619192600250244
=== Iterazione IRL 568 ===
Loss reward (iter 568): 6.636952877044678
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.36    |
|    critic_loss     | 0.00946  |
|    learning_rate   | 0.001    |
|    n_updates       | 255899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.25    |
|    critic_loss     | 0.0102   |
|    learning_rate   | 0.001    |
|    n_updates       | 256299   |
---------------------------------
=== Iterazione IRL 569 ===
Loss reward (iter 569): 6.596593856811523
=== Iterazione IRL 570 ===
Loss reward (iter 570): 6.665857315063477
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.24    |
|    critic_loss     | 0.0092   |
|    learning_rate   | 0.001    |
|    n_updates       | 256799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.25    |
|    critic_loss     | 0.0113   |
|    learning_rate   | 0.001    |
|    n_updates       | 257199   |
---------------------------------
=== Iterazione IRL 571 ===
Loss reward (iter 571): 6.772377014160156
=== Iterazione IRL 572 ===
Loss reward (iter 572): 6.652381896972656
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.4     |
|    critic_loss     | 0.00683  |
|    learning_rate   | 0.001    |
|    n_updates       | 257699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.38    |
|    critic_loss     | 0.0105   |
|    learning_rate   | 0.001    |
|    n_updates       | 258099   |
---------------------------------
=== Iterazione IRL 573 ===
Loss reward (iter 573): 6.634758472442627
=== Iterazione IRL 574 ===
Loss reward (iter 574): 6.667234897613525
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.29    |
|    critic_loss     | 0.00721  |
|    learning_rate   | 0.001    |
|    n_updates       | 258599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.26    |
|    critic_loss     | 0.00822  |
|    learning_rate   | 0.001    |
|    n_updates       | 258999   |
---------------------------------
=== Iterazione IRL 575 ===
Loss reward (iter 575): 6.621111869812012
=== Iterazione IRL 576 ===
Loss reward (iter 576): 6.7303972244262695
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.32    |
|    critic_loss     | 0.0103   |
|    learning_rate   | 0.001    |
|    n_updates       | 259499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.38    |
|    critic_loss     | 0.0124   |
|    learning_rate   | 0.001    |
|    n_updates       | 259899   |
---------------------------------
=== Iterazione IRL 577 ===
Loss reward (iter 577): 6.6369242668151855
=== Iterazione IRL 578 ===
Loss reward (iter 578): 6.539063930511475
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.28    |
|    critic_loss     | 0.0128   |
|    learning_rate   | 0.001    |
|    n_updates       | 260399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.24    |
|    critic_loss     | 0.0111   |
|    learning_rate   | 0.001    |
|    n_updates       | 260799   |
---------------------------------
=== Iterazione IRL 579 ===
Loss reward (iter 579): 6.604869842529297
=== Iterazione IRL 580 ===
Loss reward (iter 580): 6.6182661056518555
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.2     |
|    critic_loss     | 0.0125   |
|    learning_rate   | 0.001    |
|    n_updates       | 261299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.22    |
|    critic_loss     | 0.0146   |
|    learning_rate   | 0.001    |
|    n_updates       | 261699   |
---------------------------------
=== Iterazione IRL 581 ===
Loss reward (iter 581): 6.54682731628418
=== Iterazione IRL 582 ===
Loss reward (iter 582): 6.556129455566406
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.27    |
|    critic_loss     | 0.0115   |
|    learning_rate   | 0.001    |
|    n_updates       | 262199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.24    |
|    critic_loss     | 0.0107   |
|    learning_rate   | 0.001    |
|    n_updates       | 262599   |
---------------------------------
=== Iterazione IRL 583 ===
Loss reward (iter 583): 6.564507484436035
=== Iterazione IRL 584 ===
Loss reward (iter 584): 6.579992294311523
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.24    |
|    critic_loss     | 0.00921  |
|    learning_rate   | 0.001    |
|    n_updates       | 263099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.2     |
|    critic_loss     | 0.0123   |
|    learning_rate   | 0.001    |
|    n_updates       | 263499   |
---------------------------------
=== Iterazione IRL 585 ===
Loss reward (iter 585): 6.685128211975098
=== Iterazione IRL 586 ===
Loss reward (iter 586): 6.593799591064453
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6       |
|    critic_loss     | 0.0121   |
|    learning_rate   | 0.001    |
|    n_updates       | 263999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.18    |
|    critic_loss     | 0.0108   |
|    learning_rate   | 0.001    |
|    n_updates       | 264399   |
---------------------------------
=== Iterazione IRL 587 ===
Loss reward (iter 587): 6.596012592315674
=== Iterazione IRL 588 ===
Loss reward (iter 588): 6.612596035003662
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.18    |
|    critic_loss     | 0.00845  |
|    learning_rate   | 0.001    |
|    n_updates       | 264899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.38    |
|    critic_loss     | 0.0123   |
|    learning_rate   | 0.001    |
|    n_updates       | 265299   |
---------------------------------
=== Iterazione IRL 589 ===
Loss reward (iter 589): 6.628917694091797
=== Iterazione IRL 590 ===
Loss reward (iter 590): 6.588139533996582
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.21    |
|    critic_loss     | 0.0144   |
|    learning_rate   | 0.001    |
|    n_updates       | 265799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.97    |
|    critic_loss     | 0.0114   |
|    learning_rate   | 0.001    |
|    n_updates       | 266199   |
---------------------------------
=== Iterazione IRL 591 ===
Loss reward (iter 591): 6.695301532745361
=== Iterazione IRL 592 ===
Loss reward (iter 592): 6.61631441116333
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.23    |
|    critic_loss     | 0.0129   |
|    learning_rate   | 0.001    |
|    n_updates       | 266699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.21    |
|    critic_loss     | 0.0094   |
|    learning_rate   | 0.001    |
|    n_updates       | 267099   |
---------------------------------
=== Iterazione IRL 593 ===
Loss reward (iter 593): 6.75947380065918
=== Iterazione IRL 594 ===
Loss reward (iter 594): 6.676701545715332
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.2     |
|    critic_loss     | 0.0116   |
|    learning_rate   | 0.001    |
|    n_updates       | 267599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.08    |
|    critic_loss     | 0.00972  |
|    learning_rate   | 0.001    |
|    n_updates       | 267999   |
---------------------------------
=== Iterazione IRL 595 ===
Loss reward (iter 595): 6.5519490242004395
=== Iterazione IRL 596 ===
Loss reward (iter 596): 6.551487922668457
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.17    |
|    critic_loss     | 0.0103   |
|    learning_rate   | 0.001    |
|    n_updates       | 268499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.03    |
|    critic_loss     | 0.00998  |
|    learning_rate   | 0.001    |
|    n_updates       | 268899   |
---------------------------------
=== Iterazione IRL 597 ===
Loss reward (iter 597): 6.5859174728393555
=== Iterazione IRL 598 ===
Loss reward (iter 598): 6.585155487060547
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.08    |
|    critic_loss     | 0.0123   |
|    learning_rate   | 0.001    |
|    n_updates       | 269399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.26    |
|    critic_loss     | 0.00963  |
|    learning_rate   | 0.001    |
|    n_updates       | 269799   |
---------------------------------
=== Iterazione IRL 599 ===
Loss reward (iter 599): 6.594563007354736
=== Iterazione IRL 600 ===
Loss reward (iter 600): 6.614449501037598
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.99    |
|    critic_loss     | 0.00996  |
|    learning_rate   | 0.001    |
|    n_updates       | 270299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.96    |
|    critic_loss     | 0.0101   |
|    learning_rate   | 0.001    |
|    n_updates       | 270699   |
---------------------------------
=== Iterazione IRL 601 ===
Loss reward (iter 601): 6.592068672180176
=== Iterazione IRL 602 ===
Loss reward (iter 602): 6.583704948425293
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.08    |
|    critic_loss     | 0.0103   |
|    learning_rate   | 0.001    |
|    n_updates       | 271199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.23    |
|    critic_loss     | 0.0103   |
|    learning_rate   | 0.001    |
|    n_updates       | 271599   |
---------------------------------
=== Iterazione IRL 603 ===
Loss reward (iter 603): 6.738703727722168
=== Iterazione IRL 604 ===
Loss reward (iter 604): 6.701314926147461
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.08    |
|    critic_loss     | 0.0111   |
|    learning_rate   | 0.001    |
|    n_updates       | 272099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.08    |
|    critic_loss     | 0.0118   |
|    learning_rate   | 0.001    |
|    n_updates       | 272499   |
---------------------------------
=== Iterazione IRL 605 ===
Loss reward (iter 605): 6.612331390380859
=== Iterazione IRL 606 ===
Loss reward (iter 606): 6.655429840087891
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.16    |
|    critic_loss     | 0.0108   |
|    learning_rate   | 0.001    |
|    n_updates       | 272999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.93    |
|    critic_loss     | 0.00863  |
|    learning_rate   | 0.001    |
|    n_updates       | 273399   |
---------------------------------
=== Iterazione IRL 607 ===
Loss reward (iter 607): 6.6713337898254395
=== Iterazione IRL 608 ===
Loss reward (iter 608): 6.613938331604004
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.05    |
|    critic_loss     | 0.0104   |
|    learning_rate   | 0.001    |
|    n_updates       | 273899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.12    |
|    critic_loss     | 0.0102   |
|    learning_rate   | 0.001    |
|    n_updates       | 274299   |
---------------------------------
=== Iterazione IRL 609 ===
Loss reward (iter 609): 6.677638053894043
=== Iterazione IRL 610 ===
Loss reward (iter 610): 6.598716735839844
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6       |
|    critic_loss     | 0.0105   |
|    learning_rate   | 0.001    |
|    n_updates       | 274799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.04    |
|    critic_loss     | 0.0116   |
|    learning_rate   | 0.001    |
|    n_updates       | 275199   |
---------------------------------
=== Iterazione IRL 611 ===
Loss reward (iter 611): 6.678093910217285
=== Iterazione IRL 612 ===
Loss reward (iter 612): 6.747912883758545
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.85    |
|    critic_loss     | 0.00964  |
|    learning_rate   | 0.001    |
|    n_updates       | 275699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.02    |
|    critic_loss     | 0.012    |
|    learning_rate   | 0.001    |
|    n_updates       | 276099   |
---------------------------------
=== Iterazione IRL 613 ===
Loss reward (iter 613): 6.555026054382324
=== Iterazione IRL 614 ===
Loss reward (iter 614): 6.802696228027344
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.88    |
|    critic_loss     | 0.0116   |
|    learning_rate   | 0.001    |
|    n_updates       | 276599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.14    |
|    critic_loss     | 0.0118   |
|    learning_rate   | 0.001    |
|    n_updates       | 276999   |
---------------------------------
=== Iterazione IRL 615 ===
Loss reward (iter 615): 6.565856456756592
=== Iterazione IRL 616 ===
Loss reward (iter 616): 6.588778972625732
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.93    |
|    critic_loss     | 0.00894  |
|    learning_rate   | 0.001    |
|    n_updates       | 277499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.9     |
|    critic_loss     | 0.0102   |
|    learning_rate   | 0.001    |
|    n_updates       | 277899   |
---------------------------------
=== Iterazione IRL 617 ===
Loss reward (iter 617): 6.547001838684082
=== Iterazione IRL 618 ===
Loss reward (iter 618): 6.676655292510986
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.03    |
|    critic_loss     | 0.0105   |
|    learning_rate   | 0.001    |
|    n_updates       | 278399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.99    |
|    critic_loss     | 0.0102   |
|    learning_rate   | 0.001    |
|    n_updates       | 278799   |
---------------------------------
=== Iterazione IRL 619 ===
Loss reward (iter 619): 6.559652328491211
=== Iterazione IRL 620 ===
Loss reward (iter 620): 6.55368185043335
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.94    |
|    critic_loss     | 0.011    |
|    learning_rate   | 0.001    |
|    n_updates       | 279299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.94    |
|    critic_loss     | 0.00891  |
|    learning_rate   | 0.001    |
|    n_updates       | 279699   |
---------------------------------
=== Iterazione IRL 621 ===
Loss reward (iter 621): 6.586910247802734
=== Iterazione IRL 622 ===
Loss reward (iter 622): 6.649919509887695
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.86    |
|    critic_loss     | 0.00943  |
|    learning_rate   | 0.001    |
|    n_updates       | 280199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.86    |
|    critic_loss     | 0.0115   |
|    learning_rate   | 0.001    |
|    n_updates       | 280599   |
---------------------------------
=== Iterazione IRL 623 ===
Loss reward (iter 623): 6.6480021476745605
=== Iterazione IRL 624 ===
Loss reward (iter 624): 6.531853199005127
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.91    |
|    critic_loss     | 0.0126   |
|    learning_rate   | 0.001    |
|    n_updates       | 281099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.09    |
|    critic_loss     | 0.00828  |
|    learning_rate   | 0.001    |
|    n_updates       | 281499   |
---------------------------------
=== Iterazione IRL 625 ===
Loss reward (iter 625): 6.596715450286865
=== Iterazione IRL 626 ===
Loss reward (iter 626): 6.604099750518799
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.77    |
|    critic_loss     | 0.00821  |
|    learning_rate   | 0.001    |
|    n_updates       | 281999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.99    |
|    critic_loss     | 0.0114   |
|    learning_rate   | 0.001    |
|    n_updates       | 282399   |
---------------------------------
=== Iterazione IRL 627 ===
Loss reward (iter 627): 6.949408054351807
=== Iterazione IRL 628 ===
Loss reward (iter 628): 6.664259910583496
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.98    |
|    critic_loss     | 0.0105   |
|    learning_rate   | 0.001    |
|    n_updates       | 282899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.09    |
|    critic_loss     | 0.00952  |
|    learning_rate   | 0.001    |
|    n_updates       | 283299   |
---------------------------------
=== Iterazione IRL 629 ===
Loss reward (iter 629): 6.611673355102539
=== Iterazione IRL 630 ===
Loss reward (iter 630): 6.550649642944336
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.8     |
|    critic_loss     | 0.015    |
|    learning_rate   | 0.001    |
|    n_updates       | 283799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.9     |
|    critic_loss     | 0.01     |
|    learning_rate   | 0.001    |
|    n_updates       | 284199   |
---------------------------------
=== Iterazione IRL 631 ===
Loss reward (iter 631): 6.683748722076416
=== Iterazione IRL 632 ===
Loss reward (iter 632): 6.611737251281738
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.04    |
|    critic_loss     | 0.0131   |
|    learning_rate   | 0.001    |
|    n_updates       | 284699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.98    |
|    critic_loss     | 0.0101   |
|    learning_rate   | 0.001    |
|    n_updates       | 285099   |
---------------------------------
=== Iterazione IRL 633 ===
Loss reward (iter 633): 6.619208335876465
=== Iterazione IRL 634 ===
Loss reward (iter 634): 6.549434185028076
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.9     |
|    critic_loss     | 0.0116   |
|    learning_rate   | 0.001    |
|    n_updates       | 285599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.72    |
|    critic_loss     | 0.0113   |
|    learning_rate   | 0.001    |
|    n_updates       | 285999   |
---------------------------------
=== Iterazione IRL 635 ===
Loss reward (iter 635): 6.5619707107543945
=== Iterazione IRL 636 ===
Loss reward (iter 636): 6.628235816955566
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.88    |
|    critic_loss     | 0.0117   |
|    learning_rate   | 0.001    |
|    n_updates       | 286499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.86    |
|    critic_loss     | 0.0106   |
|    learning_rate   | 0.001    |
|    n_updates       | 286899   |
---------------------------------
=== Iterazione IRL 637 ===
Loss reward (iter 637): 6.557098865509033
=== Iterazione IRL 638 ===
Loss reward (iter 638): 6.580072402954102
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.74    |
|    critic_loss     | 0.012    |
|    learning_rate   | 0.001    |
|    n_updates       | 287399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.78    |
|    critic_loss     | 0.00985  |
|    learning_rate   | 0.001    |
|    n_updates       | 287799   |
---------------------------------
=== Iterazione IRL 639 ===
Loss reward (iter 639): 6.596898555755615
=== Iterazione IRL 640 ===
Loss reward (iter 640): 6.587377548217773
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.56    |
|    critic_loss     | 0.0127   |
|    learning_rate   | 0.001    |
|    n_updates       | 288299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.75    |
|    critic_loss     | 0.0115   |
|    learning_rate   | 0.001    |
|    n_updates       | 288699   |
---------------------------------
=== Iterazione IRL 641 ===
Loss reward (iter 641): 6.572265625
=== Iterazione IRL 642 ===
Loss reward (iter 642): 6.532110214233398
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.84    |
|    critic_loss     | 0.0122   |
|    learning_rate   | 0.001    |
|    n_updates       | 289199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.87    |
|    critic_loss     | 0.0107   |
|    learning_rate   | 0.001    |
|    n_updates       | 289599   |
---------------------------------
=== Iterazione IRL 643 ===
Loss reward (iter 643): 6.610106945037842
=== Iterazione IRL 644 ===
Loss reward (iter 644): 6.623347282409668
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.77    |
|    critic_loss     | 0.0127   |
|    learning_rate   | 0.001    |
|    n_updates       | 290099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.74    |
|    critic_loss     | 0.0118   |
|    learning_rate   | 0.001    |
|    n_updates       | 290499   |
---------------------------------
=== Iterazione IRL 645 ===
Loss reward (iter 645): 6.5254411697387695
=== Iterazione IRL 646 ===
Loss reward (iter 646): 6.640115737915039
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.77    |
|    critic_loss     | 0.011    |
|    learning_rate   | 0.001    |
|    n_updates       | 290999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.86    |
|    critic_loss     | 0.0119   |
|    learning_rate   | 0.001    |
|    n_updates       | 291399   |
---------------------------------
=== Iterazione IRL 647 ===
Loss reward (iter 647): 6.6877593994140625
=== Iterazione IRL 648 ===
Loss reward (iter 648): 6.62661075592041
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.74    |
|    critic_loss     | 0.0127   |
|    learning_rate   | 0.001    |
|    n_updates       | 291899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.84    |
|    critic_loss     | 0.00813  |
|    learning_rate   | 0.001    |
|    n_updates       | 292299   |
---------------------------------
=== Iterazione IRL 649 ===
Loss reward (iter 649): 6.630382537841797
=== Iterazione IRL 650 ===
Loss reward (iter 650): 6.652480602264404
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.58    |
|    critic_loss     | 0.00889  |
|    learning_rate   | 0.001    |
|    n_updates       | 292799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.74    |
|    critic_loss     | 0.0115   |
|    learning_rate   | 0.001    |
|    n_updates       | 293199   |
---------------------------------
=== Iterazione IRL 651 ===
Loss reward (iter 651): 6.718190670013428
=== Iterazione IRL 652 ===
Loss reward (iter 652): 6.63015604019165
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.6     |
|    critic_loss     | 0.0123   |
|    learning_rate   | 0.001    |
|    n_updates       | 293699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.75    |
|    critic_loss     | 0.0112   |
|    learning_rate   | 0.001    |
|    n_updates       | 294099   |
---------------------------------
=== Iterazione IRL 653 ===
Loss reward (iter 653): 6.641705513000488
=== Iterazione IRL 654 ===
Loss reward (iter 654): 6.748288154602051
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.54    |
|    critic_loss     | 0.0113   |
|    learning_rate   | 0.001    |
|    n_updates       | 294599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.75    |
|    critic_loss     | 0.0115   |
|    learning_rate   | 0.001    |
|    n_updates       | 294999   |
---------------------------------
=== Iterazione IRL 655 ===
Loss reward (iter 655): 6.543318748474121
=== Iterazione IRL 656 ===
Loss reward (iter 656): 6.600830554962158
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.89    |
|    critic_loss     | 0.0105   |
|    learning_rate   | 0.001    |
|    n_updates       | 295499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.76    |
|    critic_loss     | 0.0122   |
|    learning_rate   | 0.001    |
|    n_updates       | 295899   |
---------------------------------
=== Iterazione IRL 657 ===
Loss reward (iter 657): 6.627658843994141
=== Iterazione IRL 658 ===
Loss reward (iter 658): 6.524937629699707
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.68    |
|    critic_loss     | 0.0131   |
|    learning_rate   | 0.001    |
|    n_updates       | 296399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.69    |
|    critic_loss     | 0.0104   |
|    learning_rate   | 0.001    |
|    n_updates       | 296799   |
---------------------------------
=== Iterazione IRL 659 ===
Loss reward (iter 659): 6.552340030670166
=== Iterazione IRL 660 ===
Loss reward (iter 660): 6.614469051361084
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.59    |
|    critic_loss     | 0.0108   |
|    learning_rate   | 0.001    |
|    n_updates       | 297299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.84    |
|    critic_loss     | 0.0125   |
|    learning_rate   | 0.001    |
|    n_updates       | 297699   |
---------------------------------
=== Iterazione IRL 661 ===
Loss reward (iter 661): 6.526941776275635
=== Iterazione IRL 662 ===
Loss reward (iter 662): 6.542025566101074
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.57    |
|    critic_loss     | 0.0135   |
|    learning_rate   | 0.001    |
|    n_updates       | 298199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.58    |
|    critic_loss     | 0.0113   |
|    learning_rate   | 0.001    |
|    n_updates       | 298599   |
---------------------------------
=== Iterazione IRL 663 ===
Loss reward (iter 663): 6.57676887512207
=== Iterazione IRL 664 ===
Loss reward (iter 664): 6.564170837402344
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.74    |
|    critic_loss     | 0.0128   |
|    learning_rate   | 0.001    |
|    n_updates       | 299099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.84    |
|    critic_loss     | 0.00967  |
|    learning_rate   | 0.001    |
|    n_updates       | 299499   |
---------------------------------
=== Iterazione IRL 665 ===
Loss reward (iter 665): 6.641086578369141
=== Iterazione IRL 666 ===
Loss reward (iter 666): 6.677645206451416
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.7     |
|    critic_loss     | 0.0108   |
|    learning_rate   | 0.001    |
|    n_updates       | 299999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.64    |
|    critic_loss     | 0.00987  |
|    learning_rate   | 0.001    |
|    n_updates       | 300399   |
---------------------------------
=== Iterazione IRL 667 ===
Loss reward (iter 667): 6.570856094360352
=== Iterazione IRL 668 ===
Loss reward (iter 668): 6.662915229797363
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.65    |
|    critic_loss     | 0.0137   |
|    learning_rate   | 0.001    |
|    n_updates       | 300899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.52    |
|    critic_loss     | 0.00883  |
|    learning_rate   | 0.001    |
|    n_updates       | 301299   |
---------------------------------
=== Iterazione IRL 669 ===
Loss reward (iter 669): 6.556122303009033
=== Iterazione IRL 670 ===
Loss reward (iter 670): 6.68985652923584
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 263      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.57    |
|    critic_loss     | 0.0104   |
|    learning_rate   | 0.001    |
|    n_updates       | 301799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.32    |
|    critic_loss     | 0.00979  |
|    learning_rate   | 0.001    |
|    n_updates       | 302199   |
---------------------------------
=== Iterazione IRL 671 ===
Loss reward (iter 671): 6.590464115142822
=== Iterazione IRL 672 ===
Loss reward (iter 672): 6.604353427886963
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.5     |
|    critic_loss     | 0.0127   |
|    learning_rate   | 0.001    |
|    n_updates       | 302699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.73    |
|    critic_loss     | 0.012    |
|    learning_rate   | 0.001    |
|    n_updates       | 303099   |
---------------------------------
=== Iterazione IRL 673 ===
Loss reward (iter 673): 6.622865676879883
=== Iterazione IRL 674 ===
Loss reward (iter 674): 6.548724174499512
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 263      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.41    |
|    critic_loss     | 0.0132   |
|    learning_rate   | 0.001    |
|    n_updates       | 303599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.55    |
|    critic_loss     | 0.0146   |
|    learning_rate   | 0.001    |
|    n_updates       | 303999   |
---------------------------------
=== Iterazione IRL 675 ===
Loss reward (iter 675): 6.560905456542969
=== Iterazione IRL 676 ===
Loss reward (iter 676): 6.567300796508789
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.63    |
|    critic_loss     | 0.0109   |
|    learning_rate   | 0.001    |
|    n_updates       | 304499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.6     |
|    critic_loss     | 0.0103   |
|    learning_rate   | 0.001    |
|    n_updates       | 304899   |
---------------------------------
=== Iterazione IRL 677 ===
Loss reward (iter 677): 6.6895670890808105
=== Iterazione IRL 678 ===
Loss reward (iter 678): 6.602011680603027
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.69    |
|    critic_loss     | 0.00936  |
|    learning_rate   | 0.001    |
|    n_updates       | 305399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.39    |
|    critic_loss     | 0.0109   |
|    learning_rate   | 0.001    |
|    n_updates       | 305799   |
---------------------------------
=== Iterazione IRL 679 ===
Loss reward (iter 679): 6.735982894897461
=== Iterazione IRL 680 ===
Loss reward (iter 680): 6.553126811981201
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.37    |
|    critic_loss     | 0.0115   |
|    learning_rate   | 0.001    |
|    n_updates       | 306299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.48    |
|    critic_loss     | 0.0107   |
|    learning_rate   | 0.001    |
|    n_updates       | 306699   |
---------------------------------
=== Iterazione IRL 681 ===
Loss reward (iter 681): 6.60087251663208
=== Iterazione IRL 682 ===
Loss reward (iter 682): 6.6372175216674805
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.54    |
|    critic_loss     | 0.0108   |
|    learning_rate   | 0.001    |
|    n_updates       | 307199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.35    |
|    critic_loss     | 0.0118   |
|    learning_rate   | 0.001    |
|    n_updates       | 307599   |
---------------------------------
=== Iterazione IRL 683 ===
Loss reward (iter 683): 6.5595550537109375
=== Iterazione IRL 684 ===
Loss reward (iter 684): 6.575350761413574
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.43    |
|    critic_loss     | 0.0112   |
|    learning_rate   | 0.001    |
|    n_updates       | 308099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.64    |
|    critic_loss     | 0.0112   |
|    learning_rate   | 0.001    |
|    n_updates       | 308499   |
---------------------------------
=== Iterazione IRL 685 ===
Loss reward (iter 685): 6.587489128112793
=== Iterazione IRL 686 ===
Loss reward (iter 686): 6.633075714111328
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.42    |
|    critic_loss     | 0.0119   |
|    learning_rate   | 0.001    |
|    n_updates       | 308999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.44    |
|    critic_loss     | 0.00801  |
|    learning_rate   | 0.001    |
|    n_updates       | 309399   |
---------------------------------
=== Iterazione IRL 687 ===
Loss reward (iter 687): 6.654740810394287
=== Iterazione IRL 688 ===
Loss reward (iter 688): 6.5396528244018555
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.5     |
|    critic_loss     | 0.0135   |
|    learning_rate   | 0.001    |
|    n_updates       | 309899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.37    |
|    critic_loss     | 0.0118   |
|    learning_rate   | 0.001    |
|    n_updates       | 310299   |
---------------------------------
=== Iterazione IRL 689 ===
Loss reward (iter 689): 6.622864723205566
=== Iterazione IRL 690 ===
Loss reward (iter 690): 6.570680618286133
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.36    |
|    critic_loss     | 0.00923  |
|    learning_rate   | 0.001    |
|    n_updates       | 310799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.57    |
|    critic_loss     | 0.00956  |
|    learning_rate   | 0.001    |
|    n_updates       | 311199   |
---------------------------------
=== Iterazione IRL 691 ===
Loss reward (iter 691): 6.638902187347412
=== Iterazione IRL 692 ===
Loss reward (iter 692): 6.521551609039307
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 263      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.22    |
|    critic_loss     | 0.0103   |
|    learning_rate   | 0.001    |
|    n_updates       | 311699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.56    |
|    critic_loss     | 0.00848  |
|    learning_rate   | 0.001    |
|    n_updates       | 312099   |
---------------------------------
=== Iterazione IRL 693 ===
Loss reward (iter 693): 6.622368335723877
=== Iterazione IRL 694 ===
Loss reward (iter 694): 6.593684196472168
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 263      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.3     |
|    critic_loss     | 0.0145   |
|    learning_rate   | 0.001    |
|    n_updates       | 312599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.34    |
|    critic_loss     | 0.0113   |
|    learning_rate   | 0.001    |
|    n_updates       | 312999   |
---------------------------------
=== Iterazione IRL 695 ===
Loss reward (iter 695): 6.606403350830078
=== Iterazione IRL 696 ===
Loss reward (iter 696): 6.633510589599609
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.37    |
|    critic_loss     | 0.0108   |
|    learning_rate   | 0.001    |
|    n_updates       | 313499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.53    |
|    critic_loss     | 0.0114   |
|    learning_rate   | 0.001    |
|    n_updates       | 313899   |
---------------------------------
=== Iterazione IRL 697 ===
Loss reward (iter 697): 6.5511651039123535
=== Iterazione IRL 698 ===
Loss reward (iter 698): 6.5567402839660645
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.34    |
|    critic_loss     | 0.00892  |
|    learning_rate   | 0.001    |
|    n_updates       | 314399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.51    |
|    critic_loss     | 0.0111   |
|    learning_rate   | 0.001    |
|    n_updates       | 314799   |
---------------------------------
=== Iterazione IRL 699 ===
Loss reward (iter 699): 6.567080974578857
=== Iterazione IRL 700 ===
Loss reward (iter 700): 6.620420932769775
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.6     |
|    critic_loss     | 0.0116   |
|    learning_rate   | 0.001    |
|    n_updates       | 315299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.6     |
|    critic_loss     | 0.0133   |
|    learning_rate   | 0.001    |
|    n_updates       | 315699   |
---------------------------------
=== Iterazione IRL 701 ===
Loss reward (iter 701): 6.539188861846924
=== Iterazione IRL 702 ===
Loss reward (iter 702): 6.675740718841553
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.52    |
|    critic_loss     | 0.0118   |
|    learning_rate   | 0.001    |
|    n_updates       | 316199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.31    |
|    critic_loss     | 0.0101   |
|    learning_rate   | 0.001    |
|    n_updates       | 316599   |
---------------------------------
=== Iterazione IRL 703 ===
Loss reward (iter 703): 6.629459857940674
=== Iterazione IRL 704 ===
Loss reward (iter 704): 6.578518867492676
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.49    |
|    critic_loss     | 0.0103   |
|    learning_rate   | 0.001    |
|    n_updates       | 317099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.36    |
|    critic_loss     | 0.0113   |
|    learning_rate   | 0.001    |
|    n_updates       | 317499   |
---------------------------------
=== Iterazione IRL 705 ===
Loss reward (iter 705): 6.596673011779785
=== Iterazione IRL 706 ===
Loss reward (iter 706): 6.570679664611816
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.4     |
|    critic_loss     | 0.0105   |
|    learning_rate   | 0.001    |
|    n_updates       | 317999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.4     |
|    critic_loss     | 0.0111   |
|    learning_rate   | 0.001    |
|    n_updates       | 318399   |
---------------------------------
=== Iterazione IRL 707 ===
Loss reward (iter 707): 6.572478771209717
=== Iterazione IRL 708 ===
Loss reward (iter 708): 6.566864490509033
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.29    |
|    critic_loss     | 0.0135   |
|    learning_rate   | 0.001    |
|    n_updates       | 318899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.39    |
|    critic_loss     | 0.0111   |
|    learning_rate   | 0.001    |
|    n_updates       | 319299   |
---------------------------------
=== Iterazione IRL 709 ===
Loss reward (iter 709): 6.557607650756836
=== Iterazione IRL 710 ===
Loss reward (iter 710): 6.727142333984375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.31    |
|    critic_loss     | 0.0131   |
|    learning_rate   | 0.001    |
|    n_updates       | 319799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.35    |
|    critic_loss     | 0.0127   |
|    learning_rate   | 0.001    |
|    n_updates       | 320199   |
---------------------------------
=== Iterazione IRL 711 ===
Loss reward (iter 711): 6.579782485961914
=== Iterazione IRL 712 ===
Loss reward (iter 712): 6.738801002502441
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.32    |
|    critic_loss     | 0.0123   |
|    learning_rate   | 0.001    |
|    n_updates       | 320699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.31    |
|    critic_loss     | 0.0126   |
|    learning_rate   | 0.001    |
|    n_updates       | 321099   |
---------------------------------
=== Iterazione IRL 713 ===
Loss reward (iter 713): 6.741870880126953
=== Iterazione IRL 714 ===
Loss reward (iter 714): 6.556314945220947
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.4     |
|    critic_loss     | 0.0116   |
|    learning_rate   | 0.001    |
|    n_updates       | 321599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.02    |
|    critic_loss     | 0.0104   |
|    learning_rate   | 0.001    |
|    n_updates       | 321999   |
---------------------------------
=== Iterazione IRL 715 ===
Loss reward (iter 715): 6.548833847045898
=== Iterazione IRL 716 ===
Loss reward (iter 716): 6.72767448425293
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.35    |
|    critic_loss     | 0.00951  |
|    learning_rate   | 0.001    |
|    n_updates       | 322499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.36    |
|    critic_loss     | 0.0121   |
|    learning_rate   | 0.001    |
|    n_updates       | 322899   |
---------------------------------
=== Iterazione IRL 717 ===
Loss reward (iter 717): 6.631424903869629
=== Iterazione IRL 718 ===
Loss reward (iter 718): 6.550827980041504
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.33    |
|    critic_loss     | 0.0126   |
|    learning_rate   | 0.001    |
|    n_updates       | 323399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.17    |
|    critic_loss     | 0.0119   |
|    learning_rate   | 0.001    |
|    n_updates       | 323799   |
---------------------------------
=== Iterazione IRL 719 ===
Loss reward (iter 719): 6.553974151611328
=== Iterazione IRL 720 ===
Loss reward (iter 720): 6.5632195472717285
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.21    |
|    critic_loss     | 0.0102   |
|    learning_rate   | 0.001    |
|    n_updates       | 324299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.4     |
|    critic_loss     | 0.0102   |
|    learning_rate   | 0.001    |
|    n_updates       | 324699   |
---------------------------------
=== Iterazione IRL 721 ===
Loss reward (iter 721): 6.569573402404785
=== Iterazione IRL 722 ===
Loss reward (iter 722): 6.656946659088135
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.45    |
|    critic_loss     | 0.012    |
|    learning_rate   | 0.001    |
|    n_updates       | 325199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.33    |
|    critic_loss     | 0.0121   |
|    learning_rate   | 0.001    |
|    n_updates       | 325599   |
---------------------------------
=== Iterazione IRL 723 ===
Loss reward (iter 723): 6.649637699127197
=== Iterazione IRL 724 ===
Loss reward (iter 724): 6.548929214477539
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 263      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.11    |
|    critic_loss     | 0.0124   |
|    learning_rate   | 0.001    |
|    n_updates       | 326099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.23    |
|    critic_loss     | 0.0128   |
|    learning_rate   | 0.001    |
|    n_updates       | 326499   |
---------------------------------
=== Iterazione IRL 725 ===
Loss reward (iter 725): 6.576192378997803
=== Iterazione IRL 726 ===
Loss reward (iter 726): 6.609551906585693
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.28    |
|    critic_loss     | 0.0129   |
|    learning_rate   | 0.001    |
|    n_updates       | 326999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.25    |
|    critic_loss     | 0.0158   |
|    learning_rate   | 0.001    |
|    n_updates       | 327399   |
---------------------------------
=== Iterazione IRL 727 ===
Loss reward (iter 727): 6.6258111000061035
=== Iterazione IRL 728 ===
Loss reward (iter 728): 6.633892059326172
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.41    |
|    critic_loss     | 0.0126   |
|    learning_rate   | 0.001    |
|    n_updates       | 327899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.3     |
|    critic_loss     | 0.0125   |
|    learning_rate   | 0.001    |
|    n_updates       | 328299   |
---------------------------------
=== Iterazione IRL 729 ===
Loss reward (iter 729): 6.533044815063477
=== Iterazione IRL 730 ===
Loss reward (iter 730): 6.57592248916626
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.05    |
|    critic_loss     | 0.0158   |
|    learning_rate   | 0.001    |
|    n_updates       | 328799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.15    |
|    critic_loss     | 0.0132   |
|    learning_rate   | 0.001    |
|    n_updates       | 329199   |
---------------------------------
=== Iterazione IRL 731 ===
Loss reward (iter 731): 6.551111221313477
=== Iterazione IRL 732 ===
Loss reward (iter 732): 6.590402603149414
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.35    |
|    critic_loss     | 0.0126   |
|    learning_rate   | 0.001    |
|    n_updates       | 329699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.9     |
|    critic_loss     | 0.0135   |
|    learning_rate   | 0.001    |
|    n_updates       | 330099   |
---------------------------------
=== Iterazione IRL 733 ===
Loss reward (iter 733): 6.545961380004883
=== Iterazione IRL 734 ===
Loss reward (iter 734): 6.540161609649658
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.14    |
|    critic_loss     | 0.0123   |
|    learning_rate   | 0.001    |
|    n_updates       | 330599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.95    |
|    critic_loss     | 0.0132   |
|    learning_rate   | 0.001    |
|    n_updates       | 330999   |
---------------------------------
=== Iterazione IRL 735 ===
Loss reward (iter 735): 6.6088128089904785
=== Iterazione IRL 736 ===
Loss reward (iter 736): 6.65866756439209
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 263      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.05    |
|    critic_loss     | 0.00948  |
|    learning_rate   | 0.001    |
|    n_updates       | 331499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.17    |
|    critic_loss     | 0.0128   |
|    learning_rate   | 0.001    |
|    n_updates       | 331899   |
---------------------------------
=== Iterazione IRL 737 ===
Loss reward (iter 737): 6.547474384307861
=== Iterazione IRL 738 ===
Loss reward (iter 738): 6.531642913818359
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.04    |
|    critic_loss     | 0.0144   |
|    learning_rate   | 0.001    |
|    n_updates       | 332399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.16    |
|    critic_loss     | 0.013    |
|    learning_rate   | 0.001    |
|    n_updates       | 332799   |
---------------------------------
=== Iterazione IRL 739 ===
Loss reward (iter 739): 6.528197288513184
=== Iterazione IRL 740 ===
Loss reward (iter 740): 6.606667518615723
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.14    |
|    critic_loss     | 0.0106   |
|    learning_rate   | 0.001    |
|    n_updates       | 333299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.04    |
|    critic_loss     | 0.0155   |
|    learning_rate   | 0.001    |
|    n_updates       | 333699   |
---------------------------------
=== Iterazione IRL 741 ===
Loss reward (iter 741): 6.544447898864746
=== Iterazione IRL 742 ===
Loss reward (iter 742): 6.683313846588135
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.18    |
|    critic_loss     | 0.0121   |
|    learning_rate   | 0.001    |
|    n_updates       | 334199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.93    |
|    critic_loss     | 0.0135   |
|    learning_rate   | 0.001    |
|    n_updates       | 334599   |
---------------------------------
=== Iterazione IRL 743 ===
Loss reward (iter 743): 6.530320644378662
=== Iterazione IRL 744 ===
Loss reward (iter 744): 6.494152069091797
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.06    |
|    critic_loss     | 0.0105   |
|    learning_rate   | 0.001    |
|    n_updates       | 335099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.01    |
|    critic_loss     | 0.0126   |
|    learning_rate   | 0.001    |
|    n_updates       | 335499   |
---------------------------------
=== Iterazione IRL 745 ===
Loss reward (iter 745): 6.566786289215088
=== Iterazione IRL 746 ===
Loss reward (iter 746): 6.5181965827941895
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.15    |
|    critic_loss     | 0.0152   |
|    learning_rate   | 0.001    |
|    n_updates       | 335999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.16    |
|    critic_loss     | 0.0132   |
|    learning_rate   | 0.001    |
|    n_updates       | 336399   |
---------------------------------
=== Iterazione IRL 747 ===
Loss reward (iter 747): 6.716854095458984
=== Iterazione IRL 748 ===
Loss reward (iter 748): 6.514937877655029
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.94    |
|    critic_loss     | 0.0125   |
|    learning_rate   | 0.001    |
|    n_updates       | 336899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.99    |
|    critic_loss     | 0.0145   |
|    learning_rate   | 0.001    |
|    n_updates       | 337299   |
---------------------------------
=== Iterazione IRL 749 ===
Loss reward (iter 749): 6.57698392868042
=== Iterazione IRL 750 ===
Loss reward (iter 750): 6.6349778175354
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.01    |
|    critic_loss     | 0.0108   |
|    learning_rate   | 0.001    |
|    n_updates       | 337799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.99    |
|    critic_loss     | 0.0109   |
|    learning_rate   | 0.001    |
|    n_updates       | 338199   |
---------------------------------
=== Iterazione IRL 751 ===
Loss reward (iter 751): 6.612807750701904
=== Iterazione IRL 752 ===
Loss reward (iter 752): 6.578843593597412
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.04    |
|    critic_loss     | 0.0151   |
|    learning_rate   | 0.001    |
|    n_updates       | 338699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.1     |
|    critic_loss     | 0.0107   |
|    learning_rate   | 0.001    |
|    n_updates       | 339099   |
---------------------------------
=== Iterazione IRL 753 ===
Loss reward (iter 753): 6.6529154777526855
=== Iterazione IRL 754 ===
Loss reward (iter 754): 6.604684829711914
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.9     |
|    critic_loss     | 0.0137   |
|    learning_rate   | 0.001    |
|    n_updates       | 339599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.14    |
|    critic_loss     | 0.0147   |
|    learning_rate   | 0.001    |
|    n_updates       | 339999   |
---------------------------------
=== Iterazione IRL 755 ===
Loss reward (iter 755): 6.591187477111816
=== Iterazione IRL 756 ===
Loss reward (iter 756): 6.541251182556152
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.94    |
|    critic_loss     | 0.013    |
|    learning_rate   | 0.001    |
|    n_updates       | 340499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.03    |
|    critic_loss     | 0.0119   |
|    learning_rate   | 0.001    |
|    n_updates       | 340899   |
---------------------------------
=== Iterazione IRL 757 ===
Loss reward (iter 757): 6.704895973205566
=== Iterazione IRL 758 ===
Loss reward (iter 758): 6.607884407043457
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.84    |
|    critic_loss     | 0.0142   |
|    learning_rate   | 0.001    |
|    n_updates       | 341399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5       |
|    critic_loss     | 0.0131   |
|    learning_rate   | 0.001    |
|    n_updates       | 341799   |
---------------------------------
=== Iterazione IRL 759 ===
Loss reward (iter 759): 6.631504058837891
=== Iterazione IRL 760 ===
Loss reward (iter 760): 6.508373260498047
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.04    |
|    critic_loss     | 0.0131   |
|    learning_rate   | 0.001    |
|    n_updates       | 342299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5       |
|    critic_loss     | 0.0112   |
|    learning_rate   | 0.001    |
|    n_updates       | 342699   |
---------------------------------
=== Iterazione IRL 761 ===
Loss reward (iter 761): 6.561763286590576
=== Iterazione IRL 762 ===
Loss reward (iter 762): 6.592552661895752
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.99    |
|    critic_loss     | 0.0114   |
|    learning_rate   | 0.001    |
|    n_updates       | 343199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.8     |
|    critic_loss     | 0.0111   |
|    learning_rate   | 0.001    |
|    n_updates       | 343599   |
---------------------------------
=== Iterazione IRL 763 ===
Loss reward (iter 763): 6.595856189727783
=== Iterazione IRL 764 ===
Loss reward (iter 764): 6.538914680480957
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.88    |
|    critic_loss     | 0.0131   |
|    learning_rate   | 0.001    |
|    n_updates       | 344099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.92    |
|    critic_loss     | 0.0113   |
|    learning_rate   | 0.001    |
|    n_updates       | 344499   |
---------------------------------
=== Iterazione IRL 765 ===
Loss reward (iter 765): 6.537092208862305
=== Iterazione IRL 766 ===
Loss reward (iter 766): 6.6228837966918945
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.79    |
|    critic_loss     | 0.0147   |
|    learning_rate   | 0.001    |
|    n_updates       | 344999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.9     |
|    critic_loss     | 0.0122   |
|    learning_rate   | 0.001    |
|    n_updates       | 345399   |
---------------------------------
=== Iterazione IRL 767 ===
Loss reward (iter 767): 6.8374176025390625
=== Iterazione IRL 768 ===
Loss reward (iter 768): 6.62039041519165
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.94    |
|    critic_loss     | 0.011    |
|    learning_rate   | 0.001    |
|    n_updates       | 345899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.72    |
|    critic_loss     | 0.00906  |
|    learning_rate   | 0.001    |
|    n_updates       | 346299   |
---------------------------------
=== Iterazione IRL 769 ===
Loss reward (iter 769): 6.5638508796691895
=== Iterazione IRL 770 ===
Loss reward (iter 770): 6.543614864349365
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.03    |
|    critic_loss     | 0.0166   |
|    learning_rate   | 0.001    |
|    n_updates       | 346799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.73    |
|    critic_loss     | 0.0141   |
|    learning_rate   | 0.001    |
|    n_updates       | 347199   |
---------------------------------
=== Iterazione IRL 771 ===
Loss reward (iter 771): 6.593312740325928
=== Iterazione IRL 772 ===
Loss reward (iter 772): 6.5354695320129395
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.93    |
|    critic_loss     | 0.0105   |
|    learning_rate   | 0.001    |
|    n_updates       | 347699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.03    |
|    critic_loss     | 0.0105   |
|    learning_rate   | 0.001    |
|    n_updates       | 348099   |
---------------------------------
=== Iterazione IRL 773 ===
Loss reward (iter 773): 6.55758810043335
=== Iterazione IRL 774 ===
Loss reward (iter 774): 6.5689473152160645
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5       |
|    critic_loss     | 0.0151   |
|    learning_rate   | 0.001    |
|    n_updates       | 348599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.92    |
|    critic_loss     | 0.0102   |
|    learning_rate   | 0.001    |
|    n_updates       | 348999   |
---------------------------------
=== Iterazione IRL 775 ===
Loss reward (iter 775): 6.606383323669434
=== Iterazione IRL 776 ===
Loss reward (iter 776): 6.558675765991211
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.85    |
|    critic_loss     | 0.0114   |
|    learning_rate   | 0.001    |
|    n_updates       | 349499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.75    |
|    critic_loss     | 0.0124   |
|    learning_rate   | 0.001    |
|    n_updates       | 349899   |
---------------------------------
=== Iterazione IRL 777 ===
Loss reward (iter 777): 6.56365442276001
=== Iterazione IRL 778 ===
Loss reward (iter 778): 6.618247032165527
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.8     |
|    critic_loss     | 0.0121   |
|    learning_rate   | 0.001    |
|    n_updates       | 350399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.78    |
|    critic_loss     | 0.0124   |
|    learning_rate   | 0.001    |
|    n_updates       | 350799   |
---------------------------------
=== Iterazione IRL 779 ===
Loss reward (iter 779): 6.651815414428711
=== Iterazione IRL 780 ===
Loss reward (iter 780): 6.724812030792236
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.71    |
|    critic_loss     | 0.0142   |
|    learning_rate   | 0.001    |
|    n_updates       | 351299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.78    |
|    critic_loss     | 0.013    |
|    learning_rate   | 0.001    |
|    n_updates       | 351699   |
---------------------------------
=== Iterazione IRL 781 ===
Loss reward (iter 781): 6.620716571807861
=== Iterazione IRL 782 ===
Loss reward (iter 782): 6.5826568603515625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.81    |
|    critic_loss     | 0.0101   |
|    learning_rate   | 0.001    |
|    n_updates       | 352199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.74    |
|    critic_loss     | 0.0119   |
|    learning_rate   | 0.001    |
|    n_updates       | 352599   |
---------------------------------
=== Iterazione IRL 783 ===
Loss reward (iter 783): 6.632885456085205
=== Iterazione IRL 784 ===
Loss reward (iter 784): 6.6160383224487305
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.76    |
|    critic_loss     | 0.0133   |
|    learning_rate   | 0.001    |
|    n_updates       | 353099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.87    |
|    critic_loss     | 0.0117   |
|    learning_rate   | 0.001    |
|    n_updates       | 353499   |
---------------------------------
=== Iterazione IRL 785 ===
Loss reward (iter 785): 6.538072109222412
=== Iterazione IRL 786 ===
Loss reward (iter 786): 6.701171398162842
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.84    |
|    critic_loss     | 0.0115   |
|    learning_rate   | 0.001    |
|    n_updates       | 353999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.66    |
|    critic_loss     | 0.02     |
|    learning_rate   | 0.001    |
|    n_updates       | 354399   |
---------------------------------
=== Iterazione IRL 787 ===
Loss reward (iter 787): 6.583039283752441
=== Iterazione IRL 788 ===
Loss reward (iter 788): 6.665457725524902
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.76    |
|    critic_loss     | 0.0114   |
|    learning_rate   | 0.001    |
|    n_updates       | 354899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.86    |
|    critic_loss     | 0.0151   |
|    learning_rate   | 0.001    |
|    n_updates       | 355299   |
---------------------------------
=== Iterazione IRL 789 ===
Loss reward (iter 789): 6.512694358825684
=== Iterazione IRL 790 ===
Loss reward (iter 790): 6.637767791748047
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.62    |
|    critic_loss     | 0.0129   |
|    learning_rate   | 0.001    |
|    n_updates       | 355799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.61    |
|    critic_loss     | 0.0101   |
|    learning_rate   | 0.001    |
|    n_updates       | 356199   |
---------------------------------
=== Iterazione IRL 791 ===
Loss reward (iter 791): 6.497435092926025
=== Iterazione IRL 792 ===
Loss reward (iter 792): 6.517215728759766
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.7     |
|    critic_loss     | 0.0143   |
|    learning_rate   | 0.001    |
|    n_updates       | 356699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.78    |
|    critic_loss     | 0.015    |
|    learning_rate   | 0.001    |
|    n_updates       | 357099   |
---------------------------------
=== Iterazione IRL 793 ===
Loss reward (iter 793): 6.5638885498046875
=== Iterazione IRL 794 ===
Loss reward (iter 794): 6.620808124542236
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.55    |
|    critic_loss     | 0.00968  |
|    learning_rate   | 0.001    |
|    n_updates       | 357599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.62    |
|    critic_loss     | 0.0138   |
|    learning_rate   | 0.001    |
|    n_updates       | 357999   |
---------------------------------
=== Iterazione IRL 795 ===
Loss reward (iter 795): 6.570988178253174
=== Iterazione IRL 796 ===
Loss reward (iter 796): 6.573880672454834
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.63    |
|    critic_loss     | 0.0106   |
|    learning_rate   | 0.001    |
|    n_updates       | 358499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.74    |
|    critic_loss     | 0.0128   |
|    learning_rate   | 0.001    |
|    n_updates       | 358899   |
---------------------------------
=== Iterazione IRL 797 ===
Loss reward (iter 797): 6.619546890258789
=== Iterazione IRL 798 ===
Loss reward (iter 798): 6.681466102600098
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.47    |
|    critic_loss     | 0.0146   |
|    learning_rate   | 0.001    |
|    n_updates       | 359399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.65    |
|    critic_loss     | 0.0109   |
|    learning_rate   | 0.001    |
|    n_updates       | 359799   |
---------------------------------
=== Iterazione IRL 799 ===
Loss reward (iter 799): 6.583277225494385
=== Iterazione IRL 800 ===
Loss reward (iter 800): 6.589776039123535
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.83    |
|    critic_loss     | 0.0138   |
|    learning_rate   | 0.001    |
|    n_updates       | 360299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.68    |
|    critic_loss     | 0.0117   |
|    learning_rate   | 0.001    |
|    n_updates       | 360699   |
---------------------------------
=== Iterazione IRL 801 ===
Loss reward (iter 801): 6.648190021514893
=== Iterazione IRL 802 ===
Loss reward (iter 802): 6.61082124710083
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.51    |
|    critic_loss     | 0.0128   |
|    learning_rate   | 0.001    |
|    n_updates       | 361199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.62    |
|    critic_loss     | 0.0146   |
|    learning_rate   | 0.001    |
|    n_updates       | 361599   |
---------------------------------
=== Iterazione IRL 803 ===
Loss reward (iter 803): 6.623430252075195
=== Iterazione IRL 804 ===
Loss reward (iter 804): 6.5688042640686035
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.45    |
|    critic_loss     | 0.0114   |
|    learning_rate   | 0.001    |
|    n_updates       | 362099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.34    |
|    critic_loss     | 0.0126   |
|    learning_rate   | 0.001    |
|    n_updates       | 362499   |
---------------------------------
=== Iterazione IRL 805 ===
Loss reward (iter 805): 6.561221599578857
=== Iterazione IRL 806 ===
Loss reward (iter 806): 6.546737194061279
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.49    |
|    critic_loss     | 0.0126   |
|    learning_rate   | 0.001    |
|    n_updates       | 362999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.6     |
|    critic_loss     | 0.016    |
|    learning_rate   | 0.001    |
|    n_updates       | 363399   |
---------------------------------
=== Iterazione IRL 807 ===
Loss reward (iter 807): 6.599864959716797
=== Iterazione IRL 808 ===
Loss reward (iter 808): 6.546449661254883
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.46    |
|    critic_loss     | 0.0116   |
|    learning_rate   | 0.001    |
|    n_updates       | 363899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.52    |
|    critic_loss     | 0.0114   |
|    learning_rate   | 0.001    |
|    n_updates       | 364299   |
---------------------------------
=== Iterazione IRL 809 ===
Loss reward (iter 809): 6.619577407836914
=== Iterazione IRL 810 ===
Loss reward (iter 810): 6.670215129852295
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.44    |
|    critic_loss     | 0.0194   |
|    learning_rate   | 0.001    |
|    n_updates       | 364799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.73    |
|    critic_loss     | 0.014    |
|    learning_rate   | 0.001    |
|    n_updates       | 365199   |
---------------------------------
=== Iterazione IRL 811 ===
Loss reward (iter 811): 6.597368240356445
=== Iterazione IRL 812 ===
Loss reward (iter 812): 6.584468841552734
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.32    |
|    critic_loss     | 0.0141   |
|    learning_rate   | 0.001    |
|    n_updates       | 365699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.47    |
|    critic_loss     | 0.0122   |
|    learning_rate   | 0.001    |
|    n_updates       | 366099   |
---------------------------------
=== Iterazione IRL 813 ===
Loss reward (iter 813): 6.612802982330322
=== Iterazione IRL 814 ===
Loss reward (iter 814): 6.552867412567139
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.48    |
|    critic_loss     | 0.0143   |
|    learning_rate   | 0.001    |
|    n_updates       | 366599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.6     |
|    critic_loss     | 0.018    |
|    learning_rate   | 0.001    |
|    n_updates       | 366999   |
---------------------------------
=== Iterazione IRL 815 ===
Loss reward (iter 815): 6.5668416023254395
=== Iterazione IRL 816 ===
Loss reward (iter 816): 6.582316875457764
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.59    |
|    critic_loss     | 0.0153   |
|    learning_rate   | 0.001    |
|    n_updates       | 367499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.59    |
|    critic_loss     | 0.0129   |
|    learning_rate   | 0.001    |
|    n_updates       | 367899   |
---------------------------------
=== Iterazione IRL 817 ===
Loss reward (iter 817): 6.538781642913818
=== Iterazione IRL 818 ===
Loss reward (iter 818): 6.589439392089844
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.49    |
|    critic_loss     | 0.0157   |
|    learning_rate   | 0.001    |
|    n_updates       | 368399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.44    |
|    critic_loss     | 0.0177   |
|    learning_rate   | 0.001    |
|    n_updates       | 368799   |
---------------------------------
=== Iterazione IRL 819 ===
Loss reward (iter 819): 6.681907653808594
=== Iterazione IRL 820 ===
Loss reward (iter 820): 6.622229099273682
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.6     |
|    critic_loss     | 0.012    |
|    learning_rate   | 0.001    |
|    n_updates       | 369299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.4     |
|    critic_loss     | 0.017    |
|    learning_rate   | 0.001    |
|    n_updates       | 369699   |
---------------------------------
=== Iterazione IRL 821 ===
Loss reward (iter 821): 6.655383110046387
=== Iterazione IRL 822 ===
Loss reward (iter 822): 6.595546245574951
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.55    |
|    critic_loss     | 0.0126   |
|    learning_rate   | 0.001    |
|    n_updates       | 370199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.26    |
|    critic_loss     | 0.0138   |
|    learning_rate   | 0.001    |
|    n_updates       | 370599   |
---------------------------------
=== Iterazione IRL 823 ===
Loss reward (iter 823): 6.550899028778076
=== Iterazione IRL 824 ===
Loss reward (iter 824): 6.559141635894775
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.49    |
|    critic_loss     | 0.0148   |
|    learning_rate   | 0.001    |
|    n_updates       | 371099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.43    |
|    critic_loss     | 0.0157   |
|    learning_rate   | 0.001    |
|    n_updates       | 371499   |
---------------------------------
=== Iterazione IRL 825 ===
Loss reward (iter 825): 6.603567600250244
=== Iterazione IRL 826 ===
Loss reward (iter 826): 6.58961820602417
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.41    |
|    critic_loss     | 0.0157   |
|    learning_rate   | 0.001    |
|    n_updates       | 371999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.56    |
|    critic_loss     | 0.0137   |
|    learning_rate   | 0.001    |
|    n_updates       | 372399   |
---------------------------------
=== Iterazione IRL 827 ===
Loss reward (iter 827): 6.578774929046631
=== Iterazione IRL 828 ===
Loss reward (iter 828): 6.572401523590088
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.35    |
|    critic_loss     | 0.0101   |
|    learning_rate   | 0.001    |
|    n_updates       | 372899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.43    |
|    critic_loss     | 0.0158   |
|    learning_rate   | 0.001    |
|    n_updates       | 373299   |
---------------------------------
=== Iterazione IRL 829 ===
Loss reward (iter 829): 6.638209819793701
=== Iterazione IRL 830 ===
Loss reward (iter 830): 6.641927719116211
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.46    |
|    critic_loss     | 0.015    |
|    learning_rate   | 0.001    |
|    n_updates       | 373799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.41    |
|    critic_loss     | 0.0126   |
|    learning_rate   | 0.001    |
|    n_updates       | 374199   |
---------------------------------
=== Iterazione IRL 831 ===
Loss reward (iter 831): 6.582152366638184
=== Iterazione IRL 832 ===
Loss reward (iter 832): 6.6168718338012695
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.39    |
|    critic_loss     | 0.0158   |
|    learning_rate   | 0.001    |
|    n_updates       | 374699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.2     |
|    critic_loss     | 0.0107   |
|    learning_rate   | 0.001    |
|    n_updates       | 375099   |
---------------------------------
=== Iterazione IRL 833 ===
Loss reward (iter 833): 6.617583274841309
=== Iterazione IRL 834 ===
Loss reward (iter 834): 6.5195231437683105
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.15    |
|    critic_loss     | 0.012    |
|    learning_rate   | 0.001    |
|    n_updates       | 375599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.53    |
|    critic_loss     | 0.0187   |
|    learning_rate   | 0.001    |
|    n_updates       | 375999   |
---------------------------------
=== Iterazione IRL 835 ===
Loss reward (iter 835): 6.603095054626465
=== Iterazione IRL 836 ===
Loss reward (iter 836): 6.586966514587402
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.34    |
|    critic_loss     | 0.0134   |
|    learning_rate   | 0.001    |
|    n_updates       | 376499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.5     |
|    critic_loss     | 0.0111   |
|    learning_rate   | 0.001    |
|    n_updates       | 376899   |
---------------------------------
=== Iterazione IRL 837 ===
Loss reward (iter 837): 6.519268989562988
=== Iterazione IRL 838 ===
Loss reward (iter 838): 6.6498703956604
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.52    |
|    critic_loss     | 0.0141   |
|    learning_rate   | 0.001    |
|    n_updates       | 377399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.19    |
|    critic_loss     | 0.0131   |
|    learning_rate   | 0.001    |
|    n_updates       | 377799   |
---------------------------------
=== Iterazione IRL 839 ===
Loss reward (iter 839): 6.558078765869141
=== Iterazione IRL 840 ===
Loss reward (iter 840): 6.559574127197266
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.47    |
|    critic_loss     | 0.0108   |
|    learning_rate   | 0.001    |
|    n_updates       | 378299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.45    |
|    critic_loss     | 0.0142   |
|    learning_rate   | 0.001    |
|    n_updates       | 378699   |
---------------------------------
=== Iterazione IRL 841 ===
Loss reward (iter 841): 6.574351787567139
=== Iterazione IRL 842 ===
Loss reward (iter 842): 6.523932456970215
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.23    |
|    critic_loss     | 0.0141   |
|    learning_rate   | 0.001    |
|    n_updates       | 379199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.31    |
|    critic_loss     | 0.0125   |
|    learning_rate   | 0.001    |
|    n_updates       | 379599   |
---------------------------------
=== Iterazione IRL 843 ===
Loss reward (iter 843): 6.570711135864258
=== Iterazione IRL 844 ===
Loss reward (iter 844): 6.537741661071777
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.19    |
|    critic_loss     | 0.0118   |
|    learning_rate   | 0.001    |
|    n_updates       | 380099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.29    |
|    critic_loss     | 0.0158   |
|    learning_rate   | 0.001    |
|    n_updates       | 380499   |
---------------------------------
=== Iterazione IRL 845 ===
Loss reward (iter 845): 6.51566743850708
=== Iterazione IRL 846 ===
Loss reward (iter 846): 6.574123382568359
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.38    |
|    critic_loss     | 0.0136   |
|    learning_rate   | 0.001    |
|    n_updates       | 380999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.28    |
|    critic_loss     | 0.0134   |
|    learning_rate   | 0.001    |
|    n_updates       | 381399   |
---------------------------------
=== Iterazione IRL 847 ===
Loss reward (iter 847): 6.596529006958008
=== Iterazione IRL 848 ===
Loss reward (iter 848): 6.5832109451293945
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.19    |
|    critic_loss     | 0.0132   |
|    learning_rate   | 0.001    |
|    n_updates       | 381899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.24    |
|    critic_loss     | 0.0136   |
|    learning_rate   | 0.001    |
|    n_updates       | 382299   |
---------------------------------
=== Iterazione IRL 849 ===
Loss reward (iter 849): 6.626219272613525
=== Iterazione IRL 850 ===
Loss reward (iter 850): 6.692205905914307
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.24    |
|    critic_loss     | 0.0141   |
|    learning_rate   | 0.001    |
|    n_updates       | 382799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.16    |
|    critic_loss     | 0.013    |
|    learning_rate   | 0.001    |
|    n_updates       | 383199   |
---------------------------------
=== Iterazione IRL 851 ===
Loss reward (iter 851): 6.572617053985596
=== Iterazione IRL 852 ===
Loss reward (iter 852): 6.55996561050415
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.45    |
|    critic_loss     | 0.0156   |
|    learning_rate   | 0.001    |
|    n_updates       | 383699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.18    |
|    critic_loss     | 0.0141   |
|    learning_rate   | 0.001    |
|    n_updates       | 384099   |
---------------------------------
=== Iterazione IRL 853 ===
Loss reward (iter 853): 6.578259468078613
=== Iterazione IRL 854 ===
Loss reward (iter 854): 6.498653411865234
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.29    |
|    critic_loss     | 0.0142   |
|    learning_rate   | 0.001    |
|    n_updates       | 384599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.05    |
|    critic_loss     | 0.0172   |
|    learning_rate   | 0.001    |
|    n_updates       | 384999   |
---------------------------------
=== Iterazione IRL 855 ===
Loss reward (iter 855): 6.522205829620361
=== Iterazione IRL 856 ===
Loss reward (iter 856): 6.571381092071533
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.17    |
|    critic_loss     | 0.013    |
|    learning_rate   | 0.001    |
|    n_updates       | 385499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.21    |
|    critic_loss     | 0.0128   |
|    learning_rate   | 0.001    |
|    n_updates       | 385899   |
---------------------------------
=== Iterazione IRL 857 ===
Loss reward (iter 857): 6.553351879119873
=== Iterazione IRL 858 ===
Loss reward (iter 858): 6.435891151428223
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.31    |
|    critic_loss     | 0.0131   |
|    learning_rate   | 0.001    |
|    n_updates       | 386399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.99    |
|    critic_loss     | 0.0129   |
|    learning_rate   | 0.001    |
|    n_updates       | 386799   |
---------------------------------
=== Iterazione IRL 859 ===
Loss reward (iter 859): 6.69010591506958
=== Iterazione IRL 860 ===
Loss reward (iter 860): 6.550326347351074
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.17    |
|    critic_loss     | 0.0109   |
|    learning_rate   | 0.001    |
|    n_updates       | 387299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.1     |
|    critic_loss     | 0.0128   |
|    learning_rate   | 0.001    |
|    n_updates       | 387699   |
---------------------------------
=== Iterazione IRL 861 ===
Loss reward (iter 861): 6.573190689086914
=== Iterazione IRL 862 ===
Loss reward (iter 862): 6.573430061340332
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.18    |
|    critic_loss     | 0.0142   |
|    learning_rate   | 0.001    |
|    n_updates       | 388199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.02    |
|    critic_loss     | 0.0194   |
|    learning_rate   | 0.001    |
|    n_updates       | 388599   |
---------------------------------
=== Iterazione IRL 863 ===
Loss reward (iter 863): 6.582958698272705
=== Iterazione IRL 864 ===
Loss reward (iter 864): 6.5195417404174805
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.23    |
|    critic_loss     | 0.0143   |
|    learning_rate   | 0.001    |
|    n_updates       | 389099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.22    |
|    critic_loss     | 0.0148   |
|    learning_rate   | 0.001    |
|    n_updates       | 389499   |
---------------------------------
=== Iterazione IRL 865 ===
Loss reward (iter 865): 6.648540019989014
=== Iterazione IRL 866 ===
Loss reward (iter 866): 6.53011417388916
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.25    |
|    critic_loss     | 0.0115   |
|    learning_rate   | 0.001    |
|    n_updates       | 389999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.24    |
|    critic_loss     | 0.018    |
|    learning_rate   | 0.001    |
|    n_updates       | 390399   |
---------------------------------
=== Iterazione IRL 867 ===
Loss reward (iter 867): 6.4563822746276855
=== Iterazione IRL 868 ===
Loss reward (iter 868): 6.648674011230469
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.06    |
|    critic_loss     | 0.013    |
|    learning_rate   | 0.001    |
|    n_updates       | 390899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.15    |
|    critic_loss     | 0.0129   |
|    learning_rate   | 0.001    |
|    n_updates       | 391299   |
---------------------------------
=== Iterazione IRL 869 ===
Loss reward (iter 869): 6.709513187408447
=== Iterazione IRL 870 ===
Loss reward (iter 870): 6.577781677246094
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.97    |
|    critic_loss     | 0.0159   |
|    learning_rate   | 0.001    |
|    n_updates       | 391799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.06    |
|    critic_loss     | 0.0149   |
|    learning_rate   | 0.001    |
|    n_updates       | 392199   |
---------------------------------
=== Iterazione IRL 871 ===
Loss reward (iter 871): 6.646244049072266
=== Iterazione IRL 872 ===
Loss reward (iter 872): 6.648728370666504
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.27    |
|    critic_loss     | 0.012    |
|    learning_rate   | 0.001    |
|    n_updates       | 392699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.21    |
|    critic_loss     | 0.01     |
|    learning_rate   | 0.001    |
|    n_updates       | 393099   |
---------------------------------
=== Iterazione IRL 873 ===
Loss reward (iter 873): 6.555538654327393
=== Iterazione IRL 874 ===
Loss reward (iter 874): 6.633692264556885
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.88    |
|    critic_loss     | 0.0135   |
|    learning_rate   | 0.001    |
|    n_updates       | 393599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.05    |
|    critic_loss     | 0.0124   |
|    learning_rate   | 0.001    |
|    n_updates       | 393999   |
---------------------------------
=== Iterazione IRL 875 ===
Loss reward (iter 875): 6.579243183135986
=== Iterazione IRL 876 ===
Loss reward (iter 876): 6.637581825256348
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.05    |
|    critic_loss     | 0.0103   |
|    learning_rate   | 0.001    |
|    n_updates       | 394499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.01    |
|    critic_loss     | 0.0155   |
|    learning_rate   | 0.001    |
|    n_updates       | 394899   |
---------------------------------
=== Iterazione IRL 877 ===
Loss reward (iter 877): 6.665110111236572
=== Iterazione IRL 878 ===
Loss reward (iter 878): 6.654933929443359
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.87    |
|    critic_loss     | 0.016    |
|    learning_rate   | 0.001    |
|    n_updates       | 395399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.12    |
|    critic_loss     | 0.0122   |
|    learning_rate   | 0.001    |
|    n_updates       | 395799   |
---------------------------------
=== Iterazione IRL 879 ===
Loss reward (iter 879): 6.517202377319336
=== Iterazione IRL 880 ===
Loss reward (iter 880): 6.593328952789307
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.18    |
|    critic_loss     | 0.0155   |
|    learning_rate   | 0.001    |
|    n_updates       | 396299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.94    |
|    critic_loss     | 0.0152   |
|    learning_rate   | 0.001    |
|    n_updates       | 396699   |
---------------------------------
=== Iterazione IRL 881 ===
Loss reward (iter 881): 6.620630264282227
=== Iterazione IRL 882 ===
Loss reward (iter 882): 6.581259727478027
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.22    |
|    critic_loss     | 0.0136   |
|    learning_rate   | 0.001    |
|    n_updates       | 397199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.84    |
|    critic_loss     | 0.011    |
|    learning_rate   | 0.001    |
|    n_updates       | 397599   |
---------------------------------
=== Iterazione IRL 883 ===
Loss reward (iter 883): 6.605920314788818
=== Iterazione IRL 884 ===
Loss reward (iter 884): 6.641601085662842
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.79    |
|    critic_loss     | 0.017    |
|    learning_rate   | 0.001    |
|    n_updates       | 398099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.03    |
|    critic_loss     | 0.0141   |
|    learning_rate   | 0.001    |
|    n_updates       | 398499   |
---------------------------------
=== Iterazione IRL 885 ===
Loss reward (iter 885): 6.550347805023193
=== Iterazione IRL 886 ===
Loss reward (iter 886): 6.587922096252441
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.04    |
|    critic_loss     | 0.015    |
|    learning_rate   | 0.001    |
|    n_updates       | 398999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.04    |
|    critic_loss     | 0.0129   |
|    learning_rate   | 0.001    |
|    n_updates       | 399399   |
---------------------------------
=== Iterazione IRL 887 ===
Loss reward (iter 887): 6.620419979095459
=== Iterazione IRL 888 ===
Loss reward (iter 888): 6.648416519165039
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.12    |
|    critic_loss     | 0.0169   |
|    learning_rate   | 0.001    |
|    n_updates       | 399899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.95    |
|    critic_loss     | 0.0146   |
|    learning_rate   | 0.001    |
|    n_updates       | 400299   |
---------------------------------
=== Iterazione IRL 889 ===
Loss reward (iter 889): 6.564884662628174
=== Iterazione IRL 890 ===
Loss reward (iter 890): 6.712155818939209
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.11    |
|    critic_loss     | 0.0155   |
|    learning_rate   | 0.001    |
|    n_updates       | 400799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.06    |
|    critic_loss     | 0.0139   |
|    learning_rate   | 0.001    |
|    n_updates       | 401199   |
---------------------------------
=== Iterazione IRL 891 ===
Loss reward (iter 891): 6.54985237121582
=== Iterazione IRL 892 ===
Loss reward (iter 892): 6.568581581115723
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.13    |
|    critic_loss     | 0.0165   |
|    learning_rate   | 0.001    |
|    n_updates       | 401699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.89    |
|    critic_loss     | 0.0137   |
|    learning_rate   | 0.001    |
|    n_updates       | 402099   |
---------------------------------
=== Iterazione IRL 893 ===
Loss reward (iter 893): 6.545719623565674
=== Iterazione IRL 894 ===
Loss reward (iter 894): 6.568406105041504
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.05    |
|    critic_loss     | 0.0116   |
|    learning_rate   | 0.001    |
|    n_updates       | 402599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.91    |
|    critic_loss     | 0.0154   |
|    learning_rate   | 0.001    |
|    n_updates       | 402999   |
---------------------------------
=== Iterazione IRL 895 ===
Loss reward (iter 895): 6.654810428619385
=== Iterazione IRL 896 ===
Loss reward (iter 896): 6.64792013168335
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.96    |
|    critic_loss     | 0.0134   |
|    learning_rate   | 0.001    |
|    n_updates       | 403499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.78    |
|    critic_loss     | 0.016    |
|    learning_rate   | 0.001    |
|    n_updates       | 403899   |
---------------------------------
=== Iterazione IRL 897 ===
Loss reward (iter 897): 6.565451622009277
=== Iterazione IRL 898 ===
Loss reward (iter 898): 6.60407018661499
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.83    |
|    critic_loss     | 0.015    |
|    learning_rate   | 0.001    |
|    n_updates       | 404399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.65    |
|    critic_loss     | 0.0134   |
|    learning_rate   | 0.001    |
|    n_updates       | 404799   |
---------------------------------
=== Iterazione IRL 899 ===
Loss reward (iter 899): 6.586695194244385
=== Iterazione IRL 900 ===
Loss reward (iter 900): 6.594503402709961
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.92    |
|    critic_loss     | 0.0116   |
|    learning_rate   | 0.001    |
|    n_updates       | 405299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.76    |
|    critic_loss     | 0.0131   |
|    learning_rate   | 0.001    |
|    n_updates       | 405699   |
---------------------------------
=== Iterazione IRL 901 ===
Loss reward (iter 901): 6.565204620361328
=== Iterazione IRL 902 ===
Loss reward (iter 902): 6.553153038024902
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.63    |
|    critic_loss     | 0.0191   |
|    learning_rate   | 0.001    |
|    n_updates       | 406199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.81    |
|    critic_loss     | 0.0146   |
|    learning_rate   | 0.001    |
|    n_updates       | 406599   |
---------------------------------
=== Iterazione IRL 903 ===
Loss reward (iter 903): 6.567497253417969
=== Iterazione IRL 904 ===
Loss reward (iter 904): 6.527987957000732
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.75    |
|    critic_loss     | 0.0156   |
|    learning_rate   | 0.001    |
|    n_updates       | 407099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.83    |
|    critic_loss     | 0.015    |
|    learning_rate   | 0.001    |
|    n_updates       | 407499   |
---------------------------------
=== Iterazione IRL 905 ===
Loss reward (iter 905): 6.646432399749756
=== Iterazione IRL 906 ===
Loss reward (iter 906): 6.494040012359619
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.71    |
|    critic_loss     | 0.0129   |
|    learning_rate   | 0.001    |
|    n_updates       | 407999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.82    |
|    critic_loss     | 0.0115   |
|    learning_rate   | 0.001    |
|    n_updates       | 408399   |
---------------------------------
=== Iterazione IRL 907 ===
Loss reward (iter 907): 6.561341762542725
=== Iterazione IRL 908 ===
Loss reward (iter 908): 6.6995062828063965
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.67    |
|    critic_loss     | 0.0148   |
|    learning_rate   | 0.001    |
|    n_updates       | 408899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.87    |
|    critic_loss     | 0.0139   |
|    learning_rate   | 0.001    |
|    n_updates       | 409299   |
---------------------------------
=== Iterazione IRL 909 ===
Loss reward (iter 909): 6.628805637359619
=== Iterazione IRL 910 ===
Loss reward (iter 910): 6.568881988525391
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.82    |
|    critic_loss     | 0.0175   |
|    learning_rate   | 0.001    |
|    n_updates       | 409799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.7     |
|    critic_loss     | 0.0131   |
|    learning_rate   | 0.001    |
|    n_updates       | 410199   |
---------------------------------
=== Iterazione IRL 911 ===
Loss reward (iter 911): 6.567962169647217
=== Iterazione IRL 912 ===
Loss reward (iter 912): 6.611072540283203
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.85    |
|    critic_loss     | 0.0139   |
|    learning_rate   | 0.001    |
|    n_updates       | 410699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.79    |
|    critic_loss     | 0.0122   |
|    learning_rate   | 0.001    |
|    n_updates       | 411099   |
---------------------------------
=== Iterazione IRL 913 ===
Loss reward (iter 913): 6.568763732910156
=== Iterazione IRL 914 ===
Loss reward (iter 914): 6.594978332519531
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.81    |
|    critic_loss     | 0.0114   |
|    learning_rate   | 0.001    |
|    n_updates       | 411599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.66    |
|    critic_loss     | 0.0151   |
|    learning_rate   | 0.001    |
|    n_updates       | 411999   |
---------------------------------
=== Iterazione IRL 915 ===
Loss reward (iter 915): 6.689196586608887
=== Iterazione IRL 916 ===
Loss reward (iter 916): 6.548249244689941
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.97    |
|    critic_loss     | 0.0148   |
|    learning_rate   | 0.001    |
|    n_updates       | 412499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.66    |
|    critic_loss     | 0.018    |
|    learning_rate   | 0.001    |
|    n_updates       | 412899   |
---------------------------------
=== Iterazione IRL 917 ===
Loss reward (iter 917): 6.567557334899902
=== Iterazione IRL 918 ===
Loss reward (iter 918): 6.55658483505249
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.06    |
|    critic_loss     | 0.014    |
|    learning_rate   | 0.001    |
|    n_updates       | 413399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.68    |
|    critic_loss     | 0.0133   |
|    learning_rate   | 0.001    |
|    n_updates       | 413799   |
---------------------------------
=== Iterazione IRL 919 ===
Loss reward (iter 919): 6.545228958129883
=== Iterazione IRL 920 ===
Loss reward (iter 920): 6.552250385284424
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.79    |
|    critic_loss     | 0.0155   |
|    learning_rate   | 0.001    |
|    n_updates       | 414299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.68    |
|    critic_loss     | 0.0131   |
|    learning_rate   | 0.001    |
|    n_updates       | 414699   |
---------------------------------
=== Iterazione IRL 921 ===
Loss reward (iter 921): 6.6826348304748535
=== Iterazione IRL 922 ===
Loss reward (iter 922): 6.583950042724609
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.87    |
|    critic_loss     | 0.0142   |
|    learning_rate   | 0.001    |
|    n_updates       | 415199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.63    |
|    critic_loss     | 0.0156   |
|    learning_rate   | 0.001    |
|    n_updates       | 415599   |
---------------------------------
=== Iterazione IRL 923 ===
Loss reward (iter 923): 6.59404993057251
=== Iterazione IRL 924 ===
Loss reward (iter 924): 6.5925188064575195
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.56    |
|    critic_loss     | 0.0122   |
|    learning_rate   | 0.001    |
|    n_updates       | 416099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.75    |
|    critic_loss     | 0.0131   |
|    learning_rate   | 0.001    |
|    n_updates       | 416499   |
---------------------------------
=== Iterazione IRL 925 ===
Loss reward (iter 925): 6.559116840362549
=== Iterazione IRL 926 ===
Loss reward (iter 926): 6.64198637008667
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.8     |
|    critic_loss     | 0.0154   |
|    learning_rate   | 0.001    |
|    n_updates       | 416999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.56    |
|    critic_loss     | 0.0147   |
|    learning_rate   | 0.001    |
|    n_updates       | 417399   |
---------------------------------
=== Iterazione IRL 927 ===
Loss reward (iter 927): 6.723381042480469
=== Iterazione IRL 928 ===
Loss reward (iter 928): 6.526993274688721
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.67    |
|    critic_loss     | 0.0171   |
|    learning_rate   | 0.001    |
|    n_updates       | 417899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.63    |
|    critic_loss     | 0.0126   |
|    learning_rate   | 0.001    |
|    n_updates       | 418299   |
---------------------------------
=== Iterazione IRL 929 ===
Loss reward (iter 929): 6.565103054046631
=== Iterazione IRL 930 ===
Loss reward (iter 930): 6.5884504318237305
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.75    |
|    critic_loss     | 0.0138   |
|    learning_rate   | 0.001    |
|    n_updates       | 418799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.85    |
|    critic_loss     | 0.0135   |
|    learning_rate   | 0.001    |
|    n_updates       | 419199   |
---------------------------------
=== Iterazione IRL 931 ===
Loss reward (iter 931): 6.776226997375488
=== Iterazione IRL 932 ===
Loss reward (iter 932): 6.600334167480469
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.49    |
|    critic_loss     | 0.0136   |
|    learning_rate   | 0.001    |
|    n_updates       | 419699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.55    |
|    critic_loss     | 0.0135   |
|    learning_rate   | 0.001    |
|    n_updates       | 420099   |
---------------------------------
=== Iterazione IRL 933 ===
Loss reward (iter 933): 6.760456562042236
=== Iterazione IRL 934 ===
Loss reward (iter 934): 6.622358322143555
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.64    |
|    critic_loss     | 0.015    |
|    learning_rate   | 0.001    |
|    n_updates       | 420599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.74    |
|    critic_loss     | 0.0154   |
|    learning_rate   | 0.001    |
|    n_updates       | 420999   |
---------------------------------
=== Iterazione IRL 935 ===
Loss reward (iter 935): 6.720284461975098
=== Iterazione IRL 936 ===
Loss reward (iter 936): 6.6321282386779785
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.7     |
|    critic_loss     | 0.0154   |
|    learning_rate   | 0.001    |
|    n_updates       | 421499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.6     |
|    critic_loss     | 0.0133   |
|    learning_rate   | 0.001    |
|    n_updates       | 421899   |
---------------------------------
=== Iterazione IRL 937 ===
Loss reward (iter 937): 6.718318939208984
=== Iterazione IRL 938 ===
Loss reward (iter 938): 6.575830936431885
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.66    |
|    critic_loss     | 0.0162   |
|    learning_rate   | 0.001    |
|    n_updates       | 422399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.57    |
|    critic_loss     | 0.0198   |
|    learning_rate   | 0.001    |
|    n_updates       | 422799   |
---------------------------------
=== Iterazione IRL 939 ===
Loss reward (iter 939): 6.76206111907959
=== Iterazione IRL 940 ===
Loss reward (iter 940): 6.608407974243164
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.47    |
|    critic_loss     | 0.0147   |
|    learning_rate   | 0.001    |
|    n_updates       | 423299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.43    |
|    critic_loss     | 0.0134   |
|    learning_rate   | 0.001    |
|    n_updates       | 423699   |
---------------------------------
=== Iterazione IRL 941 ===
Loss reward (iter 941): 6.581753730773926
=== Iterazione IRL 942 ===
Loss reward (iter 942): 6.5669755935668945
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.53    |
|    critic_loss     | 0.0164   |
|    learning_rate   | 0.001    |
|    n_updates       | 424199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.55    |
|    critic_loss     | 0.0118   |
|    learning_rate   | 0.001    |
|    n_updates       | 424599   |
---------------------------------
=== Iterazione IRL 943 ===
Loss reward (iter 943): 6.5945329666137695
=== Iterazione IRL 944 ===
Loss reward (iter 944): 6.606271743774414
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.67    |
|    critic_loss     | 0.0162   |
|    learning_rate   | 0.001    |
|    n_updates       | 425099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.46    |
|    critic_loss     | 0.0174   |
|    learning_rate   | 0.001    |
|    n_updates       | 425499   |
---------------------------------
=== Iterazione IRL 945 ===
Loss reward (iter 945): 6.61722469329834
=== Iterazione IRL 946 ===
Loss reward (iter 946): 6.599531650543213
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.49    |
|    critic_loss     | 0.0136   |
|    learning_rate   | 0.001    |
|    n_updates       | 425999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.59    |
|    critic_loss     | 0.014    |
|    learning_rate   | 0.001    |
|    n_updates       | 426399   |
---------------------------------
=== Iterazione IRL 947 ===
Loss reward (iter 947): 6.598103046417236
=== Iterazione IRL 948 ===
Loss reward (iter 948): 6.603442192077637
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 263      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.53    |
|    critic_loss     | 0.0132   |
|    learning_rate   | 0.001    |
|    n_updates       | 426899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.48    |
|    critic_loss     | 0.0177   |
|    learning_rate   | 0.001    |
|    n_updates       | 427299   |
---------------------------------
=== Iterazione IRL 949 ===
Loss reward (iter 949): 6.559534549713135
=== Iterazione IRL 950 ===
Loss reward (iter 950): 6.6083664894104
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.75    |
|    critic_loss     | 0.0134   |
|    learning_rate   | 0.001    |
|    n_updates       | 427799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.64    |
|    critic_loss     | 0.0115   |
|    learning_rate   | 0.001    |
|    n_updates       | 428199   |
---------------------------------
=== Iterazione IRL 951 ===
Loss reward (iter 951): 6.586000442504883
=== Iterazione IRL 952 ===
Loss reward (iter 952): 6.588261127471924
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.48    |
|    critic_loss     | 0.0147   |
|    learning_rate   | 0.001    |
|    n_updates       | 428699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.24    |
|    critic_loss     | 0.0155   |
|    learning_rate   | 0.001    |
|    n_updates       | 429099   |
---------------------------------
=== Iterazione IRL 953 ===
Loss reward (iter 953): 6.550158977508545
=== Iterazione IRL 954 ===
Loss reward (iter 954): 6.585060119628906
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.37    |
|    critic_loss     | 0.0118   |
|    learning_rate   | 0.001    |
|    n_updates       | 429599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.53    |
|    critic_loss     | 0.0183   |
|    learning_rate   | 0.001    |
|    n_updates       | 429999   |
---------------------------------
=== Iterazione IRL 955 ===
Loss reward (iter 955): 6.600790977478027
=== Iterazione IRL 956 ===
Loss reward (iter 956): 6.582661151885986
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.58    |
|    critic_loss     | 0.0108   |
|    learning_rate   | 0.001    |
|    n_updates       | 430499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.64    |
|    critic_loss     | 0.0167   |
|    learning_rate   | 0.001    |
|    n_updates       | 430899   |
---------------------------------
=== Iterazione IRL 957 ===
Loss reward (iter 957): 6.73822546005249
=== Iterazione IRL 958 ===
Loss reward (iter 958): 6.6650896072387695
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.38    |
|    critic_loss     | 0.0181   |
|    learning_rate   | 0.001    |
|    n_updates       | 431399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.7     |
|    critic_loss     | 0.0158   |
|    learning_rate   | 0.001    |
|    n_updates       | 431799   |
---------------------------------
=== Iterazione IRL 959 ===
Loss reward (iter 959): 6.540510654449463
=== Iterazione IRL 960 ===
Loss reward (iter 960): 6.569549560546875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.54    |
|    critic_loss     | 0.0212   |
|    learning_rate   | 0.001    |
|    n_updates       | 432299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.35    |
|    critic_loss     | 0.0149   |
|    learning_rate   | 0.001    |
|    n_updates       | 432699   |
---------------------------------
=== Iterazione IRL 961 ===
Loss reward (iter 961): 6.585494041442871
=== Iterazione IRL 962 ===
Loss reward (iter 962): 6.556156158447266
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.37    |
|    critic_loss     | 0.0154   |
|    learning_rate   | 0.001    |
|    n_updates       | 433199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.35    |
|    critic_loss     | 0.0148   |
|    learning_rate   | 0.001    |
|    n_updates       | 433599   |
---------------------------------
=== Iterazione IRL 963 ===
Loss reward (iter 963): 6.543674468994141
=== Iterazione IRL 964 ===
Loss reward (iter 964): 6.729287147521973
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.4     |
|    critic_loss     | 0.0168   |
|    learning_rate   | 0.001    |
|    n_updates       | 434099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.43    |
|    critic_loss     | 0.0191   |
|    learning_rate   | 0.001    |
|    n_updates       | 434499   |
---------------------------------
=== Iterazione IRL 965 ===
Loss reward (iter 965): 6.616049289703369
=== Iterazione IRL 966 ===
Loss reward (iter 966): 6.6270880699157715
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.56    |
|    critic_loss     | 0.0127   |
|    learning_rate   | 0.001    |
|    n_updates       | 434999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.31    |
|    critic_loss     | 0.0178   |
|    learning_rate   | 0.001    |
|    n_updates       | 435399   |
---------------------------------
=== Iterazione IRL 967 ===
Loss reward (iter 967): 6.558900356292725
=== Iterazione IRL 968 ===
Loss reward (iter 968): 6.613123416900635
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.4     |
|    critic_loss     | 0.016    |
|    learning_rate   | 0.001    |
|    n_updates       | 435899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.4     |
|    critic_loss     | 0.0141   |
|    learning_rate   | 0.001    |
|    n_updates       | 436299   |
---------------------------------
=== Iterazione IRL 969 ===
Loss reward (iter 969): 6.635957717895508
=== Iterazione IRL 970 ===
Loss reward (iter 970): 6.567453861236572
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.27    |
|    critic_loss     | 0.0182   |
|    learning_rate   | 0.001    |
|    n_updates       | 436799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.16    |
|    critic_loss     | 0.0192   |
|    learning_rate   | 0.001    |
|    n_updates       | 437199   |
---------------------------------
=== Iterazione IRL 971 ===
Loss reward (iter 971): 6.544262409210205
=== Iterazione IRL 972 ===
Loss reward (iter 972): 6.624884605407715
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.36    |
|    critic_loss     | 0.0173   |
|    learning_rate   | 0.001    |
|    n_updates       | 437699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.27    |
|    critic_loss     | 0.0169   |
|    learning_rate   | 0.001    |
|    n_updates       | 438099   |
---------------------------------
=== Iterazione IRL 973 ===
Loss reward (iter 973): 6.5726704597473145
=== Iterazione IRL 974 ===
Loss reward (iter 974): 6.694864749908447
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.36    |
|    critic_loss     | 0.014    |
|    learning_rate   | 0.001    |
|    n_updates       | 438599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.31    |
|    critic_loss     | 0.0166   |
|    learning_rate   | 0.001    |
|    n_updates       | 438999   |
---------------------------------
=== Iterazione IRL 975 ===
Loss reward (iter 975): 6.651062488555908
=== Iterazione IRL 976 ===
Loss reward (iter 976): 6.52505350112915
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.25    |
|    critic_loss     | 0.0161   |
|    learning_rate   | 0.001    |
|    n_updates       | 439499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.21    |
|    critic_loss     | 0.0146   |
|    learning_rate   | 0.001    |
|    n_updates       | 439899   |
---------------------------------
=== Iterazione IRL 977 ===
Loss reward (iter 977): 6.634964942932129
=== Iterazione IRL 978 ===
Loss reward (iter 978): 6.704098701477051
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.46    |
|    critic_loss     | 0.0177   |
|    learning_rate   | 0.001    |
|    n_updates       | 440399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.22    |
|    critic_loss     | 0.0141   |
|    learning_rate   | 0.001    |
|    n_updates       | 440799   |
---------------------------------
=== Iterazione IRL 979 ===
Loss reward (iter 979): 6.684889793395996
=== Iterazione IRL 980 ===
Loss reward (iter 980): 6.621934413909912
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.4     |
|    critic_loss     | 0.0152   |
|    learning_rate   | 0.001    |
|    n_updates       | 441299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.26    |
|    critic_loss     | 0.0149   |
|    learning_rate   | 0.001    |
|    n_updates       | 441699   |
---------------------------------
=== Iterazione IRL 981 ===
Loss reward (iter 981): 6.60077428817749
=== Iterazione IRL 982 ===
Loss reward (iter 982): 6.638143062591553
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.39    |
|    critic_loss     | 0.0172   |
|    learning_rate   | 0.001    |
|    n_updates       | 442199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.18    |
|    critic_loss     | 0.0139   |
|    learning_rate   | 0.001    |
|    n_updates       | 442599   |
---------------------------------
=== Iterazione IRL 983 ===
Loss reward (iter 983): 6.605473518371582
=== Iterazione IRL 984 ===
Loss reward (iter 984): 6.568505764007568
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.36    |
|    critic_loss     | 0.016    |
|    learning_rate   | 0.001    |
|    n_updates       | 443099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.23    |
|    critic_loss     | 0.0151   |
|    learning_rate   | 0.001    |
|    n_updates       | 443499   |
---------------------------------
=== Iterazione IRL 985 ===
Loss reward (iter 985): 6.558607578277588
=== Iterazione IRL 986 ===
Loss reward (iter 986): 6.5554094314575195
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.48    |
|    critic_loss     | 0.0142   |
|    learning_rate   | 0.001    |
|    n_updates       | 443999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.02    |
|    critic_loss     | 0.011    |
|    learning_rate   | 0.001    |
|    n_updates       | 444399   |
---------------------------------
=== Iterazione IRL 987 ===
Loss reward (iter 987): 6.66684103012085
=== Iterazione IRL 988 ===
Loss reward (iter 988): 6.690847873687744
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.31    |
|    critic_loss     | 0.0173   |
|    learning_rate   | 0.001    |
|    n_updates       | 444899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.18    |
|    critic_loss     | 0.0167   |
|    learning_rate   | 0.001    |
|    n_updates       | 445299   |
---------------------------------
=== Iterazione IRL 989 ===
Loss reward (iter 989): 6.593045711517334
=== Iterazione IRL 990 ===
Loss reward (iter 990): 6.585907936096191
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.1     |
|    critic_loss     | 0.0157   |
|    learning_rate   | 0.001    |
|    n_updates       | 445799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.99    |
|    critic_loss     | 0.0179   |
|    learning_rate   | 0.001    |
|    n_updates       | 446199   |
---------------------------------
=== Iterazione IRL 991 ===
Loss reward (iter 991): 6.523577690124512
=== Iterazione IRL 992 ===
Loss reward (iter 992): 6.600647449493408
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.04    |
|    critic_loss     | 0.0177   |
|    learning_rate   | 0.001    |
|    n_updates       | 446699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.18    |
|    critic_loss     | 0.0152   |
|    learning_rate   | 0.001    |
|    n_updates       | 447099   |
---------------------------------
=== Iterazione IRL 993 ===
Loss reward (iter 993): 6.572109699249268
=== Iterazione IRL 994 ===
Loss reward (iter 994): 6.593478202819824
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.17    |
|    critic_loss     | 0.0145   |
|    learning_rate   | 0.001    |
|    n_updates       | 447599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.07    |
|    critic_loss     | 0.0154   |
|    learning_rate   | 0.001    |
|    n_updates       | 447999   |
---------------------------------
=== Iterazione IRL 995 ===
Loss reward (iter 995): 6.5689287185668945
=== Iterazione IRL 996 ===
Loss reward (iter 996): 6.573136329650879
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.23    |
|    critic_loss     | 0.0163   |
|    learning_rate   | 0.001    |
|    n_updates       | 448499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.22    |
|    critic_loss     | 0.018    |
|    learning_rate   | 0.001    |
|    n_updates       | 448899   |
---------------------------------
=== Iterazione IRL 997 ===
Loss reward (iter 997): 6.630438804626465
=== Iterazione IRL 998 ===
Loss reward (iter 998): 6.680880069732666
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.18    |
|    critic_loss     | 0.0128   |
|    learning_rate   | 0.001    |
|    n_updates       | 449399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.24    |
|    critic_loss     | 0.0155   |
|    learning_rate   | 0.001    |
|    n_updates       | 449799   |
---------------------------------
=== Iterazione IRL 999 ===
Loss reward (iter 999): 6.578252792358398
Modello TD3 salvato.
