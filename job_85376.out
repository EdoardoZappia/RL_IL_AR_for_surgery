Inizio training IRL
Using device: cuda
Using cuda device
=== Iterazione IRL 0 ===
Loss reward (iter 0): 6.276192665100098
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 246      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -0.781   |
|    critic_loss     | 0.00215  |
|    learning_rate   | 0.001    |
|    n_updates       | 299      |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 221      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -1.17    |
|    critic_loss     | 0.00209  |
|    learning_rate   | 0.001    |
|    n_updates       | 699      |
---------------------------------
=== Iterazione IRL 1 ===
Loss reward (iter 1): 6.558789253234863
=== Iterazione IRL 2 ===
Loss reward (iter 2): 6.357346057891846
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -1.44    |
|    critic_loss     | 0.035    |
|    learning_rate   | 0.001    |
|    n_updates       | 1199     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -1.48    |
|    critic_loss     | 0.0514   |
|    learning_rate   | 0.001    |
|    n_updates       | 1599     |
---------------------------------
=== Iterazione IRL 3 ===
Loss reward (iter 3): 6.1320013999938965
=== Iterazione IRL 4 ===
Loss reward (iter 4): 5.907988548278809
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 267      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -1.28    |
|    critic_loss     | 0.154    |
|    learning_rate   | 0.001    |
|    n_updates       | 2099     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -1.32    |
|    critic_loss     | 0.171    |
|    learning_rate   | 0.001    |
|    n_updates       | 2499     |
---------------------------------
=== Iterazione IRL 5 ===
Loss reward (iter 5): 5.678046226501465
=== Iterazione IRL 6 ===
Loss reward (iter 6): 5.433052062988281
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -1.49    |
|    critic_loss     | 0.238    |
|    learning_rate   | 0.001    |
|    n_updates       | 2999     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.35    |
|    critic_loss     | 0.205    |
|    learning_rate   | 0.001    |
|    n_updates       | 3399     |
---------------------------------
=== Iterazione IRL 7 ===
Loss reward (iter 7): 7.22951078414917
=== Iterazione IRL 8 ===
Loss reward (iter 8): 7.1784820556640625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.1     |
|    critic_loss     | 0.174    |
|    learning_rate   | 0.001    |
|    n_updates       | 3899     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.59    |
|    critic_loss     | 0.155    |
|    learning_rate   | 0.001    |
|    n_updates       | 4299     |
---------------------------------
=== Iterazione IRL 9 ===
Loss reward (iter 9): 7.077127933502197
=== Iterazione IRL 10 ===
Loss reward (iter 10): 6.942874431610107
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.46    |
|    critic_loss     | 0.216    |
|    learning_rate   | 0.001    |
|    n_updates       | 4799     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.13    |
|    critic_loss     | 0.259    |
|    learning_rate   | 0.001    |
|    n_updates       | 5199     |
---------------------------------
=== Iterazione IRL 11 ===
Loss reward (iter 11): 6.797637462615967
=== Iterazione IRL 12 ===
Loss reward (iter 12): 6.638551235198975
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.86    |
|    critic_loss     | 0.26     |
|    learning_rate   | 0.001    |
|    n_updates       | 5699     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.44    |
|    critic_loss     | 0.323    |
|    learning_rate   | 0.001    |
|    n_updates       | 6099     |
---------------------------------
=== Iterazione IRL 13 ===
Loss reward (iter 13): 6.512938499450684
=== Iterazione IRL 14 ===
Loss reward (iter 14): 6.323709964752197
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.98    |
|    critic_loss     | 0.406    |
|    learning_rate   | 0.001    |
|    n_updates       | 6599     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.32    |
|    critic_loss     | 0.397    |
|    learning_rate   | 0.001    |
|    n_updates       | 6999     |
---------------------------------
=== Iterazione IRL 15 ===
Loss reward (iter 15): 6.165652275085449
=== Iterazione IRL 16 ===
Loss reward (iter 16): 5.9692702293396
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 267      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.95    |
|    critic_loss     | 0.463    |
|    learning_rate   | 0.001    |
|    n_updates       | 7499     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.3     |
|    critic_loss     | 0.438    |
|    learning_rate   | 0.001    |
|    n_updates       | 7899     |
---------------------------------
=== Iterazione IRL 17 ===
Loss reward (iter 17): 5.807870864868164
=== Iterazione IRL 18 ===
Loss reward (iter 18): 5.673182010650635
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.47    |
|    critic_loss     | 0.516    |
|    learning_rate   | 0.001    |
|    n_updates       | 8399     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.67    |
|    critic_loss     | 0.522    |
|    learning_rate   | 0.001    |
|    n_updates       | 8799     |
---------------------------------
=== Iterazione IRL 19 ===
Loss reward (iter 19): 5.487916946411133
=== Iterazione IRL 20 ===
Loss reward (iter 20): 5.398192882537842
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.66    |
|    critic_loss     | 0.624    |
|    learning_rate   | 0.001    |
|    n_updates       | 9299     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.65    |
|    critic_loss     | 0.563    |
|    learning_rate   | 0.001    |
|    n_updates       | 9699     |
---------------------------------
=== Iterazione IRL 21 ===
Loss reward (iter 21): 5.231588363647461
=== Iterazione IRL 22 ===
Loss reward (iter 22): 5.070976257324219
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.33    |
|    critic_loss     | 0.776    |
|    learning_rate   | 0.001    |
|    n_updates       | 10199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.76    |
|    critic_loss     | 0.886    |
|    learning_rate   | 0.001    |
|    n_updates       | 10599    |
---------------------------------
=== Iterazione IRL 23 ===
Loss reward (iter 23): 4.95126485824585
=== Iterazione IRL 24 ===
Loss reward (iter 24): 4.790188312530518
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.31    |
|    critic_loss     | 1.08     |
|    learning_rate   | 0.001    |
|    n_updates       | 11099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.59    |
|    critic_loss     | 1.18     |
|    learning_rate   | 0.001    |
|    n_updates       | 11499    |
---------------------------------
=== Iterazione IRL 25 ===
Loss reward (iter 25): 4.6442551612854
=== Iterazione IRL 26 ===
Loss reward (iter 26): 4.528937339782715
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 267      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.46    |
|    critic_loss     | 1.21     |
|    learning_rate   | 0.001    |
|    n_updates       | 11999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.77    |
|    critic_loss     | 1.42     |
|    learning_rate   | 0.001    |
|    n_updates       | 12399    |
---------------------------------
=== Iterazione IRL 27 ===
Loss reward (iter 27): 4.358131408691406
=== Iterazione IRL 28 ===
Loss reward (iter 28): 4.171410083770752
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.49    |
|    critic_loss     | 1.54     |
|    learning_rate   | 0.001    |
|    n_updates       | 12899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.69    |
|    critic_loss     | 1.51     |
|    learning_rate   | 0.001    |
|    n_updates       | 13299    |
---------------------------------
=== Iterazione IRL 29 ===
Loss reward (iter 29): 4.045231342315674
=== Iterazione IRL 30 ===
Loss reward (iter 30): 3.8788092136383057
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.48    |
|    critic_loss     | 2.06     |
|    learning_rate   | 0.001    |
|    n_updates       | 13799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.16    |
|    critic_loss     | 2.01     |
|    learning_rate   | 0.001    |
|    n_updates       | 14199    |
---------------------------------
=== Iterazione IRL 31 ===
Loss reward (iter 31): 3.7107064723968506
=== Iterazione IRL 32 ===
Loss reward (iter 32): 3.482046127319336
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.59    |
|    critic_loss     | 1.94     |
|    learning_rate   | 0.001    |
|    n_updates       | 14699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -1.95    |
|    critic_loss     | 2.7      |
|    learning_rate   | 0.001    |
|    n_updates       | 15099    |
---------------------------------
=== Iterazione IRL 33 ===
Loss reward (iter 33): 3.3176403045654297
=== Iterazione IRL 34 ===
Loss reward (iter 34): 3.118697166442871
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -0.535   |
|    critic_loss     | 2.64     |
|    learning_rate   | 0.001    |
|    n_updates       | 15599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 0.388    |
|    critic_loss     | 2.95     |
|    learning_rate   | 0.001    |
|    n_updates       | 15999    |
---------------------------------
=== Iterazione IRL 35 ===
Loss reward (iter 35): 2.940157175064087
=== Iterazione IRL 36 ===
Loss reward (iter 36): 2.7394795417785645
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.68     |
|    critic_loss     | 3.21     |
|    learning_rate   | 0.001    |
|    n_updates       | 16499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.68     |
|    critic_loss     | 3.49     |
|    learning_rate   | 0.001    |
|    n_updates       | 16899    |
---------------------------------
=== Iterazione IRL 37 ===
Loss reward (iter 37): 2.5271685123443604
=== Iterazione IRL 38 ===
Loss reward (iter 38): 2.204078435897827
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 4.08     |
|    critic_loss     | 3.9      |
|    learning_rate   | 0.001    |
|    n_updates       | 17399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 5.76     |
|    critic_loss     | 4.23     |
|    learning_rate   | 0.001    |
|    n_updates       | 17799    |
---------------------------------
=== Iterazione IRL 39 ===
Loss reward (iter 39): 1.977015495300293
=== Iterazione IRL 40 ===
Loss reward (iter 40): 1.8163543939590454
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 7.43     |
|    critic_loss     | 5.61     |
|    learning_rate   | 0.001    |
|    n_updates       | 18299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 9.79     |
|    critic_loss     | 5.66     |
|    learning_rate   | 0.001    |
|    n_updates       | 18699    |
---------------------------------
=== Iterazione IRL 41 ===
Loss reward (iter 41): 1.5482943058013916
=== Iterazione IRL 42 ===
Loss reward (iter 42): 1.267073154449463
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 11.9     |
|    critic_loss     | 6.19     |
|    learning_rate   | 0.001    |
|    n_updates       | 19199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 14.1     |
|    critic_loss     | 8.26     |
|    learning_rate   | 0.001    |
|    n_updates       | 19599    |
---------------------------------
=== Iterazione IRL 43 ===
Loss reward (iter 43): 1.0229456424713135
=== Iterazione IRL 44 ===
Loss reward (iter 44): 0.75383061170578
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 16.1     |
|    critic_loss     | 6.71     |
|    learning_rate   | 0.001    |
|    n_updates       | 20099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 19.5     |
|    critic_loss     | 7.89     |
|    learning_rate   | 0.001    |
|    n_updates       | 20499    |
---------------------------------
=== Iterazione IRL 45 ===
Loss reward (iter 45): 0.450579971075058
=== Iterazione IRL 46 ===
Loss reward (iter 46): 0.10898716002702713
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 21.6     |
|    critic_loss     | 9.89     |
|    learning_rate   | 0.001    |
|    n_updates       | 20999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 24.7     |
|    critic_loss     | 10.4     |
|    learning_rate   | 0.001    |
|    n_updates       | 21399    |
---------------------------------
=== Iterazione IRL 47 ===
Loss reward (iter 47): -0.20996010303497314
=== Iterazione IRL 48 ===
Loss reward (iter 48): -0.47088348865509033
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 25.2     |
|    critic_loss     | 11.6     |
|    learning_rate   | 0.001    |
|    n_updates       | 21899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 28       |
|    critic_loss     | 11.8     |
|    learning_rate   | 0.001    |
|    n_updates       | 22299    |
---------------------------------
=== Iterazione IRL 49 ===
Loss reward (iter 49): -0.6856920123100281
=== Iterazione IRL 50 ===
Loss reward (iter 50): -1.0974769592285156
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 30.8     |
|    critic_loss     | 12.9     |
|    learning_rate   | 0.001    |
|    n_updates       | 22799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 34.4     |
|    critic_loss     | 18       |
|    learning_rate   | 0.001    |
|    n_updates       | 23199    |
---------------------------------
=== Iterazione IRL 51 ===
Loss reward (iter 51): -1.4654606580734253
=== Iterazione IRL 52 ===
Loss reward (iter 52): -1.9121531248092651
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 37.2     |
|    critic_loss     | 17.3     |
|    learning_rate   | 0.001    |
|    n_updates       | 23699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 40.3     |
|    critic_loss     | 18.8     |
|    learning_rate   | 0.001    |
|    n_updates       | 24099    |
---------------------------------
=== Iterazione IRL 53 ===
Loss reward (iter 53): -2.1076595783233643
=== Iterazione IRL 54 ===
Loss reward (iter 54): -2.6012415885925293
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 43.4     |
|    critic_loss     | 18.1     |
|    learning_rate   | 0.001    |
|    n_updates       | 24599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 44.1     |
|    critic_loss     | 15.6     |
|    learning_rate   | 0.001    |
|    n_updates       | 24999    |
---------------------------------
=== Iterazione IRL 55 ===
Loss reward (iter 55): -3.08573055267334
=== Iterazione IRL 56 ===
Loss reward (iter 56): -3.3048031330108643
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 49       |
|    critic_loss     | 23.7     |
|    learning_rate   | 0.001    |
|    n_updates       | 25499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 51.6     |
|    critic_loss     | 25.3     |
|    learning_rate   | 0.001    |
|    n_updates       | 25899    |
---------------------------------
=== Iterazione IRL 57 ===
Loss reward (iter 57): -3.641331195831299
=== Iterazione IRL 58 ===
Loss reward (iter 58): -4.332292556762695
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 57.4     |
|    critic_loss     | 31.7     |
|    learning_rate   | 0.001    |
|    n_updates       | 26399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 58.1     |
|    critic_loss     | 29.9     |
|    learning_rate   | 0.001    |
|    n_updates       | 26799    |
---------------------------------
=== Iterazione IRL 59 ===
Loss reward (iter 59): -4.5149102210998535
=== Iterazione IRL 60 ===
Loss reward (iter 60): -4.962170124053955
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 63.6     |
|    critic_loss     | 27.9     |
|    learning_rate   | 0.001    |
|    n_updates       | 27299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 65.2     |
|    critic_loss     | 30.7     |
|    learning_rate   | 0.001    |
|    n_updates       | 27699    |
---------------------------------
=== Iterazione IRL 61 ===
Loss reward (iter 61): -5.359394550323486
=== Iterazione IRL 62 ===
Loss reward (iter 62): -6.186193943023682
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 69.6     |
|    critic_loss     | 32.1     |
|    learning_rate   | 0.001    |
|    n_updates       | 28199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 70.9     |
|    critic_loss     | 41.9     |
|    learning_rate   | 0.001    |
|    n_updates       | 28599    |
---------------------------------
=== Iterazione IRL 63 ===
Loss reward (iter 63): -6.593834400177002
=== Iterazione IRL 64 ===
Loss reward (iter 64): -7.008632183074951
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 74.3     |
|    critic_loss     | 37.6     |
|    learning_rate   | 0.001    |
|    n_updates       | 29099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 75.7     |
|    critic_loss     | 42.7     |
|    learning_rate   | 0.001    |
|    n_updates       | 29499    |
---------------------------------
=== Iterazione IRL 65 ===
Loss reward (iter 65): -7.414577007293701
=== Iterazione IRL 66 ===
Loss reward (iter 66): -8.126806259155273
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 83.2     |
|    critic_loss     | 39.8     |
|    learning_rate   | 0.001    |
|    n_updates       | 29999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 88.7     |
|    critic_loss     | 42.7     |
|    learning_rate   | 0.001    |
|    n_updates       | 30399    |
---------------------------------
=== Iterazione IRL 67 ===
Loss reward (iter 67): -8.869279861450195
=== Iterazione IRL 68 ===
Loss reward (iter 68): -9.395086288452148
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 90.8     |
|    critic_loss     | 45.3     |
|    learning_rate   | 0.001    |
|    n_updates       | 30899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 98.3     |
|    critic_loss     | 47       |
|    learning_rate   | 0.001    |
|    n_updates       | 31299    |
---------------------------------
=== Iterazione IRL 69 ===
Loss reward (iter 69): -9.967138290405273
=== Iterazione IRL 70 ===
Loss reward (iter 70): -10.419931411743164
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 98.9     |
|    critic_loss     | 62.5     |
|    learning_rate   | 0.001    |
|    n_updates       | 31799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 113      |
|    critic_loss     | 57.8     |
|    learning_rate   | 0.001    |
|    n_updates       | 32199    |
---------------------------------
=== Iterazione IRL 71 ===
Loss reward (iter 71): -11.115206718444824
=== Iterazione IRL 72 ===
Loss reward (iter 72): -11.247146606445312
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 106      |
|    critic_loss     | 73.8     |
|    learning_rate   | 0.001    |
|    n_updates       | 32699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 114      |
|    critic_loss     | 68.2     |
|    learning_rate   | 0.001    |
|    n_updates       | 33099    |
---------------------------------
=== Iterazione IRL 73 ===
Loss reward (iter 73): -12.1520357131958
=== Iterazione IRL 74 ===
Loss reward (iter 74): -13.298977851867676
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 121      |
|    critic_loss     | 83.5     |
|    learning_rate   | 0.001    |
|    n_updates       | 33599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 126      |
|    critic_loss     | 75.7     |
|    learning_rate   | 0.001    |
|    n_updates       | 33999    |
---------------------------------
=== Iterazione IRL 75 ===
Loss reward (iter 75): -13.67069149017334
=== Iterazione IRL 76 ===
Loss reward (iter 76): -14.331482887268066
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 132      |
|    critic_loss     | 82.2     |
|    learning_rate   | 0.001    |
|    n_updates       | 34499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 134      |
|    critic_loss     | 81.9     |
|    learning_rate   | 0.001    |
|    n_updates       | 34899    |
---------------------------------
=== Iterazione IRL 77 ===
Loss reward (iter 77): -15.000088691711426
=== Iterazione IRL 78 ===
Loss reward (iter 78): -15.362936019897461
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 143      |
|    critic_loss     | 85.3     |
|    learning_rate   | 0.001    |
|    n_updates       | 35399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 146      |
|    critic_loss     | 107      |
|    learning_rate   | 0.001    |
|    n_updates       | 35799    |
---------------------------------
=== Iterazione IRL 79 ===
Loss reward (iter 79): -16.273778915405273
=== Iterazione IRL 80 ===
Loss reward (iter 80): -17.07682991027832
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 159      |
|    critic_loss     | 88.9     |
|    learning_rate   | 0.001    |
|    n_updates       | 36299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 155      |
|    critic_loss     | 108      |
|    learning_rate   | 0.001    |
|    n_updates       | 36699    |
---------------------------------
=== Iterazione IRL 81 ===
Loss reward (iter 81): -18.125547409057617
=== Iterazione IRL 82 ===
Loss reward (iter 82): -18.439273834228516
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 167      |
|    critic_loss     | 113      |
|    learning_rate   | 0.001    |
|    n_updates       | 37199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 167      |
|    critic_loss     | 169      |
|    learning_rate   | 0.001    |
|    n_updates       | 37599    |
---------------------------------
=== Iterazione IRL 83 ===
Loss reward (iter 83): -19.387123107910156
=== Iterazione IRL 84 ===
Loss reward (iter 84): -19.987979888916016
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 176      |
|    critic_loss     | 126      |
|    learning_rate   | 0.001    |
|    n_updates       | 38099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 189      |
|    critic_loss     | 135      |
|    learning_rate   | 0.001    |
|    n_updates       | 38499    |
---------------------------------
=== Iterazione IRL 85 ===
Loss reward (iter 85): -20.75603485107422
=== Iterazione IRL 86 ===
Loss reward (iter 86): -21.804424285888672
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 200      |
|    critic_loss     | 122      |
|    learning_rate   | 0.001    |
|    n_updates       | 38999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 201      |
|    critic_loss     | 152      |
|    learning_rate   | 0.001    |
|    n_updates       | 39399    |
---------------------------------
=== Iterazione IRL 87 ===
Loss reward (iter 87): -23.24976348876953
=== Iterazione IRL 88 ===
Loss reward (iter 88): -23.864519119262695
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 208      |
|    critic_loss     | 187      |
|    learning_rate   | 0.001    |
|    n_updates       | 39899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 208      |
|    critic_loss     | 153      |
|    learning_rate   | 0.001    |
|    n_updates       | 40299    |
---------------------------------
=== Iterazione IRL 89 ===
Loss reward (iter 89): -24.24935531616211
=== Iterazione IRL 90 ===
Loss reward (iter 90): -24.618444442749023
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 225      |
|    critic_loss     | 194      |
|    learning_rate   | 0.001    |
|    n_updates       | 40799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 230      |
|    critic_loss     | 159      |
|    learning_rate   | 0.001    |
|    n_updates       | 41199    |
---------------------------------
=== Iterazione IRL 91 ===
Loss reward (iter 91): -26.288373947143555
=== Iterazione IRL 92 ===
Loss reward (iter 92): -26.89625358581543
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 233      |
|    critic_loss     | 268      |
|    learning_rate   | 0.001    |
|    n_updates       | 41699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 251      |
|    critic_loss     | 223      |
|    learning_rate   | 0.001    |
|    n_updates       | 42099    |
---------------------------------
=== Iterazione IRL 93 ===
Loss reward (iter 93): -28.493852615356445
=== Iterazione IRL 94 ===
Loss reward (iter 94): -29.54825782775879
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 258      |
|    critic_loss     | 256      |
|    learning_rate   | 0.001    |
|    n_updates       | 42599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 265      |
|    critic_loss     | 242      |
|    learning_rate   | 0.001    |
|    n_updates       | 42999    |
---------------------------------
=== Iterazione IRL 95 ===
Loss reward (iter 95): -30.00696563720703
=== Iterazione IRL 96 ===
Loss reward (iter 96): -31.93535041809082
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 275      |
|    critic_loss     | 332      |
|    learning_rate   | 0.001    |
|    n_updates       | 43499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 279      |
|    critic_loss     | 323      |
|    learning_rate   | 0.001    |
|    n_updates       | 43899    |
---------------------------------
=== Iterazione IRL 97 ===
Loss reward (iter 97): -32.86252975463867
=== Iterazione IRL 98 ===
Loss reward (iter 98): -33.493988037109375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 290      |
|    critic_loss     | 298      |
|    learning_rate   | 0.001    |
|    n_updates       | 44399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 300      |
|    critic_loss     | 304      |
|    learning_rate   | 0.001    |
|    n_updates       | 44799    |
---------------------------------
=== Iterazione IRL 99 ===
Loss reward (iter 99): -34.94291687011719
=== Iterazione IRL 100 ===
Loss reward (iter 100): -36.51715087890625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 307      |
|    critic_loss     | 303      |
|    learning_rate   | 0.001    |
|    n_updates       | 45299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 318      |
|    critic_loss     | 350      |
|    learning_rate   | 0.001    |
|    n_updates       | 45699    |
---------------------------------
=== Iterazione IRL 101 ===
Loss reward (iter 101): -37.467472076416016
=== Iterazione IRL 102 ===
Loss reward (iter 102): -39.61427307128906
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 321      |
|    critic_loss     | 391      |
|    learning_rate   | 0.001    |
|    n_updates       | 46199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 329      |
|    critic_loss     | 397      |
|    learning_rate   | 0.001    |
|    n_updates       | 46599    |
---------------------------------
=== Iterazione IRL 103 ===
Loss reward (iter 103): -39.609283447265625
=== Iterazione IRL 104 ===
Loss reward (iter 104): -41.43703079223633
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 348      |
|    critic_loss     | 422      |
|    learning_rate   | 0.001    |
|    n_updates       | 47099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 355      |
|    critic_loss     | 437      |
|    learning_rate   | 0.001    |
|    n_updates       | 47499    |
---------------------------------
=== Iterazione IRL 105 ===
Loss reward (iter 105): -41.86466979980469
=== Iterazione IRL 106 ===
Loss reward (iter 106): -43.88215637207031
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 378      |
|    critic_loss     | 447      |
|    learning_rate   | 0.001    |
|    n_updates       | 47999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 386      |
|    critic_loss     | 504      |
|    learning_rate   | 0.001    |
|    n_updates       | 48399    |
---------------------------------
=== Iterazione IRL 107 ===
Loss reward (iter 107): -44.649356842041016
=== Iterazione IRL 108 ===
Loss reward (iter 108): -46.14299392700195
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 395      |
|    critic_loss     | 525      |
|    learning_rate   | 0.001    |
|    n_updates       | 48899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 410      |
|    critic_loss     | 454      |
|    learning_rate   | 0.001    |
|    n_updates       | 49299    |
---------------------------------
=== Iterazione IRL 109 ===
Loss reward (iter 109): -47.622257232666016
=== Iterazione IRL 110 ===
Loss reward (iter 110): -49.02866744995117
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 430      |
|    critic_loss     | 528      |
|    learning_rate   | 0.001    |
|    n_updates       | 49799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 430      |
|    critic_loss     | 614      |
|    learning_rate   | 0.001    |
|    n_updates       | 50199    |
---------------------------------
=== Iterazione IRL 111 ===
Loss reward (iter 111): -50.72282791137695
=== Iterazione IRL 112 ===
Loss reward (iter 112): -52.02499771118164
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 444      |
|    critic_loss     | 726      |
|    learning_rate   | 0.001    |
|    n_updates       | 50699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 462      |
|    critic_loss     | 500      |
|    learning_rate   | 0.001    |
|    n_updates       | 51099    |
---------------------------------
=== Iterazione IRL 113 ===
Loss reward (iter 113): -53.43999099731445
=== Iterazione IRL 114 ===
Loss reward (iter 114): -55.51161193847656
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 475      |
|    critic_loss     | 645      |
|    learning_rate   | 0.001    |
|    n_updates       | 51599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 478      |
|    critic_loss     | 687      |
|    learning_rate   | 0.001    |
|    n_updates       | 51999    |
---------------------------------
=== Iterazione IRL 115 ===
Loss reward (iter 115): -56.295413970947266
=== Iterazione IRL 116 ===
Loss reward (iter 116): -58.49025344848633
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 494      |
|    critic_loss     | 705      |
|    learning_rate   | 0.001    |
|    n_updates       | 52499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 528      |
|    critic_loss     | 754      |
|    learning_rate   | 0.001    |
|    n_updates       | 52899    |
---------------------------------
=== Iterazione IRL 117 ===
Loss reward (iter 117): -59.58348846435547
=== Iterazione IRL 118 ===
Loss reward (iter 118): -61.20249557495117
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 546      |
|    critic_loss     | 665      |
|    learning_rate   | 0.001    |
|    n_updates       | 53399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 555      |
|    critic_loss     | 895      |
|    learning_rate   | 0.001    |
|    n_updates       | 53799    |
---------------------------------
=== Iterazione IRL 119 ===
Loss reward (iter 119): -62.55584716796875
=== Iterazione IRL 120 ===
Loss reward (iter 120): -65.08605194091797
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 231      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 556      |
|    critic_loss     | 992      |
|    learning_rate   | 0.001    |
|    n_updates       | 54299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 203      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 586      |
|    critic_loss     | 938      |
|    learning_rate   | 0.001    |
|    n_updates       | 54699    |
---------------------------------
=== Iterazione IRL 121 ===
Loss reward (iter 121): -65.89656066894531
=== Iterazione IRL 122 ===
Loss reward (iter 122): -68.88243103027344
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 231      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 596      |
|    critic_loss     | 1.18e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 55199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 203      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 621      |
|    critic_loss     | 1.12e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 55599    |
---------------------------------
=== Iterazione IRL 123 ===
Loss reward (iter 123): -71.97489166259766
=== Iterazione IRL 124 ===
Loss reward (iter 124): -74.36792755126953
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 231      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 629      |
|    critic_loss     | 1.03e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 56099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 203      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 630      |
|    critic_loss     | 1.27e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 56499    |
---------------------------------
=== Iterazione IRL 125 ===
Loss reward (iter 125): -72.91254425048828
=== Iterazione IRL 126 ===
Loss reward (iter 126): -73.93244171142578
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 231      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 664      |
|    critic_loss     | 1.14e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 56999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 202      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 681      |
|    critic_loss     | 1.08e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 57399    |
---------------------------------
=== Iterazione IRL 127 ===
Loss reward (iter 127): -78.4903793334961
=== Iterazione IRL 128 ===
Loss reward (iter 128): -80.03872680664062
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 231      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 709      |
|    critic_loss     | 1.28e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 57899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 203      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 728      |
|    critic_loss     | 1.31e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 58299    |
---------------------------------
=== Iterazione IRL 129 ===
Loss reward (iter 129): -82.05736541748047
=== Iterazione IRL 130 ===
Loss reward (iter 130): -82.36157989501953
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 231      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 734      |
|    critic_loss     | 1.4e+03  |
|    learning_rate   | 0.001    |
|    n_updates       | 58799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 203      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 758      |
|    critic_loss     | 1.46e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 59199    |
---------------------------------
=== Iterazione IRL 131 ===
Loss reward (iter 131): -85.60858917236328
=== Iterazione IRL 132 ===
Loss reward (iter 132): -87.85208892822266
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 231      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 784      |
|    critic_loss     | 1.81e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 59699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 202      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 779      |
|    critic_loss     | 1.64e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 60099    |
---------------------------------
=== Iterazione IRL 133 ===
Loss reward (iter 133): -89.95259094238281
=== Iterazione IRL 134 ===
Loss reward (iter 134): -91.4931869506836
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 231      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 812      |
|    critic_loss     | 1.66e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 60599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 203      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 820      |
|    critic_loss     | 1.98e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 60999    |
---------------------------------
=== Iterazione IRL 135 ===
Loss reward (iter 135): -92.87359619140625
=== Iterazione IRL 136 ===
Loss reward (iter 136): -95.46942138671875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 231      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 883      |
|    critic_loss     | 1.94e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 61499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 203      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 874      |
|    critic_loss     | 1.76e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 61899    |
---------------------------------
=== Iterazione IRL 137 ===
Loss reward (iter 137): -98.75430297851562
=== Iterazione IRL 138 ===
Loss reward (iter 138): -99.66893005371094
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 231      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 887      |
|    critic_loss     | 1.7e+03  |
|    learning_rate   | 0.001    |
|    n_updates       | 62399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 203      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 924      |
|    critic_loss     | 2.14e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 62799    |
---------------------------------
=== Iterazione IRL 139 ===
Loss reward (iter 139): -104.44810485839844
=== Iterazione IRL 140 ===
Loss reward (iter 140): -105.31414031982422
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 231      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 957      |
|    critic_loss     | 2.59e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 63299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 202      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 804      |
|    critic_loss     | 1.93e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 63699    |
---------------------------------
=== Iterazione IRL 141 ===
Loss reward (iter 141): 61.064632415771484
=== Iterazione IRL 142 ===
Loss reward (iter 142): 58.74658203125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 231      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 743      |
|    critic_loss     | 1.91e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 64199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 203      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 697      |
|    critic_loss     | 1.83e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 64599    |
---------------------------------
=== Iterazione IRL 143 ===
Loss reward (iter 143): 57.892539978027344
=== Iterazione IRL 144 ===
Loss reward (iter 144): 57.292457580566406
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 231      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 672      |
|    critic_loss     | 1.86e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 65099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 212      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 641      |
|    critic_loss     | 2.32e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 65499    |
---------------------------------
=== Iterazione IRL 145 ===
Loss reward (iter 145): 56.99308395385742
=== Iterazione IRL 146 ===
Loss reward (iter 146): 55.5831298828125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 591      |
|    critic_loss     | 2.36e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 65999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 553      |
|    critic_loss     | 2.4e+03  |
|    learning_rate   | 0.001    |
|    n_updates       | 66399    |
---------------------------------
=== Iterazione IRL 147 ===
Loss reward (iter 147): 54.64995193481445
=== Iterazione IRL 148 ===
Loss reward (iter 148): 52.99748611450195
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 497      |
|    critic_loss     | 2.26e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 66899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 491      |
|    critic_loss     | 2.24e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 67299    |
---------------------------------
=== Iterazione IRL 149 ===
Loss reward (iter 149): 50.07859420776367
=== Iterazione IRL 150 ===
Loss reward (iter 150): 47.005577087402344
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 432      |
|    critic_loss     | 1.9e+03  |
|    learning_rate   | 0.001    |
|    n_updates       | 67799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 426      |
|    critic_loss     | 2.51e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 68199    |
---------------------------------
=== Iterazione IRL 151 ===
Loss reward (iter 151): 46.27615737915039
=== Iterazione IRL 152 ===
Loss reward (iter 152): 44.2614860534668
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 401      |
|    critic_loss     | 2.63e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 68699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 393      |
|    critic_loss     | 2.23e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 69099    |
---------------------------------
=== Iterazione IRL 153 ===
Loss reward (iter 153): 42.55797576904297
=== Iterazione IRL 154 ===
Loss reward (iter 154): 39.093807220458984
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 367      |
|    critic_loss     | 2.26e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 69599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 344      |
|    critic_loss     | 1.78e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 69999    |
---------------------------------
=== Iterazione IRL 155 ===
Loss reward (iter 155): 38.842674255371094
=== Iterazione IRL 156 ===
Loss reward (iter 156): 37.319881439208984
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 289      |
|    critic_loss     | 1.73e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 70499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 294      |
|    critic_loss     | 1.85e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 70899    |
---------------------------------
=== Iterazione IRL 157 ===
Loss reward (iter 157): 36.04913330078125
=== Iterazione IRL 158 ===
Loss reward (iter 158): 32.980247497558594
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 271      |
|    critic_loss     | 1.95e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 71399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 235      |
|    critic_loss     | 2.06e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 71799    |
---------------------------------
=== Iterazione IRL 159 ===
Loss reward (iter 159): 31.964900970458984
=== Iterazione IRL 160 ===
Loss reward (iter 160): 30.520647048950195
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 197      |
|    critic_loss     | 2.01e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 72299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 212      |
|    critic_loss     | 2.04e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 72699    |
---------------------------------
=== Iterazione IRL 161 ===
Loss reward (iter 161): 29.99919891357422
=== Iterazione IRL 162 ===
Loss reward (iter 162): 26.927282333374023
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 193      |
|    critic_loss     | 1.62e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 73199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 182      |
|    critic_loss     | 1.95e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 73599    |
---------------------------------
=== Iterazione IRL 163 ===
Loss reward (iter 163): 26.90582847595215
=== Iterazione IRL 164 ===
Loss reward (iter 164): 24.6793212890625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 267      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 132      |
|    critic_loss     | 1.89e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 74099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 126      |
|    critic_loss     | 1.82e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 74499    |
---------------------------------
=== Iterazione IRL 165 ===
Loss reward (iter 165): 22.05183219909668
=== Iterazione IRL 166 ===
Loss reward (iter 166): 20.332420349121094
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 90.5     |
|    critic_loss     | 2.03e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 74999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 76.6     |
|    critic_loss     | 1.94e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 75399    |
---------------------------------
=== Iterazione IRL 167 ===
Loss reward (iter 167): 19.743030548095703
=== Iterazione IRL 168 ===
Loss reward (iter 168): 17.966590881347656
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 50.5     |
|    critic_loss     | 1.89e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 75899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 23.8     |
|    critic_loss     | 1.96e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 76299    |
---------------------------------
=== Iterazione IRL 169 ===
Loss reward (iter 169): 16.24808692932129
=== Iterazione IRL 170 ===
Loss reward (iter 170): 15.28835678100586
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 231      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -30.4    |
|    critic_loss     | 2.13e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 76799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 203      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -33      |
|    critic_loss     | 2.01e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 77199    |
---------------------------------
=== Iterazione IRL 171 ===
Loss reward (iter 171): 14.224054336547852
=== Iterazione IRL 172 ===
Loss reward (iter 172): 11.962441444396973
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 231      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -49      |
|    critic_loss     | 1.82e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 77699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 203      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -85.8    |
|    critic_loss     | 1.78e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 78099    |
---------------------------------
=== Iterazione IRL 173 ===
Loss reward (iter 173): 12.02950668334961
=== Iterazione IRL 174 ===
Loss reward (iter 174): 9.965513229370117
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 232      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -119     |
|    critic_loss     | 2.11e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 78599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 203      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -134     |
|    critic_loss     | 2.02e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 78999    |
---------------------------------
=== Iterazione IRL 175 ===
Loss reward (iter 175): 9.427238464355469
=== Iterazione IRL 176 ===
Loss reward (iter 176): 5.963502883911133
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 232      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -156     |
|    critic_loss     | 1.78e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 79499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 203      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -181     |
|    critic_loss     | 1.97e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 79899    |
---------------------------------
=== Iterazione IRL 177 ===
Loss reward (iter 177): 4.855136871337891
=== Iterazione IRL 178 ===
Loss reward (iter 178): 4.090950012207031
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -239     |
|    critic_loss     | 1.64e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 80399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -216     |
|    critic_loss     | 2.08e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 80799    |
---------------------------------
=== Iterazione IRL 179 ===
Loss reward (iter 179): 3.413612127304077
=== Iterazione IRL 180 ===
Loss reward (iter 180): 1.6925151348114014
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -265     |
|    critic_loss     | 1.64e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 81299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -324     |
|    critic_loss     | 1.87e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 81699    |
---------------------------------
=== Iterazione IRL 181 ===
Loss reward (iter 181): 0.195469468832016
=== Iterazione IRL 182 ===
Loss reward (iter 182): -0.8712600469589233
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -323     |
|    critic_loss     | 1.87e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 82199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -314     |
|    critic_loss     | 1.87e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 82599    |
---------------------------------
=== Iterazione IRL 183 ===
Loss reward (iter 183): -2.086937427520752
=== Iterazione IRL 184 ===
Loss reward (iter 184): -2.368567705154419
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 262      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -379     |
|    critic_loss     | 2.02e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 83099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 214      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -430     |
|    critic_loss     | 2.54e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 83499    |
---------------------------------
=== Iterazione IRL 185 ===
Loss reward (iter 185): -4.324082374572754
=== Iterazione IRL 186 ===
Loss reward (iter 186): -5.649407863616943
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 246      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -464     |
|    critic_loss     | 1.98e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 83999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 218      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -405     |
|    critic_loss     | 1.83e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 84399    |
---------------------------------
=== Iterazione IRL 187 ===
Loss reward (iter 187): -7.845174312591553
=== Iterazione IRL 188 ===
Loss reward (iter 188): -8.653790473937988
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -463     |
|    critic_loss     | 1.88e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 84899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -469     |
|    critic_loss     | 1.76e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 85299    |
---------------------------------
=== Iterazione IRL 189 ===
Loss reward (iter 189): -9.740731239318848
=== Iterazione IRL 190 ===
Loss reward (iter 190): -9.913298606872559
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -434     |
|    critic_loss     | 1.93e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 85799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -481     |
|    critic_loss     | 1.79e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 86199    |
---------------------------------
=== Iterazione IRL 191 ===
Loss reward (iter 191): -11.560996055603027
=== Iterazione IRL 192 ===
Loss reward (iter 192): -14.37783145904541
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -468     |
|    critic_loss     | 1.78e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 86699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 215      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -527     |
|    critic_loss     | 2.36e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 87099    |
---------------------------------
=== Iterazione IRL 193 ===
Loss reward (iter 193): -15.125205993652344
=== Iterazione IRL 194 ===
Loss reward (iter 194): -15.447412490844727
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 232      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -498     |
|    critic_loss     | 2.19e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 87599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 204      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -565     |
|    critic_loss     | 1.99e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 87999    |
---------------------------------
=== Iterazione IRL 195 ===
Loss reward (iter 195): -17.396316528320312
=== Iterazione IRL 196 ===
Loss reward (iter 196): -18.59060287475586
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 233      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -505     |
|    critic_loss     | 2.06e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 88499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 204      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -515     |
|    critic_loss     | 2e+03    |
|    learning_rate   | 0.001    |
|    n_updates       | 88899    |
---------------------------------
=== Iterazione IRL 197 ===
Loss reward (iter 197): -20.248218536376953
=== Iterazione IRL 198 ===
Loss reward (iter 198): -21.291183471679688
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 233      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -504     |
|    critic_loss     | 2.05e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 89399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 204      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -507     |
|    critic_loss     | 2.13e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 89799    |
---------------------------------
=== Iterazione IRL 199 ===
Loss reward (iter 199): -24.262922286987305
=== Iterazione IRL 200 ===
Loss reward (iter 200): -24.678939819335938
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 233      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -531     |
|    critic_loss     | 2.01e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 90299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 205      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -511     |
|    critic_loss     | 1.82e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 90699    |
---------------------------------
=== Iterazione IRL 201 ===
Loss reward (iter 201): -25.99363899230957
=== Iterazione IRL 202 ===
Loss reward (iter 202): -27.542760848999023
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 233      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -585     |
|    critic_loss     | 2.03e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 91199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 205      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -574     |
|    critic_loss     | 1.87e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 91599    |
---------------------------------
=== Iterazione IRL 203 ===
Loss reward (iter 203): -29.830190658569336
=== Iterazione IRL 204 ===
Loss reward (iter 204): -31.509578704833984
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 233      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -582     |
|    critic_loss     | 2.15e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 92099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 205      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -608     |
|    critic_loss     | 2.06e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 92499    |
---------------------------------
=== Iterazione IRL 205 ===
Loss reward (iter 205): -31.788381576538086
=== Iterazione IRL 206 ===
Loss reward (iter 206): -33.92384338378906
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 233      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -555     |
|    critic_loss     | 2.1e+03  |
|    learning_rate   | 0.001    |
|    n_updates       | 92999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 205      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -583     |
|    critic_loss     | 2.49e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 93399    |
---------------------------------
=== Iterazione IRL 207 ===
Loss reward (iter 207): -36.28639602661133
=== Iterazione IRL 208 ===
Loss reward (iter 208): -37.875633239746094
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 233      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -575     |
|    critic_loss     | 2.04e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 93899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 204      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -588     |
|    critic_loss     | 2.3e+03  |
|    learning_rate   | 0.001    |
|    n_updates       | 94299    |
---------------------------------
=== Iterazione IRL 209 ===
Loss reward (iter 209): -38.91096496582031
=== Iterazione IRL 210 ===
Loss reward (iter 210): -40.14201736450195
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 233      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -610     |
|    critic_loss     | 2.05e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 94799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 205      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -551     |
|    critic_loss     | 2.01e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 95199    |
---------------------------------
=== Iterazione IRL 211 ===
Loss reward (iter 211): -41.54545211791992
=== Iterazione IRL 212 ===
Loss reward (iter 212): -45.16242218017578
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 233      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -657     |
|    critic_loss     | 2.23e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 95699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 205      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -550     |
|    critic_loss     | 2.26e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 96099    |
---------------------------------
=== Iterazione IRL 213 ===
Loss reward (iter 213): -46.18592071533203
=== Iterazione IRL 214 ===
Loss reward (iter 214): -49.104827880859375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 252      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -536     |
|    critic_loss     | 2.17e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 96599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 230      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -570     |
|    critic_loss     | 2.27e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 96999    |
---------------------------------
=== Iterazione IRL 215 ===
Loss reward (iter 215): -50.309967041015625
=== Iterazione IRL 216 ===
Loss reward (iter 216): -52.36651611328125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -512     |
|    critic_loss     | 2.17e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 97499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -473     |
|    critic_loss     | 2.31e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 97899    |
---------------------------------
=== Iterazione IRL 217 ===
Loss reward (iter 217): -53.63917922973633
=== Iterazione IRL 218 ===
Loss reward (iter 218): -56.2921142578125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -510     |
|    critic_loss     | 2.42e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 98399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -493     |
|    critic_loss     | 2.45e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 98799    |
---------------------------------
=== Iterazione IRL 219 ===
Loss reward (iter 219): -58.062992095947266
=== Iterazione IRL 220 ===
Loss reward (iter 220): -61.19145584106445
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -480     |
|    critic_loss     | 2.13e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 99299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -544     |
|    critic_loss     | 2.71e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 99699    |
---------------------------------
=== Iterazione IRL 221 ===
Loss reward (iter 221): -62.445220947265625
=== Iterazione IRL 222 ===
Loss reward (iter 222): -66.3555679321289
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -565     |
|    critic_loss     | 2.32e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 100199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -482     |
|    critic_loss     | 2.33e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 100599   |
---------------------------------
=== Iterazione IRL 223 ===
Loss reward (iter 223): -67.55037689208984
=== Iterazione IRL 224 ===
Loss reward (iter 224): -71.12542724609375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -430     |
|    critic_loss     | 1.87e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 101099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -460     |
|    critic_loss     | 2.49e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 101499   |
---------------------------------
=== Iterazione IRL 225 ===
Loss reward (iter 225): -72.16486358642578
=== Iterazione IRL 226 ===
Loss reward (iter 226): -74.01580047607422
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -437     |
|    critic_loss     | 2.1e+03  |
|    learning_rate   | 0.001    |
|    n_updates       | 101999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -466     |
|    critic_loss     | 2.74e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 102399   |
---------------------------------
=== Iterazione IRL 227 ===
Loss reward (iter 227): -76.1722183227539
=== Iterazione IRL 228 ===
Loss reward (iter 228): -79.23219299316406
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -515     |
|    critic_loss     | 2.64e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 102899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -452     |
|    critic_loss     | 2.42e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 103299   |
---------------------------------
=== Iterazione IRL 229 ===
Loss reward (iter 229): -81.66569519042969
=== Iterazione IRL 230 ===
Loss reward (iter 230): -86.0234603881836
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -495     |
|    critic_loss     | 2.82e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 103799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -490     |
|    critic_loss     | 2.47e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 104199   |
---------------------------------
=== Iterazione IRL 231 ===
Loss reward (iter 231): -88.213134765625
=== Iterazione IRL 232 ===
Loss reward (iter 232): -89.91321563720703
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -408     |
|    critic_loss     | 2.89e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 104699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -413     |
|    critic_loss     | 2.66e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 105099   |
---------------------------------
=== Iterazione IRL 233 ===
Loss reward (iter 233): -93.5753173828125
=== Iterazione IRL 234 ===
Loss reward (iter 234): -95.65005493164062
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -404     |
|    critic_loss     | 3.12e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 105599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -429     |
|    critic_loss     | 3.09e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 105999   |
---------------------------------
=== Iterazione IRL 235 ===
Loss reward (iter 235): -98.46586608886719
=== Iterazione IRL 236 ===
Loss reward (iter 236): -102.37525939941406
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -484     |
|    critic_loss     | 3.2e+03  |
|    learning_rate   | 0.001    |
|    n_updates       | 106499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -399     |
|    critic_loss     | 3.25e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 106899   |
---------------------------------
=== Iterazione IRL 237 ===
Loss reward (iter 237): -105.73506164550781
=== Iterazione IRL 238 ===
Loss reward (iter 238): -108.41864013671875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -380     |
|    critic_loss     | 2.58e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 107399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -299     |
|    critic_loss     | 2.66e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 107799   |
---------------------------------
=== Iterazione IRL 239 ===
Loss reward (iter 239): -111.88292694091797
=== Iterazione IRL 240 ===
Loss reward (iter 240): -112.23876953125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -376     |
|    critic_loss     | 3.42e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 108299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -435     |
|    critic_loss     | 3.55e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 108699   |
---------------------------------
=== Iterazione IRL 241 ===
Loss reward (iter 241): -117.64341735839844
=== Iterazione IRL 242 ===
Loss reward (iter 242): -120.50418853759766
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 257      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -274     |
|    critic_loss     | 3.22e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 109199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 214      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -283     |
|    critic_loss     | 3.73e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 109599   |
---------------------------------
=== Iterazione IRL 243 ===
Loss reward (iter 243): -125.85906219482422
=== Iterazione IRL 244 ===
Loss reward (iter 244): -129.14537048339844
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 235      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -350     |
|    critic_loss     | 3.8e+03  |
|    learning_rate   | 0.001    |
|    n_updates       | 110099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 206      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -313     |
|    critic_loss     | 3.74e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 110499   |
---------------------------------
=== Iterazione IRL 245 ===
Loss reward (iter 245): -131.72711181640625
=== Iterazione IRL 246 ===
Loss reward (iter 246): -134.96690368652344
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 235      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -275     |
|    critic_loss     | 3.09e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 110999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 206      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -158     |
|    critic_loss     | 3.89e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 111399   |
---------------------------------
=== Iterazione IRL 247 ===
Loss reward (iter 247): -139.66928100585938
=== Iterazione IRL 248 ===
Loss reward (iter 248): -142.616455078125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 235      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -237     |
|    critic_loss     | 4.19e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 111899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 217      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -195     |
|    critic_loss     | 4.07e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 112299   |
---------------------------------
=== Iterazione IRL 249 ===
Loss reward (iter 249): -146.72642517089844
=== Iterazione IRL 250 ===
Loss reward (iter 250): -149.70965576171875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -104     |
|    critic_loss     | 3.91e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 112799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -260     |
|    critic_loss     | 4.2e+03  |
|    learning_rate   | 0.001    |
|    n_updates       | 113199   |
---------------------------------
=== Iterazione IRL 251 ===
Loss reward (iter 251): -151.68910217285156
=== Iterazione IRL 252 ===
Loss reward (iter 252): -156.3589630126953
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -197     |
|    critic_loss     | 3.84e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 113699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -219     |
|    critic_loss     | 4.62e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 114099   |
---------------------------------
=== Iterazione IRL 253 ===
Loss reward (iter 253): -162.89988708496094
=== Iterazione IRL 254 ===
Loss reward (iter 254): -167.02471923828125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -80.1    |
|    critic_loss     | 4.5e+03  |
|    learning_rate   | 0.001    |
|    n_updates       | 114599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -163     |
|    critic_loss     | 5.03e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 114999   |
---------------------------------
=== Iterazione IRL 255 ===
Loss reward (iter 255): -170.056396484375
=== Iterazione IRL 256 ===
Loss reward (iter 256): -171.11712646484375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 26.7     |
|    critic_loss     | 5.01e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 115499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -60.7    |
|    critic_loss     | 5.65e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 115899   |
---------------------------------
=== Iterazione IRL 257 ===
Loss reward (iter 257): -174.7349395751953
=== Iterazione IRL 258 ===
Loss reward (iter 258): -177.5914306640625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 8.26     |
|    critic_loss     | 5.84e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 116399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 93.1     |
|    critic_loss     | 6.69e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 116799   |
---------------------------------
=== Iterazione IRL 259 ===
Loss reward (iter 259): -183.96731567382812
=== Iterazione IRL 260 ===
Loss reward (iter 260): -189.346435546875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 176      |
|    critic_loss     | 5.91e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 117299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 208      |
|    critic_loss     | 6.1e+03  |
|    learning_rate   | 0.001    |
|    n_updates       | 117699   |
---------------------------------
=== Iterazione IRL 261 ===
Loss reward (iter 261): -196.40914916992188
=== Iterazione IRL 262 ===
Loss reward (iter 262): -199.07443237304688
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 284      |
|    critic_loss     | 6.18e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 118199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 268      |
|    critic_loss     | 6.4e+03  |
|    learning_rate   | 0.001    |
|    n_updates       | 118599   |
---------------------------------
=== Iterazione IRL 263 ===
Loss reward (iter 263): -200.06472778320312
=== Iterazione IRL 264 ===
Loss reward (iter 264): -208.5959014892578
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 238      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 330      |
|    critic_loss     | 7.16e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 119099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 436      |
|    critic_loss     | 6.7e+03  |
|    learning_rate   | 0.001    |
|    n_updates       | 119499   |
---------------------------------
=== Iterazione IRL 265 ===
Loss reward (iter 265): -211.1407470703125
=== Iterazione IRL 266 ===
Loss reward (iter 266): -216.16610717773438
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 234      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 466      |
|    critic_loss     | 6.31e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 119999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 206      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 448      |
|    critic_loss     | 6.41e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 120399   |
---------------------------------
=== Iterazione IRL 267 ===
Loss reward (iter 267): -219.8184814453125
=== Iterazione IRL 268 ===
Loss reward (iter 268): -225.85403442382812
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 235      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 481      |
|    critic_loss     | 6.82e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 120899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 206      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 554      |
|    critic_loss     | 7.62e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 121299   |
---------------------------------
=== Iterazione IRL 269 ===
Loss reward (iter 269): -228.40394592285156
=== Iterazione IRL 270 ===
Loss reward (iter 270): -236.31239318847656
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 235      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 514      |
|    critic_loss     | 8.01e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 121799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 206      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 507      |
|    critic_loss     | 6.78e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 122199   |
---------------------------------
=== Iterazione IRL 271 ===
Loss reward (iter 271): -233.87779235839844
=== Iterazione IRL 272 ===
Loss reward (iter 272): -248.55532836914062
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 235      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 687      |
|    critic_loss     | 7.15e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 122699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 206      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 790      |
|    critic_loss     | 7.35e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 123099   |
---------------------------------
=== Iterazione IRL 273 ===
Loss reward (iter 273): -250.87086486816406
=== Iterazione IRL 274 ===
Loss reward (iter 274): -254.16748046875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 234      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 713      |
|    critic_loss     | 8.21e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 123599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 206      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 841      |
|    critic_loss     | 7.79e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 123999   |
---------------------------------
=== Iterazione IRL 275 ===
Loss reward (iter 275): -257.1142578125
=== Iterazione IRL 276 ===
Loss reward (iter 276): -265.6429138183594
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 234      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 704      |
|    critic_loss     | 9.1e+03  |
|    learning_rate   | 0.001    |
|    n_updates       | 124499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 206      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 937      |
|    critic_loss     | 8.98e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 124899   |
---------------------------------
=== Iterazione IRL 277 ===
Loss reward (iter 277): -271.3797302246094
=== Iterazione IRL 278 ===
Loss reward (iter 278): -279.79412841796875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 235      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 912      |
|    critic_loss     | 1.1e+04  |
|    learning_rate   | 0.001    |
|    n_updates       | 125399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 206      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 849      |
|    critic_loss     | 1.15e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 125799   |
---------------------------------
=== Iterazione IRL 279 ===
Loss reward (iter 279): -283.9485168457031
=== Iterazione IRL 280 ===
Loss reward (iter 280): -288.4528503417969
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 235      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 997      |
|    critic_loss     | 1.26e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 126299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 206      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.02e+03 |
|    critic_loss     | 1.22e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 126699   |
---------------------------------
=== Iterazione IRL 281 ===
Loss reward (iter 281): -294.7432556152344
=== Iterazione IRL 282 ===
Loss reward (iter 282): -295.8204650878906
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 234      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 987      |
|    critic_loss     | 1.12e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 127199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 205      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.15e+03 |
|    critic_loss     | 1.14e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 127599   |
---------------------------------
=== Iterazione IRL 283 ===
Loss reward (iter 283): -304.9300842285156
=== Iterazione IRL 284 ===
Loss reward (iter 284): -311.6487121582031
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 234      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.05e+03 |
|    critic_loss     | 1.19e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 128099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 206      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.13e+03 |
|    critic_loss     | 1.41e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 128499   |
---------------------------------
=== Iterazione IRL 285 ===
Loss reward (iter 285): -317.9125061035156
=== Iterazione IRL 286 ===
Loss reward (iter 286): -324.3752746582031
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 235      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.23e+03 |
|    critic_loss     | 1.29e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 128999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 206      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.34e+03 |
|    critic_loss     | 1.26e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 129399   |
---------------------------------
=== Iterazione IRL 287 ===
Loss reward (iter 287): -328.6377868652344
=== Iterazione IRL 288 ===
Loss reward (iter 288): -338.4730529785156
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 234      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.27e+03 |
|    critic_loss     | 1.24e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 129899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 206      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.32e+03 |
|    critic_loss     | 1.34e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 130299   |
---------------------------------
=== Iterazione IRL 289 ===
Loss reward (iter 289): -340.01861572265625
=== Iterazione IRL 290 ===
Loss reward (iter 290): -352.70526123046875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 234      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.51e+03 |
|    critic_loss     | 1.43e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 130799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 206      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.4e+03  |
|    critic_loss     | 1.69e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 131199   |
---------------------------------
=== Iterazione IRL 291 ===
Loss reward (iter 291): -353.6533508300781
=== Iterazione IRL 292 ===
Loss reward (iter 292): -365.0395202636719
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 235      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.48e+03 |
|    critic_loss     | 1.41e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 131699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 206      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.54e+03 |
|    critic_loss     | 1.52e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 132099   |
---------------------------------
=== Iterazione IRL 293 ===
Loss reward (iter 293): -374.3119201660156
=== Iterazione IRL 294 ===
Loss reward (iter 294): -377.135986328125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 234      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.6e+03  |
|    critic_loss     | 1.55e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 132599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 206      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.61e+03 |
|    critic_loss     | 1.46e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 132999   |
---------------------------------
=== Iterazione IRL 295 ===
Loss reward (iter 295): -376.704833984375
=== Iterazione IRL 296 ===
Loss reward (iter 296): -394.3110046386719
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 235      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.78e+03 |
|    critic_loss     | 2.13e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 133499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 206      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.77e+03 |
|    critic_loss     | 1.46e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 133899   |
---------------------------------
=== Iterazione IRL 297 ===
Loss reward (iter 297): -396.89422607421875
=== Iterazione IRL 298 ===
Loss reward (iter 298): -404.4497985839844
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 235      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.69e+03 |
|    critic_loss     | 1.87e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 134399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 206      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.84e+03 |
|    critic_loss     | 2.15e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 134799   |
---------------------------------
=== Iterazione IRL 299 ===
Loss reward (iter 299): -411.6211853027344
=== Iterazione IRL 300 ===
Loss reward (iter 300): -418.23577880859375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 235      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.1e+03  |
|    critic_loss     | 1.96e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 135299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 206      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.03e+03 |
|    critic_loss     | 1.92e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 135699   |
---------------------------------
=== Iterazione IRL 301 ===
Loss reward (iter 301): -426.8011474609375
=== Iterazione IRL 302 ===
Loss reward (iter 302): -431.93548583984375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 235      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.04e+03 |
|    critic_loss     | 2.22e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 136199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 222      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.16e+03 |
|    critic_loss     | 2.51e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 136599   |
---------------------------------
=== Iterazione IRL 303 ===
Loss reward (iter 303): -440.43377685546875
=== Iterazione IRL 304 ===
Loss reward (iter 304): -446.39715576171875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.19e+03 |
|    critic_loss     | 2.32e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 137099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.3e+03  |
|    critic_loss     | 2.64e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 137499   |
---------------------------------
=== Iterazione IRL 305 ===
Loss reward (iter 305): -454.971435546875
=== Iterazione IRL 306 ===
Loss reward (iter 306): -462.1185302734375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.27e+03 |
|    critic_loss     | 2.06e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 137999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.32e+03 |
|    critic_loss     | 2.14e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 138399   |
---------------------------------
=== Iterazione IRL 307 ===
Loss reward (iter 307): -466.505615234375
=== Iterazione IRL 308 ===
Loss reward (iter 308): -477.1175231933594
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.65e+03 |
|    critic_loss     | 2.81e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 138899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.69e+03 |
|    critic_loss     | 2.75e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 139299   |
---------------------------------
=== Iterazione IRL 309 ===
Loss reward (iter 309): -485.8027038574219
=== Iterazione IRL 310 ===
Loss reward (iter 310): -498.39166259765625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.6e+03  |
|    critic_loss     | 2.36e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 139799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.65e+03 |
|    critic_loss     | 2.95e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 140199   |
---------------------------------
=== Iterazione IRL 311 ===
Loss reward (iter 311): -501.985107421875
=== Iterazione IRL 312 ===
Loss reward (iter 312): -512.4445190429688
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.84e+03 |
|    critic_loss     | 2.36e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 140699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.84e+03 |
|    critic_loss     | 2.1e+04  |
|    learning_rate   | 0.001    |
|    n_updates       | 141099   |
---------------------------------
=== Iterazione IRL 313 ===
Loss reward (iter 313): -521.4810791015625
=== Iterazione IRL 314 ===
Loss reward (iter 314): -518.6184692382812
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.84e+03 |
|    critic_loss     | 3.59e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 141599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.81e+03 |
|    critic_loss     | 2.83e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 141999   |
---------------------------------
=== Iterazione IRL 315 ===
Loss reward (iter 315): -538.6892700195312
=== Iterazione IRL 316 ===
Loss reward (iter 316): -548.898681640625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.2e+03  |
|    critic_loss     | 2.13e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 142499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.05e+03 |
|    critic_loss     | 3.47e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 142899   |
---------------------------------
=== Iterazione IRL 317 ===
Loss reward (iter 317): -549.524658203125
=== Iterazione IRL 318 ===
Loss reward (iter 318): -560.7376708984375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.29e+03 |
|    critic_loss     | 3.05e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 143399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.17e+03 |
|    critic_loss     | 3.27e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 143799   |
---------------------------------
=== Iterazione IRL 319 ===
Loss reward (iter 319): -569.7696533203125
=== Iterazione IRL 320 ===
Loss reward (iter 320): -577.4716796875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.15e+03 |
|    critic_loss     | 3.5e+04  |
|    learning_rate   | 0.001    |
|    n_updates       | 144299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 228      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.3e+03  |
|    critic_loss     | 4.79e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 144699   |
---------------------------------
=== Iterazione IRL 321 ===
Loss reward (iter 321): -582.2822265625
=== Iterazione IRL 322 ===
Loss reward (iter 322): -593.1340942382812
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 235      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.42e+03 |
|    critic_loss     | 3.84e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 145199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 206      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.7e+03  |
|    critic_loss     | 4.13e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 145599   |
---------------------------------
=== Iterazione IRL 323 ===
Loss reward (iter 323): -601.0942993164062
=== Iterazione IRL 324 ===
Loss reward (iter 324): -615.1263427734375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 237      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.56e+03 |
|    critic_loss     | 3.84e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 146099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 224      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.95e+03 |
|    critic_loss     | 4.74e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 146499   |
---------------------------------
=== Iterazione IRL 325 ===
Loss reward (iter 325): -627.3143310546875
=== Iterazione IRL 326 ===
Loss reward (iter 326): -629.7949829101562
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.91e+03 |
|    critic_loss     | 4.43e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 146999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 4.02e+03 |
|    critic_loss     | 5.17e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 147399   |
---------------------------------
=== Iterazione IRL 327 ===
Loss reward (iter 327): -641.542236328125
=== Iterazione IRL 328 ===
Loss reward (iter 328): -662.00146484375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 4.29e+03 |
|    critic_loss     | 3.94e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 147899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 4e+03    |
|    critic_loss     | 4.31e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 148299   |
---------------------------------
=== Iterazione IRL 329 ===
Loss reward (iter 329): -661.0215454101562
=== Iterazione IRL 330 ===
Loss reward (iter 330): -677.3622436523438
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 4.31e+03 |
|    critic_loss     | 4.37e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 148799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 4.3e+03  |
|    critic_loss     | 5.06e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 149199   |
---------------------------------
=== Iterazione IRL 331 ===
Loss reward (iter 331): -685.6929321289062
=== Iterazione IRL 332 ===
Loss reward (iter 332): -689.088134765625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 237      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 4.26e+03 |
|    critic_loss     | 5.32e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 149699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 4.55e+03 |
|    critic_loss     | 5.51e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 150099   |
---------------------------------
=== Iterazione IRL 333 ===
Loss reward (iter 333): -703.0159301757812
=== Iterazione IRL 334 ===
Loss reward (iter 334): -710.6256103515625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 235      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 4.71e+03 |
|    critic_loss     | 6.52e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 150599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 206      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 4.75e+03 |
|    critic_loss     | 4.6e+04  |
|    learning_rate   | 0.001    |
|    n_updates       | 150999   |
---------------------------------
=== Iterazione IRL 335 ===
Loss reward (iter 335): -719.70361328125
=== Iterazione IRL 336 ===
Loss reward (iter 336): -730.7775268554688
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 235      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 5.08e+03 |
|    critic_loss     | 5.72e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 151499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 206      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 4.92e+03 |
|    critic_loss     | 6.22e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 151899   |
---------------------------------
=== Iterazione IRL 337 ===
Loss reward (iter 337): -736.2948608398438
=== Iterazione IRL 338 ===
Loss reward (iter 338): -757.3873901367188
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 235      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 5.31e+03 |
|    critic_loss     | 6.91e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 152399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 206      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 5.23e+03 |
|    critic_loss     | 5.12e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 152799   |
---------------------------------
=== Iterazione IRL 339 ===
Loss reward (iter 339): -764.03662109375
=== Iterazione IRL 340 ===
Loss reward (iter 340): -771.6963500976562
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 235      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 5.36e+03 |
|    critic_loss     | 5.87e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 153299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 206      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 5.34e+03 |
|    critic_loss     | 6.39e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 153699   |
---------------------------------
=== Iterazione IRL 341 ===
Loss reward (iter 341): -785.6751708984375
=== Iterazione IRL 342 ===
Loss reward (iter 342): -806.1128540039062
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 235      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 5.4e+03  |
|    critic_loss     | 7.43e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 154199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 206      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 5.67e+03 |
|    critic_loss     | 7.66e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 154599   |
---------------------------------
=== Iterazione IRL 343 ===
Loss reward (iter 343): -813.8770751953125
=== Iterazione IRL 344 ===
Loss reward (iter 344): -817.361083984375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 235      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 5.79e+03 |
|    critic_loss     | 8.04e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 155099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 206      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 5.67e+03 |
|    critic_loss     | 7.34e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 155499   |
---------------------------------
=== Iterazione IRL 345 ===
Loss reward (iter 345): -832.8651123046875
=== Iterazione IRL 346 ===
Loss reward (iter 346): -851.9474487304688
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 235      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 6.02e+03 |
|    critic_loss     | 6.77e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 155999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 6.35e+03 |
|    critic_loss     | 7.48e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 156399   |
---------------------------------
=== Iterazione IRL 347 ===
Loss reward (iter 347): -858.2662353515625
=== Iterazione IRL 348 ===
Loss reward (iter 348): -871.4887084960938
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 6.31e+03 |
|    critic_loss     | 6.87e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 156899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 6.19e+03 |
|    critic_loss     | 8.17e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 157299   |
---------------------------------
=== Iterazione IRL 349 ===
Loss reward (iter 349): -875.9749145507812
=== Iterazione IRL 350 ===
Loss reward (iter 350): -887.2330322265625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 6.29e+03 |
|    critic_loss     | 8.53e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 157799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 6.35e+03 |
|    critic_loss     | 9.33e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 158199   |
---------------------------------
=== Iterazione IRL 351 ===
Loss reward (iter 351): -904.5152587890625
=== Iterazione IRL 352 ===
Loss reward (iter 352): -924.48193359375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 6.6e+03  |
|    critic_loss     | 7.53e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 158699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 6.55e+03 |
|    critic_loss     | 9.18e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 159099   |
---------------------------------
=== Iterazione IRL 353 ===
Loss reward (iter 353): -930.6414794921875
=== Iterazione IRL 354 ===
Loss reward (iter 354): -941.370361328125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 6.62e+03 |
|    critic_loss     | 7.47e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 159599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 6.64e+03 |
|    critic_loss     | 9.2e+04  |
|    learning_rate   | 0.001    |
|    n_updates       | 159999   |
---------------------------------
=== Iterazione IRL 355 ===
Loss reward (iter 355): -960.5012817382812
=== Iterazione IRL 356 ===
Loss reward (iter 356): -967.0415649414062
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 6.97e+03 |
|    critic_loss     | 1.09e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 160499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 7.01e+03 |
|    critic_loss     | 9.58e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 160899   |
---------------------------------
=== Iterazione IRL 357 ===
Loss reward (iter 357): -982.5135498046875
=== Iterazione IRL 358 ===
Loss reward (iter 358): -994.5515747070312
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 7.24e+03 |
|    critic_loss     | 9.1e+04  |
|    learning_rate   | 0.001    |
|    n_updates       | 161399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 7.46e+03 |
|    critic_loss     | 9.23e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 161799   |
---------------------------------
=== Iterazione IRL 359 ===
Loss reward (iter 359): -998.7540893554688
=== Iterazione IRL 360 ===
Loss reward (iter 360): -1008.4295043945312
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 7.72e+03 |
|    critic_loss     | 1.06e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 162299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 7.92e+03 |
|    critic_loss     | 9.66e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 162699   |
---------------------------------
=== Iterazione IRL 361 ===
Loss reward (iter 361): -1019.443603515625
=== Iterazione IRL 362 ===
Loss reward (iter 362): -1051.5035400390625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 7.47e+03 |
|    critic_loss     | 1.27e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 163199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 7.97e+03 |
|    critic_loss     | 1.26e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 163599   |
---------------------------------
=== Iterazione IRL 363 ===
Loss reward (iter 363): -1056.623046875
=== Iterazione IRL 364 ===
Loss reward (iter 364): -1066.87158203125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 8.07e+03 |
|    critic_loss     | 1.33e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 164099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 7.99e+03 |
|    critic_loss     | 1.16e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 164499   |
---------------------------------
=== Iterazione IRL 365 ===
Loss reward (iter 365): -1074.3658447265625
=== Iterazione IRL 366 ===
Loss reward (iter 366): -1087.3543701171875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 8.13e+03 |
|    critic_loss     | 1.16e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 164999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 8.46e+03 |
|    critic_loss     | 1.25e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 165399   |
---------------------------------
=== Iterazione IRL 367 ===
Loss reward (iter 367): -1114.26806640625
=== Iterazione IRL 368 ===
Loss reward (iter 368): -1119.24658203125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 8.77e+03 |
|    critic_loss     | 1.58e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 165899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 8.82e+03 |
|    critic_loss     | 1.45e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 166299   |
---------------------------------
=== Iterazione IRL 369 ===
Loss reward (iter 369): -1127.9093017578125
=== Iterazione IRL 370 ===
Loss reward (iter 370): -1155.3717041015625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 8.91e+03 |
|    critic_loss     | 1.72e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 166799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 8.76e+03 |
|    critic_loss     | 1.31e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 167199   |
---------------------------------
=== Iterazione IRL 371 ===
Loss reward (iter 371): -1170.0068359375
=== Iterazione IRL 372 ===
Loss reward (iter 372): -1163.08837890625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 9.33e+03 |
|    critic_loss     | 1.47e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 167699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 9.29e+03 |
|    critic_loss     | 1.66e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 168099   |
---------------------------------
=== Iterazione IRL 373 ===
Loss reward (iter 373): -1193.1207275390625
=== Iterazione IRL 374 ===
Loss reward (iter 374): -1201.723388671875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 9.38e+03 |
|    critic_loss     | 1.62e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 168599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 9.91e+03 |
|    critic_loss     | 1.86e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 168999   |
---------------------------------
=== Iterazione IRL 375 ===
Loss reward (iter 375): -1213.665283203125
=== Iterazione IRL 376 ===
Loss reward (iter 376): -1250.2271728515625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 9.97e+03 |
|    critic_loss     | 1.72e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 169499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.02e+04 |
|    critic_loss     | 1.64e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 169899   |
---------------------------------
=== Iterazione IRL 377 ===
Loss reward (iter 377): -1248.93408203125
=== Iterazione IRL 378 ===
Loss reward (iter 378): -1271.153076171875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 9.95e+03 |
|    critic_loss     | 1.71e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 170399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.03e+04 |
|    critic_loss     | 1.67e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 170799   |
---------------------------------
=== Iterazione IRL 379 ===
Loss reward (iter 379): -1291.5386962890625
=== Iterazione IRL 380 ===
Loss reward (iter 380): -1296.76513671875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.07e+04 |
|    critic_loss     | 1.91e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 171299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.06e+04 |
|    critic_loss     | 1.92e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 171699   |
---------------------------------
=== Iterazione IRL 381 ===
Loss reward (iter 381): -1295.0946044921875
=== Iterazione IRL 382 ===
Loss reward (iter 382): -1322.5291748046875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.09e+04 |
|    critic_loss     | 2.1e+05  |
|    learning_rate   | 0.001    |
|    n_updates       | 172199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.12e+04 |
|    critic_loss     | 2.11e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 172599   |
---------------------------------
=== Iterazione IRL 383 ===
Loss reward (iter 383): -1344.855712890625
=== Iterazione IRL 384 ===
Loss reward (iter 384): -1381.370361328125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.09e+04 |
|    critic_loss     | 2.02e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 173099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.15e+04 |
|    critic_loss     | 1.91e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 173499   |
---------------------------------
=== Iterazione IRL 385 ===
Loss reward (iter 385): -1376.3035888671875
=== Iterazione IRL 386 ===
Loss reward (iter 386): -1389.0513916015625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 235      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.14e+04 |
|    critic_loss     | 2.05e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 173999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.14e+04 |
|    critic_loss     | 2.68e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 174399   |
---------------------------------
=== Iterazione IRL 387 ===
Loss reward (iter 387): -1400.4190673828125
=== Iterazione IRL 388 ===
Loss reward (iter 388): -1427.3099365234375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.19e+04 |
|    critic_loss     | 2.49e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 174899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.18e+04 |
|    critic_loss     | 2.46e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 175299   |
---------------------------------
=== Iterazione IRL 389 ===
Loss reward (iter 389): -1452.498046875
=== Iterazione IRL 390 ===
Loss reward (iter 390): -1449.691650390625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.23e+04 |
|    critic_loss     | 2.28e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 175799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.25e+04 |
|    critic_loss     | 1.96e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 176199   |
---------------------------------
=== Iterazione IRL 391 ===
Loss reward (iter 391): -1446.5960693359375
=== Iterazione IRL 392 ===
Loss reward (iter 392): -1477.5877685546875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.23e+04 |
|    critic_loss     | 2.34e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 176699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.27e+04 |
|    critic_loss     | 2.04e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 177099   |
---------------------------------
=== Iterazione IRL 393 ===
Loss reward (iter 393): -1503.5167236328125
=== Iterazione IRL 394 ===
Loss reward (iter 394): -1494.2991943359375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.29e+04 |
|    critic_loss     | 2.68e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 177599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.32e+04 |
|    critic_loss     | 2.81e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 177999   |
---------------------------------
=== Iterazione IRL 395 ===
Loss reward (iter 395): -1540.796630859375
=== Iterazione IRL 396 ===
Loss reward (iter 396): -1534.921875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.33e+04 |
|    critic_loss     | 2.57e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 178499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.35e+04 |
|    critic_loss     | 2.8e+05  |
|    learning_rate   | 0.001    |
|    n_updates       | 178899   |
---------------------------------
=== Iterazione IRL 397 ===
Loss reward (iter 397): -1564.3231201171875
=== Iterazione IRL 398 ===
Loss reward (iter 398): -1587.518798828125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.39e+04 |
|    critic_loss     | 2.92e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 179399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.4e+04  |
|    critic_loss     | 2.55e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 179799   |
---------------------------------
=== Iterazione IRL 399 ===
Loss reward (iter 399): -1600.697509765625
=== Iterazione IRL 400 ===
Loss reward (iter 400): -1618.8211669921875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.44e+04 |
|    critic_loss     | 2.87e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 180299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.43e+04 |
|    critic_loss     | 2.58e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 180699   |
---------------------------------
=== Iterazione IRL 401 ===
Loss reward (iter 401): -1646.7259521484375
=== Iterazione IRL 402 ===
Loss reward (iter 402): -1656.7275390625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.43e+04 |
|    critic_loss     | 2.81e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 181199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.48e+04 |
|    critic_loss     | 2.64e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 181599   |
---------------------------------
=== Iterazione IRL 403 ===
Loss reward (iter 403): -1658.4815673828125
=== Iterazione IRL 404 ===
Loss reward (iter 404): -1680.6400146484375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.54e+04 |
|    critic_loss     | 3.04e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 182099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.52e+04 |
|    critic_loss     | 3.12e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 182499   |
---------------------------------
=== Iterazione IRL 405 ===
Loss reward (iter 405): -1736.271728515625
=== Iterazione IRL 406 ===
Loss reward (iter 406): -1742.7728271484375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.57e+04 |
|    critic_loss     | 3.5e+05  |
|    learning_rate   | 0.001    |
|    n_updates       | 182999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.5e+04  |
|    critic_loss     | 3.73e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 183399   |
---------------------------------
=== Iterazione IRL 407 ===
Loss reward (iter 407): -1744.8463134765625
=== Iterazione IRL 408 ===
Loss reward (iter 408): -1770.8604736328125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.57e+04 |
|    critic_loss     | 3.99e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 183899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.6e+04  |
|    critic_loss     | 4.45e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 184299   |
---------------------------------
=== Iterazione IRL 409 ===
Loss reward (iter 409): -1783.543701171875
=== Iterazione IRL 410 ===
Loss reward (iter 410): -1799.2103271484375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.59e+04 |
|    critic_loss     | 2.83e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 184799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.62e+04 |
|    critic_loss     | 3.95e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 185199   |
---------------------------------
=== Iterazione IRL 411 ===
Loss reward (iter 411): -1817.9453125
=== Iterazione IRL 412 ===
Loss reward (iter 412): -1835.36962890625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.66e+04 |
|    critic_loss     | 3.42e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 185699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.74e+04 |
|    critic_loss     | 3.81e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 186099   |
---------------------------------
=== Iterazione IRL 413 ===
Loss reward (iter 413): -1861.909912109375
=== Iterazione IRL 414 ===
Loss reward (iter 414): -1893.1453857421875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.69e+04 |
|    critic_loss     | 4.23e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 186599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.68e+04 |
|    critic_loss     | 3.48e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 186999   |
---------------------------------
=== Iterazione IRL 415 ===
Loss reward (iter 415): -1904.1766357421875
=== Iterazione IRL 416 ===
Loss reward (iter 416): -1931.372802734375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.74e+04 |
|    critic_loss     | 3.72e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 187499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.78e+04 |
|    critic_loss     | 4.24e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 187899   |
---------------------------------
=== Iterazione IRL 417 ===
Loss reward (iter 417): -1935.174072265625
=== Iterazione IRL 418 ===
Loss reward (iter 418): -1952.91162109375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.8e+04  |
|    critic_loss     | 4.95e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 188399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.8e+04  |
|    critic_loss     | 4.06e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 188799   |
---------------------------------
=== Iterazione IRL 419 ===
Loss reward (iter 419): -1973.616455078125
=== Iterazione IRL 420 ===
Loss reward (iter 420): -1972.9447021484375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.82e+04 |
|    critic_loss     | 4.08e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 189299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.85e+04 |
|    critic_loss     | 4.42e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 189699   |
---------------------------------
=== Iterazione IRL 421 ===
Loss reward (iter 421): -2023.531982421875
=== Iterazione IRL 422 ===
Loss reward (iter 422): -2048.48974609375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.88e+04 |
|    critic_loss     | 4.53e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 190199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.89e+04 |
|    critic_loss     | 5.42e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 190599   |
---------------------------------
=== Iterazione IRL 423 ===
Loss reward (iter 423): -2029.490478515625
=== Iterazione IRL 424 ===
Loss reward (iter 424): -2081.538818359375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.97e+04 |
|    critic_loss     | 4.31e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 191099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.89e+04 |
|    critic_loss     | 6.11e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 191499   |
---------------------------------
=== Iterazione IRL 425 ===
Loss reward (iter 425): -2069.22509765625
=== Iterazione IRL 426 ===
Loss reward (iter 426): -2116.92431640625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.9e+04  |
|    critic_loss     | 5.29e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 191999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.97e+04 |
|    critic_loss     | 5.92e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 192399   |
---------------------------------
=== Iterazione IRL 427 ===
Loss reward (iter 427): -2139.39697265625
=== Iterazione IRL 428 ===
Loss reward (iter 428): -2166.41943359375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.98e+04 |
|    critic_loss     | 4.03e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 192899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.06e+04 |
|    critic_loss     | 6.07e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 193299   |
---------------------------------
=== Iterazione IRL 429 ===
Loss reward (iter 429): -2175.382080078125
=== Iterazione IRL 430 ===
Loss reward (iter 430): -2161.957275390625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.09e+04 |
|    critic_loss     | 6.03e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 193799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.06e+04 |
|    critic_loss     | 6.43e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 194199   |
---------------------------------
=== Iterazione IRL 431 ===
Loss reward (iter 431): -2207.70068359375
=== Iterazione IRL 432 ===
Loss reward (iter 432): -2257.6279296875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.05e+04 |
|    critic_loss     | 5.81e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 194699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.12e+04 |
|    critic_loss     | 6.26e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 195099   |
---------------------------------
=== Iterazione IRL 433 ===
Loss reward (iter 433): -2269.417236328125
=== Iterazione IRL 434 ===
Loss reward (iter 434): -2276.76611328125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.13e+04 |
|    critic_loss     | 6.64e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 195599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.15e+04 |
|    critic_loss     | 6.1e+05  |
|    learning_rate   | 0.001    |
|    n_updates       | 195999   |
---------------------------------
=== Iterazione IRL 435 ===
Loss reward (iter 435): -2278.248291015625
=== Iterazione IRL 436 ===
Loss reward (iter 436): -2317.06640625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.15e+04 |
|    critic_loss     | 5.5e+05  |
|    learning_rate   | 0.001    |
|    n_updates       | 196499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.24e+04 |
|    critic_loss     | 5.57e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 196899   |
---------------------------------
=== Iterazione IRL 437 ===
Loss reward (iter 437): -2376.8701171875
=== Iterazione IRL 438 ===
Loss reward (iter 438): -2365.845458984375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.24e+04 |
|    critic_loss     | 5.8e+05  |
|    learning_rate   | 0.001    |
|    n_updates       | 197399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.26e+04 |
|    critic_loss     | 7.35e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 197799   |
---------------------------------
=== Iterazione IRL 439 ===
Loss reward (iter 439): -2420.171875
=== Iterazione IRL 440 ===
Loss reward (iter 440): -2425.5302734375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.33e+04 |
|    critic_loss     | 7.23e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 198299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.38e+04 |
|    critic_loss     | 7.22e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 198699   |
---------------------------------
=== Iterazione IRL 441 ===
Loss reward (iter 441): -2423.632568359375
=== Iterazione IRL 442 ===
Loss reward (iter 442): -2463.11279296875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.39e+04 |
|    critic_loss     | 5.91e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 199199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.38e+04 |
|    critic_loss     | 7.61e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 199599   |
---------------------------------
=== Iterazione IRL 443 ===
Loss reward (iter 443): -2488.205078125
=== Iterazione IRL 444 ===
Loss reward (iter 444): -2547.861572265625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.42e+04 |
|    critic_loss     | 6.97e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 200099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.47e+04 |
|    critic_loss     | 7.51e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 200499   |
---------------------------------
=== Iterazione IRL 445 ===
Loss reward (iter 445): -2561.46630859375
=== Iterazione IRL 446 ===
Loss reward (iter 446): -2561.33154296875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.46e+04 |
|    critic_loss     | 6.63e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 200999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.52e+04 |
|    critic_loss     | 7.47e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 201399   |
---------------------------------
=== Iterazione IRL 447 ===
Loss reward (iter 447): -2578.140380859375
=== Iterazione IRL 448 ===
Loss reward (iter 448): -2602.64599609375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.56e+04 |
|    critic_loss     | 7.77e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 201899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.54e+04 |
|    critic_loss     | 7.1e+05  |
|    learning_rate   | 0.001    |
|    n_updates       | 202299   |
---------------------------------
=== Iterazione IRL 449 ===
Loss reward (iter 449): -2635.993896484375
=== Iterazione IRL 450 ===
Loss reward (iter 450): -2639.20166015625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.59e+04 |
|    critic_loss     | 8e+05    |
|    learning_rate   | 0.001    |
|    n_updates       | 202799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.64e+04 |
|    critic_loss     | 1.03e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 203199   |
---------------------------------
=== Iterazione IRL 451 ===
Loss reward (iter 451): -2681.083251953125
=== Iterazione IRL 452 ===
Loss reward (iter 452): -2693.605712890625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.7e+04  |
|    critic_loss     | 8.11e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 203699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.73e+04 |
|    critic_loss     | 8.61e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 204099   |
---------------------------------
=== Iterazione IRL 453 ===
Loss reward (iter 453): -2728.931640625
=== Iterazione IRL 454 ===
Loss reward (iter 454): -2757.84912109375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.73e+04 |
|    critic_loss     | 9.89e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 204599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.77e+04 |
|    critic_loss     | 8.79e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 204999   |
---------------------------------
=== Iterazione IRL 455 ===
Loss reward (iter 455): -2742.8359375
=== Iterazione IRL 456 ===
Loss reward (iter 456): -2774.84423828125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.73e+04 |
|    critic_loss     | 9.39e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 205499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.81e+04 |
|    critic_loss     | 7.48e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 205899   |
---------------------------------
=== Iterazione IRL 457 ===
Loss reward (iter 457): -2806.805419921875
=== Iterazione IRL 458 ===
Loss reward (iter 458): -2858.06640625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.86e+04 |
|    critic_loss     | 1.03e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 206399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.93e+04 |
|    critic_loss     | 8.35e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 206799   |
---------------------------------
=== Iterazione IRL 459 ===
Loss reward (iter 459): -2840.932861328125
=== Iterazione IRL 460 ===
Loss reward (iter 460): -2903.216796875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.94e+04 |
|    critic_loss     | 1.16e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 207299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.93e+04 |
|    critic_loss     | 9.85e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 207699   |
---------------------------------
=== Iterazione IRL 461 ===
Loss reward (iter 461): -2934.665771484375
=== Iterazione IRL 462 ===
Loss reward (iter 462): -2915.05859375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.03e+04 |
|    critic_loss     | 8.12e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 208199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.03e+04 |
|    critic_loss     | 8.83e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 208599   |
---------------------------------
=== Iterazione IRL 463 ===
Loss reward (iter 463): -3004.23046875
=== Iterazione IRL 464 ===
Loss reward (iter 464): -3000.561767578125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.05e+04 |
|    critic_loss     | 8.57e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 209099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.04e+04 |
|    critic_loss     | 1.01e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 209499   |
---------------------------------
=== Iterazione IRL 465 ===
Loss reward (iter 465): -3043.355712890625
=== Iterazione IRL 466 ===
Loss reward (iter 466): -3017.41748046875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.18e+04 |
|    critic_loss     | 8.92e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 209999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.11e+04 |
|    critic_loss     | 1.14e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 210399   |
---------------------------------
=== Iterazione IRL 467 ===
Loss reward (iter 467): -3100.02490234375
=== Iterazione IRL 468 ===
Loss reward (iter 468): -3118.405029296875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.21e+04 |
|    critic_loss     | 1.13e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 210899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.18e+04 |
|    critic_loss     | 9.29e+05 |
|    learning_rate   | 0.001    |
|    n_updates       | 211299   |
---------------------------------
=== Iterazione IRL 469 ===
Loss reward (iter 469): -3089.564453125
=== Iterazione IRL 470 ===
Loss reward (iter 470): -3185.062744140625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.29e+04 |
|    critic_loss     | 1.11e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 211799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.27e+04 |
|    critic_loss     | 1.04e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 212199   |
---------------------------------
=== Iterazione IRL 471 ===
Loss reward (iter 471): -3171.841796875
=== Iterazione IRL 472 ===
Loss reward (iter 472): -3272.7978515625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.31e+04 |
|    critic_loss     | 1.13e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 212699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.37e+04 |
|    critic_loss     | 9.6e+05  |
|    learning_rate   | 0.001    |
|    n_updates       | 213099   |
---------------------------------
=== Iterazione IRL 473 ===
Loss reward (iter 473): -3272.17041015625
=== Iterazione IRL 474 ===
Loss reward (iter 474): -3252.13037109375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.39e+04 |
|    critic_loss     | 1.1e+06  |
|    learning_rate   | 0.001    |
|    n_updates       | 213599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.42e+04 |
|    critic_loss     | 1.25e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 213999   |
---------------------------------
=== Iterazione IRL 475 ===
Loss reward (iter 475): -3279.396728515625
=== Iterazione IRL 476 ===
Loss reward (iter 476): -3311.32763671875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.51e+04 |
|    critic_loss     | 1.09e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 214499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.46e+04 |
|    critic_loss     | 1.24e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 214899   |
---------------------------------
=== Iterazione IRL 477 ===
Loss reward (iter 477): -3346.396240234375
=== Iterazione IRL 478 ===
Loss reward (iter 478): -3335.656982421875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.49e+04 |
|    critic_loss     | 1.33e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 215399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.62e+04 |
|    critic_loss     | 1.38e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 215799   |
---------------------------------
=== Iterazione IRL 479 ===
Loss reward (iter 479): -3438.429443359375
=== Iterazione IRL 480 ===
Loss reward (iter 480): -3425.1259765625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.57e+04 |
|    critic_loss     | 1.42e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 216299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.62e+04 |
|    critic_loss     | 1.3e+06  |
|    learning_rate   | 0.001    |
|    n_updates       | 216699   |
---------------------------------
=== Iterazione IRL 481 ===
Loss reward (iter 481): -3416.488525390625
=== Iterazione IRL 482 ===
Loss reward (iter 482): -3481.642333984375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.62e+04 |
|    critic_loss     | 1.28e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 217199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.75e+04 |
|    critic_loss     | 1.5e+06  |
|    learning_rate   | 0.001    |
|    n_updates       | 217599   |
---------------------------------
=== Iterazione IRL 483 ===
Loss reward (iter 483): -3524.527099609375
=== Iterazione IRL 484 ===
Loss reward (iter 484): -3572.244384765625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.68e+04 |
|    critic_loss     | 1.26e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 218099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.77e+04 |
|    critic_loss     | 1.86e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 218499   |
---------------------------------
=== Iterazione IRL 485 ===
Loss reward (iter 485): -3594.38330078125
=== Iterazione IRL 486 ===
Loss reward (iter 486): -3599.6728515625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.85e+04 |
|    critic_loss     | 1.45e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 218999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.89e+04 |
|    critic_loss     | 1.33e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 219399   |
---------------------------------
=== Iterazione IRL 487 ===
Loss reward (iter 487): -3623.224365234375
=== Iterazione IRL 488 ===
Loss reward (iter 488): -3653.614013671875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.95e+04 |
|    critic_loss     | 1.47e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 219899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.93e+04 |
|    critic_loss     | 1.44e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 220299   |
---------------------------------
=== Iterazione IRL 489 ===
Loss reward (iter 489): -3669.43505859375
=== Iterazione IRL 490 ===
Loss reward (iter 490): -3695.177734375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.96e+04 |
|    critic_loss     | 1.75e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 220799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.96e+04 |
|    critic_loss     | 1.9e+06  |
|    learning_rate   | 0.001    |
|    n_updates       | 221199   |
---------------------------------
=== Iterazione IRL 491 ===
Loss reward (iter 491): -3773.069091796875
=== Iterazione IRL 492 ===
Loss reward (iter 492): -3736.335205078125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 4e+04    |
|    critic_loss     | 1.5e+06  |
|    learning_rate   | 0.001    |
|    n_updates       | 221699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 4.1e+04  |
|    critic_loss     | 1.63e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 222099   |
---------------------------------
=== Iterazione IRL 493 ===
Loss reward (iter 493): -3756.019775390625
=== Iterazione IRL 494 ===
Loss reward (iter 494): -3816.91552734375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 4.12e+04 |
|    critic_loss     | 1.57e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 222599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 4.2e+04  |
|    critic_loss     | 1.57e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 222999   |
---------------------------------
=== Iterazione IRL 495 ===
Loss reward (iter 495): -3832.18505859375
=== Iterazione IRL 496 ===
Loss reward (iter 496): -3907.531494140625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 4.21e+04 |
|    critic_loss     | 1.75e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 223499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 4.26e+04 |
|    critic_loss     | 1.61e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 223899   |
---------------------------------
=== Iterazione IRL 497 ===
Loss reward (iter 497): -3878.42578125
=== Iterazione IRL 498 ===
Loss reward (iter 498): -3954.087646484375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 4.35e+04 |
|    critic_loss     | 2.08e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 224399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 4.28e+04 |
|    critic_loss     | 1.61e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 224799   |
---------------------------------
=== Iterazione IRL 499 ===
Loss reward (iter 499): -4001.6669921875
=== Iterazione IRL 500 ===
Loss reward (iter 500): -4030.65380859375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 276      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 4.3e+04  |
|    critic_loss     | 1.77e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 225299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 242      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 4.4e+04  |
|    critic_loss     | 1.99e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 225699   |
---------------------------------
=== Iterazione IRL 501 ===
Loss reward (iter 501): -4023.37158203125
=== Iterazione IRL 502 ===
Loss reward (iter 502): -4075.833251953125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 276      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 4.48e+04 |
|    critic_loss     | 1.76e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 226199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 242      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 4.51e+04 |
|    critic_loss     | 1.65e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 226599   |
---------------------------------
=== Iterazione IRL 503 ===
Loss reward (iter 503): -4096.4091796875
=== Iterazione IRL 504 ===
Loss reward (iter 504): -4109.08349609375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 276      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 4.45e+04 |
|    critic_loss     | 1.98e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 227099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 242      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 4.44e+04 |
|    critic_loss     | 1.91e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 227499   |
---------------------------------
=== Iterazione IRL 505 ===
Loss reward (iter 505): -4170.0263671875
=== Iterazione IRL 506 ===
Loss reward (iter 506): -4190.65966796875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 276      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 4.62e+04 |
|    critic_loss     | 2.22e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 227999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 242      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 4.67e+04 |
|    critic_loss     | 2.56e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 228399   |
---------------------------------
=== Iterazione IRL 507 ===
Loss reward (iter 507): -4234.60546875
=== Iterazione IRL 508 ===
Loss reward (iter 508): -4247.8466796875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 276      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 4.64e+04 |
|    critic_loss     | 2.25e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 228899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 243      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 4.78e+04 |
|    critic_loss     | 1.97e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 229299   |
---------------------------------
=== Iterazione IRL 509 ===
Loss reward (iter 509): -4326.53466796875
=== Iterazione IRL 510 ===
Loss reward (iter 510): -4263.0869140625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 277      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 4.76e+04 |
|    critic_loss     | 1.86e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 229799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 243      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 4.82e+04 |
|    critic_loss     | 1.97e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 230199   |
---------------------------------
=== Iterazione IRL 511 ===
Loss reward (iter 511): -4348.1787109375
=== Iterazione IRL 512 ===
Loss reward (iter 512): -4392.0400390625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 278      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 4.81e+04 |
|    critic_loss     | 2.03e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 230699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 243      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 4.87e+04 |
|    critic_loss     | 2.49e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 231099   |
---------------------------------
=== Iterazione IRL 513 ===
Loss reward (iter 513): -4409.93994140625
=== Iterazione IRL 514 ===
Loss reward (iter 514): -4462.7509765625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 277      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 4.96e+04 |
|    critic_loss     | 2.21e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 231599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 243      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 4.98e+04 |
|    critic_loss     | 2.24e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 231999   |
---------------------------------
=== Iterazione IRL 515 ===
Loss reward (iter 515): -4431.46533203125
=== Iterazione IRL 516 ===
Loss reward (iter 516): -4558.87646484375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 277      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 5e+04    |
|    critic_loss     | 2.06e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 232499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 243      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 5.1e+04  |
|    critic_loss     | 2.54e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 232899   |
---------------------------------
=== Iterazione IRL 517 ===
Loss reward (iter 517): -4581.76318359375
=== Iterazione IRL 518 ===
Loss reward (iter 518): -4581.0380859375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 278      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 5.17e+04 |
|    critic_loss     | 2.43e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 233399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 243      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 5.19e+04 |
|    critic_loss     | 2.9e+06  |
|    learning_rate   | 0.001    |
|    n_updates       | 233799   |
---------------------------------
=== Iterazione IRL 519 ===
Loss reward (iter 519): -4597.1015625
=== Iterazione IRL 520 ===
Loss reward (iter 520): -4670.07568359375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 277      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 5.21e+04 |
|    critic_loss     | 2.7e+06  |
|    learning_rate   | 0.001    |
|    n_updates       | 234299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 243      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 5.31e+04 |
|    critic_loss     | 2.65e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 234699   |
---------------------------------
=== Iterazione IRL 521 ===
Loss reward (iter 521): -4712.3359375
=== Iterazione IRL 522 ===
Loss reward (iter 522): -4759.80712890625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 277      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 5.37e+04 |
|    critic_loss     | 2.44e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 235199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 242      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 5.34e+04 |
|    critic_loss     | 2.26e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 235599   |
---------------------------------
=== Iterazione IRL 523 ===
Loss reward (iter 523): -4787.34375
=== Iterazione IRL 524 ===
Loss reward (iter 524): -4785.8095703125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 276      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 5.36e+04 |
|    critic_loss     | 2.61e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 236099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 243      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 5.36e+04 |
|    critic_loss     | 2.56e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 236499   |
---------------------------------
=== Iterazione IRL 525 ===
Loss reward (iter 525): -4823.40625
=== Iterazione IRL 526 ===
Loss reward (iter 526): -4865.11376953125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 277      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 5.48e+04 |
|    critic_loss     | 2.76e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 236999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 243      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 5.56e+04 |
|    critic_loss     | 3.19e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 237399   |
---------------------------------
=== Iterazione IRL 527 ===
Loss reward (iter 527): -4869.7744140625
=== Iterazione IRL 528 ===
Loss reward (iter 528): -4990.27587890625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 277      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 5.64e+04 |
|    critic_loss     | 2.73e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 237899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 243      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 5.7e+04  |
|    critic_loss     | 2.33e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 238299   |
---------------------------------
=== Iterazione IRL 529 ===
Loss reward (iter 529): -4975.00439453125
=== Iterazione IRL 530 ===
Loss reward (iter 530): -5010.02685546875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 5.71e+04 |
|    critic_loss     | 2.96e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 238799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 242      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 5.73e+04 |
|    critic_loss     | 2.52e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 239199   |
---------------------------------
=== Iterazione IRL 531 ===
Loss reward (iter 531): -5063.68798828125
=== Iterazione IRL 532 ===
Loss reward (iter 532): -5070.796875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 277      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 5.82e+04 |
|    critic_loss     | 3.45e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 239699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 243      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 5.93e+04 |
|    critic_loss     | 3.26e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 240099   |
---------------------------------
=== Iterazione IRL 533 ===
Loss reward (iter 533): -5110.32763671875
=== Iterazione IRL 534 ===
Loss reward (iter 534): -5062.8935546875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 278      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 5.81e+04 |
|    critic_loss     | 3.39e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 240599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 244      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 5.87e+04 |
|    critic_loss     | 3.46e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 240999   |
---------------------------------
=== Iterazione IRL 535 ===
Loss reward (iter 535): -5184.5751953125
=== Iterazione IRL 536 ===
Loss reward (iter 536): -5192.29150390625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 278      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 6.04e+04 |
|    critic_loss     | 2.71e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 241499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 243      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 5.97e+04 |
|    critic_loss     | 2.72e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 241899   |
---------------------------------
=== Iterazione IRL 537 ===
Loss reward (iter 537): -5292.85400390625
=== Iterazione IRL 538 ===
Loss reward (iter 538): -5320.77099609375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 278      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 6.14e+04 |
|    critic_loss     | 2.57e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 242399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 243      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 6.12e+04 |
|    critic_loss     | 3.43e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 242799   |
---------------------------------
=== Iterazione IRL 539 ===
Loss reward (iter 539): -5235.9765625
=== Iterazione IRL 540 ===
Loss reward (iter 540): -5356.599609375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 278      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 6.2e+04  |
|    critic_loss     | 3.57e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 243299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 244      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 6.19e+04 |
|    critic_loss     | 3.37e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 243699   |
---------------------------------
=== Iterazione IRL 541 ===
Loss reward (iter 541): -5409.9013671875
=== Iterazione IRL 542 ===
Loss reward (iter 542): -5367.466796875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 278      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 6.39e+04 |
|    critic_loss     | 3.17e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 244199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 244      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 6.34e+04 |
|    critic_loss     | 3.68e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 244599   |
---------------------------------
=== Iterazione IRL 543 ===
Loss reward (iter 543): -5472.49365234375
=== Iterazione IRL 544 ===
Loss reward (iter 544): -5463.08642578125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 278      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 6.54e+04 |
|    critic_loss     | 3.19e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 245099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 242      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 6.51e+04 |
|    critic_loss     | 2.84e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 245499   |
---------------------------------
=== Iterazione IRL 545 ===
Loss reward (iter 545): -5533.7197265625
=== Iterazione IRL 546 ===
Loss reward (iter 546): -5568.48876953125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 6.57e+04 |
|    critic_loss     | 3.12e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 245999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 6.66e+04 |
|    critic_loss     | 3.08e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 246399   |
---------------------------------
=== Iterazione IRL 547 ===
Loss reward (iter 547): -5666.1572265625
=== Iterazione IRL 548 ===
Loss reward (iter 548): -5564.70849609375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 6.69e+04 |
|    critic_loss     | 3.66e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 246899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 6.66e+04 |
|    critic_loss     | 4.14e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 247299   |
---------------------------------
=== Iterazione IRL 549 ===
Loss reward (iter 549): -5697.4423828125
=== Iterazione IRL 550 ===
Loss reward (iter 550): -5689.33544921875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 6.75e+04 |
|    critic_loss     | 3.85e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 247799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 6.78e+04 |
|    critic_loss     | 3.88e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 248199   |
---------------------------------
=== Iterazione IRL 551 ===
Loss reward (iter 551): -5770.01220703125
=== Iterazione IRL 552 ===
Loss reward (iter 552): -5858.57763671875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 6.85e+04 |
|    critic_loss     | 3.54e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 248699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 6.96e+04 |
|    critic_loss     | 3.87e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 249099   |
---------------------------------
=== Iterazione IRL 553 ===
Loss reward (iter 553): -5835.03759765625
=== Iterazione IRL 554 ===
Loss reward (iter 554): -5985.91259765625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 6.97e+04 |
|    critic_loss     | 3.41e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 249599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 6.98e+04 |
|    critic_loss     | 4.05e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 249999   |
---------------------------------
=== Iterazione IRL 555 ===
Loss reward (iter 555): -6018.78515625
=== Iterazione IRL 556 ===
Loss reward (iter 556): -5974.83544921875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 7.09e+04 |
|    critic_loss     | 4.11e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 250499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 7.16e+04 |
|    critic_loss     | 3.22e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 250899   |
---------------------------------
=== Iterazione IRL 557 ===
Loss reward (iter 557): -5993.919921875
=== Iterazione IRL 558 ===
Loss reward (iter 558): -6018.60986328125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 7.16e+04 |
|    critic_loss     | 5.41e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 251399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 7.29e+04 |
|    critic_loss     | 4.05e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 251799   |
---------------------------------
=== Iterazione IRL 559 ===
Loss reward (iter 559): -6023.19384765625
=== Iterazione IRL 560 ===
Loss reward (iter 560): -6069.45703125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 7.4e+04  |
|    critic_loss     | 4.57e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 252299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 7.39e+04 |
|    critic_loss     | 3.85e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 252699   |
---------------------------------
=== Iterazione IRL 561 ===
Loss reward (iter 561): -6184.15771484375
=== Iterazione IRL 562 ===
Loss reward (iter 562): -6131.6142578125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 275      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 7.42e+04 |
|    critic_loss     | 4.17e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 253199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 7.46e+04 |
|    critic_loss     | 5.25e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 253599   |
---------------------------------
=== Iterazione IRL 563 ===
Loss reward (iter 563): -6301.1328125
=== Iterazione IRL 564 ===
Loss reward (iter 564): -6355.10498046875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 7.59e+04 |
|    critic_loss     | 4.19e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 254099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 7.66e+04 |
|    critic_loss     | 4.22e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 254499   |
---------------------------------
=== Iterazione IRL 565 ===
Loss reward (iter 565): -6330.017578125
=== Iterazione IRL 566 ===
Loss reward (iter 566): -6377.078125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 7.68e+04 |
|    critic_loss     | 4.59e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 254999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 242      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 7.71e+04 |
|    critic_loss     | 4.92e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 255399   |
---------------------------------
=== Iterazione IRL 567 ===
Loss reward (iter 567): -6371.2802734375
=== Iterazione IRL 568 ===
Loss reward (iter 568): -6534.1044921875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 277      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 7.85e+04 |
|    critic_loss     | 6.07e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 255899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 243      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 7.84e+04 |
|    critic_loss     | 4.2e+06  |
|    learning_rate   | 0.001    |
|    n_updates       | 256299   |
---------------------------------
=== Iterazione IRL 569 ===
Loss reward (iter 569): -6510.39453125
=== Iterazione IRL 570 ===
Loss reward (iter 570): -6469.9931640625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 277      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 7.8e+04  |
|    critic_loss     | 4.77e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 256799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 7.87e+04 |
|    critic_loss     | 4.86e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 257199   |
---------------------------------
=== Iterazione IRL 571 ===
Loss reward (iter 571): -6588.25146484375
=== Iterazione IRL 572 ===
Loss reward (iter 572): -6682.0615234375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 238      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 8.13e+04 |
|    critic_loss     | 4.56e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 257699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 209      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 7.98e+04 |
|    critic_loss     | 4.63e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 258099   |
---------------------------------
=== Iterazione IRL 573 ===
Loss reward (iter 573): -6687.90234375
=== Iterazione IRL 574 ===
Loss reward (iter 574): -6790.89404296875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 239      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 8.24e+04 |
|    critic_loss     | 5.19e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 258599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 209      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 8.25e+04 |
|    critic_loss     | 5.03e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 258999   |
---------------------------------
=== Iterazione IRL 575 ===
Loss reward (iter 575): -6760.6728515625
=== Iterazione IRL 576 ===
Loss reward (iter 576): -6657.52783203125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 261      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 8.21e+04 |
|    critic_loss     | 6.35e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 259499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 217      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 8.51e+04 |
|    critic_loss     | 5.48e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 259899   |
---------------------------------
=== Iterazione IRL 577 ===
Loss reward (iter 577): -6785.8955078125
=== Iterazione IRL 578 ===
Loss reward (iter 578): -6770.99560546875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 239      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 8.34e+04 |
|    critic_loss     | 5.85e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 260399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 209      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 8.62e+04 |
|    critic_loss     | 5.85e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 260799   |
---------------------------------
=== Iterazione IRL 579 ===
Loss reward (iter 579): -6943.81689453125
=== Iterazione IRL 580 ===
Loss reward (iter 580): -6951.404296875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 238      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 8.61e+04 |
|    critic_loss     | 5.05e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 261299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 209      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 8.65e+04 |
|    critic_loss     | 6.31e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 261699   |
---------------------------------
=== Iterazione IRL 581 ===
Loss reward (iter 581): -6982.4619140625
=== Iterazione IRL 582 ===
Loss reward (iter 582): -7005.2939453125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 239      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 8.6e+04  |
|    critic_loss     | 5.08e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 262199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 210      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 8.65e+04 |
|    critic_loss     | 5.8e+06  |
|    learning_rate   | 0.001    |
|    n_updates       | 262599   |
---------------------------------
=== Iterazione IRL 583 ===
Loss reward (iter 583): -7098.33642578125
=== Iterazione IRL 584 ===
Loss reward (iter 584): -7092.21923828125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 238      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 8.64e+04 |
|    critic_loss     | 5.62e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 263099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 209      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 8.92e+04 |
|    critic_loss     | 5e+06    |
|    learning_rate   | 0.001    |
|    n_updates       | 263499   |
---------------------------------
=== Iterazione IRL 585 ===
Loss reward (iter 585): -7193.298828125
=== Iterazione IRL 586 ===
Loss reward (iter 586): -7257.12451171875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 239      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 8.85e+04 |
|    critic_loss     | 7.02e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 263999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 209      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 9.03e+04 |
|    critic_loss     | 6.58e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 264399   |
---------------------------------
=== Iterazione IRL 587 ===
Loss reward (iter 587): -7294.11328125
=== Iterazione IRL 588 ===
Loss reward (iter 588): -7300.02587890625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 238      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 9.09e+04 |
|    critic_loss     | 6.62e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 264899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 209      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 8.95e+04 |
|    critic_loss     | 6.38e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 265299   |
---------------------------------
=== Iterazione IRL 589 ===
Loss reward (iter 589): -7358.25830078125
=== Iterazione IRL 590 ===
Loss reward (iter 590): -7413.10986328125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 239      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 9.22e+04 |
|    critic_loss     | 5.93e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 265799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 209      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 9.29e+04 |
|    critic_loss     | 7.25e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 266199   |
---------------------------------
=== Iterazione IRL 591 ===
Loss reward (iter 591): -7493.3076171875
=== Iterazione IRL 592 ===
Loss reward (iter 592): -7525.5439453125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 239      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 9.27e+04 |
|    critic_loss     | 6.66e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 266699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 209      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 9.47e+04 |
|    critic_loss     | 5.53e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 267099   |
---------------------------------
=== Iterazione IRL 593 ===
Loss reward (iter 593): -7441.88232421875
=== Iterazione IRL 594 ===
Loss reward (iter 594): -7549.7607421875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 9.36e+04 |
|    critic_loss     | 7.47e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 267599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 9.78e+04 |
|    critic_loss     | 6.91e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 267999   |
---------------------------------
=== Iterazione IRL 595 ===
Loss reward (iter 595): -7674.93701171875
=== Iterazione IRL 596 ===
Loss reward (iter 596): -7661.8115234375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 9.55e+04 |
|    critic_loss     | 6.67e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 268499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 9.59e+04 |
|    critic_loss     | 6.77e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 268899   |
---------------------------------
=== Iterazione IRL 597 ===
Loss reward (iter 597): -7669.951171875
=== Iterazione IRL 598 ===
Loss reward (iter 598): -7643.8046875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 9.78e+04 |
|    critic_loss     | 6.66e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 269399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 9.72e+04 |
|    critic_loss     | 7.18e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 269799   |
---------------------------------
=== Iterazione IRL 599 ===
Loss reward (iter 599): -7825.306640625
=== Iterazione IRL 600 ===
Loss reward (iter 600): -7829.3603515625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 9.85e+04 |
|    critic_loss     | 6.76e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 270299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 9.86e+04 |
|    critic_loss     | 5.99e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 270699   |
---------------------------------
=== Iterazione IRL 601 ===
Loss reward (iter 601): -7939.5595703125
=== Iterazione IRL 602 ===
Loss reward (iter 602): -8015.212890625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1e+05    |
|    critic_loss     | 6.66e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 271199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 9.91e+04 |
|    critic_loss     | 8.1e+06  |
|    learning_rate   | 0.001    |
|    n_updates       | 271599   |
---------------------------------
=== Iterazione IRL 603 ===
Loss reward (iter 603): -7980.30322265625
=== Iterazione IRL 604 ===
Loss reward (iter 604): -8065.99267578125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.02e+05 |
|    critic_loss     | 6.93e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 272099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.03e+05 |
|    critic_loss     | 8.98e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 272499   |
---------------------------------
=== Iterazione IRL 605 ===
Loss reward (iter 605): -8115.54345703125
=== Iterazione IRL 606 ===
Loss reward (iter 606): -8044.89013671875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 9.93e+04 |
|    critic_loss     | 7.69e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 272999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.03e+05 |
|    critic_loss     | 8.56e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 273399   |
---------------------------------
=== Iterazione IRL 607 ===
Loss reward (iter 607): -8179.8681640625
=== Iterazione IRL 608 ===
Loss reward (iter 608): -8270.1171875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.05e+05 |
|    critic_loss     | 7.79e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 273899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.03e+05 |
|    critic_loss     | 7.91e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 274299   |
---------------------------------
=== Iterazione IRL 609 ===
Loss reward (iter 609): -8244.625
=== Iterazione IRL 610 ===
Loss reward (iter 610): -8289.205078125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.05e+05 |
|    critic_loss     | 8.72e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 274799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.08e+05 |
|    critic_loss     | 9.73e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 275199   |
---------------------------------
=== Iterazione IRL 611 ===
Loss reward (iter 611): -8425.255859375
=== Iterazione IRL 612 ===
Loss reward (iter 612): -8395.900390625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.08e+05 |
|    critic_loss     | 7.42e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 275699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.07e+05 |
|    critic_loss     | 9.12e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 276099   |
---------------------------------
=== Iterazione IRL 613 ===
Loss reward (iter 613): -8485.2734375
=== Iterazione IRL 614 ===
Loss reward (iter 614): -8491.1044921875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.11e+05 |
|    critic_loss     | 7.33e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 276599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.07e+05 |
|    critic_loss     | 9.28e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 276999   |
---------------------------------
=== Iterazione IRL 615 ===
Loss reward (iter 615): -8545.173828125
=== Iterazione IRL 616 ===
Loss reward (iter 616): -8567.6171875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.1e+05  |
|    critic_loss     | 7.42e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 277499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.1e+05  |
|    critic_loss     | 9.62e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 277899   |
---------------------------------
=== Iterazione IRL 617 ===
Loss reward (iter 617): -8706.244140625
=== Iterazione IRL 618 ===
Loss reward (iter 618): -8830.8203125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.11e+05 |
|    critic_loss     | 8.6e+06  |
|    learning_rate   | 0.001    |
|    n_updates       | 278399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.12e+05 |
|    critic_loss     | 8.66e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 278799   |
---------------------------------
=== Iterazione IRL 619 ===
Loss reward (iter 619): -8797.8525390625
=== Iterazione IRL 620 ===
Loss reward (iter 620): -8883.48046875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.14e+05 |
|    critic_loss     | 1.07e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 279299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.14e+05 |
|    critic_loss     | 7.33e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 279699   |
---------------------------------
=== Iterazione IRL 621 ===
Loss reward (iter 621): -8922.62890625
=== Iterazione IRL 622 ===
Loss reward (iter 622): -8914.033203125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.13e+05 |
|    critic_loss     | 9.09e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 280199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.16e+05 |
|    critic_loss     | 9.01e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 280599   |
---------------------------------
=== Iterazione IRL 623 ===
Loss reward (iter 623): -8910.3232421875
=== Iterazione IRL 624 ===
Loss reward (iter 624): -9065.9345703125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.17e+05 |
|    critic_loss     | 9.62e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 281099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.18e+05 |
|    critic_loss     | 8.99e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 281499   |
---------------------------------
=== Iterazione IRL 625 ===
Loss reward (iter 625): -9156.736328125
=== Iterazione IRL 626 ===
Loss reward (iter 626): -9148.3984375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.2e+05  |
|    critic_loss     | 1.14e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 281999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.16e+05 |
|    critic_loss     | 1.1e+07  |
|    learning_rate   | 0.001    |
|    n_updates       | 282399   |
---------------------------------
=== Iterazione IRL 627 ===
Loss reward (iter 627): -9148.9736328125
=== Iterazione IRL 628 ===
Loss reward (iter 628): -9223.7509765625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.2e+05  |
|    critic_loss     | 9.34e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 282899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.2e+05  |
|    critic_loss     | 1.06e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 283299   |
---------------------------------
=== Iterazione IRL 629 ===
Loss reward (iter 629): -9348.8349609375
=== Iterazione IRL 630 ===
Loss reward (iter 630): -9276.083984375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.2e+05  |
|    critic_loss     | 9.51e+06 |
|    learning_rate   | 0.001    |
|    n_updates       | 283799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.2e+05  |
|    critic_loss     | 1.19e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 284199   |
---------------------------------
=== Iterazione IRL 631 ===
Loss reward (iter 631): -9384.9736328125
=== Iterazione IRL 632 ===
Loss reward (iter 632): -9390.9775390625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.22e+05 |
|    critic_loss     | 1.01e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 284699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.26e+05 |
|    critic_loss     | 1.1e+07  |
|    learning_rate   | 0.001    |
|    n_updates       | 285099   |
---------------------------------
=== Iterazione IRL 633 ===
Loss reward (iter 633): -9454.6875
=== Iterazione IRL 634 ===
Loss reward (iter 634): -9562.0712890625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.24e+05 |
|    critic_loss     | 1.25e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 285599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.25e+05 |
|    critic_loss     | 1.04e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 285999   |
---------------------------------
=== Iterazione IRL 635 ===
Loss reward (iter 635): -9596.4228515625
=== Iterazione IRL 636 ===
Loss reward (iter 636): -9732.6240234375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.25e+05 |
|    critic_loss     | 1.17e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 286499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.28e+05 |
|    critic_loss     | 1.17e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 286899   |
---------------------------------
=== Iterazione IRL 637 ===
Loss reward (iter 637): -9729.783203125
=== Iterazione IRL 638 ===
Loss reward (iter 638): -9775.328125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.26e+05 |
|    critic_loss     | 1.02e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 287399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.3e+05  |
|    critic_loss     | 1.09e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 287799   |
---------------------------------
=== Iterazione IRL 639 ===
Loss reward (iter 639): -9817.5751953125
=== Iterazione IRL 640 ===
Loss reward (iter 640): -9911.0615234375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.31e+05 |
|    critic_loss     | 1.18e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 288299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.3e+05  |
|    critic_loss     | 1.24e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 288699   |
---------------------------------
=== Iterazione IRL 641 ===
Loss reward (iter 641): -9921.04296875
=== Iterazione IRL 642 ===
Loss reward (iter 642): -9779.8037109375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.32e+05 |
|    critic_loss     | 1.12e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 289199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.27e+05 |
|    critic_loss     | 1.13e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 289599   |
---------------------------------
=== Iterazione IRL 643 ===
Loss reward (iter 643): -10048.0
=== Iterazione IRL 644 ===
Loss reward (iter 644): -10192.4462890625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.31e+05 |
|    critic_loss     | 1.06e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 290099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.32e+05 |
|    critic_loss     | 1.33e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 290499   |
---------------------------------
=== Iterazione IRL 645 ===
Loss reward (iter 645): -10028.1171875
=== Iterazione IRL 646 ===
Loss reward (iter 646): -10142.048828125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.34e+05 |
|    critic_loss     | 1.19e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 290999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.35e+05 |
|    critic_loss     | 1.17e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 291399   |
---------------------------------
=== Iterazione IRL 647 ===
Loss reward (iter 647): -10127.6357421875
=== Iterazione IRL 648 ===
Loss reward (iter 648): -10274.904296875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.35e+05 |
|    critic_loss     | 1.3e+07  |
|    learning_rate   | 0.001    |
|    n_updates       | 291899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.36e+05 |
|    critic_loss     | 1.12e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 292299   |
---------------------------------
=== Iterazione IRL 649 ===
Loss reward (iter 649): -10406.88671875
=== Iterazione IRL 650 ===
Loss reward (iter 650): -10311.8935546875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.39e+05 |
|    critic_loss     | 1.19e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 292799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.36e+05 |
|    critic_loss     | 1.38e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 293199   |
---------------------------------
=== Iterazione IRL 651 ===
Loss reward (iter 651): -10418.1728515625
=== Iterazione IRL 652 ===
Loss reward (iter 652): -10481.953125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.41e+05 |
|    critic_loss     | 1.07e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 293699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.39e+05 |
|    critic_loss     | 1.3e+07  |
|    learning_rate   | 0.001    |
|    n_updates       | 294099   |
---------------------------------
=== Iterazione IRL 653 ===
Loss reward (iter 653): -10474.234375
=== Iterazione IRL 654 ===
Loss reward (iter 654): -10661.6474609375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.43e+05 |
|    critic_loss     | 1.26e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 294599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.42e+05 |
|    critic_loss     | 1.31e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 294999   |
---------------------------------
=== Iterazione IRL 655 ===
Loss reward (iter 655): -10713.298828125
=== Iterazione IRL 656 ===
Loss reward (iter 656): -10808.271484375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.41e+05 |
|    critic_loss     | 1.22e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 295499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.44e+05 |
|    critic_loss     | 1.42e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 295899   |
---------------------------------
=== Iterazione IRL 657 ===
Loss reward (iter 657): -10748.2177734375
=== Iterazione IRL 658 ===
Loss reward (iter 658): -10828.19921875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.44e+05 |
|    critic_loss     | 1.44e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 296399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.44e+05 |
|    critic_loss     | 1.73e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 296799   |
---------------------------------
=== Iterazione IRL 659 ===
Loss reward (iter 659): -10829.54296875
=== Iterazione IRL 660 ===
Loss reward (iter 660): -11016.201171875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.47e+05 |
|    critic_loss     | 1.45e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 297299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.48e+05 |
|    critic_loss     | 1.41e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 297699   |
---------------------------------
=== Iterazione IRL 661 ===
Loss reward (iter 661): -11074.662109375
=== Iterazione IRL 662 ===
Loss reward (iter 662): -11287.9111328125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.44e+05 |
|    critic_loss     | 1.45e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 298199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.48e+05 |
|    critic_loss     | 1.4e+07  |
|    learning_rate   | 0.001    |
|    n_updates       | 298599   |
---------------------------------
=== Iterazione IRL 663 ===
Loss reward (iter 663): -11169.326171875
=== Iterazione IRL 664 ===
Loss reward (iter 664): -11183.9951171875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.5e+05  |
|    critic_loss     | 1.29e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 299099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.49e+05 |
|    critic_loss     | 1.4e+07  |
|    learning_rate   | 0.001    |
|    n_updates       | 299499   |
---------------------------------
=== Iterazione IRL 665 ===
Loss reward (iter 665): -11309.9716796875
=== Iterazione IRL 666 ===
Loss reward (iter 666): -11289.9443359375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.57e+05 |
|    critic_loss     | 1.71e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 299999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.55e+05 |
|    critic_loss     | 1.29e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 300399   |
---------------------------------
=== Iterazione IRL 667 ===
Loss reward (iter 667): -11402.2314453125
=== Iterazione IRL 668 ===
Loss reward (iter 668): -11475.6669921875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.52e+05 |
|    critic_loss     | 1.77e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 300899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.52e+05 |
|    critic_loss     | 1.46e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 301299   |
---------------------------------
=== Iterazione IRL 669 ===
Loss reward (iter 669): -11450.73046875
=== Iterazione IRL 670 ===
Loss reward (iter 670): -11581.060546875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.54e+05 |
|    critic_loss     | 1.6e+07  |
|    learning_rate   | 0.001    |
|    n_updates       | 301799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.59e+05 |
|    critic_loss     | 1.46e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 302199   |
---------------------------------
=== Iterazione IRL 671 ===
Loss reward (iter 671): -11727.3291015625
=== Iterazione IRL 672 ===
Loss reward (iter 672): -11632.8369140625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.56e+05 |
|    critic_loss     | 1.72e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 302699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.57e+05 |
|    critic_loss     | 1.57e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 303099   |
---------------------------------
=== Iterazione IRL 673 ===
Loss reward (iter 673): -11831.61328125
=== Iterazione IRL 674 ===
Loss reward (iter 674): -11871.9296875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.6e+05  |
|    critic_loss     | 1.6e+07  |
|    learning_rate   | 0.001    |
|    n_updates       | 303599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.6e+05  |
|    critic_loss     | 1.51e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 303999   |
---------------------------------
=== Iterazione IRL 675 ===
Loss reward (iter 675): -11804.7236328125
=== Iterazione IRL 676 ===
Loss reward (iter 676): -11974.2255859375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.61e+05 |
|    critic_loss     | 1.34e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 304499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.61e+05 |
|    critic_loss     | 1.82e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 304899   |
---------------------------------
=== Iterazione IRL 677 ===
Loss reward (iter 677): -11998.732421875
=== Iterazione IRL 678 ===
Loss reward (iter 678): -11988.712890625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.64e+05 |
|    critic_loss     | 2.16e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 305399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.66e+05 |
|    critic_loss     | 1.76e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 305799   |
---------------------------------
=== Iterazione IRL 679 ===
Loss reward (iter 679): -12149.6552734375
=== Iterazione IRL 680 ===
Loss reward (iter 680): -12208.4970703125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.62e+05 |
|    critic_loss     | 1.99e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 306299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.66e+05 |
|    critic_loss     | 1.94e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 306699   |
---------------------------------
=== Iterazione IRL 681 ===
Loss reward (iter 681): -12137.7294921875
=== Iterazione IRL 682 ===
Loss reward (iter 682): -12279.6865234375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.67e+05 |
|    critic_loss     | 1.76e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 307199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.65e+05 |
|    critic_loss     | 2.1e+07  |
|    learning_rate   | 0.001    |
|    n_updates       | 307599   |
---------------------------------
=== Iterazione IRL 683 ===
Loss reward (iter 683): -12403.080078125
=== Iterazione IRL 684 ===
Loss reward (iter 684): -12499.15625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.7e+05  |
|    critic_loss     | 1.81e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 308099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.71e+05 |
|    critic_loss     | 1.7e+07  |
|    learning_rate   | 0.001    |
|    n_updates       | 308499   |
---------------------------------
=== Iterazione IRL 685 ===
Loss reward (iter 685): -12466.060546875
=== Iterazione IRL 686 ===
Loss reward (iter 686): -12652.9326171875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.68e+05 |
|    critic_loss     | 1.65e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 308999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.75e+05 |
|    critic_loss     | 1.85e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 309399   |
---------------------------------
=== Iterazione IRL 687 ===
Loss reward (iter 687): -12520.0361328125
=== Iterazione IRL 688 ===
Loss reward (iter 688): -12847.98046875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.76e+05 |
|    critic_loss     | 1.93e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 309899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.73e+05 |
|    critic_loss     | 1.8e+07  |
|    learning_rate   | 0.001    |
|    n_updates       | 310299   |
---------------------------------
=== Iterazione IRL 689 ===
Loss reward (iter 689): -12715.896484375
=== Iterazione IRL 690 ===
Loss reward (iter 690): -12834.2646484375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.77e+05 |
|    critic_loss     | 1.59e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 310799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.76e+05 |
|    critic_loss     | 2.02e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 311199   |
---------------------------------
=== Iterazione IRL 691 ===
Loss reward (iter 691): -12760.8642578125
=== Iterazione IRL 692 ===
Loss reward (iter 692): -12897.9658203125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.79e+05 |
|    critic_loss     | 2.12e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 311699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.79e+05 |
|    critic_loss     | 2.01e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 312099   |
---------------------------------
=== Iterazione IRL 693 ===
Loss reward (iter 693): -12842.8544921875
=== Iterazione IRL 694 ===
Loss reward (iter 694): -13174.5068359375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.8e+05  |
|    critic_loss     | 2.09e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 312599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.82e+05 |
|    critic_loss     | 2.06e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 312999   |
---------------------------------
=== Iterazione IRL 695 ===
Loss reward (iter 695): -13110.1962890625
=== Iterazione IRL 696 ===
Loss reward (iter 696): -13148.837890625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.84e+05 |
|    critic_loss     | 1.94e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 313499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.82e+05 |
|    critic_loss     | 2.32e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 313899   |
---------------------------------
=== Iterazione IRL 697 ===
Loss reward (iter 697): -13257.865234375
=== Iterazione IRL 698 ===
Loss reward (iter 698): -13346.73828125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.83e+05 |
|    critic_loss     | 2.03e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 314399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.82e+05 |
|    critic_loss     | 1.75e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 314799   |
---------------------------------
=== Iterazione IRL 699 ===
Loss reward (iter 699): -13411.5830078125
=== Iterazione IRL 700 ===
Loss reward (iter 700): -13432.54296875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.84e+05 |
|    critic_loss     | 1.99e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 315299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.88e+05 |
|    critic_loss     | 2.26e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 315699   |
---------------------------------
=== Iterazione IRL 701 ===
Loss reward (iter 701): -13404.2568359375
=== Iterazione IRL 702 ===
Loss reward (iter 702): -13443.388671875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.91e+05 |
|    critic_loss     | 2.12e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 316199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.87e+05 |
|    critic_loss     | 2.16e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 316599   |
---------------------------------
=== Iterazione IRL 703 ===
Loss reward (iter 703): -13667.08984375
=== Iterazione IRL 704 ===
Loss reward (iter 704): -13832.650390625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.92e+05 |
|    critic_loss     | 2.14e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 317099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.91e+05 |
|    critic_loss     | 2.09e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 317499   |
---------------------------------
=== Iterazione IRL 705 ===
Loss reward (iter 705): -13948.7529296875
=== Iterazione IRL 706 ===
Loss reward (iter 706): -13910.544921875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.95e+05 |
|    critic_loss     | 1.89e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 317999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.94e+05 |
|    critic_loss     | 2.22e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 318399   |
---------------------------------
=== Iterazione IRL 707 ===
Loss reward (iter 707): -13938.951171875
=== Iterazione IRL 708 ===
Loss reward (iter 708): -13870.5791015625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.98e+05 |
|    critic_loss     | 2.53e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 318899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2e+05    |
|    critic_loss     | 2.39e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 319299   |
---------------------------------
=== Iterazione IRL 709 ===
Loss reward (iter 709): -14212.5205078125
=== Iterazione IRL 710 ===
Loss reward (iter 710): -14174.7412109375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.9e+05  |
|    critic_loss     | 2.54e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 319799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.96e+05 |
|    critic_loss     | 2.6e+07  |
|    learning_rate   | 0.001    |
|    n_updates       | 320199   |
---------------------------------
=== Iterazione IRL 711 ===
Loss reward (iter 711): -13984.5869140625
=== Iterazione IRL 712 ===
Loss reward (iter 712): -14151.6015625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.99e+05 |
|    critic_loss     | 2.52e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 320699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.98e+05 |
|    critic_loss     | 2.57e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 321099   |
---------------------------------
=== Iterazione IRL 713 ===
Loss reward (iter 713): -14241.0205078125
=== Iterazione IRL 714 ===
Loss reward (iter 714): -14334.875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.98e+05 |
|    critic_loss     | 1.97e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 321599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.01e+05 |
|    critic_loss     | 2.49e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 321999   |
---------------------------------
=== Iterazione IRL 715 ===
Loss reward (iter 715): -14591.265625
=== Iterazione IRL 716 ===
Loss reward (iter 716): -14706.8056640625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.02e+05 |
|    critic_loss     | 2.3e+07  |
|    learning_rate   | 0.001    |
|    n_updates       | 322499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.02e+05 |
|    critic_loss     | 2.37e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 322899   |
---------------------------------
=== Iterazione IRL 717 ===
Loss reward (iter 717): -14375.779296875
=== Iterazione IRL 718 ===
Loss reward (iter 718): -14655.2548828125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.09e+05 |
|    critic_loss     | 2.84e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 323399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.07e+05 |
|    critic_loss     | 2.49e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 323799   |
---------------------------------
=== Iterazione IRL 719 ===
Loss reward (iter 719): -14830.423828125
=== Iterazione IRL 720 ===
Loss reward (iter 720): -14793.3134765625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.05e+05 |
|    critic_loss     | 2.39e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 324299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.11e+05 |
|    critic_loss     | 2.91e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 324699   |
---------------------------------
=== Iterazione IRL 721 ===
Loss reward (iter 721): -14852.412109375
=== Iterazione IRL 722 ===
Loss reward (iter 722): -15077.6318359375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.06e+05 |
|    critic_loss     | 2.6e+07  |
|    learning_rate   | 0.001    |
|    n_updates       | 325199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.14e+05 |
|    critic_loss     | 2.82e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 325599   |
---------------------------------
=== Iterazione IRL 723 ===
Loss reward (iter 723): -15181.060546875
=== Iterazione IRL 724 ===
Loss reward (iter 724): -15200.525390625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.08e+05 |
|    critic_loss     | 2.76e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 326099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.15e+05 |
|    critic_loss     | 2.63e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 326499   |
---------------------------------
=== Iterazione IRL 725 ===
Loss reward (iter 725): -15221.6142578125
=== Iterazione IRL 726 ===
Loss reward (iter 726): -15300.0458984375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.16e+05 |
|    critic_loss     | 2.71e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 326999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.2e+05  |
|    critic_loss     | 2.49e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 327399   |
---------------------------------
=== Iterazione IRL 727 ===
Loss reward (iter 727): -15376.33984375
=== Iterazione IRL 728 ===
Loss reward (iter 728): -15596.60546875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.15e+05 |
|    critic_loss     | 2.54e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 327899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.22e+05 |
|    critic_loss     | 2.68e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 328299   |
---------------------------------
=== Iterazione IRL 729 ===
Loss reward (iter 729): -15585.779296875
=== Iterazione IRL 730 ===
Loss reward (iter 730): -15445.330078125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.18e+05 |
|    critic_loss     | 2.79e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 328799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.21e+05 |
|    critic_loss     | 3.03e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 329199   |
---------------------------------
=== Iterazione IRL 731 ===
Loss reward (iter 731): -15566.6552734375
=== Iterazione IRL 732 ===
Loss reward (iter 732): -15647.2392578125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.23e+05 |
|    critic_loss     | 3.32e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 329699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.2e+05  |
|    critic_loss     | 3.04e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 330099   |
---------------------------------
=== Iterazione IRL 733 ===
Loss reward (iter 733): -15783.5517578125
=== Iterazione IRL 734 ===
Loss reward (iter 734): -15792.3486328125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.25e+05 |
|    critic_loss     | 2.52e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 330599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.26e+05 |
|    critic_loss     | 2.77e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 330999   |
---------------------------------
=== Iterazione IRL 735 ===
Loss reward (iter 735): -15958.4140625
=== Iterazione IRL 736 ===
Loss reward (iter 736): -16017.837890625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.3e+05  |
|    critic_loss     | 3.26e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 331499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.28e+05 |
|    critic_loss     | 2.92e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 331899   |
---------------------------------
=== Iterazione IRL 737 ===
Loss reward (iter 737): -15890.5986328125
=== Iterazione IRL 738 ===
Loss reward (iter 738): -16029.2685546875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.27e+05 |
|    critic_loss     | 2.63e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 332399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.27e+05 |
|    critic_loss     | 2.99e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 332799   |
---------------------------------
=== Iterazione IRL 739 ===
Loss reward (iter 739): -16103.298828125
=== Iterazione IRL 740 ===
Loss reward (iter 740): -16083.888671875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.32e+05 |
|    critic_loss     | 2.64e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 333299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.27e+05 |
|    critic_loss     | 3.28e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 333699   |
---------------------------------
=== Iterazione IRL 741 ===
Loss reward (iter 741): -16493.81640625
=== Iterazione IRL 742 ===
Loss reward (iter 742): -16545.998046875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 276      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.39e+05 |
|    critic_loss     | 3.08e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 334199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 242      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.33e+05 |
|    critic_loss     | 3.07e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 334599   |
---------------------------------
=== Iterazione IRL 743 ===
Loss reward (iter 743): -16577.9921875
=== Iterazione IRL 744 ===
Loss reward (iter 744): -16578.685546875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.35e+05 |
|    critic_loss     | 3.44e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 335099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 241      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.37e+05 |
|    critic_loss     | 3.06e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 335499   |
---------------------------------
=== Iterazione IRL 745 ===
Loss reward (iter 745): -16640.43359375
=== Iterazione IRL 746 ===
Loss reward (iter 746): -16912.68359375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 277      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.37e+05 |
|    critic_loss     | 2.56e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 335999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 243      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.4e+05  |
|    critic_loss     | 2.97e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 336399   |
---------------------------------
=== Iterazione IRL 747 ===
Loss reward (iter 747): -16898.869140625
=== Iterazione IRL 748 ===
Loss reward (iter 748): -16948.427734375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 277      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.49e+05 |
|    critic_loss     | 3.29e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 336899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 243      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.47e+05 |
|    critic_loss     | 3.57e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 337299   |
---------------------------------
=== Iterazione IRL 749 ===
Loss reward (iter 749): -17136.189453125
=== Iterazione IRL 750 ===
Loss reward (iter 750): -16991.140625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 277      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.39e+05 |
|    critic_loss     | 2.63e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 337799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 243      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.49e+05 |
|    critic_loss     | 3.43e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 338199   |
---------------------------------
=== Iterazione IRL 751 ===
Loss reward (iter 751): -17411.6796875
=== Iterazione IRL 752 ===
Loss reward (iter 752): -17280.908203125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 277      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.47e+05 |
|    critic_loss     | 3.91e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 338699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 243      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.51e+05 |
|    critic_loss     | 3.35e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 339099   |
---------------------------------
=== Iterazione IRL 753 ===
Loss reward (iter 753): -17425.029296875
=== Iterazione IRL 754 ===
Loss reward (iter 754): -17611.8984375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 277      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.51e+05 |
|    critic_loss     | 3.39e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 339599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 243      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.47e+05 |
|    critic_loss     | 3.91e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 339999   |
---------------------------------
=== Iterazione IRL 755 ===
Loss reward (iter 755): -17442.41796875
=== Iterazione IRL 756 ===
Loss reward (iter 756): -17484.7890625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 277      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.46e+05 |
|    critic_loss     | 4.14e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 340499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 243      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.56e+05 |
|    critic_loss     | 3.28e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 340899   |
---------------------------------
=== Iterazione IRL 757 ===
Loss reward (iter 757): -17560.267578125
=== Iterazione IRL 758 ===
Loss reward (iter 758): -17798.365234375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 277      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.52e+05 |
|    critic_loss     | 3.43e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 341399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 243      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.58e+05 |
|    critic_loss     | 3.4e+07  |
|    learning_rate   | 0.001    |
|    n_updates       | 341799   |
---------------------------------
=== Iterazione IRL 759 ===
Loss reward (iter 759): -17744.81640625
=== Iterazione IRL 760 ===
Loss reward (iter 760): -17911.111328125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 277      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.58e+05 |
|    critic_loss     | 4.06e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 342299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 243      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.59e+05 |
|    critic_loss     | 3.46e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 342699   |
---------------------------------
=== Iterazione IRL 761 ===
Loss reward (iter 761): -18128.3046875
=== Iterazione IRL 762 ===
Loss reward (iter 762): -18044.90234375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 277      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.55e+05 |
|    critic_loss     | 3.34e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 343199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 243      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.59e+05 |
|    critic_loss     | 3.54e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 343599   |
---------------------------------
=== Iterazione IRL 763 ===
Loss reward (iter 763): -18052.05859375
=== Iterazione IRL 764 ===
Loss reward (iter 764): -18354.8125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 277      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.67e+05 |
|    critic_loss     | 4.2e+07  |
|    learning_rate   | 0.001    |
|    n_updates       | 344099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 243      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.59e+05 |
|    critic_loss     | 3.8e+07  |
|    learning_rate   | 0.001    |
|    n_updates       | 344499   |
---------------------------------
=== Iterazione IRL 765 ===
Loss reward (iter 765): -18207.58984375
=== Iterazione IRL 766 ===
Loss reward (iter 766): -18441.080078125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 277      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.65e+05 |
|    critic_loss     | 3.93e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 344999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 243      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.65e+05 |
|    critic_loss     | 4e+07    |
|    learning_rate   | 0.001    |
|    n_updates       | 345399   |
---------------------------------
=== Iterazione IRL 767 ===
Loss reward (iter 767): -18393.009765625
=== Iterazione IRL 768 ===
Loss reward (iter 768): -18554.79296875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 278      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.69e+05 |
|    critic_loss     | 3.53e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 345899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 243      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.67e+05 |
|    critic_loss     | 4.05e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 346299   |
---------------------------------
=== Iterazione IRL 769 ===
Loss reward (iter 769): -18605.580078125
=== Iterazione IRL 770 ===
Loss reward (iter 770): -18749.96484375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.72e+05 |
|    critic_loss     | 3.65e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 346799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.74e+05 |
|    critic_loss     | 3.34e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 347199   |
---------------------------------
=== Iterazione IRL 771 ===
Loss reward (iter 771): -18831.603515625
=== Iterazione IRL 772 ===
Loss reward (iter 772): -18739.013671875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.73e+05 |
|    critic_loss     | 3.75e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 347699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.74e+05 |
|    critic_loss     | 4.21e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 348099   |
---------------------------------
=== Iterazione IRL 773 ===
Loss reward (iter 773): -19034.525390625
=== Iterazione IRL 774 ===
Loss reward (iter 774): -19202.89453125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.78e+05 |
|    critic_loss     | 4.52e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 348599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.77e+05 |
|    critic_loss     | 4.67e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 348999   |
---------------------------------
=== Iterazione IRL 775 ===
Loss reward (iter 775): -19238.3515625
=== Iterazione IRL 776 ===
Loss reward (iter 776): -19371.22265625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.8e+05  |
|    critic_loss     | 3.77e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 349499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.78e+05 |
|    critic_loss     | 4.12e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 349899   |
---------------------------------
=== Iterazione IRL 777 ===
Loss reward (iter 777): -19239.19140625
=== Iterazione IRL 778 ===
Loss reward (iter 778): -19388.484375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.88e+05 |
|    critic_loss     | 3.98e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 350399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.87e+05 |
|    critic_loss     | 4.42e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 350799   |
---------------------------------
=== Iterazione IRL 779 ===
Loss reward (iter 779): -19405.74609375
=== Iterazione IRL 780 ===
Loss reward (iter 780): -19451.423828125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.87e+05 |
|    critic_loss     | 4.71e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 351299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.83e+05 |
|    critic_loss     | 4.25e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 351699   |
---------------------------------
=== Iterazione IRL 781 ===
Loss reward (iter 781): -19617.60546875
=== Iterazione IRL 782 ===
Loss reward (iter 782): -19705.07421875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.87e+05 |
|    critic_loss     | 4.55e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 352199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.87e+05 |
|    critic_loss     | 4.87e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 352599   |
---------------------------------
=== Iterazione IRL 783 ===
Loss reward (iter 783): -19707.1796875
=== Iterazione IRL 784 ===
Loss reward (iter 784): -19737.1796875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.97e+05 |
|    critic_loss     | 4.43e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 353099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.92e+05 |
|    critic_loss     | 4.57e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 353499   |
---------------------------------
=== Iterazione IRL 785 ===
Loss reward (iter 785): -19967.57421875
=== Iterazione IRL 786 ===
Loss reward (iter 786): -19942.681640625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.85e+05 |
|    critic_loss     | 4.06e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 353999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.92e+05 |
|    critic_loss     | 4.97e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 354399   |
---------------------------------
=== Iterazione IRL 787 ===
Loss reward (iter 787): -20341.98046875
=== Iterazione IRL 788 ===
Loss reward (iter 788): -20094.912109375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.96e+05 |
|    critic_loss     | 4.02e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 354899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.96e+05 |
|    critic_loss     | 3.88e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 355299   |
---------------------------------
=== Iterazione IRL 789 ===
Loss reward (iter 789): -20005.216796875
=== Iterazione IRL 790 ===
Loss reward (iter 790): -20608.337890625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.97e+05 |
|    critic_loss     | 6.18e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 355799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.98e+05 |
|    critic_loss     | 4.66e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 356199   |
---------------------------------
=== Iterazione IRL 791 ===
Loss reward (iter 791): -20335.0859375
=== Iterazione IRL 792 ===
Loss reward (iter 792): -20758.279296875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 274      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.99e+05 |
|    critic_loss     | 5.21e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 356699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 242      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.06e+05 |
|    critic_loss     | 5.26e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 357099   |
---------------------------------
=== Iterazione IRL 793 ===
Loss reward (iter 793): -20685.76171875
=== Iterazione IRL 794 ===
Loss reward (iter 794): -20806.458984375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 277      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.02e+05 |
|    critic_loss     | 5.24e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 357599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 243      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.01e+05 |
|    critic_loss     | 5.01e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 357999   |
---------------------------------
=== Iterazione IRL 795 ===
Loss reward (iter 795): -20998.841796875
=== Iterazione IRL 796 ===
Loss reward (iter 796): -21011.37109375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 277      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.02e+05 |
|    critic_loss     | 4.71e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 358499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 243      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.07e+05 |
|    critic_loss     | 4.78e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 358899   |
---------------------------------
=== Iterazione IRL 797 ===
Loss reward (iter 797): -20952.091796875
=== Iterazione IRL 798 ===
Loss reward (iter 798): -21156.515625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 278      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.07e+05 |
|    critic_loss     | 5.85e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 359399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 243      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.1e+05  |
|    critic_loss     | 5.57e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 359799   |
---------------------------------
=== Iterazione IRL 799 ===
Loss reward (iter 799): -21083.416015625
=== Iterazione IRL 800 ===
Loss reward (iter 800): -21293.130859375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 277      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.09e+05 |
|    critic_loss     | 5.98e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 360299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 243      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.09e+05 |
|    critic_loss     | 5.9e+07  |
|    learning_rate   | 0.001    |
|    n_updates       | 360699   |
---------------------------------
=== Iterazione IRL 801 ===
Loss reward (iter 801): -21675.482421875
=== Iterazione IRL 802 ===
Loss reward (iter 802): -21245.6953125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 277      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.15e+05 |
|    critic_loss     | 5.9e+07  |
|    learning_rate   | 0.001    |
|    n_updates       | 361199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 243      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.16e+05 |
|    critic_loss     | 5.88e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 361599   |
---------------------------------
=== Iterazione IRL 803 ===
Loss reward (iter 803): -21671.611328125
=== Iterazione IRL 804 ===
Loss reward (iter 804): -21555.064453125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 278      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.16e+05 |
|    critic_loss     | 4.34e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 362099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 244      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.18e+05 |
|    critic_loss     | 5.5e+07  |
|    learning_rate   | 0.001    |
|    n_updates       | 362499   |
---------------------------------
=== Iterazione IRL 805 ===
Loss reward (iter 805): -21827.599609375
=== Iterazione IRL 806 ===
Loss reward (iter 806): -21699.287109375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 278      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.2e+05  |
|    critic_loss     | 6.48e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 362999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 243      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.21e+05 |
|    critic_loss     | 6.39e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 363399   |
---------------------------------
=== Iterazione IRL 807 ===
Loss reward (iter 807): -21970.9765625
=== Iterazione IRL 808 ===
Loss reward (iter 808): -22051.333984375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 277      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.22e+05 |
|    critic_loss     | 5.37e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 363899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 243      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.22e+05 |
|    critic_loss     | 6.61e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 364299   |
---------------------------------
=== Iterazione IRL 809 ===
Loss reward (iter 809): -22159.21484375
=== Iterazione IRL 810 ===
Loss reward (iter 810): -22008.04296875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 277      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.25e+05 |
|    critic_loss     | 6.33e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 364799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 243      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.27e+05 |
|    critic_loss     | 5.07e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 365199   |
---------------------------------
=== Iterazione IRL 811 ===
Loss reward (iter 811): -22460.67578125
=== Iterazione IRL 812 ===
Loss reward (iter 812): -22384.59375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 278      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.26e+05 |
|    critic_loss     | 5.55e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 365699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 243      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.32e+05 |
|    critic_loss     | 6.01e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 366099   |
---------------------------------
=== Iterazione IRL 813 ===
Loss reward (iter 813): -22572.970703125
=== Iterazione IRL 814 ===
Loss reward (iter 814): -22686.78125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 278      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.33e+05 |
|    critic_loss     | 5.84e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 366599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 243      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.32e+05 |
|    critic_loss     | 6.1e+07  |
|    learning_rate   | 0.001    |
|    n_updates       | 366999   |
---------------------------------
=== Iterazione IRL 815 ===
Loss reward (iter 815): -22582.4375
=== Iterazione IRL 816 ===
Loss reward (iter 816): -22781.220703125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 277      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.35e+05 |
|    critic_loss     | 5.96e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 367499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 243      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.37e+05 |
|    critic_loss     | 5.78e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 367899   |
---------------------------------
=== Iterazione IRL 817 ===
Loss reward (iter 817): -23014.576171875
=== Iterazione IRL 818 ===
Loss reward (iter 818): -23307.857421875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 277      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.34e+05 |
|    critic_loss     | 5.99e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 368399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 243      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.41e+05 |
|    critic_loss     | 6.03e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 368799   |
---------------------------------
=== Iterazione IRL 819 ===
Loss reward (iter 819): -22862.125
=== Iterazione IRL 820 ===
Loss reward (iter 820): -23093.708984375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 277      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.37e+05 |
|    critic_loss     | 6.49e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 369299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 243      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.41e+05 |
|    critic_loss     | 6e+07    |
|    learning_rate   | 0.001    |
|    n_updates       | 369699   |
---------------------------------
=== Iterazione IRL 821 ===
Loss reward (iter 821): -23479.087890625
=== Iterazione IRL 822 ===
Loss reward (iter 822): -23793.3828125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 277      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.42e+05 |
|    critic_loss     | 6.45e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 370199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 243      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.48e+05 |
|    critic_loss     | 6.9e+07  |
|    learning_rate   | 0.001    |
|    n_updates       | 370599   |
---------------------------------
=== Iterazione IRL 823 ===
Loss reward (iter 823): -23503.322265625
=== Iterazione IRL 824 ===
Loss reward (iter 824): -23855.708984375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 238      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.43e+05 |
|    critic_loss     | 6.84e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 371099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 209      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.43e+05 |
|    critic_loss     | 6.02e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 371499   |
---------------------------------
=== Iterazione IRL 825 ===
Loss reward (iter 825): -23612.25390625
=== Iterazione IRL 826 ===
Loss reward (iter 826): -23787.3828125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 239      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.45e+05 |
|    critic_loss     | 6.94e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 371999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 209      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.4e+05  |
|    critic_loss     | 6.7e+07  |
|    learning_rate   | 0.001    |
|    n_updates       | 372399   |
---------------------------------
=== Iterazione IRL 827 ===
Loss reward (iter 827): -24007.21875
=== Iterazione IRL 828 ===
Loss reward (iter 828): -24007.310546875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 238      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.56e+05 |
|    critic_loss     | 6.78e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 372899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 209      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.57e+05 |
|    critic_loss     | 7.14e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 373299   |
---------------------------------
=== Iterazione IRL 829 ===
Loss reward (iter 829): -23801.841796875
=== Iterazione IRL 830 ===
Loss reward (iter 830): -24278.12890625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 239      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.53e+05 |
|    critic_loss     | 5.84e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 373799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 209      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.59e+05 |
|    critic_loss     | 7.43e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 374199   |
---------------------------------
=== Iterazione IRL 831 ===
Loss reward (iter 831): -24083.158203125
=== Iterazione IRL 832 ===
Loss reward (iter 832): -24584.35546875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 239      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.62e+05 |
|    critic_loss     | 6.74e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 374699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 209      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.53e+05 |
|    critic_loss     | 6.51e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 375099   |
---------------------------------
=== Iterazione IRL 833 ===
Loss reward (iter 833): -24497.580078125
=== Iterazione IRL 834 ===
Loss reward (iter 834): -24231.1015625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 239      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.62e+05 |
|    critic_loss     | 7.48e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 375599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 209      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.68e+05 |
|    critic_loss     | 7.71e+07 |
|    learning_rate   | 0.001    |
|    n_updates       | 375999   |
---------------------------------
