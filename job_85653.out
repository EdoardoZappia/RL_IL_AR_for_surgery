Inizio training IRL
Using device: cuda
Using cuda device
=== Iterazione IRL 0 ===
Loss reward (iter 0): 6.976498126983643
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 261      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -1.76    |
|    critic_loss     | 0.00603  |
|    learning_rate   | 0.001    |
|    n_updates       | 299      |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.49    |
|    critic_loss     | 0.0094   |
|    learning_rate   | 0.001    |
|    n_updates       | 699      |
---------------------------------
=== Iterazione IRL 1 ===
Loss reward (iter 1): 7.629086494445801
=== Iterazione IRL 2 ===
Loss reward (iter 2): 7.60587739944458
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.45    |
|    critic_loss     | 0.016    |
|    learning_rate   | 0.001    |
|    n_updates       | 1199     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.22    |
|    critic_loss     | 0.03     |
|    learning_rate   | 0.001    |
|    n_updates       | 1599     |
---------------------------------
=== Iterazione IRL 3 ===
Loss reward (iter 3): 7.577645301818848
=== Iterazione IRL 4 ===
Loss reward (iter 4): 7.553390026092529
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.98    |
|    critic_loss     | 0.0323   |
|    learning_rate   | 0.001    |
|    n_updates       | 2099     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.68    |
|    critic_loss     | 0.0489   |
|    learning_rate   | 0.001    |
|    n_updates       | 2499     |
---------------------------------
=== Iterazione IRL 5 ===
Loss reward (iter 5): 7.535996913909912
=== Iterazione IRL 6 ===
Loss reward (iter 6): 7.510037422180176
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.35    |
|    critic_loss     | 0.0584   |
|    learning_rate   | 0.001    |
|    n_updates       | 2999     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.98    |
|    critic_loss     | 0.0811   |
|    learning_rate   | 0.001    |
|    n_updates       | 3399     |
---------------------------------
