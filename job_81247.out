Inizio training IRL
Using device: cuda
Using cuda device
=== Iterazione IRL 0 ===
Loss reward (iter 0): 4.651638031005859
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 143      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.3    |
|    critic_loss     | 0.459    |
|    ent_coef        | 0.918    |
|    ent_coef_loss   | -0.252   |
|    learning_rate   | 0.0003   |
|    n_updates       | 299      |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 126      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -18.6    |
|    critic_loss     | 0.26     |
|    ent_coef        | 0.819    |
|    ent_coef_loss   | -0.551   |
|    learning_rate   | 0.0003   |
|    n_updates       | 699      |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 122      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -27.5    |
|    critic_loss     | 0.353    |
|    ent_coef        | 0.73     |
|    ent_coef_loss   | -0.875   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1099     |
---------------------------------
=== Iterazione IRL 1 ===
Loss reward (iter 1): 3.9703314304351807
=== Iterazione IRL 2 ===
Loss reward (iter 2): -1.7914624214172363
=== Iterazione IRL 3 ===
Loss reward (iter 3): -9.206846237182617
=== Iterazione IRL 4 ===
Loss reward (iter 4): -21.285140991210938
=== Iterazione IRL 5 ===
Loss reward (iter 5): -38.434478759765625
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 147      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -126     |
|    critic_loss     | 185      |
|    ent_coef        | 0.718    |
|    ent_coef_loss   | 1.45     |
|    learning_rate   | 0.0003   |
|    n_updates       | 1699     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 125      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -259     |
|    critic_loss     | 145      |
|    ent_coef        | 0.846    |
|    ent_coef_loss   | 0.811    |
|    learning_rate   | 0.0003   |
|    n_updates       | 2099     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 117      |
|    time_elapsed    | 10       |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -462     |
|    critic_loss     | 201      |
|    ent_coef        | 0.989    |
|    ent_coef_loss   | 0.0559   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2499     |
---------------------------------
=== Iterazione IRL 6 ===
Loss reward (iter 6): 82.4168472290039
=== Iterazione IRL 7 ===
Loss reward (iter 7): 39.95772171020508
=== Iterazione IRL 8 ===
Loss reward (iter 8): 12.622386932373047
=== Iterazione IRL 9 ===
Loss reward (iter 9): -6.866530895233154
=== Iterazione IRL 10 ===
Loss reward (iter 10): -22.452877044677734
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 136      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -798     |
|    critic_loss     | 1.47e+03 |
|    ent_coef        | 1.23     |
|    ent_coef_loss   | -1.15    |
|    learning_rate   | 0.0003   |
|    n_updates       | 3099     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 118      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -919     |
|    critic_loss     | 2.05e+03 |
|    ent_coef        | 1.4      |
|    ent_coef_loss   | -1.75    |
|    learning_rate   | 0.0003   |
|    n_updates       | 3499     |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 113       |
|    time_elapsed    | 10        |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.05e+03 |
|    critic_loss     | 2.62e+03  |
|    ent_coef        | 1.58      |
|    ent_coef_loss   | -2.37     |
|    learning_rate   | 0.0003    |
|    n_updates       | 3899      |
----------------------------------
=== Iterazione IRL 11 ===
Loss reward (iter 11): -39.7630729675293
=== Iterazione IRL 12 ===
Loss reward (iter 12): -62.482826232910156
=== Iterazione IRL 13 ===
Loss reward (iter 13): -92.46981811523438
=== Iterazione IRL 14 ===
Loss reward (iter 14): -121.37471771240234
=== Iterazione IRL 15 ===
Loss reward (iter 15): -178.0167999267578
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 135       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.27e+03 |
|    critic_loss     | 3.8e+03   |
|    ent_coef        | 1.87      |
|    ent_coef_loss   | -2.72     |
|    learning_rate   | 0.0003    |
|    n_updates       | 4499      |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 118       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.33e+03 |
|    critic_loss     | 3.37e+03  |
|    ent_coef        | 2.08      |
|    ent_coef_loss   | -2.8      |
|    learning_rate   | 0.0003    |
|    n_updates       | 4899      |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 113      |
|    time_elapsed    | 10       |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -1.4e+03 |
|    critic_loss     | 3.72e+03 |
|    ent_coef        | 2.3      |
|    ent_coef_loss   | -3.27    |
|    learning_rate   | 0.0003   |
|    n_updates       | 5299     |
---------------------------------
=== Iterazione IRL 16 ===
Loss reward (iter 16): -233.96290588378906
=== Iterazione IRL 17 ===
Loss reward (iter 17): -311.5299072265625
=== Iterazione IRL 18 ===
Loss reward (iter 18): -411.1493225097656
=== Iterazione IRL 19 ===
Loss reward (iter 19): -505.8297119140625
=== Iterazione IRL 20 ===
Loss reward (iter 20): -654.7068481445312
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 135       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.95e+03 |
|    critic_loss     | 1.87e+04  |
|    ent_coef        | 2.81      |
|    ent_coef_loss   | -5.54     |
|    learning_rate   | 0.0003    |
|    n_updates       | 5899      |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 118       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -4.45e+03 |
|    critic_loss     | 2.6e+04   |
|    ent_coef        | 3.16      |
|    ent_coef_loss   | -6.08     |
|    learning_rate   | 0.0003    |
|    n_updates       | 6299      |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 113       |
|    time_elapsed    | 10        |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -6.23e+03 |
|    critic_loss     | 3.37e+04  |
|    ent_coef        | 3.6       |
|    ent_coef_loss   | -7.32     |
|    learning_rate   | 0.0003    |
|    n_updates       | 6699      |
----------------------------------
=== Iterazione IRL 21 ===
Loss reward (iter 21): 780.4384155273438
=== Iterazione IRL 22 ===
Loss reward (iter 22): 579.4154663085938
=== Iterazione IRL 23 ===
Loss reward (iter 23): 423.3137512207031
=== Iterazione IRL 24 ===
Loss reward (iter 24): 297.5062561035156
=== Iterazione IRL 25 ===
Loss reward (iter 25): 224.10643005371094
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 135       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -9.58e+03 |
|    critic_loss     | 5.43e+04  |
|    ent_coef        | 4.37      |
|    ent_coef_loss   | -8.29     |
|    learning_rate   | 0.0003    |
|    n_updates       | 7299      |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 118       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.16e+04 |
|    critic_loss     | 4.79e+04  |
|    ent_coef        | 4.94      |
|    ent_coef_loss   | -9.8      |
|    learning_rate   | 0.0003    |
|    n_updates       | 7699      |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 113       |
|    time_elapsed    | 10        |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.39e+04 |
|    critic_loss     | 6.5e+04   |
|    ent_coef        | 5.57      |
|    ent_coef_loss   | -8.99     |
|    learning_rate   | 0.0003    |
|    n_updates       | 8099      |
----------------------------------
=== Iterazione IRL 26 ===
Loss reward (iter 26): 141.48117065429688
=== Iterazione IRL 27 ===
Loss reward (iter 27): 19.90764808654785
=== Iterazione IRL 28 ===
Loss reward (iter 28): -46.70927810668945
=== Iterazione IRL 29 ===
Loss reward (iter 29): -96.0494613647461
=== Iterazione IRL 30 ===
Loss reward (iter 30): -176.35678100585938
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 135       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.67e+04 |
|    critic_loss     | 6.44e+04  |
|    ent_coef        | 6.62      |
|    ent_coef_loss   | -9.67     |
|    learning_rate   | 0.0003    |
|    n_updates       | 8699      |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 118       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.77e+04 |
|    critic_loss     | 6.67e+04  |
|    ent_coef        | 7.43      |
|    ent_coef_loss   | -10.1     |
|    learning_rate   | 0.0003    |
|    n_updates       | 9099      |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 113       |
|    time_elapsed    | 10        |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.94e+04 |
|    critic_loss     | 7.23e+04  |
|    ent_coef        | 8.32      |
|    ent_coef_loss   | -10.7     |
|    learning_rate   | 0.0003    |
|    n_updates       | 9499      |
----------------------------------
=== Iterazione IRL 31 ===
Loss reward (iter 31): 1.296097755432129
=== Iterazione IRL 32 ===
Loss reward (iter 32): -146.958740234375
=== Iterazione IRL 33 ===
Loss reward (iter 33): -213.55792236328125
=== Iterazione IRL 34 ===
Loss reward (iter 34): -295.265380859375
=== Iterazione IRL 35 ===
Loss reward (iter 35): -348.4162292480469
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 136       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.29e+04 |
|    critic_loss     | 1.04e+05  |
|    ent_coef        | 9.82      |
|    ent_coef_loss   | -10.4     |
|    learning_rate   | 0.0003    |
|    n_updates       | 10099     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 118       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.31e+04 |
|    critic_loss     | 1.47e+05  |
|    ent_coef        | 10.9      |
|    ent_coef_loss   | -10.3     |
|    learning_rate   | 0.0003    |
|    n_updates       | 10499     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 113       |
|    time_elapsed    | 10        |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.51e+04 |
|    critic_loss     | 1.23e+05  |
|    ent_coef        | 12.1      |
|    ent_coef_loss   | -9.33     |
|    learning_rate   | 0.0003    |
|    n_updates       | 10899     |
----------------------------------
=== Iterazione IRL 36 ===
Loss reward (iter 36): -536.8040161132812
=== Iterazione IRL 37 ===
Loss reward (iter 37): -567.995849609375
=== Iterazione IRL 38 ===
Loss reward (iter 38): -696.036376953125
=== Iterazione IRL 39 ===
Loss reward (iter 39): -818.6410522460938
=== Iterazione IRL 40 ===
Loss reward (iter 40): -1028.520263671875
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 147       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.61e+04 |
|    critic_loss     | 1.94e+05  |
|    ent_coef        | 14.1      |
|    ent_coef_loss   | -9.32     |
|    learning_rate   | 0.0003    |
|    n_updates       | 11499     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 128       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.73e+04 |
|    critic_loss     | 2.02e+05  |
|    ent_coef        | 15.6      |
|    ent_coef_loss   | -8.6      |
|    learning_rate   | 0.0003    |
|    n_updates       | 11899     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 123       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.82e+04 |
|    critic_loss     | 2.15e+05  |
|    ent_coef        | 17.2      |
|    ent_coef_loss   | -8.7      |
|    learning_rate   | 0.0003    |
|    n_updates       | 12299     |
----------------------------------
=== Iterazione IRL 41 ===
Loss reward (iter 41): -1008.8046875
=== Iterazione IRL 42 ===
Loss reward (iter 42): -1378.421630859375
=== Iterazione IRL 43 ===
Loss reward (iter 43): -1351.1607666015625
=== Iterazione IRL 44 ===
Loss reward (iter 44): -622.2498168945312
=== Iterazione IRL 45 ===
Loss reward (iter 45): -1333.829345703125
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 147      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.9e+04 |
|    critic_loss     | 2.31e+05 |
|    ent_coef        | 19.8     |
|    ent_coef_loss   | -6.66    |
|    learning_rate   | 0.0003   |
|    n_updates       | 12899    |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 128       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.97e+04 |
|    critic_loss     | 5.1e+05   |
|    ent_coef        | 21.3      |
|    ent_coef_loss   | -5.28     |
|    learning_rate   | 0.0003    |
|    n_updates       | 13299     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 123       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -3.18e+04 |
|    critic_loss     | 4.38e+05  |
|    ent_coef        | 23.5      |
|    ent_coef_loss   | -7.32     |
|    learning_rate   | 0.0003    |
|    n_updates       | 13699     |
----------------------------------
=== Iterazione IRL 46 ===
Loss reward (iter 46): 6342.5400390625
=== Iterazione IRL 47 ===
Loss reward (iter 47): 4855.21826171875
=== Iterazione IRL 48 ===
Loss reward (iter 48): 3856.324462890625
=== Iterazione IRL 49 ===
Loss reward (iter 49): 2909.6787109375
=== Iterazione IRL 50 ===
Loss reward (iter 50): 2217.013671875
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 148       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -3.62e+04 |
|    critic_loss     | 9.72e+05  |
|    ent_coef        | 27.8      |
|    ent_coef_loss   | -10.5     |
|    learning_rate   | 0.0003    |
|    n_updates       | 14299     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 128       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -3.85e+04 |
|    critic_loss     | 7.99e+05  |
|    ent_coef        | 31.6      |
|    ent_coef_loss   | -9.13     |
|    learning_rate   | 0.0003    |
|    n_updates       | 14699     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 123       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -4.11e+04 |
|    critic_loss     | 1.33e+06  |
|    ent_coef        | 35.8      |
|    ent_coef_loss   | -10.5     |
|    learning_rate   | 0.0003    |
|    n_updates       | 15099     |
----------------------------------
=== Iterazione IRL 51 ===
Loss reward (iter 51): 1782.769775390625
=== Iterazione IRL 52 ===
Loss reward (iter 52): 1473.6876220703125
=== Iterazione IRL 53 ===
Loss reward (iter 53): 1287.68994140625
=== Iterazione IRL 54 ===
Loss reward (iter 54): 1028.1119384765625
=== Iterazione IRL 55 ===
Loss reward (iter 55): 896.2914428710938
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 147       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -4.53e+04 |
|    critic_loss     | 1.47e+06  |
|    ent_coef        | 42.8      |
|    ent_coef_loss   | -10.2     |
|    learning_rate   | 0.0003    |
|    n_updates       | 15699     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 128       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -4.94e+04 |
|    critic_loss     | 1.21e+06  |
|    ent_coef        | 47.9      |
|    ent_coef_loss   | -8.94     |
|    learning_rate   | 0.0003    |
|    n_updates       | 16099     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 122       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -5.09e+04 |
|    critic_loss     | 1.14e+06  |
|    ent_coef        | 52.8      |
|    ent_coef_loss   | -8.21     |
|    learning_rate   | 0.0003    |
|    n_updates       | 16499     |
----------------------------------
=== Iterazione IRL 56 ===
Loss reward (iter 56): 826.4622802734375
=== Iterazione IRL 57 ===
Loss reward (iter 57): 836.7454223632812
=== Iterazione IRL 58 ===
Loss reward (iter 58): 615.7061157226562
=== Iterazione IRL 59 ===
Loss reward (iter 59): 537.4395141601562
=== Iterazione IRL 60 ===
Loss reward (iter 60): 511.2593688964844
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 135       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -5.88e+04 |
|    critic_loss     | 1.26e+06  |
|    ent_coef        | 60.6      |
|    ent_coef_loss   | -6.46     |
|    learning_rate   | 0.0003    |
|    n_updates       | 17099     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 117       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -5.96e+04 |
|    critic_loss     | 1.28e+06  |
|    ent_coef        | 65.7      |
|    ent_coef_loss   | -6.95     |
|    learning_rate   | 0.0003    |
|    n_updates       | 17499     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 113       |
|    time_elapsed    | 10        |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -5.98e+04 |
|    critic_loss     | 1.05e+06  |
|    ent_coef        | 70.7      |
|    ent_coef_loss   | -4.2      |
|    learning_rate   | 0.0003    |
|    n_updates       | 17899     |
----------------------------------
=== Iterazione IRL 61 ===
Loss reward (iter 61): 623.8999633789062
=== Iterazione IRL 62 ===
Loss reward (iter 62): 524.6474609375
=== Iterazione IRL 63 ===
Loss reward (iter 63): 465.6973571777344
=== Iterazione IRL 64 ===
Loss reward (iter 64): 366.05145263671875
=== Iterazione IRL 65 ===
Loss reward (iter 65): 346.67254638671875
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 136       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -6.51e+04 |
|    critic_loss     | 9.6e+05   |
|    ent_coef        | 77.4      |
|    ent_coef_loss   | -1.14     |
|    learning_rate   | 0.0003    |
|    n_updates       | 18499     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 118       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -7.04e+04 |
|    critic_loss     | 1.52e+06  |
|    ent_coef        | 82        |
|    ent_coef_loss   | -2.23     |
|    learning_rate   | 0.0003    |
|    n_updates       | 18899     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 113       |
|    time_elapsed    | 10        |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -7.17e+04 |
|    critic_loss     | 1.28e+06  |
|    ent_coef        | 87.5      |
|    ent_coef_loss   | -2.31     |
|    learning_rate   | 0.0003    |
|    n_updates       | 19299     |
----------------------------------
=== Iterazione IRL 66 ===
Loss reward (iter 66): 286.34405517578125
=== Iterazione IRL 67 ===
Loss reward (iter 67): 252.83323669433594
=== Iterazione IRL 68 ===
Loss reward (iter 68): 236.08572387695312
=== Iterazione IRL 69 ===
Loss reward (iter 69): 207.17056274414062
=== Iterazione IRL 70 ===
Loss reward (iter 70): 188.36251831054688
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 136       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -7.41e+04 |
|    critic_loss     | 7.79e+05  |
|    ent_coef        | 93.2      |
|    ent_coef_loss   | -1.22     |
|    learning_rate   | 0.0003    |
|    n_updates       | 19899     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 118       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -7.59e+04 |
|    critic_loss     | 1.04e+06  |
|    ent_coef        | 94.2      |
|    ent_coef_loss   | -0.0608   |
|    learning_rate   | 0.0003    |
|    n_updates       | 20299     |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 114      |
|    time_elapsed    | 10       |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -7.9e+04 |
|    critic_loss     | 1.01e+06 |
|    ent_coef        | 95.1     |
|    ent_coef_loss   | -0.712   |
|    learning_rate   | 0.0003   |
|    n_updates       | 20699    |
---------------------------------
=== Iterazione IRL 71 ===
Loss reward (iter 71): 153.7625732421875
=== Iterazione IRL 72 ===
Loss reward (iter 72): 136.5045928955078
=== Iterazione IRL 73 ===
Loss reward (iter 73): 125.92890167236328
=== Iterazione IRL 74 ===
Loss reward (iter 74): 112.05501556396484
=== Iterazione IRL 75 ===
Loss reward (iter 75): 92.63579559326172
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 147       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -7.92e+04 |
|    critic_loss     | 8.73e+05  |
|    ent_coef        | 101       |
|    ent_coef_loss   | -0.607    |
|    learning_rate   | 0.0003    |
|    n_updates       | 21299     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 128       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -8.02e+04 |
|    critic_loss     | 1.47e+06  |
|    ent_coef        | 106       |
|    ent_coef_loss   | -0.833    |
|    learning_rate   | 0.0003    |
|    n_updates       | 21699     |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 123      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -8.3e+04 |
|    critic_loss     | 1.45e+06 |
|    ent_coef        | 111      |
|    ent_coef_loss   | -0.0487  |
|    learning_rate   | 0.0003   |
|    n_updates       | 22099    |
---------------------------------
=== Iterazione IRL 76 ===
Loss reward (iter 76): 114.2064437866211
=== Iterazione IRL 77 ===
Loss reward (iter 77): 99.19768524169922
=== Iterazione IRL 78 ===
Loss reward (iter 78): 83.2544174194336
=== Iterazione IRL 79 ===
Loss reward (iter 79): 65.72867584228516
=== Iterazione IRL 80 ===
Loss reward (iter 80): 53.47254180908203
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 136       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -8.58e+04 |
|    critic_loss     | 1.05e+06  |
|    ent_coef        | 115       |
|    ent_coef_loss   | 0.995     |
|    learning_rate   | 0.0003    |
|    n_updates       | 22699     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 118       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -8.31e+04 |
|    critic_loss     | 1.15e+06  |
|    ent_coef        | 117       |
|    ent_coef_loss   | 0.135     |
|    learning_rate   | 0.0003    |
|    n_updates       | 23099     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 113       |
|    time_elapsed    | 10        |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -8.89e+04 |
|    critic_loss     | 1.24e+06  |
|    ent_coef        | 118       |
|    ent_coef_loss   | 0.373     |
|    learning_rate   | 0.0003    |
|    n_updates       | 23499     |
----------------------------------
=== Iterazione IRL 81 ===
Loss reward (iter 81): 37.432735443115234
=== Iterazione IRL 82 ===
Loss reward (iter 82): 15.018234252929688
=== Iterazione IRL 83 ===
Loss reward (iter 83): 21.113309860229492
=== Iterazione IRL 84 ===
Loss reward (iter 84): 12.036462783813477
=== Iterazione IRL 85 ===
Loss reward (iter 85): 8.29996109008789
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 136       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -8.82e+04 |
|    critic_loss     | 1.14e+06  |
|    ent_coef        | 117       |
|    ent_coef_loss   | 0.534     |
|    learning_rate   | 0.0003    |
|    n_updates       | 24099     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 118       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -8.83e+04 |
|    critic_loss     | 1.18e+06  |
|    ent_coef        | 116       |
|    ent_coef_loss   | 0.91      |
|    learning_rate   | 0.0003    |
|    n_updates       | 24499     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 113       |
|    time_elapsed    | 10        |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -9.06e+04 |
|    critic_loss     | 1.29e+06  |
|    ent_coef        | 116       |
|    ent_coef_loss   | -0.278    |
|    learning_rate   | 0.0003    |
|    n_updates       | 24899     |
----------------------------------
=== Iterazione IRL 86 ===
Loss reward (iter 86): 9.14424991607666
=== Iterazione IRL 87 ===
Loss reward (iter 87): 4.2762885093688965
=== Iterazione IRL 88 ===
Loss reward (iter 88): 4.457388877868652
=== Iterazione IRL 89 ===
Loss reward (iter 89): 1.4558132886886597
=== Iterazione IRL 90 ===
Loss reward (iter 90): -12.2859525680542
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 136       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -9.07e+04 |
|    critic_loss     | 1.32e+06  |
|    ent_coef        | 114       |
|    ent_coef_loss   | -0.582    |
|    learning_rate   | 0.0003    |
|    n_updates       | 25499     |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 118      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -9.2e+04 |
|    critic_loss     | 1.96e+06 |
|    ent_coef        | 114      |
|    ent_coef_loss   | 0.649    |
|    learning_rate   | 0.0003   |
|    n_updates       | 25899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 113      |
|    time_elapsed    | 10       |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -8.9e+04 |
|    critic_loss     | 1.44e+06 |
|    ent_coef        | 112      |
|    ent_coef_loss   | -0.834   |
|    learning_rate   | 0.0003   |
|    n_updates       | 26299    |
---------------------------------
=== Iterazione IRL 91 ===
Loss reward (iter 91): -19.834308624267578
=== Iterazione IRL 92 ===
Loss reward (iter 92): -40.38639831542969
=== Iterazione IRL 93 ===
Loss reward (iter 93): 19.427001953125
=== Iterazione IRL 94 ===
Loss reward (iter 94): -46.797691345214844
=== Iterazione IRL 95 ===
Loss reward (iter 95): -0.5236655473709106
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 136       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -9.02e+04 |
|    critic_loss     | 9.09e+05  |
|    ent_coef        | 109       |
|    ent_coef_loss   | 1.52      |
|    learning_rate   | 0.0003    |
|    n_updates       | 26899     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 118       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -9.13e+04 |
|    critic_loss     | 1.63e+06  |
|    ent_coef        | 107       |
|    ent_coef_loss   | 1.11      |
|    learning_rate   | 0.0003    |
|    n_updates       | 27299     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 114       |
|    time_elapsed    | 10        |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -9.17e+04 |
|    critic_loss     | 9.74e+05  |
|    ent_coef        | 104       |
|    ent_coef_loss   | 1.26      |
|    learning_rate   | 0.0003    |
|    n_updates       | 27699     |
----------------------------------
=== Iterazione IRL 96 ===
Loss reward (iter 96): 13.880109786987305
=== Iterazione IRL 97 ===
Loss reward (iter 97): 14.600804328918457
=== Iterazione IRL 98 ===
Loss reward (iter 98): -25.16806411743164
=== Iterazione IRL 99 ===
Loss reward (iter 99): -10.282388687133789
=== Iterazione IRL 100 ===
Loss reward (iter 100): -14.512920379638672
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 136       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -9.13e+04 |
|    critic_loss     | 1.27e+06  |
|    ent_coef        | 99.4      |
|    ent_coef_loss   | 1.31      |
|    learning_rate   | 0.0003    |
|    n_updates       | 28299     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 119       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -9.07e+04 |
|    critic_loss     | 6.98e+05  |
|    ent_coef        | 98.7      |
|    ent_coef_loss   | -2.31     |
|    learning_rate   | 0.0003    |
|    n_updates       | 28699     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 114       |
|    time_elapsed    | 10        |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -9.21e+04 |
|    critic_loss     | 8.15e+05  |
|    ent_coef        | 97.7      |
|    ent_coef_loss   | -0.515    |
|    learning_rate   | 0.0003    |
|    n_updates       | 29099     |
----------------------------------
=== Iterazione IRL 101 ===
Loss reward (iter 101): 33.74359130859375
=== Iterazione IRL 102 ===
Loss reward (iter 102): 16.03317642211914
=== Iterazione IRL 103 ===
Loss reward (iter 103): 13.582294464111328
=== Iterazione IRL 104 ===
Loss reward (iter 104): 6.510841369628906
=== Iterazione IRL 105 ===
Loss reward (iter 105): 3.2645556926727295
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 136       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -9.06e+04 |
|    critic_loss     | 1.78e+06  |
|    ent_coef        | 101       |
|    ent_coef_loss   | -0.741    |
|    learning_rate   | 0.0003    |
|    n_updates       | 29699     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 119       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -8.98e+04 |
|    critic_loss     | 1.22e+06  |
|    ent_coef        | 103       |
|    ent_coef_loss   | -1.47     |
|    learning_rate   | 0.0003    |
|    n_updates       | 30099     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 114       |
|    time_elapsed    | 10        |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -8.86e+04 |
|    critic_loss     | 1.26e+06  |
|    ent_coef        | 107       |
|    ent_coef_loss   | -1.08     |
|    learning_rate   | 0.0003    |
|    n_updates       | 30499     |
----------------------------------
=== Iterazione IRL 106 ===
Loss reward (iter 106): 6.29310417175293
=== Iterazione IRL 107 ===
Loss reward (iter 107): 4.162749290466309
=== Iterazione IRL 108 ===
Loss reward (iter 108): 2.85721492767334
=== Iterazione IRL 109 ===
Loss reward (iter 109): 0.7582857608795166
=== Iterazione IRL 110 ===
Loss reward (iter 110): -1.3607048988342285
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 136       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -8.82e+04 |
|    critic_loss     | 1.07e+06  |
|    ent_coef        | 109       |
|    ent_coef_loss   | -1.02     |
|    learning_rate   | 0.0003    |
|    n_updates       | 31099     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 118       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -8.85e+04 |
|    critic_loss     | 1.3e+06   |
|    ent_coef        | 108       |
|    ent_coef_loss   | -0.114    |
|    learning_rate   | 0.0003    |
|    n_updates       | 31499     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 113       |
|    time_elapsed    | 10        |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -8.85e+04 |
|    critic_loss     | 1.2e+06   |
|    ent_coef        | 108       |
|    ent_coef_loss   | 1.13      |
|    learning_rate   | 0.0003    |
|    n_updates       | 31899     |
----------------------------------
=== Iterazione IRL 111 ===
Loss reward (iter 111): -1.7633907794952393
=== Iterazione IRL 112 ===
Loss reward (iter 112): -2.2225306034088135
=== Iterazione IRL 113 ===
Loss reward (iter 113): -5.210055351257324
=== Iterazione IRL 114 ===
Loss reward (iter 114): 0.45735597610473633
=== Iterazione IRL 115 ===
Loss reward (iter 115): -1.887235403060913
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 136       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -8.75e+04 |
|    critic_loss     | 9.82e+05  |
|    ent_coef        | 111       |
|    ent_coef_loss   | 0.79      |
|    learning_rate   | 0.0003    |
|    n_updates       | 32499     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 118       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -8.62e+04 |
|    critic_loss     | 8.3e+05   |
|    ent_coef        | 111       |
|    ent_coef_loss   | 0.719     |
|    learning_rate   | 0.0003    |
|    n_updates       | 32899     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 113       |
|    time_elapsed    | 10        |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -8.59e+04 |
|    critic_loss     | 6.83e+05  |
|    ent_coef        | 109       |
|    ent_coef_loss   | 0.354     |
|    learning_rate   | 0.0003    |
|    n_updates       | 33299     |
----------------------------------
=== Iterazione IRL 116 ===
Loss reward (iter 116): -1.2249948978424072
=== Iterazione IRL 117 ===
Loss reward (iter 117): -1.653164267539978
=== Iterazione IRL 118 ===
Loss reward (iter 118): -3.8122076988220215
=== Iterazione IRL 119 ===
Loss reward (iter 119): 5.634336471557617
=== Iterazione IRL 120 ===
Loss reward (iter 120): -1.663187861442566
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 136       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -8.62e+04 |
|    critic_loss     | 1.59e+06  |
|    ent_coef        | 109       |
|    ent_coef_loss   | -0.569    |
|    learning_rate   | 0.0003    |
|    n_updates       | 33899     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 118       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -8.57e+04 |
|    critic_loss     | 1.22e+06  |
|    ent_coef        | 112       |
|    ent_coef_loss   | 1.39      |
|    learning_rate   | 0.0003    |
|    n_updates       | 34299     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 113       |
|    time_elapsed    | 10        |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -8.47e+04 |
|    critic_loss     | 1.01e+06  |
|    ent_coef        | 111       |
|    ent_coef_loss   | -0.5      |
|    learning_rate   | 0.0003    |
|    n_updates       | 34699     |
----------------------------------
=== Iterazione IRL 121 ===
Loss reward (iter 121): -3.8240818977355957
=== Iterazione IRL 122 ===
Loss reward (iter 122): 1.7526808977127075
=== Iterazione IRL 123 ===
Loss reward (iter 123): 6.223227024078369
=== Iterazione IRL 124 ===
Loss reward (iter 124): 0.030449271202087402
=== Iterazione IRL 125 ===
Loss reward (iter 125): -2.3492209911346436
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 136       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -8.47e+04 |
|    critic_loss     | 1.15e+06  |
|    ent_coef        | 111       |
|    ent_coef_loss   | -0.399    |
|    learning_rate   | 0.0003    |
|    n_updates       | 35299     |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 119      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.4e+04 |
|    critic_loss     | 9.45e+05 |
|    ent_coef        | 112      |
|    ent_coef_loss   | 1.47     |
|    learning_rate   | 0.0003   |
|    n_updates       | 35699    |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 114       |
|    time_elapsed    | 10        |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -8.33e+04 |
|    critic_loss     | 1.01e+06  |
|    ent_coef        | 113       |
|    ent_coef_loss   | 0.923     |
|    learning_rate   | 0.0003    |
|    n_updates       | 36099     |
----------------------------------
=== Iterazione IRL 126 ===
Loss reward (iter 126): -6.543615818023682
=== Iterazione IRL 127 ===
Loss reward (iter 127): -12.092194557189941
=== Iterazione IRL 128 ===
Loss reward (iter 128): -20.09604263305664
=== Iterazione IRL 129 ===
Loss reward (iter 129): -15.486174583435059
=== Iterazione IRL 130 ===
Loss reward (iter 130): -29.772539138793945
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 149       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -8.33e+04 |
|    critic_loss     | 1.36e+06  |
|    ent_coef        | 113       |
|    ent_coef_loss   | 0.621     |
|    learning_rate   | 0.0003    |
|    n_updates       | 36699     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 130       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -8.22e+04 |
|    critic_loss     | 9.56e+05  |
|    ent_coef        | 111       |
|    ent_coef_loss   | -0.0876   |
|    learning_rate   | 0.0003    |
|    n_updates       | 37099     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 124       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -8.17e+04 |
|    critic_loss     | 1.1e+06   |
|    ent_coef        | 115       |
|    ent_coef_loss   | -0.594    |
|    learning_rate   | 0.0003    |
|    n_updates       | 37499     |
----------------------------------
=== Iterazione IRL 131 ===
Loss reward (iter 131): -28.01068878173828
=== Iterazione IRL 132 ===
Loss reward (iter 132): -36.42678451538086
=== Iterazione IRL 133 ===
Loss reward (iter 133): -55.302738189697266
=== Iterazione IRL 134 ===
Loss reward (iter 134): -74.73596954345703
=== Iterazione IRL 135 ===
Loss reward (iter 135): -89.03616333007812
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 149       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -8.03e+04 |
|    critic_loss     | 9.41e+05  |
|    ent_coef        | 109       |
|    ent_coef_loss   | 0.408     |
|    learning_rate   | 0.0003    |
|    n_updates       | 38099     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 129       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -7.95e+04 |
|    critic_loss     | 8.74e+05  |
|    ent_coef        | 107       |
|    ent_coef_loss   | -1.79     |
|    learning_rate   | 0.0003    |
|    n_updates       | 38499     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 124       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -7.96e+04 |
|    critic_loss     | 1.2e+06   |
|    ent_coef        | 105       |
|    ent_coef_loss   | -1.97     |
|    learning_rate   | 0.0003    |
|    n_updates       | 38899     |
----------------------------------
=== Iterazione IRL 136 ===
Loss reward (iter 136): -143.56002807617188
=== Iterazione IRL 137 ===
Loss reward (iter 137): -157.51153564453125
=== Iterazione IRL 138 ===
Loss reward (iter 138): -112.42993927001953
=== Iterazione IRL 139 ===
Loss reward (iter 139): -286.3773193359375
=== Iterazione IRL 140 ===
Loss reward (iter 140): -346.0268859863281
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 149       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -7.91e+04 |
|    critic_loss     | 1.47e+06  |
|    ent_coef        | 102       |
|    ent_coef_loss   | 0.298     |
|    learning_rate   | 0.0003    |
|    n_updates       | 39499     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 130       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -7.79e+04 |
|    critic_loss     | 2.03e+06  |
|    ent_coef        | 100       |
|    ent_coef_loss   | -0.272    |
|    learning_rate   | 0.0003    |
|    n_updates       | 39899     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 124       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -7.68e+04 |
|    critic_loss     | 1.45e+06  |
|    ent_coef        | 96.7      |
|    ent_coef_loss   | -0.329    |
|    learning_rate   | 0.0003    |
|    n_updates       | 40299     |
----------------------------------
=== Iterazione IRL 141 ===
Loss reward (iter 141): -33.847599029541016
=== Iterazione IRL 142 ===
Loss reward (iter 142): -128.77186584472656
=== Iterazione IRL 143 ===
Loss reward (iter 143): -207.63575744628906
=== Iterazione IRL 144 ===
Loss reward (iter 144): -144.21763610839844
=== Iterazione IRL 145 ===
Loss reward (iter 145): 111.9072265625
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 149       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -7.66e+04 |
|    critic_loss     | 1.3e+06   |
|    ent_coef        | 89.9      |
|    ent_coef_loss   | 1.83      |
|    learning_rate   | 0.0003    |
|    n_updates       | 40899     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 130       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -7.56e+04 |
|    critic_loss     | 1.2e+06   |
|    ent_coef        | 88        |
|    ent_coef_loss   | -0.203    |
|    learning_rate   | 0.0003    |
|    n_updates       | 41299     |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 124      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -7.5e+04 |
|    critic_loss     | 1.16e+06 |
|    ent_coef        | 86.7     |
|    ent_coef_loss   | 1.85     |
|    learning_rate   | 0.0003   |
|    n_updates       | 41699    |
---------------------------------
=== Iterazione IRL 146 ===
Loss reward (iter 146): 28.195465087890625
=== Iterazione IRL 147 ===
Loss reward (iter 147): -408.6607971191406
=== Iterazione IRL 148 ===
Loss reward (iter 148): -295.38153076171875
=== Iterazione IRL 149 ===
Loss reward (iter 149): -122.8808822631836
=== Iterazione IRL 150 ===
Loss reward (iter 150): -457.8493347167969
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 149       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -7.32e+04 |
|    critic_loss     | 1.22e+06  |
|    ent_coef        | 81.5      |
|    ent_coef_loss   | -1.09     |
|    learning_rate   | 0.0003    |
|    n_updates       | 42299     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 130       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -7.28e+04 |
|    critic_loss     | 9.68e+05  |
|    ent_coef        | 79.3      |
|    ent_coef_loss   | 0.261     |
|    learning_rate   | 0.0003    |
|    n_updates       | 42699     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 124       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -7.26e+04 |
|    critic_loss     | 1.11e+06  |
|    ent_coef        | 76.1      |
|    ent_coef_loss   | 0.395     |
|    learning_rate   | 0.0003    |
|    n_updates       | 43099     |
----------------------------------
=== Iterazione IRL 151 ===
Loss reward (iter 151): 3.629051923751831
=== Iterazione IRL 152 ===
Loss reward (iter 152): -38.98676681518555
=== Iterazione IRL 153 ===
Loss reward (iter 153): -512.294921875
=== Iterazione IRL 154 ===
Loss reward (iter 154): -198.07240295410156
=== Iterazione IRL 155 ===
Loss reward (iter 155): -4.68194580078125
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 149       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -7.08e+04 |
|    critic_loss     | 1.46e+06  |
|    ent_coef        | 73.3      |
|    ent_coef_loss   | 1.25      |
|    learning_rate   | 0.0003    |
|    n_updates       | 43699     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 124       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -7.03e+04 |
|    critic_loss     | 1.22e+06  |
|    ent_coef        | 69.9      |
|    ent_coef_loss   | 0.537     |
|    learning_rate   | 0.0003    |
|    n_updates       | 44099     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 117       |
|    time_elapsed    | 10        |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -6.97e+04 |
|    critic_loss     | 1.19e+06  |
|    ent_coef        | 67.6      |
|    ent_coef_loss   | -1.99     |
|    learning_rate   | 0.0003    |
|    n_updates       | 44499     |
----------------------------------
=== Iterazione IRL 156 ===
Loss reward (iter 156): 329.9988708496094
=== Iterazione IRL 157 ===
Loss reward (iter 157): -129.80104064941406
=== Iterazione IRL 158 ===
Loss reward (iter 158): 233.55589294433594
=== Iterazione IRL 159 ===
Loss reward (iter 159): 240.55984497070312
=== Iterazione IRL 160 ===
Loss reward (iter 160): -249.54193115234375
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 138       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -6.76e+04 |
|    critic_loss     | 1.43e+06  |
|    ent_coef        | 63.6      |
|    ent_coef_loss   | -0.298    |
|    learning_rate   | 0.0003    |
|    n_updates       | 45099     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 120       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -6.67e+04 |
|    critic_loss     | 1.41e+06  |
|    ent_coef        | 61.9      |
|    ent_coef_loss   | -0.044    |
|    learning_rate   | 0.0003    |
|    n_updates       | 45499     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 115       |
|    time_elapsed    | 10        |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -6.65e+04 |
|    critic_loss     | 1.37e+06  |
|    ent_coef        | 61.1      |
|    ent_coef_loss   | 1.35      |
|    learning_rate   | 0.0003    |
|    n_updates       | 45899     |
----------------------------------
=== Iterazione IRL 161 ===
Loss reward (iter 161): 568.6449584960938
=== Iterazione IRL 162 ===
Loss reward (iter 162): 527.3015747070312
=== Iterazione IRL 163 ===
Loss reward (iter 163): 443.51202392578125
=== Iterazione IRL 164 ===
Loss reward (iter 164): 347.181884765625
=== Iterazione IRL 165 ===
Loss reward (iter 165): 289.56005859375
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 137       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -6.49e+04 |
|    critic_loss     | 1.62e+06  |
|    ent_coef        | 60.1      |
|    ent_coef_loss   | 0.371     |
|    learning_rate   | 0.0003    |
|    n_updates       | 46499     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 119       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -6.36e+04 |
|    critic_loss     | 1.47e+06  |
|    ent_coef        | 60        |
|    ent_coef_loss   | -0.759    |
|    learning_rate   | 0.0003    |
|    n_updates       | 46899     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 114       |
|    time_elapsed    | 10        |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -6.41e+04 |
|    critic_loss     | 6.86e+05  |
|    ent_coef        | 61.5      |
|    ent_coef_loss   | -0.563    |
|    learning_rate   | 0.0003    |
|    n_updates       | 47299     |
----------------------------------
=== Iterazione IRL 166 ===
Loss reward (iter 166): 282.4017028808594
=== Iterazione IRL 167 ===
Loss reward (iter 167): 228.13015747070312
=== Iterazione IRL 168 ===
Loss reward (iter 168): 203.90353393554688
=== Iterazione IRL 169 ===
Loss reward (iter 169): 167.0643768310547
=== Iterazione IRL 170 ===
Loss reward (iter 170): 131.45907592773438
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 137       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -6.29e+04 |
|    critic_loss     | 9.73e+05  |
|    ent_coef        | 64.1      |
|    ent_coef_loss   | -0.0334   |
|    learning_rate   | 0.0003    |
|    n_updates       | 47899     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 119       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -6.18e+04 |
|    critic_loss     | 1.27e+06  |
|    ent_coef        | 64.9      |
|    ent_coef_loss   | 0.564     |
|    learning_rate   | 0.0003    |
|    n_updates       | 48299     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 114       |
|    time_elapsed    | 10        |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -6.07e+04 |
|    critic_loss     | 1.28e+06  |
|    ent_coef        | 64.4      |
|    ent_coef_loss   | -0.906    |
|    learning_rate   | 0.0003    |
|    n_updates       | 48699     |
----------------------------------
=== Iterazione IRL 171 ===
Loss reward (iter 171): 120.1507568359375
=== Iterazione IRL 172 ===
Loss reward (iter 172): 94.86386108398438
=== Iterazione IRL 173 ===
Loss reward (iter 173): 80.79588317871094
=== Iterazione IRL 174 ===
Loss reward (iter 174): 75.82758331298828
=== Iterazione IRL 175 ===
Loss reward (iter 175): 73.30194854736328
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 140       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -5.99e+04 |
|    critic_loss     | 1.15e+06  |
|    ent_coef        | 64.2      |
|    ent_coef_loss   | -0.0168   |
|    learning_rate   | 0.0003    |
|    n_updates       | 49299     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 126       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -6.01e+04 |
|    critic_loss     | 1.79e+06  |
|    ent_coef        | 64.6      |
|    ent_coef_loss   | -1.88     |
|    learning_rate   | 0.0003    |
|    n_updates       | 49699     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 122       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -5.84e+04 |
|    critic_loss     | 1.3e+06   |
|    ent_coef        | 64.3      |
|    ent_coef_loss   | 1.48      |
|    learning_rate   | 0.0003    |
|    n_updates       | 50099     |
----------------------------------
=== Iterazione IRL 176 ===
Loss reward (iter 176): 60.477664947509766
=== Iterazione IRL 177 ===
Loss reward (iter 177): 53.54472732543945
=== Iterazione IRL 178 ===
Loss reward (iter 178): 36.47344970703125
=== Iterazione IRL 179 ===
Loss reward (iter 179): 35.47697830200195
=== Iterazione IRL 180 ===
Loss reward (iter 180): 19.80722427368164
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 149       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -5.81e+04 |
|    critic_loss     | 7.72e+05  |
|    ent_coef        | 63.5      |
|    ent_coef_loss   | 0.764     |
|    learning_rate   | 0.0003    |
|    n_updates       | 50699     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 130       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -5.81e+04 |
|    critic_loss     | 9.97e+05  |
|    ent_coef        | 62        |
|    ent_coef_loss   | 0.488     |
|    learning_rate   | 0.0003    |
|    n_updates       | 51099     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 124       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -5.71e+04 |
|    critic_loss     | 1.19e+06  |
|    ent_coef        | 61.2      |
|    ent_coef_loss   | 0.334     |
|    learning_rate   | 0.0003    |
|    n_updates       | 51499     |
----------------------------------
=== Iterazione IRL 181 ===
Loss reward (iter 181): 17.661378860473633
=== Iterazione IRL 182 ===
Loss reward (iter 182): 13.096864700317383
=== Iterazione IRL 183 ===
Loss reward (iter 183): 2.0746355056762695
=== Iterazione IRL 184 ===
Loss reward (iter 184): -4.279560089111328
=== Iterazione IRL 185 ===
Loss reward (iter 185): -9.728694915771484
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 149      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.6e+04 |
|    critic_loss     | 1.01e+06 |
|    ent_coef        | 59.5     |
|    ent_coef_loss   | -1.21    |
|    learning_rate   | 0.0003   |
|    n_updates       | 52099    |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 129       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -5.58e+04 |
|    critic_loss     | 1.04e+06  |
|    ent_coef        | 59.4      |
|    ent_coef_loss   | 0.731     |
|    learning_rate   | 0.0003    |
|    n_updates       | 52499     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 124       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -5.51e+04 |
|    critic_loss     | 1.06e+06  |
|    ent_coef        | 58        |
|    ent_coef_loss   | 0.931     |
|    learning_rate   | 0.0003    |
|    n_updates       | 52899     |
----------------------------------
=== Iterazione IRL 186 ===
Loss reward (iter 186): -21.217761993408203
=== Iterazione IRL 187 ===
Loss reward (iter 187): -29.323902130126953
=== Iterazione IRL 188 ===
Loss reward (iter 188): -55.28159713745117
=== Iterazione IRL 189 ===
Loss reward (iter 189): -6.985889434814453
=== Iterazione IRL 190 ===
Loss reward (iter 190): -48.304962158203125
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 149       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -5.35e+04 |
|    critic_loss     | 1.63e+06  |
|    ent_coef        | 56.7      |
|    ent_coef_loss   | -0.388    |
|    learning_rate   | 0.0003    |
|    n_updates       | 53499     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 129       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -5.34e+04 |
|    critic_loss     | 6.81e+05  |
|    ent_coef        | 54.8      |
|    ent_coef_loss   | -1.27     |
|    learning_rate   | 0.0003    |
|    n_updates       | 53899     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 124       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -5.2e+04  |
|    critic_loss     | 1.2e+06   |
|    ent_coef        | 54.4      |
|    ent_coef_loss   | -0.000381 |
|    learning_rate   | 0.0003    |
|    n_updates       | 54299     |
----------------------------------
=== Iterazione IRL 191 ===
Loss reward (iter 191): 49.360774993896484
=== Iterazione IRL 192 ===
Loss reward (iter 192): 13.549836158752441
=== Iterazione IRL 193 ===
Loss reward (iter 193): 20.939682006835938
=== Iterazione IRL 194 ===
Loss reward (iter 194): -7.571360111236572
=== Iterazione IRL 195 ===
Loss reward (iter 195): -15.514976501464844
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 149       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -5.19e+04 |
|    critic_loss     | 1.21e+06  |
|    ent_coef        | 52.1      |
|    ent_coef_loss   | 0.222     |
|    learning_rate   | 0.0003    |
|    n_updates       | 54899     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 130       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -5.11e+04 |
|    critic_loss     | 8.56e+05  |
|    ent_coef        | 51.8      |
|    ent_coef_loss   | -0.389    |
|    learning_rate   | 0.0003    |
|    n_updates       | 55299     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 124       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -5.03e+04 |
|    critic_loss     | 1.02e+06  |
|    ent_coef        | 51.1      |
|    ent_coef_loss   | -0.918    |
|    learning_rate   | 0.0003    |
|    n_updates       | 55699     |
----------------------------------
=== Iterazione IRL 196 ===
Loss reward (iter 196): 13.33848762512207
=== Iterazione IRL 197 ===
Loss reward (iter 197): 8.637255668640137
=== Iterazione IRL 198 ===
Loss reward (iter 198): -8.534647941589355
=== Iterazione IRL 199 ===
Loss reward (iter 199): -2.537503242492676
=== Iterazione IRL 200 ===
Loss reward (iter 200): 3.2627992630004883
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 148       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -4.95e+04 |
|    critic_loss     | 1.13e+06  |
|    ent_coef        | 49.1      |
|    ent_coef_loss   | 0.355     |
|    learning_rate   | 0.0003    |
|    n_updates       | 56299     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 129       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -4.89e+04 |
|    critic_loss     | 1.34e+06  |
|    ent_coef        | 48.4      |
|    ent_coef_loss   | 0.133     |
|    learning_rate   | 0.0003    |
|    n_updates       | 56699     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 124       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -4.88e+04 |
|    critic_loss     | 1.49e+06  |
|    ent_coef        | 47.7      |
|    ent_coef_loss   | -1.52     |
|    learning_rate   | 0.0003    |
|    n_updates       | 57099     |
----------------------------------
=== Iterazione IRL 201 ===
Loss reward (iter 201): 15.357972145080566
=== Iterazione IRL 202 ===
Loss reward (iter 202): 4.147759437561035
=== Iterazione IRL 203 ===
Loss reward (iter 203): -3.678116798400879
=== Iterazione IRL 204 ===
Loss reward (iter 204): 19.177690505981445
=== Iterazione IRL 205 ===
Loss reward (iter 205): 3.671323299407959
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 148       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -4.84e+04 |
|    critic_loss     | 1.05e+06  |
|    ent_coef        | 47.4      |
|    ent_coef_loss   | 0.241     |
|    learning_rate   | 0.0003    |
|    n_updates       | 57699     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 129       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -4.71e+04 |
|    critic_loss     | 9.46e+05  |
|    ent_coef        | 47.3      |
|    ent_coef_loss   | 0.2       |
|    learning_rate   | 0.0003    |
|    n_updates       | 58099     |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 124      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -4.7e+04 |
|    critic_loss     | 8.9e+05  |
|    ent_coef        | 47.2     |
|    ent_coef_loss   | -0.38    |
|    learning_rate   | 0.0003   |
|    n_updates       | 58499    |
---------------------------------
=== Iterazione IRL 206 ===
Loss reward (iter 206): -10.23058795928955
=== Iterazione IRL 207 ===
Loss reward (iter 207): -7.371902942657471
=== Iterazione IRL 208 ===
Loss reward (iter 208): -5.117883205413818
=== Iterazione IRL 209 ===
Loss reward (iter 209): -6.001061916351318
=== Iterazione IRL 210 ===
Loss reward (iter 210): 2.4209213256835938
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 148       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -4.63e+04 |
|    critic_loss     | 1.42e+06  |
|    ent_coef        | 47        |
|    ent_coef_loss   | 0.394     |
|    learning_rate   | 0.0003    |
|    n_updates       | 59099     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 129       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -4.51e+04 |
|    critic_loss     | 6.27e+05  |
|    ent_coef        | 45.9      |
|    ent_coef_loss   | -0.19     |
|    learning_rate   | 0.0003    |
|    n_updates       | 59499     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 124       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -4.57e+04 |
|    critic_loss     | 8.01e+05  |
|    ent_coef        | 45.2      |
|    ent_coef_loss   | 0.404     |
|    learning_rate   | 0.0003    |
|    n_updates       | 59899     |
----------------------------------
=== Iterazione IRL 211 ===
Loss reward (iter 211): -4.833740711212158
=== Iterazione IRL 212 ===
Loss reward (iter 212): 16.621837615966797
=== Iterazione IRL 213 ===
Loss reward (iter 213): -11.183052062988281
=== Iterazione IRL 214 ===
Loss reward (iter 214): 23.82708740234375
=== Iterazione IRL 215 ===
Loss reward (iter 215): 7.740278244018555
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 148       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -4.47e+04 |
|    critic_loss     | 9.84e+05  |
|    ent_coef        | 44.5      |
|    ent_coef_loss   | -0.922    |
|    learning_rate   | 0.0003    |
|    n_updates       | 60499     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 129       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -4.35e+04 |
|    critic_loss     | 1.3e+06   |
|    ent_coef        | 44.4      |
|    ent_coef_loss   | 0.627     |
|    learning_rate   | 0.0003    |
|    n_updates       | 60899     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 124       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -4.36e+04 |
|    critic_loss     | 8.99e+05  |
|    ent_coef        | 43.4      |
|    ent_coef_loss   | 0.349     |
|    learning_rate   | 0.0003    |
|    n_updates       | 61299     |
----------------------------------
=== Iterazione IRL 216 ===
Loss reward (iter 216): 36.3519287109375
=== Iterazione IRL 217 ===
Loss reward (iter 217): 17.485370635986328
=== Iterazione IRL 218 ===
Loss reward (iter 218): 22.250267028808594
=== Iterazione IRL 219 ===
Loss reward (iter 219): 8.88607406616211
=== Iterazione IRL 220 ===
Loss reward (iter 220): 5.4409990310668945
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 149       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -4.26e+04 |
|    critic_loss     | 1.05e+06  |
|    ent_coef        | 42.5      |
|    ent_coef_loss   | 0.563     |
|    learning_rate   | 0.0003    |
|    n_updates       | 61899     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 130       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -4.29e+04 |
|    critic_loss     | 5.86e+05  |
|    ent_coef        | 41.9      |
|    ent_coef_loss   | 1.75      |
|    learning_rate   | 0.0003    |
|    n_updates       | 62299     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 125       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -4.21e+04 |
|    critic_loss     | 1e+06     |
|    ent_coef        | 42.3      |
|    ent_coef_loss   | 0.104     |
|    learning_rate   | 0.0003    |
|    n_updates       | 62699     |
----------------------------------
=== Iterazione IRL 221 ===
Loss reward (iter 221): 30.602739334106445
=== Iterazione IRL 222 ===
Loss reward (iter 222): 23.205747604370117
=== Iterazione IRL 223 ===
Loss reward (iter 223): 26.323423385620117
=== Iterazione IRL 224 ===
Loss reward (iter 224): 16.240570068359375
=== Iterazione IRL 225 ===
Loss reward (iter 225): 7.6813435554504395
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 150       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -4.08e+04 |
|    critic_loss     | 8.18e+05  |
|    ent_coef        | 41.9      |
|    ent_coef_loss   | -0.57     |
|    learning_rate   | 0.0003    |
|    n_updates       | 63299     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 130       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -4.07e+04 |
|    critic_loss     | 6.83e+05  |
|    ent_coef        | 42.2      |
|    ent_coef_loss   | 0.262     |
|    learning_rate   | 0.0003    |
|    n_updates       | 63699     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 125       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -4.12e+04 |
|    critic_loss     | 9.52e+05  |
|    ent_coef        | 41.9      |
|    ent_coef_loss   | -1.26     |
|    learning_rate   | 0.0003    |
|    n_updates       | 64099     |
----------------------------------
=== Iterazione IRL 226 ===
Loss reward (iter 226): 28.498027801513672
=== Iterazione IRL 227 ===
Loss reward (iter 227): 15.655378341674805
=== Iterazione IRL 228 ===
Loss reward (iter 228): 18.74787712097168
=== Iterazione IRL 229 ===
Loss reward (iter 229): 14.012054443359375
=== Iterazione IRL 230 ===
Loss reward (iter 230): 11.180766105651855
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 150       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -4.04e+04 |
|    critic_loss     | 1.09e+06  |
|    ent_coef        | 43.2      |
|    ent_coef_loss   | 0.383     |
|    learning_rate   | 0.0003    |
|    n_updates       | 64699     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 130       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -3.95e+04 |
|    critic_loss     | 5.59e+05  |
|    ent_coef        | 42.5      |
|    ent_coef_loss   | 0.584     |
|    learning_rate   | 0.0003    |
|    n_updates       | 65099     |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -3.9e+04 |
|    critic_loss     | 1.25e+06 |
|    ent_coef        | 43.8     |
|    ent_coef_loss   | -1.49    |
|    learning_rate   | 0.0003   |
|    n_updates       | 65499    |
---------------------------------
=== Iterazione IRL 231 ===
Loss reward (iter 231): 14.656737327575684
=== Iterazione IRL 232 ===
Loss reward (iter 232): 10.784415245056152
=== Iterazione IRL 233 ===
Loss reward (iter 233): 11.29426383972168
=== Iterazione IRL 234 ===
Loss reward (iter 234): 6.7127580642700195
=== Iterazione IRL 235 ===
Loss reward (iter 235): 10.504006385803223
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 139      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.9e+04 |
|    critic_loss     | 1.19e+06 |
|    ent_coef        | 44.8     |
|    ent_coef_loss   | -0.285   |
|    learning_rate   | 0.0003   |
|    n_updates       | 66099    |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 120       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -3.81e+04 |
|    critic_loss     | 1.27e+06  |
|    ent_coef        | 44.8      |
|    ent_coef_loss   | 0.585     |
|    learning_rate   | 0.0003    |
|    n_updates       | 66499     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 115       |
|    time_elapsed    | 10        |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -3.83e+04 |
|    critic_loss     | 5.94e+05  |
|    ent_coef        | 45.7      |
|    ent_coef_loss   | -0.56     |
|    learning_rate   | 0.0003    |
|    n_updates       | 66899     |
----------------------------------
=== Iterazione IRL 236 ===
Loss reward (iter 236): 7.269860744476318
=== Iterazione IRL 237 ===
Loss reward (iter 237): 12.194208145141602
=== Iterazione IRL 238 ===
Loss reward (iter 238): 14.611019134521484
=== Iterazione IRL 239 ===
Loss reward (iter 239): 7.733387470245361
=== Iterazione IRL 240 ===
Loss reward (iter 240): 11.680985450744629
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 138       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -3.79e+04 |
|    critic_loss     | 4.8e+05   |
|    ent_coef        | 46.4      |
|    ent_coef_loss   | 0.105     |
|    learning_rate   | 0.0003    |
|    n_updates       | 67499     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 120       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -3.74e+04 |
|    critic_loss     | 1.2e+06   |
|    ent_coef        | 46.2      |
|    ent_coef_loss   | -0.325    |
|    learning_rate   | 0.0003    |
|    n_updates       | 67899     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 115       |
|    time_elapsed    | 10        |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -3.76e+04 |
|    critic_loss     | 1.17e+06  |
|    ent_coef        | 46        |
|    ent_coef_loss   | 0.246     |
|    learning_rate   | 0.0003    |
|    n_updates       | 68299     |
----------------------------------
=== Iterazione IRL 241 ===
Loss reward (iter 241): 7.920877456665039
=== Iterazione IRL 242 ===
Loss reward (iter 242): 2.9954514503479004
=== Iterazione IRL 243 ===
Loss reward (iter 243): 1.105290174484253
=== Iterazione IRL 244 ===
Loss reward (iter 244): 6.698544025421143
=== Iterazione IRL 245 ===
Loss reward (iter 245): 7.30344820022583
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 138       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -3.67e+04 |
|    critic_loss     | 5.83e+05  |
|    ent_coef        | 47.1      |
|    ent_coef_loss   | 0.405     |
|    learning_rate   | 0.0003    |
|    n_updates       | 68899     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 120       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -3.67e+04 |
|    critic_loss     | 9.05e+05  |
|    ent_coef        | 46.2      |
|    ent_coef_loss   | -0.197    |
|    learning_rate   | 0.0003    |
|    n_updates       | 69299     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 115       |
|    time_elapsed    | 10        |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -3.59e+04 |
|    critic_loss     | 6.6e+05   |
|    ent_coef        | 46.7      |
|    ent_coef_loss   | 1.03      |
|    learning_rate   | 0.0003    |
|    n_updates       | 69699     |
----------------------------------
=== Iterazione IRL 246 ===
Loss reward (iter 246): 5.489447116851807
=== Iterazione IRL 247 ===
Loss reward (iter 247): 1.4880871772766113
=== Iterazione IRL 248 ===
Loss reward (iter 248): 5.6743292808532715
=== Iterazione IRL 249 ===
Loss reward (iter 249): 4.520678520202637
=== Iterazione IRL 250 ===
Loss reward (iter 250): 1.0086636543273926
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 138       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -3.55e+04 |
|    critic_loss     | 3.48e+05  |
|    ent_coef        | 47.6      |
|    ent_coef_loss   | 1.62      |
|    learning_rate   | 0.0003    |
|    n_updates       | 70299     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 124       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -3.52e+04 |
|    critic_loss     | 1.4e+06   |
|    ent_coef        | 47.2      |
|    ent_coef_loss   | -1.04     |
|    learning_rate   | 0.0003    |
|    n_updates       | 70699     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 121       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -3.48e+04 |
|    critic_loss     | 1.18e+06  |
|    ent_coef        | 47.4      |
|    ent_coef_loss   | 1.02      |
|    learning_rate   | 0.0003    |
|    n_updates       | 71099     |
----------------------------------
=== Iterazione IRL 251 ===
Loss reward (iter 251): 0.3238341808319092
=== Iterazione IRL 252 ===
Loss reward (iter 252): -3.6580986976623535
=== Iterazione IRL 253 ===
Loss reward (iter 253): 17.97140121459961
=== Iterazione IRL 254 ===
Loss reward (iter 254): 5.719869613647461
=== Iterazione IRL 255 ===
Loss reward (iter 255): 4.480419158935547
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 150       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -3.51e+04 |
|    critic_loss     | 6.31e+05  |
|    ent_coef        | 47.2      |
|    ent_coef_loss   | -0.175    |
|    learning_rate   | 0.0003    |
|    n_updates       | 71699     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -3.43e+04 |
|    critic_loss     | 1.03e+06  |
|    ent_coef        | 47.7      |
|    ent_coef_loss   | -0.131    |
|    learning_rate   | 0.0003    |
|    n_updates       | 72099     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 125       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -3.44e+04 |
|    critic_loss     | 8.92e+05  |
|    ent_coef        | 47.3      |
|    ent_coef_loss   | -0.772    |
|    learning_rate   | 0.0003    |
|    n_updates       | 72499     |
----------------------------------
=== Iterazione IRL 256 ===
Loss reward (iter 256): 0.04614424705505371
=== Iterazione IRL 257 ===
Loss reward (iter 257): -5.154962539672852
=== Iterazione IRL 258 ===
Loss reward (iter 258): -7.804164886474609
=== Iterazione IRL 259 ===
Loss reward (iter 259): -12.771440505981445
=== Iterazione IRL 260 ===
Loss reward (iter 260): -12.08319091796875
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 150       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -3.38e+04 |
|    critic_loss     | 1.29e+06  |
|    ent_coef        | 48        |
|    ent_coef_loss   | -0.353    |
|    learning_rate   | 0.0003    |
|    n_updates       | 73099     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -3.36e+04 |
|    critic_loss     | 8.13e+05  |
|    ent_coef        | 48.5      |
|    ent_coef_loss   | 0.447     |
|    learning_rate   | 0.0003    |
|    n_updates       | 73499     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 125       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -3.31e+04 |
|    critic_loss     | 9.99e+05  |
|    ent_coef        | 47.7      |
|    ent_coef_loss   | 0.501     |
|    learning_rate   | 0.0003    |
|    n_updates       | 73899     |
----------------------------------
=== Iterazione IRL 261 ===
Loss reward (iter 261): 1.7012929916381836
=== Iterazione IRL 262 ===
Loss reward (iter 262): 25.15402603149414
=== Iterazione IRL 263 ===
Loss reward (iter 263): 6.712580680847168
=== Iterazione IRL 264 ===
Loss reward (iter 264): 2.4516243934631348
=== Iterazione IRL 265 ===
Loss reward (iter 265): -3.1240768432617188
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 150       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -3.31e+04 |
|    critic_loss     | 8.62e+05  |
|    ent_coef        | 47.1      |
|    ent_coef_loss   | -0.919    |
|    learning_rate   | 0.0003    |
|    n_updates       | 74499     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -3.24e+04 |
|    critic_loss     | 8.42e+05  |
|    ent_coef        | 47.2      |
|    ent_coef_loss   | 0.386     |
|    learning_rate   | 0.0003    |
|    n_updates       | 74899     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 125       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -3.24e+04 |
|    critic_loss     | 7.67e+05  |
|    ent_coef        | 47.2      |
|    ent_coef_loss   | -0.271    |
|    learning_rate   | 0.0003    |
|    n_updates       | 75299     |
----------------------------------
=== Iterazione IRL 266 ===
Loss reward (iter 266): -7.713822841644287
=== Iterazione IRL 267 ===
Loss reward (iter 267): 9.035587310791016
=== Iterazione IRL 268 ===
Loss reward (iter 268): 3.898635149002075
=== Iterazione IRL 269 ===
Loss reward (iter 269): -0.3498547077178955
=== Iterazione IRL 270 ===
Loss reward (iter 270): -8.110697746276855
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.2e+04 |
|    critic_loss     | 1.05e+06 |
|    ent_coef        | 47.2     |
|    ent_coef_loss   | 0.616    |
|    learning_rate   | 0.0003   |
|    n_updates       | 75899    |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 130       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -3.15e+04 |
|    critic_loss     | 1.08e+06  |
|    ent_coef        | 47.3      |
|    ent_coef_loss   | 0.999     |
|    learning_rate   | 0.0003    |
|    n_updates       | 76299     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 125       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -3.15e+04 |
|    critic_loss     | 1.12e+06  |
|    ent_coef        | 46.9      |
|    ent_coef_loss   | -0.0729   |
|    learning_rate   | 0.0003    |
|    n_updates       | 76699     |
----------------------------------
=== Iterazione IRL 271 ===
Loss reward (iter 271): 5.859824180603027
=== Iterazione IRL 272 ===
Loss reward (iter 272): -6.745548248291016
=== Iterazione IRL 273 ===
Loss reward (iter 273): -3.7869763374328613
=== Iterazione IRL 274 ===
Loss reward (iter 274): -9.265580177307129
=== Iterazione IRL 275 ===
Loss reward (iter 275): -10.697755813598633
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 149       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -3.12e+04 |
|    critic_loss     | 1.38e+06  |
|    ent_coef        | 46.9      |
|    ent_coef_loss   | -0.624    |
|    learning_rate   | 0.0003    |
|    n_updates       | 77299     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 130       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -3.07e+04 |
|    critic_loss     | 7.05e+05  |
|    ent_coef        | 47.1      |
|    ent_coef_loss   | 0.215     |
|    learning_rate   | 0.0003    |
|    n_updates       | 77699     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 125       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -3.05e+04 |
|    critic_loss     | 3.98e+05  |
|    ent_coef        | 47.2      |
|    ent_coef_loss   | 0.639     |
|    learning_rate   | 0.0003    |
|    n_updates       | 78099     |
----------------------------------
=== Iterazione IRL 276 ===
Loss reward (iter 276): -29.03839111328125
=== Iterazione IRL 277 ===
Loss reward (iter 277): -26.223373413085938
=== Iterazione IRL 278 ===
Loss reward (iter 278): -0.7391198873519897
=== Iterazione IRL 279 ===
Loss reward (iter 279): -21.10690689086914
=== Iterazione IRL 280 ===
Loss reward (iter 280): -22.864662170410156
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 149       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -3.03e+04 |
|    critic_loss     | 5.27e+05  |
|    ent_coef        | 46.4      |
|    ent_coef_loss   | 0.414     |
|    learning_rate   | 0.0003    |
|    n_updates       | 78699     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 130       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.97e+04 |
|    critic_loss     | 1.01e+06  |
|    ent_coef        | 46.1      |
|    ent_coef_loss   | -0.309    |
|    learning_rate   | 0.0003    |
|    n_updates       | 79099     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 125       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.95e+04 |
|    critic_loss     | 5.8e+05   |
|    ent_coef        | 44.9      |
|    ent_coef_loss   | -0.251    |
|    learning_rate   | 0.0003    |
|    n_updates       | 79499     |
----------------------------------
=== Iterazione IRL 281 ===
Loss reward (iter 281): -24.071760177612305
=== Iterazione IRL 282 ===
Loss reward (iter 282): -11.251218795776367
=== Iterazione IRL 283 ===
Loss reward (iter 283): -18.61261558532715
=== Iterazione IRL 284 ===
Loss reward (iter 284): -22.461090087890625
=== Iterazione IRL 285 ===
Loss reward (iter 285): -22.05112648010254
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 150       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.96e+04 |
|    critic_loss     | 7.12e+05  |
|    ent_coef        | 44.1      |
|    ent_coef_loss   | 0.276     |
|    learning_rate   | 0.0003    |
|    n_updates       | 80099     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.91e+04 |
|    critic_loss     | 5.23e+05  |
|    ent_coef        | 43.6      |
|    ent_coef_loss   | -0.242    |
|    learning_rate   | 0.0003    |
|    n_updates       | 80499     |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -2.9e+04 |
|    critic_loss     | 3.93e+05 |
|    ent_coef        | 42.8     |
|    ent_coef_loss   | -0.11    |
|    learning_rate   | 0.0003   |
|    n_updates       | 80899    |
---------------------------------
=== Iterazione IRL 286 ===
Loss reward (iter 286): 48.211673736572266
=== Iterazione IRL 287 ===
Loss reward (iter 287): -3.1137566566467285
=== Iterazione IRL 288 ===
Loss reward (iter 288): -7.913609027862549
=== Iterazione IRL 289 ===
Loss reward (iter 289): -30.613903045654297
=== Iterazione IRL 290 ===
Loss reward (iter 290): -11.079316139221191
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 150       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.88e+04 |
|    critic_loss     | 1.54e+06  |
|    ent_coef        | 41.7      |
|    ent_coef_loss   | 0.551     |
|    learning_rate   | 0.0003    |
|    n_updates       | 81499     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.86e+04 |
|    critic_loss     | 6.25e+05  |
|    ent_coef        | 40.3      |
|    ent_coef_loss   | 0.637     |
|    learning_rate   | 0.0003    |
|    n_updates       | 81899     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 125       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.77e+04 |
|    critic_loss     | 5.12e+05  |
|    ent_coef        | 39.5      |
|    ent_coef_loss   | 0.389     |
|    learning_rate   | 0.0003    |
|    n_updates       | 82299     |
----------------------------------
=== Iterazione IRL 291 ===
Loss reward (iter 291): 42.75596618652344
=== Iterazione IRL 292 ===
Loss reward (iter 292): -11.234718322753906
=== Iterazione IRL 293 ===
Loss reward (iter 293): 12.219647407531738
=== Iterazione IRL 294 ===
Loss reward (iter 294): -25.36049461364746
=== Iterazione IRL 295 ===
Loss reward (iter 295): -14.25680160522461
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 150       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.77e+04 |
|    critic_loss     | 5.55e+05  |
|    ent_coef        | 37.9      |
|    ent_coef_loss   | -0.302    |
|    learning_rate   | 0.0003    |
|    n_updates       | 82899     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.75e+04 |
|    critic_loss     | 5.11e+05  |
|    ent_coef        | 37.4      |
|    ent_coef_loss   | -1.07     |
|    learning_rate   | 0.0003    |
|    n_updates       | 83299     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.71e+04 |
|    critic_loss     | 7.63e+05  |
|    ent_coef        | 37        |
|    ent_coef_loss   | 0.303     |
|    learning_rate   | 0.0003    |
|    n_updates       | 83699     |
----------------------------------
=== Iterazione IRL 296 ===
Loss reward (iter 296): 1.7910187244415283
=== Iterazione IRL 297 ===
Loss reward (iter 297): 9.810530662536621
=== Iterazione IRL 298 ===
Loss reward (iter 298): -11.477150917053223
=== Iterazione IRL 299 ===
Loss reward (iter 299): -0.717941164970398
=== Iterazione IRL 300 ===
Loss reward (iter 300): -27.918066024780273
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 150       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.68e+04 |
|    critic_loss     | 6.22e+05  |
|    ent_coef        | 36.5      |
|    ent_coef_loss   | 1.29      |
|    learning_rate   | 0.0003    |
|    n_updates       | 84299     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.67e+04 |
|    critic_loss     | 7.63e+05  |
|    ent_coef        | 36.6      |
|    ent_coef_loss   | 0.738     |
|    learning_rate   | 0.0003    |
|    n_updates       | 84699     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 125       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.66e+04 |
|    critic_loss     | 6.74e+05  |
|    ent_coef        | 36.3      |
|    ent_coef_loss   | -0.642    |
|    learning_rate   | 0.0003    |
|    n_updates       | 85099     |
----------------------------------
=== Iterazione IRL 301 ===
Loss reward (iter 301): 13.152067184448242
=== Iterazione IRL 302 ===
Loss reward (iter 302): 1.8005090951919556
=== Iterazione IRL 303 ===
Loss reward (iter 303): 0.7488812208175659
=== Iterazione IRL 304 ===
Loss reward (iter 304): -6.89845085144043
=== Iterazione IRL 305 ===
Loss reward (iter 305): -10.360723495483398
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.6e+04 |
|    critic_loss     | 4.55e+05 |
|    ent_coef        | 36.3     |
|    ent_coef_loss   | 0.0344   |
|    learning_rate   | 0.0003   |
|    n_updates       | 85699    |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.59e+04 |
|    critic_loss     | 3.81e+05  |
|    ent_coef        | 36.6      |
|    ent_coef_loss   | 0.307     |
|    learning_rate   | 0.0003    |
|    n_updates       | 86099     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.56e+04 |
|    critic_loss     | 4.91e+05  |
|    ent_coef        | 36.2      |
|    ent_coef_loss   | 1.05      |
|    learning_rate   | 0.0003    |
|    n_updates       | 86499     |
----------------------------------
=== Iterazione IRL 306 ===
Loss reward (iter 306): 2.2011961936950684
=== Iterazione IRL 307 ===
Loss reward (iter 307): 4.367305278778076
=== Iterazione IRL 308 ===
Loss reward (iter 308): -0.919775128364563
=== Iterazione IRL 309 ===
Loss reward (iter 309): 12.349602699279785
=== Iterazione IRL 310 ===
Loss reward (iter 310): -3.8807730674743652
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.54e+04 |
|    critic_loss     | 1.23e+06  |
|    ent_coef        | 35.8      |
|    ent_coef_loss   | -1.01     |
|    learning_rate   | 0.0003    |
|    n_updates       | 87099     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.52e+04 |
|    critic_loss     | 5.01e+05  |
|    ent_coef        | 35.6      |
|    ent_coef_loss   | -0.449    |
|    learning_rate   | 0.0003    |
|    n_updates       | 87499     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.44e+04 |
|    critic_loss     | 5.51e+05  |
|    ent_coef        | 35.2      |
|    ent_coef_loss   | 1.1       |
|    learning_rate   | 0.0003    |
|    n_updates       | 87899     |
----------------------------------
=== Iterazione IRL 311 ===
Loss reward (iter 311): 20.824657440185547
=== Iterazione IRL 312 ===
Loss reward (iter 312): 9.684123039245605
=== Iterazione IRL 313 ===
Loss reward (iter 313): 17.4111385345459
=== Iterazione IRL 314 ===
Loss reward (iter 314): 0.1925957202911377
=== Iterazione IRL 315 ===
Loss reward (iter 315): 5.528078556060791
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.47e+04 |
|    critic_loss     | 1.06e+06  |
|    ent_coef        | 35.3      |
|    ent_coef_loss   | 0.41      |
|    learning_rate   | 0.0003    |
|    n_updates       | 88499     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.44e+04 |
|    critic_loss     | 6.35e+05  |
|    ent_coef        | 35.5      |
|    ent_coef_loss   | 0.716     |
|    learning_rate   | 0.0003    |
|    n_updates       | 88899     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.39e+04 |
|    critic_loss     | 6.65e+05  |
|    ent_coef        | 35.7      |
|    ent_coef_loss   | 0.0643    |
|    learning_rate   | 0.0003    |
|    n_updates       | 89299     |
----------------------------------
=== Iterazione IRL 316 ===
Loss reward (iter 316): 9.263227462768555
=== Iterazione IRL 317 ===
Loss reward (iter 317): 11.759167671203613
=== Iterazione IRL 318 ===
Loss reward (iter 318): 4.243458271026611
=== Iterazione IRL 319 ===
Loss reward (iter 319): 11.731267929077148
=== Iterazione IRL 320 ===
Loss reward (iter 320): 12.730484008789062
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.35e+04 |
|    critic_loss     | 4.85e+05  |
|    ent_coef        | 36.4      |
|    ent_coef_loss   | 0.123     |
|    learning_rate   | 0.0003    |
|    n_updates       | 89899     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.38e+04 |
|    critic_loss     | 1.47e+06  |
|    ent_coef        | 37.1      |
|    ent_coef_loss   | -0.0752   |
|    learning_rate   | 0.0003    |
|    n_updates       | 90299     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.33e+04 |
|    critic_loss     | 5.3e+05   |
|    ent_coef        | 37.3      |
|    ent_coef_loss   | 0.479     |
|    learning_rate   | 0.0003    |
|    n_updates       | 90699     |
----------------------------------
=== Iterazione IRL 321 ===
Loss reward (iter 321): 12.344062805175781
=== Iterazione IRL 322 ===
Loss reward (iter 322): 2.396998405456543
=== Iterazione IRL 323 ===
Loss reward (iter 323): 5.020468711853027
=== Iterazione IRL 324 ===
Loss reward (iter 324): 12.46647834777832
=== Iterazione IRL 325 ===
Loss reward (iter 325): 9.78138256072998
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 150       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.32e+04 |
|    critic_loss     | 3.71e+05  |
|    ent_coef        | 37.4      |
|    ent_coef_loss   | -0.0135   |
|    learning_rate   | 0.0003    |
|    n_updates       | 91299     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.28e+04 |
|    critic_loss     | 1.09e+06  |
|    ent_coef        | 36.7      |
|    ent_coef_loss   | -1.06     |
|    learning_rate   | 0.0003    |
|    n_updates       | 91699     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 125       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.24e+04 |
|    critic_loss     | 4.5e+05   |
|    ent_coef        | 36.8      |
|    ent_coef_loss   | -0.0599   |
|    learning_rate   | 0.0003    |
|    n_updates       | 92099     |
----------------------------------
=== Iterazione IRL 326 ===
Loss reward (iter 326): 6.461296081542969
=== Iterazione IRL 327 ===
Loss reward (iter 327): 6.309072017669678
=== Iterazione IRL 328 ===
Loss reward (iter 328): 9.96419906616211
=== Iterazione IRL 329 ===
Loss reward (iter 329): 5.7734785079956055
=== Iterazione IRL 330 ===
Loss reward (iter 330): -5.351015567779541
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 150       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.24e+04 |
|    critic_loss     | 3.64e+05  |
|    ent_coef        | 36.4      |
|    ent_coef_loss   | -0.792    |
|    learning_rate   | 0.0003    |
|    n_updates       | 92699     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.23e+04 |
|    critic_loss     | 7.21e+05  |
|    ent_coef        | 36.6      |
|    ent_coef_loss   | 1.34      |
|    learning_rate   | 0.0003    |
|    n_updates       | 93099     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.25e+04 |
|    critic_loss     | 8.19e+05  |
|    ent_coef        | 35.8      |
|    ent_coef_loss   | -1.66     |
|    learning_rate   | 0.0003    |
|    n_updates       | 93499     |
----------------------------------
=== Iterazione IRL 331 ===
Loss reward (iter 331): 12.338964462280273
=== Iterazione IRL 332 ===
Loss reward (iter 332): 13.508557319641113
=== Iterazione IRL 333 ===
Loss reward (iter 333): 2.928084373474121
=== Iterazione IRL 334 ===
Loss reward (iter 334): 9.138009071350098
=== Iterazione IRL 335 ===
Loss reward (iter 335): 6.935283184051514
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 150       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.18e+04 |
|    critic_loss     | 1.01e+06  |
|    ent_coef        | 36.1      |
|    ent_coef_loss   | 0.119     |
|    learning_rate   | 0.0003    |
|    n_updates       | 94099     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.14e+04 |
|    critic_loss     | 5.61e+05  |
|    ent_coef        | 35.7      |
|    ent_coef_loss   | 0.219     |
|    learning_rate   | 0.0003    |
|    n_updates       | 94499     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.12e+04 |
|    critic_loss     | 3.74e+05  |
|    ent_coef        | 35.1      |
|    ent_coef_loss   | 0.677     |
|    learning_rate   | 0.0003    |
|    n_updates       | 94899     |
----------------------------------
=== Iterazione IRL 336 ===
Loss reward (iter 336): 5.853123664855957
=== Iterazione IRL 337 ===
Loss reward (iter 337): 6.510161399841309
=== Iterazione IRL 338 ===
Loss reward (iter 338): 5.302068710327148
=== Iterazione IRL 339 ===
Loss reward (iter 339): 4.182075500488281
=== Iterazione IRL 340 ===
Loss reward (iter 340): 5.566226482391357
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 150       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.12e+04 |
|    critic_loss     | 3.12e+05  |
|    ent_coef        | 35.2      |
|    ent_coef_loss   | -0.162    |
|    learning_rate   | 0.0003    |
|    n_updates       | 95499     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.07e+04 |
|    critic_loss     | 4.98e+05  |
|    ent_coef        | 35.1      |
|    ent_coef_loss   | 0.963     |
|    learning_rate   | 0.0003    |
|    n_updates       | 95899     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 125       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.07e+04 |
|    critic_loss     | 5.83e+05  |
|    ent_coef        | 34.6      |
|    ent_coef_loss   | 0.227     |
|    learning_rate   | 0.0003    |
|    n_updates       | 96299     |
----------------------------------
=== Iterazione IRL 341 ===
Loss reward (iter 341): 4.414894104003906
=== Iterazione IRL 342 ===
Loss reward (iter 342): 2.1842710971832275
=== Iterazione IRL 343 ===
Loss reward (iter 343): 3.20115065574646
=== Iterazione IRL 344 ===
Loss reward (iter 344): 5.383625030517578
=== Iterazione IRL 345 ===
Loss reward (iter 345): 4.264285087585449
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 150       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.01e+04 |
|    critic_loss     | 4.63e+05  |
|    ent_coef        | 34.4      |
|    ent_coef_loss   | 0.346     |
|    learning_rate   | 0.0003    |
|    n_updates       | 96899     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.98e+04 |
|    critic_loss     | 6.63e+05  |
|    ent_coef        | 33.5      |
|    ent_coef_loss   | -1.21     |
|    learning_rate   | 0.0003    |
|    n_updates       | 97299     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.99e+04 |
|    critic_loss     | 9.72e+05  |
|    ent_coef        | 33.2      |
|    ent_coef_loss   | -0.295    |
|    learning_rate   | 0.0003    |
|    n_updates       | 97699     |
----------------------------------
=== Iterazione IRL 346 ===
Loss reward (iter 346): 6.837800025939941
=== Iterazione IRL 347 ===
Loss reward (iter 347): 2.292954921722412
=== Iterazione IRL 348 ===
Loss reward (iter 348): 1.7235521078109741
=== Iterazione IRL 349 ===
Loss reward (iter 349): 4.4789252281188965
=== Iterazione IRL 350 ===
Loss reward (iter 350): 5.18035888671875
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 150       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.98e+04 |
|    critic_loss     | 4.9e+05   |
|    ent_coef        | 33.9      |
|    ent_coef_loss   | -2.19     |
|    learning_rate   | 0.0003    |
|    n_updates       | 98299     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.93e+04 |
|    critic_loss     | 7.99e+05  |
|    ent_coef        | 33.5      |
|    ent_coef_loss   | -0.406    |
|    learning_rate   | 0.0003    |
|    n_updates       | 98699     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.92e+04 |
|    critic_loss     | 8.89e+05  |
|    ent_coef        | 33.3      |
|    ent_coef_loss   | -0.117    |
|    learning_rate   | 0.0003    |
|    n_updates       | 99099     |
----------------------------------
=== Iterazione IRL 351 ===
Loss reward (iter 351): 2.5220229625701904
=== Iterazione IRL 352 ===
Loss reward (iter 352): 6.9955315589904785
=== Iterazione IRL 353 ===
Loss reward (iter 353): 5.256094932556152
=== Iterazione IRL 354 ===
Loss reward (iter 354): 3.583263397216797
=== Iterazione IRL 355 ===
Loss reward (iter 355): 5.4746994972229
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.92e+04 |
|    critic_loss     | 4.07e+05  |
|    ent_coef        | 33        |
|    ent_coef_loss   | -0.579    |
|    learning_rate   | 0.0003    |
|    n_updates       | 99699     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.87e+04 |
|    critic_loss     | 1.33e+06  |
|    ent_coef        | 32.1      |
|    ent_coef_loss   | 0.247     |
|    learning_rate   | 0.0003    |
|    n_updates       | 100099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.85e+04 |
|    critic_loss     | 3.35e+05  |
|    ent_coef        | 31.9      |
|    ent_coef_loss   | 1.3       |
|    learning_rate   | 0.0003    |
|    n_updates       | 100499    |
----------------------------------
=== Iterazione IRL 356 ===
Loss reward (iter 356): 6.712953567504883
=== Iterazione IRL 357 ===
Loss reward (iter 357): 5.079195022583008
=== Iterazione IRL 358 ===
Loss reward (iter 358): 6.97715950012207
=== Iterazione IRL 359 ===
Loss reward (iter 359): 6.858063220977783
=== Iterazione IRL 360 ===
Loss reward (iter 360): 4.685077667236328
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 150       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.85e+04 |
|    critic_loss     | 6.63e+05  |
|    ent_coef        | 30.9      |
|    ent_coef_loss   | 0.264     |
|    learning_rate   | 0.0003    |
|    n_updates       | 101099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.83e+04 |
|    critic_loss     | 4.43e+05  |
|    ent_coef        | 30.2      |
|    ent_coef_loss   | -0.777    |
|    learning_rate   | 0.0003    |
|    n_updates       | 101499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.78e+04 |
|    critic_loss     | 3.55e+05  |
|    ent_coef        | 30.8      |
|    ent_coef_loss   | 0.645     |
|    learning_rate   | 0.0003    |
|    n_updates       | 101899    |
----------------------------------
=== Iterazione IRL 361 ===
Loss reward (iter 361): 5.6258544921875
=== Iterazione IRL 362 ===
Loss reward (iter 362): 6.155238151550293
=== Iterazione IRL 363 ===
Loss reward (iter 363): 5.14785623550415
=== Iterazione IRL 364 ===
Loss reward (iter 364): 4.7290239334106445
=== Iterazione IRL 365 ===
Loss reward (iter 365): 6.200591564178467
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.75e+04 |
|    critic_loss     | 7.59e+05  |
|    ent_coef        | 29.5      |
|    ent_coef_loss   | 0.62      |
|    learning_rate   | 0.0003    |
|    n_updates       | 102499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.71e+04 |
|    critic_loss     | 2.72e+05  |
|    ent_coef        | 29.4      |
|    ent_coef_loss   | 0.788     |
|    learning_rate   | 0.0003    |
|    n_updates       | 102899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.72e+04 |
|    critic_loss     | 7.77e+05  |
|    ent_coef        | 28.9      |
|    ent_coef_loss   | 0.863     |
|    learning_rate   | 0.0003    |
|    n_updates       | 103299    |
----------------------------------
=== Iterazione IRL 366 ===
Loss reward (iter 366): 5.494311809539795
=== Iterazione IRL 367 ===
Loss reward (iter 367): 6.802945613861084
=== Iterazione IRL 368 ===
Loss reward (iter 368): 7.200149059295654
=== Iterazione IRL 369 ===
Loss reward (iter 369): 5.8699750900268555
=== Iterazione IRL 370 ===
Loss reward (iter 370): 5.96614933013916
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.69e+04 |
|    critic_loss     | 5.38e+05  |
|    ent_coef        | 28.3      |
|    ent_coef_loss   | -1.08     |
|    learning_rate   | 0.0003    |
|    n_updates       | 103899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.69e+04 |
|    critic_loss     | 5.36e+05  |
|    ent_coef        | 28.7      |
|    ent_coef_loss   | 0.209     |
|    learning_rate   | 0.0003    |
|    n_updates       | 104299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.68e+04 |
|    critic_loss     | 7.12e+05  |
|    ent_coef        | 28.5      |
|    ent_coef_loss   | -1.19     |
|    learning_rate   | 0.0003    |
|    n_updates       | 104699    |
----------------------------------
=== Iterazione IRL 371 ===
Loss reward (iter 371): 5.4464263916015625
=== Iterazione IRL 372 ===
Loss reward (iter 372): 5.547092437744141
=== Iterazione IRL 373 ===
Loss reward (iter 373): 5.168642997741699
=== Iterazione IRL 374 ===
Loss reward (iter 374): 5.702231407165527
=== Iterazione IRL 375 ===
Loss reward (iter 375): 5.216671466827393
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.63e+04 |
|    critic_loss     | 7.08e+05  |
|    ent_coef        | 28.5      |
|    ent_coef_loss   | -0.483    |
|    learning_rate   | 0.0003    |
|    n_updates       | 105299    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -1.6e+04 |
|    critic_loss     | 3.19e+05 |
|    ent_coef        | 28.6     |
|    ent_coef_loss   | -0.311   |
|    learning_rate   | 0.0003   |
|    n_updates       | 105699   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.61e+04 |
|    critic_loss     | 3.39e+05  |
|    ent_coef        | 28.4      |
|    ent_coef_loss   | 0.27      |
|    learning_rate   | 0.0003    |
|    n_updates       | 106099    |
----------------------------------
=== Iterazione IRL 376 ===
Loss reward (iter 376): 6.540004730224609
=== Iterazione IRL 377 ===
Loss reward (iter 377): 5.508664131164551
=== Iterazione IRL 378 ===
Loss reward (iter 378): 4.928193092346191
=== Iterazione IRL 379 ===
Loss reward (iter 379): 3.4968109130859375
=== Iterazione IRL 380 ===
Loss reward (iter 380): 5.316595554351807
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.61e+04 |
|    critic_loss     | 1.53e+06  |
|    ent_coef        | 27.9      |
|    ent_coef_loss   | -0.305    |
|    learning_rate   | 0.0003    |
|    n_updates       | 106699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.58e+04 |
|    critic_loss     | 5.4e+05   |
|    ent_coef        | 28.3      |
|    ent_coef_loss   | -0.349    |
|    learning_rate   | 0.0003    |
|    n_updates       | 107099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.57e+04 |
|    critic_loss     | 4.25e+05  |
|    ent_coef        | 28.4      |
|    ent_coef_loss   | -1.53     |
|    learning_rate   | 0.0003    |
|    n_updates       | 107499    |
----------------------------------
=== Iterazione IRL 381 ===
Loss reward (iter 381): 5.551764011383057
=== Iterazione IRL 382 ===
Loss reward (iter 382): 4.647209167480469
=== Iterazione IRL 383 ===
Loss reward (iter 383): 5.998292922973633
=== Iterazione IRL 384 ===
Loss reward (iter 384): 4.658281326293945
=== Iterazione IRL 385 ===
Loss reward (iter 385): 5.473991394042969
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.52e+04 |
|    critic_loss     | 3.04e+05  |
|    ent_coef        | 28.1      |
|    ent_coef_loss   | -1.01     |
|    learning_rate   | 0.0003    |
|    n_updates       | 108099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.51e+04 |
|    critic_loss     | 7.03e+05  |
|    ent_coef        | 28        |
|    ent_coef_loss   | -0.231    |
|    learning_rate   | 0.0003    |
|    n_updates       | 108499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.52e+04 |
|    critic_loss     | 3.95e+05  |
|    ent_coef        | 28.1      |
|    ent_coef_loss   | -0.21     |
|    learning_rate   | 0.0003    |
|    n_updates       | 108899    |
----------------------------------
=== Iterazione IRL 386 ===
Loss reward (iter 386): 6.157909393310547
=== Iterazione IRL 387 ===
Loss reward (iter 387): 3.629499673843384
=== Iterazione IRL 388 ===
Loss reward (iter 388): 8.370295524597168
=== Iterazione IRL 389 ===
Loss reward (iter 389): 3.69045352935791
=== Iterazione IRL 390 ===
Loss reward (iter 390): 4.024069786071777
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.49e+04 |
|    critic_loss     | 4.51e+05  |
|    ent_coef        | 28.2      |
|    ent_coef_loss   | -0.153    |
|    learning_rate   | 0.0003    |
|    n_updates       | 109499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.48e+04 |
|    critic_loss     | 6.02e+05  |
|    ent_coef        | 28.3      |
|    ent_coef_loss   | -0.447    |
|    learning_rate   | 0.0003    |
|    n_updates       | 109899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.45e+04 |
|    critic_loss     | 4.37e+05  |
|    ent_coef        | 27.9      |
|    ent_coef_loss   | 0.297     |
|    learning_rate   | 0.0003    |
|    n_updates       | 110299    |
----------------------------------
=== Iterazione IRL 391 ===
Loss reward (iter 391): 3.1044015884399414
=== Iterazione IRL 392 ===
Loss reward (iter 392): 6.126160621643066
=== Iterazione IRL 393 ===
Loss reward (iter 393): 4.8648552894592285
=== Iterazione IRL 394 ===
Loss reward (iter 394): 5.042853832244873
=== Iterazione IRL 395 ===
Loss reward (iter 395): 3.326089382171631
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.43e+04 |
|    critic_loss     | 4.72e+05  |
|    ent_coef        | 27.7      |
|    ent_coef_loss   | -0.616    |
|    learning_rate   | 0.0003    |
|    n_updates       | 110899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.42e+04 |
|    critic_loss     | 8.67e+05  |
|    ent_coef        | 27.9      |
|    ent_coef_loss   | 0.165     |
|    learning_rate   | 0.0003    |
|    n_updates       | 111299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.42e+04 |
|    critic_loss     | 7.33e+05  |
|    ent_coef        | 27.3      |
|    ent_coef_loss   | 1.99      |
|    learning_rate   | 0.0003    |
|    n_updates       | 111699    |
----------------------------------
=== Iterazione IRL 396 ===
Loss reward (iter 396): 4.3500823974609375
=== Iterazione IRL 397 ===
Loss reward (iter 397): 5.382411003112793
=== Iterazione IRL 398 ===
Loss reward (iter 398): 5.050979137420654
=== Iterazione IRL 399 ===
Loss reward (iter 399): 5.917385578155518
=== Iterazione IRL 400 ===
Loss reward (iter 400): 4.083123683929443
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -1.4e+04 |
|    critic_loss     | 2.73e+05 |
|    ent_coef        | 27.3     |
|    ent_coef_loss   | 0.992    |
|    learning_rate   | 0.0003   |
|    n_updates       | 112299   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.36e+04 |
|    critic_loss     | 1.08e+06  |
|    ent_coef        | 27.6      |
|    ent_coef_loss   | 0.485     |
|    learning_rate   | 0.0003    |
|    n_updates       | 112699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.36e+04 |
|    critic_loss     | 5.78e+05  |
|    ent_coef        | 27.6      |
|    ent_coef_loss   | 0.947     |
|    learning_rate   | 0.0003    |
|    n_updates       | 113099    |
----------------------------------
=== Iterazione IRL 401 ===
Loss reward (iter 401): 3.143289089202881
=== Iterazione IRL 402 ===
Loss reward (iter 402): 2.4796509742736816
=== Iterazione IRL 403 ===
Loss reward (iter 403): 2.500368595123291
=== Iterazione IRL 404 ===
Loss reward (iter 404): 2.944746971130371
=== Iterazione IRL 405 ===
Loss reward (iter 405): 2.7829067707061768
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.36e+04 |
|    critic_loss     | 3.72e+05  |
|    ent_coef        | 28        |
|    ent_coef_loss   | 0.367     |
|    learning_rate   | 0.0003    |
|    n_updates       | 113699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.33e+04 |
|    critic_loss     | 7.67e+05  |
|    ent_coef        | 26.7      |
|    ent_coef_loss   | -1.44     |
|    learning_rate   | 0.0003    |
|    n_updates       | 114099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.34e+04 |
|    critic_loss     | 2.6e+05   |
|    ent_coef        | 27        |
|    ent_coef_loss   | 0.384     |
|    learning_rate   | 0.0003    |
|    n_updates       | 114499    |
----------------------------------
=== Iterazione IRL 406 ===
Loss reward (iter 406): -2.206827163696289
=== Iterazione IRL 407 ===
Loss reward (iter 407): 4.653210639953613
=== Iterazione IRL 408 ===
Loss reward (iter 408): 3.0948047637939453
=== Iterazione IRL 409 ===
Loss reward (iter 409): -2.781035900115967
=== Iterazione IRL 410 ===
Loss reward (iter 410): -3.212545394897461
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.34e+04 |
|    critic_loss     | 4.18e+05  |
|    ent_coef        | 27.2      |
|    ent_coef_loss   | -0.106    |
|    learning_rate   | 0.0003    |
|    n_updates       | 115099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.31e+04 |
|    critic_loss     | 4.75e+05  |
|    ent_coef        | 27.1      |
|    ent_coef_loss   | -1.03     |
|    learning_rate   | 0.0003    |
|    n_updates       | 115499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.28e+04 |
|    critic_loss     | 4e+05     |
|    ent_coef        | 27.9      |
|    ent_coef_loss   | -0.516    |
|    learning_rate   | 0.0003    |
|    n_updates       | 115899    |
----------------------------------
=== Iterazione IRL 411 ===
Loss reward (iter 411): 8.946632385253906
=== Iterazione IRL 412 ===
Loss reward (iter 412): 0.39584827423095703
=== Iterazione IRL 413 ===
Loss reward (iter 413): -1.4484069347381592
=== Iterazione IRL 414 ===
Loss reward (iter 414): -0.1712876558303833
=== Iterazione IRL 415 ===
Loss reward (iter 415): -3.391679048538208
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 150       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.29e+04 |
|    critic_loss     | 8e+05     |
|    ent_coef        | 27.5      |
|    ent_coef_loss   | 0.709     |
|    learning_rate   | 0.0003    |
|    n_updates       | 116499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.26e+04 |
|    critic_loss     | 5.97e+05  |
|    ent_coef        | 27.2      |
|    ent_coef_loss   | -0.512    |
|    learning_rate   | 0.0003    |
|    n_updates       | 116899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.27e+04 |
|    critic_loss     | 5.65e+05  |
|    ent_coef        | 27.4      |
|    ent_coef_loss   | -0.563    |
|    learning_rate   | 0.0003    |
|    n_updates       | 117299    |
----------------------------------
=== Iterazione IRL 416 ===
Loss reward (iter 416): -6.594508171081543
=== Iterazione IRL 417 ===
Loss reward (iter 417): -4.8228912353515625
=== Iterazione IRL 418 ===
Loss reward (iter 418): -6.679838180541992
=== Iterazione IRL 419 ===
Loss reward (iter 419): -11.328444480895996
=== Iterazione IRL 420 ===
Loss reward (iter 420): -16.748857498168945
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.24e+04 |
|    critic_loss     | 6.02e+05  |
|    ent_coef        | 27.7      |
|    ent_coef_loss   | 1.34      |
|    learning_rate   | 0.0003    |
|    n_updates       | 117899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.23e+04 |
|    critic_loss     | 5.94e+05  |
|    ent_coef        | 27.4      |
|    ent_coef_loss   | 0.268     |
|    learning_rate   | 0.0003    |
|    n_updates       | 118299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.25e+04 |
|    critic_loss     | 3.76e+05  |
|    ent_coef        | 28.1      |
|    ent_coef_loss   | -0.274    |
|    learning_rate   | 0.0003    |
|    n_updates       | 118699    |
----------------------------------
=== Iterazione IRL 421 ===
Loss reward (iter 421): -8.950723648071289
=== Iterazione IRL 422 ===
Loss reward (iter 422): 12.851397514343262
=== Iterazione IRL 423 ===
Loss reward (iter 423): -2.9437451362609863
=== Iterazione IRL 424 ===
Loss reward (iter 424): -6.301899433135986
=== Iterazione IRL 425 ===
Loss reward (iter 425): -5.443625450134277
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.24e+04 |
|    critic_loss     | 5.43e+05  |
|    ent_coef        | 27.9      |
|    ent_coef_loss   | -0.331    |
|    learning_rate   | 0.0003    |
|    n_updates       | 119299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.21e+04 |
|    critic_loss     | 9.03e+05  |
|    ent_coef        | 27.6      |
|    ent_coef_loss   | -0.404    |
|    learning_rate   | 0.0003    |
|    n_updates       | 119699    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -1.2e+04 |
|    critic_loss     | 8.34e+05 |
|    ent_coef        | 27.7     |
|    ent_coef_loss   | -0.895   |
|    learning_rate   | 0.0003   |
|    n_updates       | 120099   |
---------------------------------
=== Iterazione IRL 426 ===
Loss reward (iter 426): 7.7880330085754395
=== Iterazione IRL 427 ===
Loss reward (iter 427): -13.720165252685547
=== Iterazione IRL 428 ===
Loss reward (iter 428): -29.844432830810547
=== Iterazione IRL 429 ===
Loss reward (iter 429): 4.051365375518799
=== Iterazione IRL 430 ===
Loss reward (iter 430): 9.234033584594727
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.17e+04 |
|    critic_loss     | 8.05e+05  |
|    ent_coef        | 27.7      |
|    ent_coef_loss   | -1.54     |
|    learning_rate   | 0.0003    |
|    n_updates       | 120699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.17e+04 |
|    critic_loss     | 4.15e+05  |
|    ent_coef        | 27.4      |
|    ent_coef_loss   | -0.671    |
|    learning_rate   | 0.0003    |
|    n_updates       | 121099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.15e+04 |
|    critic_loss     | 4.32e+05  |
|    ent_coef        | 27.6      |
|    ent_coef_loss   | -0.255    |
|    learning_rate   | 0.0003    |
|    n_updates       | 121499    |
----------------------------------
=== Iterazione IRL 431 ===
Loss reward (iter 431): -14.37561321258545
=== Iterazione IRL 432 ===
Loss reward (iter 432): -10.205336570739746
=== Iterazione IRL 433 ===
Loss reward (iter 433): 10.535839080810547
=== Iterazione IRL 434 ===
Loss reward (iter 434): -7.949074745178223
=== Iterazione IRL 435 ===
Loss reward (iter 435): -14.123007774353027
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.14e+04 |
|    critic_loss     | 2.85e+05  |
|    ent_coef        | 27        |
|    ent_coef_loss   | 1         |
|    learning_rate   | 0.0003    |
|    n_updates       | 122099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.14e+04 |
|    critic_loss     | 4.05e+05  |
|    ent_coef        | 27.2      |
|    ent_coef_loss   | 0.714     |
|    learning_rate   | 0.0003    |
|    n_updates       | 122499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.15e+04 |
|    critic_loss     | 2.49e+05  |
|    ent_coef        | 27.1      |
|    ent_coef_loss   | -0.266    |
|    learning_rate   | 0.0003    |
|    n_updates       | 122899    |
----------------------------------
=== Iterazione IRL 436 ===
Loss reward (iter 436): -21.98839569091797
=== Iterazione IRL 437 ===
Loss reward (iter 437): -25.026201248168945
=== Iterazione IRL 438 ===
Loss reward (iter 438): -32.355167388916016
=== Iterazione IRL 439 ===
Loss reward (iter 439): -45.6942024230957
=== Iterazione IRL 440 ===
Loss reward (iter 440): -59.509315490722656
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 150       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.12e+04 |
|    critic_loss     | 3.57e+05  |
|    ent_coef        | 26.5      |
|    ent_coef_loss   | 0.0133    |
|    learning_rate   | 0.0003    |
|    n_updates       | 123499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.13e+04 |
|    critic_loss     | 3.83e+05  |
|    ent_coef        | 26.3      |
|    ent_coef_loss   | 0.248     |
|    learning_rate   | 0.0003    |
|    n_updates       | 123899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.11e+04 |
|    critic_loss     | 8.14e+05  |
|    ent_coef        | 26.6      |
|    ent_coef_loss   | -0.564    |
|    learning_rate   | 0.0003    |
|    n_updates       | 124299    |
----------------------------------
=== Iterazione IRL 441 ===
Loss reward (iter 441): -13.930166244506836
=== Iterazione IRL 442 ===
Loss reward (iter 442): -62.679237365722656
=== Iterazione IRL 443 ===
Loss reward (iter 443): -27.307960510253906
=== Iterazione IRL 444 ===
Loss reward (iter 444): -5.501590728759766
=== Iterazione IRL 445 ===
Loss reward (iter 445): -16.662181854248047
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -1.1e+04 |
|    critic_loss     | 5.41e+05 |
|    ent_coef        | 25.9     |
|    ent_coef_loss   | -0.169   |
|    learning_rate   | 0.0003   |
|    n_updates       | 124899   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.11e+04 |
|    critic_loss     | 3.28e+05  |
|    ent_coef        | 26        |
|    ent_coef_loss   | -1.8      |
|    learning_rate   | 0.0003    |
|    n_updates       | 125299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.07e+04 |
|    critic_loss     | 3.24e+05  |
|    ent_coef        | 25.4      |
|    ent_coef_loss   | 0.966     |
|    learning_rate   | 0.0003    |
|    n_updates       | 125699    |
----------------------------------
=== Iterazione IRL 446 ===
Loss reward (iter 446): -18.28815460205078
=== Iterazione IRL 447 ===
Loss reward (iter 447): -22.58390235900879
=== Iterazione IRL 448 ===
Loss reward (iter 448): -36.628719329833984
=== Iterazione IRL 449 ===
Loss reward (iter 449): -36.57367706298828
=== Iterazione IRL 450 ===
Loss reward (iter 450): 16.26734161376953
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.08e+04 |
|    critic_loss     | 2.74e+05  |
|    ent_coef        | 24.9      |
|    ent_coef_loss   | 0.407     |
|    learning_rate   | 0.0003    |
|    n_updates       | 126299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.05e+04 |
|    critic_loss     | 4.17e+05  |
|    ent_coef        | 24.6      |
|    ent_coef_loss   | -0.117    |
|    learning_rate   | 0.0003    |
|    n_updates       | 126699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.05e+04 |
|    critic_loss     | 4.75e+05  |
|    ent_coef        | 24.3      |
|    ent_coef_loss   | 0.976     |
|    learning_rate   | 0.0003    |
|    n_updates       | 127099    |
----------------------------------
=== Iterazione IRL 451 ===
Loss reward (iter 451): -5.417707443237305
=== Iterazione IRL 452 ===
Loss reward (iter 452): 26.373327255249023
=== Iterazione IRL 453 ===
Loss reward (iter 453): -28.778400421142578
=== Iterazione IRL 454 ===
Loss reward (iter 454): -11.026962280273438
=== Iterazione IRL 455 ===
Loss reward (iter 455): -7.959733486175537
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.02e+04 |
|    critic_loss     | 7.53e+05  |
|    ent_coef        | 23.2      |
|    ent_coef_loss   | -0.854    |
|    learning_rate   | 0.0003    |
|    n_updates       | 127699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.04e+04 |
|    critic_loss     | 6.34e+05  |
|    ent_coef        | 22.3      |
|    ent_coef_loss   | -0.396    |
|    learning_rate   | 0.0003    |
|    n_updates       | 128099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -9.85e+03 |
|    critic_loss     | 2.76e+05  |
|    ent_coef        | 22.1      |
|    ent_coef_loss   | 0.838     |
|    learning_rate   | 0.0003    |
|    n_updates       | 128499    |
----------------------------------
=== Iterazione IRL 456 ===
Loss reward (iter 456): 1.259268045425415
=== Iterazione IRL 457 ===
Loss reward (iter 457): -14.05859661102295
=== Iterazione IRL 458 ===
Loss reward (iter 458): -19.0966854095459
=== Iterazione IRL 459 ===
Loss reward (iter 459): 11.555303573608398
=== Iterazione IRL 460 ===
Loss reward (iter 460): -11.412569046020508
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -9.71e+03 |
|    critic_loss     | 2.54e+05  |
|    ent_coef        | 21.7      |
|    ent_coef_loss   | 0.401     |
|    learning_rate   | 0.0003    |
|    n_updates       | 129099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -9.97e+03 |
|    critic_loss     | 6.31e+05  |
|    ent_coef        | 21        |
|    ent_coef_loss   | -0.0481   |
|    learning_rate   | 0.0003    |
|    n_updates       | 129499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -9.61e+03 |
|    critic_loss     | 4.75e+05  |
|    ent_coef        | 19.8      |
|    ent_coef_loss   | -0.433    |
|    learning_rate   | 0.0003    |
|    n_updates       | 129899    |
----------------------------------
=== Iterazione IRL 461 ===
Loss reward (iter 461): 8.958949089050293
=== Iterazione IRL 462 ===
Loss reward (iter 462): 10.967166900634766
=== Iterazione IRL 463 ===
Loss reward (iter 463): 12.0701904296875
=== Iterazione IRL 464 ===
Loss reward (iter 464): 13.152387619018555
=== Iterazione IRL 465 ===
Loss reward (iter 465): 5.506189823150635
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -9.55e+03 |
|    critic_loss     | 3.24e+05  |
|    ent_coef        | 19.5      |
|    ent_coef_loss   | 0.0488    |
|    learning_rate   | 0.0003    |
|    n_updates       | 130499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -9.28e+03 |
|    critic_loss     | 6.55e+05  |
|    ent_coef        | 19        |
|    ent_coef_loss   | -0.831    |
|    learning_rate   | 0.0003    |
|    n_updates       | 130899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -9.19e+03 |
|    critic_loss     | 5.4e+05   |
|    ent_coef        | 19        |
|    ent_coef_loss   | -0.395    |
|    learning_rate   | 0.0003    |
|    n_updates       | 131299    |
----------------------------------
=== Iterazione IRL 466 ===
Loss reward (iter 466): 2.985806941986084
=== Iterazione IRL 467 ===
Loss reward (iter 467): 6.729735374450684
=== Iterazione IRL 468 ===
Loss reward (iter 468): 10.244140625
=== Iterazione IRL 469 ===
Loss reward (iter 469): 2.9552524089813232
=== Iterazione IRL 470 ===
Loss reward (iter 470): -4.88392448425293
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -9.09e+03 |
|    critic_loss     | 5.89e+05  |
|    ent_coef        | 18.5      |
|    ent_coef_loss   | 0.991     |
|    learning_rate   | 0.0003    |
|    n_updates       | 131899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -8.98e+03 |
|    critic_loss     | 3.83e+05  |
|    ent_coef        | 17.4      |
|    ent_coef_loss   | -0.261    |
|    learning_rate   | 0.0003    |
|    n_updates       | 132299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -8.93e+03 |
|    critic_loss     | 3.75e+05  |
|    ent_coef        | 17.4      |
|    ent_coef_loss   | 0.0711    |
|    learning_rate   | 0.0003    |
|    n_updates       | 132699    |
----------------------------------
=== Iterazione IRL 471 ===
Loss reward (iter 471): 12.460265159606934
=== Iterazione IRL 472 ===
Loss reward (iter 472): 10.065712928771973
=== Iterazione IRL 473 ===
Loss reward (iter 473): 8.400693893432617
=== Iterazione IRL 474 ===
Loss reward (iter 474): 6.010076522827148
=== Iterazione IRL 475 ===
Loss reward (iter 475): 8.239032745361328
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -8.87e+03 |
|    critic_loss     | 4.7e+05   |
|    ent_coef        | 17.5      |
|    ent_coef_loss   | 0.0138    |
|    learning_rate   | 0.0003    |
|    n_updates       | 133299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -8.92e+03 |
|    critic_loss     | 4.32e+05  |
|    ent_coef        | 17.8      |
|    ent_coef_loss   | 0.536     |
|    learning_rate   | 0.0003    |
|    n_updates       | 133699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -8.75e+03 |
|    critic_loss     | 8.13e+05  |
|    ent_coef        | 18.1      |
|    ent_coef_loss   | -0.843    |
|    learning_rate   | 0.0003    |
|    n_updates       | 134099    |
----------------------------------
=== Iterazione IRL 476 ===
Loss reward (iter 476): 6.003344535827637
=== Iterazione IRL 477 ===
Loss reward (iter 477): 5.454686164855957
=== Iterazione IRL 478 ===
Loss reward (iter 478): 6.133626937866211
=== Iterazione IRL 479 ===
Loss reward (iter 479): 7.052077293395996
=== Iterazione IRL 480 ===
Loss reward (iter 480): 5.440293312072754
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -8.56e+03 |
|    critic_loss     | 7.01e+05  |
|    ent_coef        | 19.2      |
|    ent_coef_loss   | 0.0322    |
|    learning_rate   | 0.0003    |
|    n_updates       | 134699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -8.58e+03 |
|    critic_loss     | 2.63e+05  |
|    ent_coef        | 20.6      |
|    ent_coef_loss   | 0.722     |
|    learning_rate   | 0.0003    |
|    n_updates       | 135099    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -8.5e+03 |
|    critic_loss     | 2.28e+05 |
|    ent_coef        | 21.1     |
|    ent_coef_loss   | 0.411    |
|    learning_rate   | 0.0003   |
|    n_updates       | 135499   |
---------------------------------
=== Iterazione IRL 481 ===
Loss reward (iter 481): 6.137736797332764
=== Iterazione IRL 482 ===
Loss reward (iter 482): 4.472004413604736
=== Iterazione IRL 483 ===
Loss reward (iter 483): 3.457954168319702
=== Iterazione IRL 484 ===
Loss reward (iter 484): 2.6585776805877686
=== Iterazione IRL 485 ===
Loss reward (iter 485): 4.2707390785217285
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -8.45e+03 |
|    critic_loss     | 4.3e+05   |
|    ent_coef        | 21.5      |
|    ent_coef_loss   | 1.5       |
|    learning_rate   | 0.0003    |
|    n_updates       | 136099    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.3e+03 |
|    critic_loss     | 4.62e+05 |
|    ent_coef        | 21       |
|    ent_coef_loss   | -1.01    |
|    learning_rate   | 0.0003   |
|    n_updates       | 136499   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -8.21e+03 |
|    critic_loss     | 3.02e+05  |
|    ent_coef        | 20.5      |
|    ent_coef_loss   | -0.373    |
|    learning_rate   | 0.0003    |
|    n_updates       | 136899    |
----------------------------------
=== Iterazione IRL 486 ===
Loss reward (iter 486): 3.819283962249756
=== Iterazione IRL 487 ===
Loss reward (iter 487): 3.072962999343872
=== Iterazione IRL 488 ===
Loss reward (iter 488): 4.135327339172363
=== Iterazione IRL 489 ===
Loss reward (iter 489): 3.046557903289795
=== Iterazione IRL 490 ===
Loss reward (iter 490): 2.4822559356689453
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 150       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -7.94e+03 |
|    critic_loss     | 5.99e+05  |
|    ent_coef        | 20.9      |
|    ent_coef_loss   | -0.313    |
|    learning_rate   | 0.0003    |
|    n_updates       | 137499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -8.02e+03 |
|    critic_loss     | 3.74e+05  |
|    ent_coef        | 20.5      |
|    ent_coef_loss   | -0.313    |
|    learning_rate   | 0.0003    |
|    n_updates       | 137899    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -8.1e+03 |
|    critic_loss     | 3.94e+05 |
|    ent_coef        | 20.9     |
|    ent_coef_loss   | -0.0285  |
|    learning_rate   | 0.0003   |
|    n_updates       | 138299   |
---------------------------------
=== Iterazione IRL 491 ===
Loss reward (iter 491): 1.85316002368927
=== Iterazione IRL 492 ===
Loss reward (iter 492): 3.391334295272827
=== Iterazione IRL 493 ===
Loss reward (iter 493): 6.251258373260498
=== Iterazione IRL 494 ===
Loss reward (iter 494): 6.55954647064209
=== Iterazione IRL 495 ===
Loss reward (iter 495): 5.417540550231934
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -7.83e+03 |
|    critic_loss     | 2.48e+05  |
|    ent_coef        | 21        |
|    ent_coef_loss   | -0.0641   |
|    learning_rate   | 0.0003    |
|    n_updates       | 138899    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.8e+03 |
|    critic_loss     | 2.13e+05 |
|    ent_coef        | 21       |
|    ent_coef_loss   | -0.557   |
|    learning_rate   | 0.0003   |
|    n_updates       | 139299   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -7.75e+03 |
|    critic_loss     | 8.63e+05  |
|    ent_coef        | 21        |
|    ent_coef_loss   | -0.182    |
|    learning_rate   | 0.0003    |
|    n_updates       | 139699    |
----------------------------------
=== Iterazione IRL 496 ===
Loss reward (iter 496): 1.0110069513320923
=== Iterazione IRL 497 ===
Loss reward (iter 497): 3.515016794204712
=== Iterazione IRL 498 ===
Loss reward (iter 498): 2.49849271774292
=== Iterazione IRL 499 ===
Loss reward (iter 499): 4.902087211608887
=== Iterazione IRL 500 ===
Loss reward (iter 500): 2.398453712463379
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -7.61e+03 |
|    critic_loss     | 1.66e+05  |
|    ent_coef        | 20.4      |
|    ent_coef_loss   | 0.631     |
|    learning_rate   | 0.0003    |
|    n_updates       | 140299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -7.51e+03 |
|    critic_loss     | 8.41e+05  |
|    ent_coef        | 21        |
|    ent_coef_loss   | -0.118    |
|    learning_rate   | 0.0003    |
|    n_updates       | 140699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -7.49e+03 |
|    critic_loss     | 2.83e+05  |
|    ent_coef        | 21        |
|    ent_coef_loss   | 0.487     |
|    learning_rate   | 0.0003    |
|    n_updates       | 141099    |
----------------------------------
=== Iterazione IRL 501 ===
Loss reward (iter 501): 0.3297663927078247
=== Iterazione IRL 502 ===
Loss reward (iter 502): 3.5613386631011963
=== Iterazione IRL 503 ===
Loss reward (iter 503): -0.8922021389007568
=== Iterazione IRL 504 ===
Loss reward (iter 504): 1.0322844982147217
=== Iterazione IRL 505 ===
Loss reward (iter 505): -0.47781968116760254
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 150       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -7.39e+03 |
|    critic_loss     | 2.78e+05  |
|    ent_coef        | 20.8      |
|    ent_coef_loss   | 0.678     |
|    learning_rate   | 0.0003    |
|    n_updates       | 141699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -7.25e+03 |
|    critic_loss     | 5.62e+05  |
|    ent_coef        | 20.5      |
|    ent_coef_loss   | -0.809    |
|    learning_rate   | 0.0003    |
|    n_updates       | 142099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -7.31e+03 |
|    critic_loss     | 4e+05     |
|    ent_coef        | 20.4      |
|    ent_coef_loss   | 0.43      |
|    learning_rate   | 0.0003    |
|    n_updates       | 142499    |
----------------------------------
=== Iterazione IRL 506 ===
Loss reward (iter 506): 4.2830986976623535
=== Iterazione IRL 507 ===
Loss reward (iter 507): 7.663563251495361
=== Iterazione IRL 508 ===
Loss reward (iter 508): 7.8539018630981445
=== Iterazione IRL 509 ===
Loss reward (iter 509): 8.919240951538086
=== Iterazione IRL 510 ===
Loss reward (iter 510): 3.5771713256835938
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.1e+03 |
|    critic_loss     | 3.42e+05 |
|    ent_coef        | 20.4     |
|    ent_coef_loss   | 1.07     |
|    learning_rate   | 0.0003   |
|    n_updates       | 143099   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -7.14e+03 |
|    critic_loss     | 7.83e+05  |
|    ent_coef        | 20.2      |
|    ent_coef_loss   | 0.327     |
|    learning_rate   | 0.0003    |
|    n_updates       | 143499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -7.16e+03 |
|    critic_loss     | 4.51e+05  |
|    ent_coef        | 20.1      |
|    ent_coef_loss   | -0.705    |
|    learning_rate   | 0.0003    |
|    n_updates       | 143899    |
----------------------------------
=== Iterazione IRL 511 ===
Loss reward (iter 511): 3.690267562866211
=== Iterazione IRL 512 ===
Loss reward (iter 512): 5.028628349304199
=== Iterazione IRL 513 ===
Loss reward (iter 513): 3.6658456325531006
=== Iterazione IRL 514 ===
Loss reward (iter 514): 5.265789031982422
=== Iterazione IRL 515 ===
Loss reward (iter 515): 2.440396785736084
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -6.98e+03 |
|    critic_loss     | 4.63e+05  |
|    ent_coef        | 19.9      |
|    ent_coef_loss   | 0.348     |
|    learning_rate   | 0.0003    |
|    n_updates       | 144499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -6.86e+03 |
|    critic_loss     | 3.3e+05   |
|    ent_coef        | 20.4      |
|    ent_coef_loss   | -0.884    |
|    learning_rate   | 0.0003    |
|    n_updates       | 144899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -6.98e+03 |
|    critic_loss     | 3.98e+05  |
|    ent_coef        | 19.9      |
|    ent_coef_loss   | -0.687    |
|    learning_rate   | 0.0003    |
|    n_updates       | 145299    |
----------------------------------
=== Iterazione IRL 516 ===
Loss reward (iter 516): 7.654620170593262
=== Iterazione IRL 517 ===
Loss reward (iter 517): 4.677311420440674
=== Iterazione IRL 518 ===
Loss reward (iter 518): 4.632263660430908
=== Iterazione IRL 519 ===
Loss reward (iter 519): 4.04548454284668
=== Iterazione IRL 520 ===
Loss reward (iter 520): 4.310549736022949
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -6.65e+03 |
|    critic_loss     | 3.09e+05  |
|    ent_coef        | 19.7      |
|    ent_coef_loss   | 0.329     |
|    learning_rate   | 0.0003    |
|    n_updates       | 145899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -6.63e+03 |
|    critic_loss     | 2.95e+05  |
|    ent_coef        | 19.7      |
|    ent_coef_loss   | -0.00588  |
|    learning_rate   | 0.0003    |
|    n_updates       | 146299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -6.56e+03 |
|    critic_loss     | 5.5e+05   |
|    ent_coef        | 19.5      |
|    ent_coef_loss   | 0.0109    |
|    learning_rate   | 0.0003    |
|    n_updates       | 146699    |
----------------------------------
=== Iterazione IRL 521 ===
Loss reward (iter 521): 3.952122449874878
=== Iterazione IRL 522 ===
Loss reward (iter 522): 2.49434757232666
=== Iterazione IRL 523 ===
Loss reward (iter 523): 2.0843114852905273
=== Iterazione IRL 524 ===
Loss reward (iter 524): 7.36079740524292
=== Iterazione IRL 525 ===
Loss reward (iter 525): 3.5817923545837402
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 150       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -6.54e+03 |
|    critic_loss     | 5.56e+05  |
|    ent_coef        | 19.2      |
|    ent_coef_loss   | 1.09      |
|    learning_rate   | 0.0003    |
|    n_updates       | 147299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -6.55e+03 |
|    critic_loss     | 6.85e+05  |
|    ent_coef        | 18.9      |
|    ent_coef_loss   | 0.148     |
|    learning_rate   | 0.0003    |
|    n_updates       | 147699    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -6.4e+03 |
|    critic_loss     | 9.22e+05 |
|    ent_coef        | 19       |
|    ent_coef_loss   | 0.246    |
|    learning_rate   | 0.0003   |
|    n_updates       | 148099   |
---------------------------------
=== Iterazione IRL 526 ===
Loss reward (iter 526): 5.31130313873291
=== Iterazione IRL 527 ===
Loss reward (iter 527): 2.4401798248291016
=== Iterazione IRL 528 ===
Loss reward (iter 528): 4.055462837219238
=== Iterazione IRL 529 ===
Loss reward (iter 529): 4.412655830383301
=== Iterazione IRL 530 ===
Loss reward (iter 530): 7.1748552322387695
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -6.47e+03 |
|    critic_loss     | 8.31e+05  |
|    ent_coef        | 18.4      |
|    ent_coef_loss   | -0.167    |
|    learning_rate   | 0.0003    |
|    n_updates       | 148699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -6.35e+03 |
|    critic_loss     | 8.13e+05  |
|    ent_coef        | 18.7      |
|    ent_coef_loss   | 0.191     |
|    learning_rate   | 0.0003    |
|    n_updates       | 149099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -5.98e+03 |
|    critic_loss     | 2.49e+05  |
|    ent_coef        | 19        |
|    ent_coef_loss   | -0.251    |
|    learning_rate   | 0.0003    |
|    n_updates       | 149499    |
----------------------------------
=== Iterazione IRL 531 ===
Loss reward (iter 531): 2.4177656173706055
=== Iterazione IRL 532 ===
Loss reward (iter 532): 4.323629379272461
=== Iterazione IRL 533 ===
Loss reward (iter 533): 4.80809211730957
=== Iterazione IRL 534 ===
Loss reward (iter 534): 4.038107872009277
=== Iterazione IRL 535 ===
Loss reward (iter 535): 4.732380390167236
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -5.95e+03 |
|    critic_loss     | 2.78e+05  |
|    ent_coef        | 18.9      |
|    ent_coef_loss   | -1.18     |
|    learning_rate   | 0.0003    |
|    n_updates       | 150099    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.2e+03 |
|    critic_loss     | 2.61e+05 |
|    ent_coef        | 18.8     |
|    ent_coef_loss   | -1.3     |
|    learning_rate   | 0.0003   |
|    n_updates       | 150499   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -6.07e+03 |
|    critic_loss     | 1.88e+05  |
|    ent_coef        | 18.6      |
|    ent_coef_loss   | -0.679    |
|    learning_rate   | 0.0003    |
|    n_updates       | 150899    |
----------------------------------
=== Iterazione IRL 536 ===
Loss reward (iter 536): 2.5224671363830566
=== Iterazione IRL 537 ===
Loss reward (iter 537): 3.175565242767334
=== Iterazione IRL 538 ===
Loss reward (iter 538): 2.8456759452819824
=== Iterazione IRL 539 ===
Loss reward (iter 539): 3.0613865852355957
=== Iterazione IRL 540 ===
Loss reward (iter 540): 3.1773228645324707
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -5.65e+03 |
|    critic_loss     | 1.74e+05  |
|    ent_coef        | 18.9      |
|    ent_coef_loss   | -0.0336   |
|    learning_rate   | 0.0003    |
|    n_updates       | 151499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -5.72e+03 |
|    critic_loss     | 3.56e+05  |
|    ent_coef        | 19.5      |
|    ent_coef_loss   | -0.122    |
|    learning_rate   | 0.0003    |
|    n_updates       | 151899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -5.66e+03 |
|    critic_loss     | 2.14e+05  |
|    ent_coef        | 19.5      |
|    ent_coef_loss   | 0.171     |
|    learning_rate   | 0.0003    |
|    n_updates       | 152299    |
----------------------------------
=== Iterazione IRL 541 ===
Loss reward (iter 541): 4.803212642669678
=== Iterazione IRL 542 ===
Loss reward (iter 542): 3.886021375656128
=== Iterazione IRL 543 ===
Loss reward (iter 543): 4.580461502075195
=== Iterazione IRL 544 ===
Loss reward (iter 544): 4.658974647521973
=== Iterazione IRL 545 ===
Loss reward (iter 545): 2.438042402267456
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -5.65e+03 |
|    critic_loss     | 3.06e+05  |
|    ent_coef        | 18.7      |
|    ent_coef_loss   | 0.529     |
|    learning_rate   | 0.0003    |
|    n_updates       | 152899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -5.72e+03 |
|    critic_loss     | 2.77e+05  |
|    ent_coef        | 19        |
|    ent_coef_loss   | 0.484     |
|    learning_rate   | 0.0003    |
|    n_updates       | 153299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -5.62e+03 |
|    critic_loss     | 4.86e+05  |
|    ent_coef        | 18.7      |
|    ent_coef_loss   | -0.323    |
|    learning_rate   | 0.0003    |
|    n_updates       | 153699    |
----------------------------------
=== Iterazione IRL 546 ===
Loss reward (iter 546): -0.7600041627883911
=== Iterazione IRL 547 ===
Loss reward (iter 547): 0.1673983335494995
=== Iterazione IRL 548 ===
Loss reward (iter 548): 1.020295262336731
=== Iterazione IRL 549 ===
Loss reward (iter 549): 3.0914788246154785
=== Iterazione IRL 550 ===
Loss reward (iter 550): -1.2998592853546143
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -5.35e+03 |
|    critic_loss     | 2.71e+05  |
|    ent_coef        | 19        |
|    ent_coef_loss   | 0.231     |
|    learning_rate   | 0.0003    |
|    n_updates       | 154299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -5.43e+03 |
|    critic_loss     | 3.67e+05  |
|    ent_coef        | 18.9      |
|    ent_coef_loss   | 0.816     |
|    learning_rate   | 0.0003    |
|    n_updates       | 154699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -5.29e+03 |
|    critic_loss     | 9.58e+05  |
|    ent_coef        | 19.1      |
|    ent_coef_loss   | 0.64      |
|    learning_rate   | 0.0003    |
|    n_updates       | 155099    |
----------------------------------
=== Iterazione IRL 551 ===
Loss reward (iter 551): 1.6616216897964478
=== Iterazione IRL 552 ===
Loss reward (iter 552): 3.8764748573303223
=== Iterazione IRL 553 ===
Loss reward (iter 553): 1.778054118156433
=== Iterazione IRL 554 ===
Loss reward (iter 554): 1.5618354082107544
=== Iterazione IRL 555 ===
Loss reward (iter 555): 1.2364298105239868
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -5.16e+03 |
|    critic_loss     | 5.35e+05  |
|    ent_coef        | 18.9      |
|    ent_coef_loss   | 0.259     |
|    learning_rate   | 0.0003    |
|    n_updates       | 155699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -5.42e+03 |
|    critic_loss     | 5.39e+05  |
|    ent_coef        | 18.7      |
|    ent_coef_loss   | -0.532    |
|    learning_rate   | 0.0003    |
|    n_updates       | 156099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -5.18e+03 |
|    critic_loss     | 3.03e+05  |
|    ent_coef        | 18.5      |
|    ent_coef_loss   | -0.921    |
|    learning_rate   | 0.0003    |
|    n_updates       | 156499    |
----------------------------------
=== Iterazione IRL 556 ===
Loss reward (iter 556): 3.639434814453125
=== Iterazione IRL 557 ===
Loss reward (iter 557): 0.11944842338562012
=== Iterazione IRL 558 ===
Loss reward (iter 558): 1.9771746397018433
=== Iterazione IRL 559 ===
Loss reward (iter 559): 1.4148821830749512
=== Iterazione IRL 560 ===
Loss reward (iter 560): 2.628347635269165
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -5.19e+03 |
|    critic_loss     | 1.2e+05   |
|    ent_coef        | 18.6      |
|    ent_coef_loss   | 0.241     |
|    learning_rate   | 0.0003    |
|    n_updates       | 157099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -5.16e+03 |
|    critic_loss     | 4.96e+05  |
|    ent_coef        | 18.1      |
|    ent_coef_loss   | 0.678     |
|    learning_rate   | 0.0003    |
|    n_updates       | 157499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -4.97e+03 |
|    critic_loss     | 1.57e+05  |
|    ent_coef        | 17.8      |
|    ent_coef_loss   | 1.06      |
|    learning_rate   | 0.0003    |
|    n_updates       | 157899    |
----------------------------------
=== Iterazione IRL 561 ===
Loss reward (iter 561): -2.4534597396850586
=== Iterazione IRL 562 ===
Loss reward (iter 562): 2.9754438400268555
=== Iterazione IRL 563 ===
Loss reward (iter 563): 2.0147485733032227
=== Iterazione IRL 564 ===
Loss reward (iter 564): 3.2576169967651367
=== Iterazione IRL 565 ===
Loss reward (iter 565): 0.26536762714385986
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 150       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -4.92e+03 |
|    critic_loss     | 4.34e+05  |
|    ent_coef        | 17.5      |
|    ent_coef_loss   | -0.922    |
|    learning_rate   | 0.0003    |
|    n_updates       | 158499    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.8e+03 |
|    critic_loss     | 2.22e+05 |
|    ent_coef        | 17.7     |
|    ent_coef_loss   | 0.507    |
|    learning_rate   | 0.0003   |
|    n_updates       | 158899   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 125       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -4.67e+03 |
|    critic_loss     | 5.52e+05  |
|    ent_coef        | 17.9      |
|    ent_coef_loss   | 0.88      |
|    learning_rate   | 0.0003    |
|    n_updates       | 159299    |
----------------------------------
=== Iterazione IRL 566 ===
Loss reward (iter 566): 0.5529296398162842
=== Iterazione IRL 567 ===
Loss reward (iter 567): -0.04221534729003906
=== Iterazione IRL 568 ===
Loss reward (iter 568): -0.21402406692504883
=== Iterazione IRL 569 ===
Loss reward (iter 569): -2.705275535583496
=== Iterazione IRL 570 ===
Loss reward (iter 570): 0.7977432012557983
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 150       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -4.87e+03 |
|    critic_loss     | 4.19e+05  |
|    ent_coef        | 18.1      |
|    ent_coef_loss   | -0.354    |
|    learning_rate   | 0.0003    |
|    n_updates       | 159899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -4.83e+03 |
|    critic_loss     | 2.45e+05  |
|    ent_coef        | 18        |
|    ent_coef_loss   | -0.0692   |
|    learning_rate   | 0.0003    |
|    n_updates       | 160299    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -4.7e+03 |
|    critic_loss     | 5.71e+05 |
|    ent_coef        | 17.7     |
|    ent_coef_loss   | 0.345    |
|    learning_rate   | 0.0003   |
|    n_updates       | 160699   |
---------------------------------
=== Iterazione IRL 571 ===
Loss reward (iter 571): -0.35381484031677246
=== Iterazione IRL 572 ===
Loss reward (iter 572): -1.3549118041992188
=== Iterazione IRL 573 ===
Loss reward (iter 573): -0.44298267364501953
=== Iterazione IRL 574 ===
Loss reward (iter 574): -1.4880585670471191
=== Iterazione IRL 575 ===
Loss reward (iter 575): -2.829078197479248
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -4.65e+03 |
|    critic_loss     | 2.48e+05  |
|    ent_coef        | 17.6      |
|    ent_coef_loss   | 0.811     |
|    learning_rate   | 0.0003    |
|    n_updates       | 161299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -4.62e+03 |
|    critic_loss     | 8e+05     |
|    ent_coef        | 17.8      |
|    ent_coef_loss   | -0.178    |
|    learning_rate   | 0.0003    |
|    n_updates       | 161699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -4.39e+03 |
|    critic_loss     | 1.83e+05  |
|    ent_coef        | 17.7      |
|    ent_coef_loss   | 0.0535    |
|    learning_rate   | 0.0003    |
|    n_updates       | 162099    |
----------------------------------
=== Iterazione IRL 576 ===
Loss reward (iter 576): 1.5923080444335938
=== Iterazione IRL 577 ===
Loss reward (iter 577): -2.1376187801361084
=== Iterazione IRL 578 ===
Loss reward (iter 578): -2.3876209259033203
=== Iterazione IRL 579 ===
Loss reward (iter 579): -1.7640985250473022
=== Iterazione IRL 580 ===
Loss reward (iter 580): -7.536191940307617
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 150       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -4.59e+03 |
|    critic_loss     | 1.76e+05  |
|    ent_coef        | 17.9      |
|    ent_coef_loss   | 0.187     |
|    learning_rate   | 0.0003    |
|    n_updates       | 162699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -4.56e+03 |
|    critic_loss     | 2.8e+05   |
|    ent_coef        | 17.6      |
|    ent_coef_loss   | -0.706    |
|    learning_rate   | 0.0003    |
|    n_updates       | 163099    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -4.5e+03 |
|    critic_loss     | 1.88e+05 |
|    ent_coef        | 17.4     |
|    ent_coef_loss   | 0.734    |
|    learning_rate   | 0.0003   |
|    n_updates       | 163499   |
---------------------------------
=== Iterazione IRL 581 ===
Loss reward (iter 581): 2.5281124114990234
=== Iterazione IRL 582 ===
Loss reward (iter 582): 13.001296043395996
=== Iterazione IRL 583 ===
Loss reward (iter 583): 2.5609025955200195
=== Iterazione IRL 584 ===
Loss reward (iter 584): 2.817673921585083
=== Iterazione IRL 585 ===
Loss reward (iter 585): -2.9480319023132324
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -4.33e+03 |
|    critic_loss     | 3.76e+05  |
|    ent_coef        | 17.3      |
|    ent_coef_loss   | 0.171     |
|    learning_rate   | 0.0003    |
|    n_updates       | 164099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -4.29e+03 |
|    critic_loss     | 2.4e+05   |
|    ent_coef        | 16.9      |
|    ent_coef_loss   | -1.14     |
|    learning_rate   | 0.0003    |
|    n_updates       | 164499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -4.27e+03 |
|    critic_loss     | 2.05e+05  |
|    ent_coef        | 17        |
|    ent_coef_loss   | 0.549     |
|    learning_rate   | 0.0003    |
|    n_updates       | 164899    |
----------------------------------
=== Iterazione IRL 586 ===
Loss reward (iter 586): -2.027773141860962
=== Iterazione IRL 587 ===
Loss reward (iter 587): -3.8584532737731934
=== Iterazione IRL 588 ===
Loss reward (iter 588): -3.452310562133789
=== Iterazione IRL 589 ===
Loss reward (iter 589): -6.621426105499268
=== Iterazione IRL 590 ===
Loss reward (iter 590): -6.096837997436523
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -4.26e+03 |
|    critic_loss     | 3.19e+05  |
|    ent_coef        | 16.9      |
|    ent_coef_loss   | -0.103    |
|    learning_rate   | 0.0003    |
|    n_updates       | 165499    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.2e+03 |
|    critic_loss     | 4.8e+05  |
|    ent_coef        | 16.7     |
|    ent_coef_loss   | -0.18    |
|    learning_rate   | 0.0003   |
|    n_updates       | 165899   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -4.12e+03 |
|    critic_loss     | 5.07e+05  |
|    ent_coef        | 16.7      |
|    ent_coef_loss   | 0.102     |
|    learning_rate   | 0.0003    |
|    n_updates       | 166299    |
----------------------------------
=== Iterazione IRL 591 ===
Loss reward (iter 591): 12.225154876708984
=== Iterazione IRL 592 ===
Loss reward (iter 592): 10.473809242248535
=== Iterazione IRL 593 ===
Loss reward (iter 593): 6.465608596801758
=== Iterazione IRL 594 ===
Loss reward (iter 594): 2.0909085273742676
=== Iterazione IRL 595 ===
Loss reward (iter 595): 1.9510524272918701
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -3.96e+03 |
|    critic_loss     | 3.96e+05  |
|    ent_coef        | 16.8      |
|    ent_coef_loss   | -0.422    |
|    learning_rate   | 0.0003    |
|    n_updates       | 166899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -3.98e+03 |
|    critic_loss     | 2.97e+05  |
|    ent_coef        | 16.8      |
|    ent_coef_loss   | 0.506     |
|    learning_rate   | 0.0003    |
|    n_updates       | 167299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -3.71e+03 |
|    critic_loss     | 2.09e+05  |
|    ent_coef        | 16.4      |
|    ent_coef_loss   | -0.175    |
|    learning_rate   | 0.0003    |
|    n_updates       | 167699    |
----------------------------------
=== Iterazione IRL 596 ===
Loss reward (iter 596): -0.7098588943481445
=== Iterazione IRL 597 ===
Loss reward (iter 597): -1.7346464395523071
=== Iterazione IRL 598 ===
Loss reward (iter 598): -2.7921528816223145
=== Iterazione IRL 599 ===
Loss reward (iter 599): -1.4845751523971558
=== Iterazione IRL 600 ===
Loss reward (iter 600): -3.869098663330078
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -3.75e+03 |
|    critic_loss     | 2.15e+05  |
|    ent_coef        | 16.2      |
|    ent_coef_loss   | 1.29      |
|    learning_rate   | 0.0003    |
|    n_updates       | 168299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -3.79e+03 |
|    critic_loss     | 1.18e+05  |
|    ent_coef        | 16.6      |
|    ent_coef_loss   | 0.34      |
|    learning_rate   | 0.0003    |
|    n_updates       | 168699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -3.65e+03 |
|    critic_loss     | 4.11e+05  |
|    ent_coef        | 16.5      |
|    ent_coef_loss   | 0.152     |
|    learning_rate   | 0.0003    |
|    n_updates       | 169099    |
----------------------------------
=== Iterazione IRL 601 ===
Loss reward (iter 601): 1.4311432838439941
=== Iterazione IRL 602 ===
Loss reward (iter 602): -1.363936424255371
=== Iterazione IRL 603 ===
Loss reward (iter 603): 0.07524669170379639
=== Iterazione IRL 604 ===
Loss reward (iter 604): 7.039134502410889
=== Iterazione IRL 605 ===
Loss reward (iter 605): 2.367872714996338
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -3.71e+03 |
|    critic_loss     | 2e+05     |
|    ent_coef        | 16.3      |
|    ent_coef_loss   | -1.47     |
|    learning_rate   | 0.0003    |
|    n_updates       | 169699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -3.59e+03 |
|    critic_loss     | 3.44e+05  |
|    ent_coef        | 16        |
|    ent_coef_loss   | 0.18      |
|    learning_rate   | 0.0003    |
|    n_updates       | 170099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -3.54e+03 |
|    critic_loss     | 4.04e+05  |
|    ent_coef        | 15.6      |
|    ent_coef_loss   | 1.59      |
|    learning_rate   | 0.0003    |
|    n_updates       | 170499    |
----------------------------------
=== Iterazione IRL 606 ===
Loss reward (iter 606): 2.697538375854492
=== Iterazione IRL 607 ===
Loss reward (iter 607): 2.119540214538574
=== Iterazione IRL 608 ===
Loss reward (iter 608): 3.322913646697998
=== Iterazione IRL 609 ===
Loss reward (iter 609): -0.5605952739715576
=== Iterazione IRL 610 ===
Loss reward (iter 610): -3.3972043991088867
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -3.41e+03 |
|    critic_loss     | 4.23e+05  |
|    ent_coef        | 15.4      |
|    ent_coef_loss   | -0.165    |
|    learning_rate   | 0.0003    |
|    n_updates       | 171099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -3.31e+03 |
|    critic_loss     | 5.3e+05   |
|    ent_coef        | 15.4      |
|    ent_coef_loss   | 0.813     |
|    learning_rate   | 0.0003    |
|    n_updates       | 171499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -3.35e+03 |
|    critic_loss     | 1.33e+05  |
|    ent_coef        | 15.1      |
|    ent_coef_loss   | 0.0778    |
|    learning_rate   | 0.0003    |
|    n_updates       | 171899    |
----------------------------------
=== Iterazione IRL 611 ===
Loss reward (iter 611): -2.685840129852295
=== Iterazione IRL 612 ===
Loss reward (iter 612): -3.5785677433013916
=== Iterazione IRL 613 ===
Loss reward (iter 613): 6.567615985870361
=== Iterazione IRL 614 ===
Loss reward (iter 614): 5.829201698303223
=== Iterazione IRL 615 ===
Loss reward (iter 615): 0.3078676462173462
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 150       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -3.36e+03 |
|    critic_loss     | 5.26e+05  |
|    ent_coef        | 14.8      |
|    ent_coef_loss   | -0.459    |
|    learning_rate   | 0.0003    |
|    n_updates       | 172499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -3.32e+03 |
|    critic_loss     | 3.33e+05  |
|    ent_coef        | 15        |
|    ent_coef_loss   | 0.0495    |
|    learning_rate   | 0.0003    |
|    n_updates       | 172899    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -3.3e+03 |
|    critic_loss     | 3.63e+05 |
|    ent_coef        | 15.2     |
|    ent_coef_loss   | -0.784   |
|    learning_rate   | 0.0003   |
|    n_updates       | 173299   |
---------------------------------
=== Iterazione IRL 616 ===
Loss reward (iter 616): 8.263099670410156
=== Iterazione IRL 617 ===
Loss reward (iter 617): -2.37273907661438
=== Iterazione IRL 618 ===
Loss reward (iter 618): -0.8396803140640259
=== Iterazione IRL 619 ===
Loss reward (iter 619): -0.9193128347396851
=== Iterazione IRL 620 ===
Loss reward (iter 620): -3.9179179668426514
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 150       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -3.15e+03 |
|    critic_loss     | 5.9e+05   |
|    ent_coef        | 15.2      |
|    ent_coef_loss   | -0.533    |
|    learning_rate   | 0.0003    |
|    n_updates       | 173899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -3.24e+03 |
|    critic_loss     | 4.81e+05  |
|    ent_coef        | 15.7      |
|    ent_coef_loss   | -0.321    |
|    learning_rate   | 0.0003    |
|    n_updates       | 174299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 125       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.99e+03 |
|    critic_loss     | 1.8e+05   |
|    ent_coef        | 15.9      |
|    ent_coef_loss   | -0.615    |
|    learning_rate   | 0.0003    |
|    n_updates       | 174699    |
----------------------------------
=== Iterazione IRL 621 ===
Loss reward (iter 621): 6.820151329040527
=== Iterazione IRL 622 ===
Loss reward (iter 622): -4.515130043029785
=== Iterazione IRL 623 ===
Loss reward (iter 623): -1.161233901977539
=== Iterazione IRL 624 ===
Loss reward (iter 624): 0.6778398752212524
=== Iterazione IRL 625 ===
Loss reward (iter 625): -2.0986552238464355
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 150       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -3.01e+03 |
|    critic_loss     | 3.04e+05  |
|    ent_coef        | 15.9      |
|    ent_coef_loss   | 0.0566    |
|    learning_rate   | 0.0003    |
|    n_updates       | 175299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.98e+03 |
|    critic_loss     | 2.27e+05  |
|    ent_coef        | 15.5      |
|    ent_coef_loss   | 1.66      |
|    learning_rate   | 0.0003    |
|    n_updates       | 175699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.74e+03 |
|    critic_loss     | 2.36e+05  |
|    ent_coef        | 15.5      |
|    ent_coef_loss   | 0.605     |
|    learning_rate   | 0.0003    |
|    n_updates       | 176099    |
----------------------------------
=== Iterazione IRL 626 ===
Loss reward (iter 626): -0.828319787979126
=== Iterazione IRL 627 ===
Loss reward (iter 627): -4.12750244140625
=== Iterazione IRL 628 ===
Loss reward (iter 628): -0.24782252311706543
=== Iterazione IRL 629 ===
Loss reward (iter 629): -9.417924880981445
=== Iterazione IRL 630 ===
Loss reward (iter 630): -2.1404309272766113
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 150       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.82e+03 |
|    critic_loss     | 2.52e+05  |
|    ent_coef        | 16.1      |
|    ent_coef_loss   | 0.48      |
|    learning_rate   | 0.0003    |
|    n_updates       | 176699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.64e+03 |
|    critic_loss     | 4.77e+05  |
|    ent_coef        | 16.1      |
|    ent_coef_loss   | 0.118     |
|    learning_rate   | 0.0003    |
|    n_updates       | 177099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.92e+03 |
|    critic_loss     | 5.99e+05  |
|    ent_coef        | 15.5      |
|    ent_coef_loss   | -0.092    |
|    learning_rate   | 0.0003    |
|    n_updates       | 177499    |
----------------------------------
=== Iterazione IRL 631 ===
Loss reward (iter 631): 4.022309303283691
=== Iterazione IRL 632 ===
Loss reward (iter 632): 8.693031311035156
=== Iterazione IRL 633 ===
Loss reward (iter 633): 1.1997509002685547
=== Iterazione IRL 634 ===
Loss reward (iter 634): 2.2067928314208984
=== Iterazione IRL 635 ===
Loss reward (iter 635): 1.366724967956543
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.72e+03 |
|    critic_loss     | 2.49e+05  |
|    ent_coef        | 14.9      |
|    ent_coef_loss   | 1.3       |
|    learning_rate   | 0.0003    |
|    n_updates       | 178099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.71e+03 |
|    critic_loss     | 2.7e+05   |
|    ent_coef        | 14.7      |
|    ent_coef_loss   | -0.508    |
|    learning_rate   | 0.0003    |
|    n_updates       | 178499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.44e+03 |
|    critic_loss     | 2.95e+05  |
|    ent_coef        | 15        |
|    ent_coef_loss   | 0.316     |
|    learning_rate   | 0.0003    |
|    n_updates       | 178899    |
----------------------------------
=== Iterazione IRL 636 ===
Loss reward (iter 636): -1.7049341201782227
=== Iterazione IRL 637 ===
Loss reward (iter 637): -3.9998490810394287
=== Iterazione IRL 638 ===
Loss reward (iter 638): 4.822993278503418
=== Iterazione IRL 639 ===
Loss reward (iter 639): 2.0820188522338867
=== Iterazione IRL 640 ===
Loss reward (iter 640): -0.07780313491821289
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.54e+03 |
|    critic_loss     | 1.9e+05   |
|    ent_coef        | 14.8      |
|    ent_coef_loss   | 0.272     |
|    learning_rate   | 0.0003    |
|    n_updates       | 179499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.59e+03 |
|    critic_loss     | 3.06e+05  |
|    ent_coef        | 14.6      |
|    ent_coef_loss   | -0.18     |
|    learning_rate   | 0.0003    |
|    n_updates       | 179899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.43e+03 |
|    critic_loss     | 2.72e+05  |
|    ent_coef        | 14.5      |
|    ent_coef_loss   | -1.21     |
|    learning_rate   | 0.0003    |
|    n_updates       | 180299    |
----------------------------------
=== Iterazione IRL 641 ===
Loss reward (iter 641): 0.9577655792236328
=== Iterazione IRL 642 ===
Loss reward (iter 642): 1.0990216732025146
=== Iterazione IRL 643 ===
Loss reward (iter 643): -1.8237959146499634
=== Iterazione IRL 644 ===
Loss reward (iter 644): 0.6919201612472534
=== Iterazione IRL 645 ===
Loss reward (iter 645): 1.1342371702194214
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 150       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.36e+03 |
|    critic_loss     | 2.63e+05  |
|    ent_coef        | 14.5      |
|    ent_coef_loss   | 0.436     |
|    learning_rate   | 0.0003    |
|    n_updates       | 180899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.13e+03 |
|    critic_loss     | 3.16e+05  |
|    ent_coef        | 14.4      |
|    ent_coef_loss   | 0.116     |
|    learning_rate   | 0.0003    |
|    n_updates       | 181299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 125       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.33e+03 |
|    critic_loss     | 2.68e+05  |
|    ent_coef        | 14.7      |
|    ent_coef_loss   | 0.0494    |
|    learning_rate   | 0.0003    |
|    n_updates       | 181699    |
----------------------------------
=== Iterazione IRL 646 ===
Loss reward (iter 646): 9.969032287597656
=== Iterazione IRL 647 ===
Loss reward (iter 647): 3.048462390899658
=== Iterazione IRL 648 ===
Loss reward (iter 648): 2.2405881881713867
=== Iterazione IRL 649 ===
Loss reward (iter 649): -0.13419461250305176
=== Iterazione IRL 650 ===
Loss reward (iter 650): -0.20844244956970215
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 150       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.14e+03 |
|    critic_loss     | 4.84e+05  |
|    ent_coef        | 14.9      |
|    ent_coef_loss   | 0.548     |
|    learning_rate   | 0.0003    |
|    n_updates       | 182299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.14e+03 |
|    critic_loss     | 3.64e+05  |
|    ent_coef        | 14.8      |
|    ent_coef_loss   | -0.00832  |
|    learning_rate   | 0.0003    |
|    n_updates       | 182699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.12e+03 |
|    critic_loss     | 2.03e+05  |
|    ent_coef        | 14.9      |
|    ent_coef_loss   | -0.69     |
|    learning_rate   | 0.0003    |
|    n_updates       | 183099    |
----------------------------------
=== Iterazione IRL 651 ===
Loss reward (iter 651): -0.7043408155441284
=== Iterazione IRL 652 ===
Loss reward (iter 652): 1.9546598196029663
=== Iterazione IRL 653 ===
Loss reward (iter 653): -1.3012442588806152
=== Iterazione IRL 654 ===
Loss reward (iter 654): 2.182400703430176
=== Iterazione IRL 655 ===
Loss reward (iter 655): 1.76573646068573
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.07e+03 |
|    critic_loss     | 2.96e+05  |
|    ent_coef        | 14.8      |
|    ent_coef_loss   | -0.727    |
|    learning_rate   | 0.0003    |
|    n_updates       | 183699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.88e+03 |
|    critic_loss     | 2.45e+05  |
|    ent_coef        | 14.5      |
|    ent_coef_loss   | 0.335     |
|    learning_rate   | 0.0003    |
|    n_updates       | 184099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.18e+03 |
|    critic_loss     | 4.47e+05  |
|    ent_coef        | 14.2      |
|    ent_coef_loss   | 0.968     |
|    learning_rate   | 0.0003    |
|    n_updates       | 184499    |
----------------------------------
=== Iterazione IRL 656 ===
Loss reward (iter 656): 7.996827125549316
=== Iterazione IRL 657 ===
Loss reward (iter 657): -1.0421346426010132
=== Iterazione IRL 658 ===
Loss reward (iter 658): -0.297382116317749
=== Iterazione IRL 659 ===
Loss reward (iter 659): -1.7608317136764526
=== Iterazione IRL 660 ===
Loss reward (iter 660): 2.4426379203796387
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.97e+03 |
|    critic_loss     | 5.62e+05  |
|    ent_coef        | 14.3      |
|    ent_coef_loss   | 0.253     |
|    learning_rate   | 0.0003    |
|    n_updates       | 185099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.78e+03 |
|    critic_loss     | 1.63e+05  |
|    ent_coef        | 14.4      |
|    ent_coef_loss   | 0.0038    |
|    learning_rate   | 0.0003    |
|    n_updates       | 185499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.81e+03 |
|    critic_loss     | 5.35e+05  |
|    ent_coef        | 14.4      |
|    ent_coef_loss   | -0.785    |
|    learning_rate   | 0.0003    |
|    n_updates       | 185899    |
----------------------------------
=== Iterazione IRL 661 ===
Loss reward (iter 661): 5.188013553619385
=== Iterazione IRL 662 ===
Loss reward (iter 662): 1.6509642601013184
=== Iterazione IRL 663 ===
Loss reward (iter 663): -1.9978541135787964
=== Iterazione IRL 664 ===
Loss reward (iter 664): 4.469418048858643
=== Iterazione IRL 665 ===
Loss reward (iter 665): 3.2130179405212402
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.84e+03 |
|    critic_loss     | 1.47e+05  |
|    ent_coef        | 14.1      |
|    ent_coef_loss   | 0.227     |
|    learning_rate   | 0.0003    |
|    n_updates       | 186499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.89e+03 |
|    critic_loss     | 1.4e+05   |
|    ent_coef        | 14.1      |
|    ent_coef_loss   | -0.0509   |
|    learning_rate   | 0.0003    |
|    n_updates       | 186899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.63e+03 |
|    critic_loss     | 3.8e+05   |
|    ent_coef        | 14        |
|    ent_coef_loss   | 1.03      |
|    learning_rate   | 0.0003    |
|    n_updates       | 187299    |
----------------------------------
=== Iterazione IRL 666 ===
Loss reward (iter 666): 2.192972183227539
=== Iterazione IRL 667 ===
Loss reward (iter 667): 3.0823376178741455
=== Iterazione IRL 668 ===
Loss reward (iter 668): 2.539703845977783
=== Iterazione IRL 669 ===
Loss reward (iter 669): 2.3158702850341797
=== Iterazione IRL 670 ===
Loss reward (iter 670): 1.1150931119918823
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.67e+03 |
|    critic_loss     | 2.4e+05   |
|    ent_coef        | 13.8      |
|    ent_coef_loss   | -0.0337   |
|    learning_rate   | 0.0003    |
|    n_updates       | 187899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.75e+03 |
|    critic_loss     | 5.47e+05  |
|    ent_coef        | 13.7      |
|    ent_coef_loss   | 0.525     |
|    learning_rate   | 0.0003    |
|    n_updates       | 188299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.84e+03 |
|    critic_loss     | 5.34e+05  |
|    ent_coef        | 13.9      |
|    ent_coef_loss   | -0.191    |
|    learning_rate   | 0.0003    |
|    n_updates       | 188699    |
----------------------------------
=== Iterazione IRL 671 ===
Loss reward (iter 671): -1.3548431396484375
=== Iterazione IRL 672 ===
Loss reward (iter 672): -0.3870227336883545
=== Iterazione IRL 673 ===
Loss reward (iter 673): -1.1252412796020508
=== Iterazione IRL 674 ===
Loss reward (iter 674): -1.5050647258758545
=== Iterazione IRL 675 ===
Loss reward (iter 675): 2.735464572906494
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 150       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.47e+03 |
|    critic_loss     | 4.85e+05  |
|    ent_coef        | 13.6      |
|    ent_coef_loss   | 0.828     |
|    learning_rate   | 0.0003    |
|    n_updates       | 189299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.55e+03 |
|    critic_loss     | 4.84e+05  |
|    ent_coef        | 13.4      |
|    ent_coef_loss   | -0.299    |
|    learning_rate   | 0.0003    |
|    n_updates       | 189699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.56e+03 |
|    critic_loss     | 2.87e+05  |
|    ent_coef        | 13.3      |
|    ent_coef_loss   | 0.599     |
|    learning_rate   | 0.0003    |
|    n_updates       | 190099    |
----------------------------------
=== Iterazione IRL 676 ===
Loss reward (iter 676): 1.4650641679763794
=== Iterazione IRL 677 ===
Loss reward (iter 677): -0.7132689952850342
=== Iterazione IRL 678 ===
Loss reward (iter 678): -0.8487899303436279
=== Iterazione IRL 679 ===
Loss reward (iter 679): -2.4397711753845215
=== Iterazione IRL 680 ===
Loss reward (iter 680): -2.555698871612549
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 150       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.47e+03 |
|    critic_loss     | 4.3e+05   |
|    ent_coef        | 13.2      |
|    ent_coef_loss   | -0.842    |
|    learning_rate   | 0.0003    |
|    n_updates       | 190699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.52e+03 |
|    critic_loss     | 1.75e+05  |
|    ent_coef        | 13.3      |
|    ent_coef_loss   | 0.0147    |
|    learning_rate   | 0.0003    |
|    n_updates       | 191099    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -1.5e+03 |
|    critic_loss     | 6.04e+05 |
|    ent_coef        | 13.4     |
|    ent_coef_loss   | -0.162   |
|    learning_rate   | 0.0003   |
|    n_updates       | 191499   |
---------------------------------
=== Iterazione IRL 681 ===
Loss reward (iter 681): 4.044979095458984
=== Iterazione IRL 682 ===
Loss reward (iter 682): -1.1523346900939941
=== Iterazione IRL 683 ===
Loss reward (iter 683): -3.227199077606201
=== Iterazione IRL 684 ===
Loss reward (iter 684): 0.2310333251953125
=== Iterazione IRL 685 ===
Loss reward (iter 685): -2.177682399749756
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 150       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.31e+03 |
|    critic_loss     | 2.82e+05  |
|    ent_coef        | 13.7      |
|    ent_coef_loss   | -0.494    |
|    learning_rate   | 0.0003    |
|    n_updates       | 192099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.27e+03 |
|    critic_loss     | 2.23e+05  |
|    ent_coef        | 13.4      |
|    ent_coef_loss   | 0.0861    |
|    learning_rate   | 0.0003    |
|    n_updates       | 192499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.27e+03 |
|    critic_loss     | 1.71e+05  |
|    ent_coef        | 13.1      |
|    ent_coef_loss   | 1.49      |
|    learning_rate   | 0.0003    |
|    n_updates       | 192899    |
----------------------------------
=== Iterazione IRL 686 ===
Loss reward (iter 686): 4.292649269104004
=== Iterazione IRL 687 ===
Loss reward (iter 687): -0.8507436513900757
=== Iterazione IRL 688 ===
Loss reward (iter 688): 4.324235916137695
=== Iterazione IRL 689 ===
Loss reward (iter 689): 1.8868441581726074
=== Iterazione IRL 690 ===
Loss reward (iter 690): 1.0911281108856201
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.34e+03 |
|    critic_loss     | 1.02e+05  |
|    ent_coef        | 13.1      |
|    ent_coef_loss   | -0.211    |
|    learning_rate   | 0.0003    |
|    n_updates       | 193499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.37e+03 |
|    critic_loss     | 2.37e+05  |
|    ent_coef        | 13        |
|    ent_coef_loss   | 0.623     |
|    learning_rate   | 0.0003    |
|    n_updates       | 193899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.24e+03 |
|    critic_loss     | 1.22e+05  |
|    ent_coef        | 13        |
|    ent_coef_loss   | 0.173     |
|    learning_rate   | 0.0003    |
|    n_updates       | 194299    |
----------------------------------
=== Iterazione IRL 691 ===
Loss reward (iter 691): 0.2325727939605713
=== Iterazione IRL 692 ===
Loss reward (iter 692): -3.3568644523620605
=== Iterazione IRL 693 ===
Loss reward (iter 693): 1.339236855506897
=== Iterazione IRL 694 ===
Loss reward (iter 694): -0.49506163597106934
=== Iterazione IRL 695 ===
Loss reward (iter 695): -0.8943644762039185
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -1.2e+03 |
|    critic_loss     | 2.29e+05 |
|    ent_coef        | 13.1     |
|    ent_coef_loss   | 0.154    |
|    learning_rate   | 0.0003   |
|    n_updates       | 194899   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.21e+03 |
|    critic_loss     | 2.86e+05  |
|    ent_coef        | 13.1      |
|    ent_coef_loss   | -0.396    |
|    learning_rate   | 0.0003    |
|    n_updates       | 195299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.06e+03 |
|    critic_loss     | 1.4e+05   |
|    ent_coef        | 13.2      |
|    ent_coef_loss   | 0.205     |
|    learning_rate   | 0.0003    |
|    n_updates       | 195699    |
----------------------------------
=== Iterazione IRL 696 ===
Loss reward (iter 696): 0.7500590085983276
=== Iterazione IRL 697 ===
Loss reward (iter 697): 1.333577036857605
=== Iterazione IRL 698 ===
Loss reward (iter 698): -4.266557693481445
=== Iterazione IRL 699 ===
Loss reward (iter 699): -5.034684658050537
=== Iterazione IRL 700 ===
Loss reward (iter 700): 0.7318823337554932
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -999     |
|    critic_loss     | 2.59e+05 |
|    ent_coef        | 14       |
|    ent_coef_loss   | 0.567    |
|    learning_rate   | 0.0003   |
|    n_updates       | 196299   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.09e+03 |
|    critic_loss     | 5.58e+05  |
|    ent_coef        | 14.1      |
|    ent_coef_loss   | -1.05     |
|    learning_rate   | 0.0003    |
|    n_updates       | 196699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.02e+03 |
|    critic_loss     | 4.74e+05  |
|    ent_coef        | 14.1      |
|    ent_coef_loss   | 0.397     |
|    learning_rate   | 0.0003    |
|    n_updates       | 197099    |
----------------------------------
=== Iterazione IRL 701 ===
Loss reward (iter 701): -2.9485013484954834
=== Iterazione IRL 702 ===
Loss reward (iter 702): -1.1522610187530518
=== Iterazione IRL 703 ===
Loss reward (iter 703): 3.2101094722747803
=== Iterazione IRL 704 ===
Loss reward (iter 704): 0.09731054306030273
=== Iterazione IRL 705 ===
Loss reward (iter 705): 3.7467312812805176
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -973     |
|    critic_loss     | 1.54e+05 |
|    ent_coef        | 13.9     |
|    ent_coef_loss   | 0.662    |
|    learning_rate   | 0.0003   |
|    n_updates       | 197699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -1e+03   |
|    critic_loss     | 1.56e+05 |
|    ent_coef        | 13.8     |
|    ent_coef_loss   | -0.182   |
|    learning_rate   | 0.0003   |
|    n_updates       | 198099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -935     |
|    critic_loss     | 4.98e+05 |
|    ent_coef        | 13.6     |
|    ent_coef_loss   | 0.628    |
|    learning_rate   | 0.0003   |
|    n_updates       | 198499   |
---------------------------------
=== Iterazione IRL 706 ===
Loss reward (iter 706): -2.2322447299957275
=== Iterazione IRL 707 ===
Loss reward (iter 707): 1.9209959506988525
=== Iterazione IRL 708 ===
Loss reward (iter 708): -0.7870856523513794
=== Iterazione IRL 709 ===
Loss reward (iter 709): 0.40591228008270264
=== Iterazione IRL 710 ===
Loss reward (iter 710): -0.5064688920974731
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -965     |
|    critic_loss     | 6.07e+05 |
|    ent_coef        | 13.8     |
|    ent_coef_loss   | 0.572    |
|    learning_rate   | 0.0003   |
|    n_updates       | 199099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -981     |
|    critic_loss     | 4.05e+05 |
|    ent_coef        | 13.9     |
|    ent_coef_loss   | 0.187    |
|    learning_rate   | 0.0003   |
|    n_updates       | 199499   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.02e+03 |
|    critic_loss     | 1.95e+05  |
|    ent_coef        | 13.9      |
|    ent_coef_loss   | 0.273     |
|    learning_rate   | 0.0003    |
|    n_updates       | 199899    |
----------------------------------
=== Iterazione IRL 711 ===
Loss reward (iter 711): 1.122910976409912
=== Iterazione IRL 712 ===
Loss reward (iter 712): -3.284052848815918
=== Iterazione IRL 713 ===
Loss reward (iter 713): -0.6689702272415161
=== Iterazione IRL 714 ===
Loss reward (iter 714): 0.5872845649719238
=== Iterazione IRL 715 ===
Loss reward (iter 715): -1.557462453842163
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -787     |
|    critic_loss     | 1.23e+05 |
|    ent_coef        | 13.4     |
|    ent_coef_loss   | -0.508   |
|    learning_rate   | 0.0003   |
|    n_updates       | 200499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -764     |
|    critic_loss     | 2.07e+05 |
|    ent_coef        | 13.6     |
|    ent_coef_loss   | 0.174    |
|    learning_rate   | 0.0003   |
|    n_updates       | 200899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -804     |
|    critic_loss     | 1.89e+05 |
|    ent_coef        | 13.5     |
|    ent_coef_loss   | 0.0127   |
|    learning_rate   | 0.0003   |
|    n_updates       | 201299   |
---------------------------------
=== Iterazione IRL 716 ===
Loss reward (iter 716): -1.679427146911621
=== Iterazione IRL 717 ===
Loss reward (iter 717): -1.3718401193618774
=== Iterazione IRL 718 ===
Loss reward (iter 718): -0.5548826456069946
=== Iterazione IRL 719 ===
Loss reward (iter 719): -1.2122122049331665
=== Iterazione IRL 720 ===
Loss reward (iter 720): -5.613546371459961
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -733     |
|    critic_loss     | 1.74e+05 |
|    ent_coef        | 13.6     |
|    ent_coef_loss   | -0.487   |
|    learning_rate   | 0.0003   |
|    n_updates       | 201899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -878     |
|    critic_loss     | 4.67e+05 |
|    ent_coef        | 13.6     |
|    ent_coef_loss   | 0.587    |
|    learning_rate   | 0.0003   |
|    n_updates       | 202299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -803     |
|    critic_loss     | 5.52e+05 |
|    ent_coef        | 13.6     |
|    ent_coef_loss   | -0.488   |
|    learning_rate   | 0.0003   |
|    n_updates       | 202699   |
---------------------------------
=== Iterazione IRL 721 ===
Loss reward (iter 721): -6.201417922973633
=== Iterazione IRL 722 ===
Loss reward (iter 722): 5.27965784072876
=== Iterazione IRL 723 ===
Loss reward (iter 723): 5.349262714385986
=== Iterazione IRL 724 ===
Loss reward (iter 724): 2.380411148071289
=== Iterazione IRL 725 ===
Loss reward (iter 725): -0.32822954654693604
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -688     |
|    critic_loss     | 2.63e+05 |
|    ent_coef        | 13.9     |
|    ent_coef_loss   | -0.0565  |
|    learning_rate   | 0.0003   |
|    n_updates       | 203299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -559     |
|    critic_loss     | 3.92e+05 |
|    ent_coef        | 13.6     |
|    ent_coef_loss   | -0.819   |
|    learning_rate   | 0.0003   |
|    n_updates       | 203699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -665     |
|    critic_loss     | 6.39e+05 |
|    ent_coef        | 13.5     |
|    ent_coef_loss   | -0.779   |
|    learning_rate   | 0.0003   |
|    n_updates       | 204099   |
---------------------------------
=== Iterazione IRL 726 ===
Loss reward (iter 726): -0.7923069000244141
=== Iterazione IRL 727 ===
Loss reward (iter 727): -2.0776376724243164
=== Iterazione IRL 728 ===
Loss reward (iter 728): -7.391683101654053
=== Iterazione IRL 729 ===
Loss reward (iter 729): 1.5490660667419434
=== Iterazione IRL 730 ===
Loss reward (iter 730): -0.700412392616272
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -850     |
|    critic_loss     | 2.8e+05  |
|    ent_coef        | 13.3     |
|    ent_coef_loss   | -0.49    |
|    learning_rate   | 0.0003   |
|    n_updates       | 204699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -613     |
|    critic_loss     | 2.78e+05 |
|    ent_coef        | 13.3     |
|    ent_coef_loss   | -0.0783  |
|    learning_rate   | 0.0003   |
|    n_updates       | 205099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -645     |
|    critic_loss     | 4.44e+05 |
|    ent_coef        | 13.2     |
|    ent_coef_loss   | -0.0105  |
|    learning_rate   | 0.0003   |
|    n_updates       | 205499   |
---------------------------------
=== Iterazione IRL 731 ===
Loss reward (iter 731): -5.589077949523926
=== Iterazione IRL 732 ===
Loss reward (iter 732): -1.8541215658187866
=== Iterazione IRL 733 ===
Loss reward (iter 733): 2.290876865386963
=== Iterazione IRL 734 ===
Loss reward (iter 734): -4.978482246398926
=== Iterazione IRL 735 ===
Loss reward (iter 735): -2.4979054927825928
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -675     |
|    critic_loss     | 2.29e+05 |
|    ent_coef        | 13       |
|    ent_coef_loss   | 0.674    |
|    learning_rate   | 0.0003   |
|    n_updates       | 206099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -551     |
|    critic_loss     | 1.6e+05  |
|    ent_coef        | 13       |
|    ent_coef_loss   | -0.497   |
|    learning_rate   | 0.0003   |
|    n_updates       | 206499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -593     |
|    critic_loss     | 2.94e+05 |
|    ent_coef        | 12.7     |
|    ent_coef_loss   | 0.397    |
|    learning_rate   | 0.0003   |
|    n_updates       | 206899   |
---------------------------------
=== Iterazione IRL 736 ===
Loss reward (iter 736): 1.900843858718872
=== Iterazione IRL 737 ===
Loss reward (iter 737): 1.3134995698928833
=== Iterazione IRL 738 ===
Loss reward (iter 738): -0.532380223274231
=== Iterazione IRL 739 ===
Loss reward (iter 739): -5.430878162384033
=== Iterazione IRL 740 ===
Loss reward (iter 740): -1.1282278299331665
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -659     |
|    critic_loss     | 1.08e+05 |
|    ent_coef        | 12.6     |
|    ent_coef_loss   | 1.1      |
|    learning_rate   | 0.0003   |
|    n_updates       | 207499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -553     |
|    critic_loss     | 2.18e+05 |
|    ent_coef        | 12.6     |
|    ent_coef_loss   | -0.397   |
|    learning_rate   | 0.0003   |
|    n_updates       | 207899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -604     |
|    critic_loss     | 2.18e+05 |
|    ent_coef        | 12.3     |
|    ent_coef_loss   | 0.297    |
|    learning_rate   | 0.0003   |
|    n_updates       | 208299   |
---------------------------------
=== Iterazione IRL 741 ===
Loss reward (iter 741): -0.8357465267181396
=== Iterazione IRL 742 ===
Loss reward (iter 742): -4.030494213104248
=== Iterazione IRL 743 ===
Loss reward (iter 743): -4.04459810256958
=== Iterazione IRL 744 ===
Loss reward (iter 744): 0.578648567199707
=== Iterazione IRL 745 ===
Loss reward (iter 745): -1.7317736148834229
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -483     |
|    critic_loss     | 3.59e+05 |
|    ent_coef        | 11.8     |
|    ent_coef_loss   | 0.144    |
|    learning_rate   | 0.0003   |
|    n_updates       | 208899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -367     |
|    critic_loss     | 3.1e+05  |
|    ent_coef        | 11.3     |
|    ent_coef_loss   | -1.06    |
|    learning_rate   | 0.0003   |
|    n_updates       | 209299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -396     |
|    critic_loss     | 1.29e+05 |
|    ent_coef        | 11.4     |
|    ent_coef_loss   | -0.576   |
|    learning_rate   | 0.0003   |
|    n_updates       | 209699   |
---------------------------------
=== Iterazione IRL 746 ===
Loss reward (iter 746): 5.926778316497803
=== Iterazione IRL 747 ===
Loss reward (iter 747): -2.3296456336975098
=== Iterazione IRL 748 ===
Loss reward (iter 748): 7.926688194274902
=== Iterazione IRL 749 ===
Loss reward (iter 749): -3.7304258346557617
=== Iterazione IRL 750 ===
Loss reward (iter 750): -2.6364023685455322
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -265     |
|    critic_loss     | 1.71e+05 |
|    ent_coef        | 11.2     |
|    ent_coef_loss   | 0.977    |
|    learning_rate   | 0.0003   |
|    n_updates       | 210299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -459     |
|    critic_loss     | 1.13e+05 |
|    ent_coef        | 11.1     |
|    ent_coef_loss   | 0.715    |
|    learning_rate   | 0.0003   |
|    n_updates       | 210699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -424     |
|    critic_loss     | 5.72e+05 |
|    ent_coef        | 11.1     |
|    ent_coef_loss   | 0.594    |
|    learning_rate   | 0.0003   |
|    n_updates       | 211099   |
---------------------------------
=== Iterazione IRL 751 ===
Loss reward (iter 751): -26.04471206665039
=== Iterazione IRL 752 ===
Loss reward (iter 752): -6.166868209838867
=== Iterazione IRL 753 ===
Loss reward (iter 753): -11.42772388458252
=== Iterazione IRL 754 ===
Loss reward (iter 754): -15.82675838470459
=== Iterazione IRL 755 ===
Loss reward (iter 755): -30.9585018157959
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -264     |
|    critic_loss     | 1.17e+05 |
|    ent_coef        | 11       |
|    ent_coef_loss   | 0.176    |
|    learning_rate   | 0.0003   |
|    n_updates       | 211699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -322     |
|    critic_loss     | 1.69e+05 |
|    ent_coef        | 11       |
|    ent_coef_loss   | 0.922    |
|    learning_rate   | 0.0003   |
|    n_updates       | 212099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -348     |
|    critic_loss     | 1.84e+05 |
|    ent_coef        | 10.8     |
|    ent_coef_loss   | -0.915   |
|    learning_rate   | 0.0003   |
|    n_updates       | 212499   |
---------------------------------
=== Iterazione IRL 756 ===
Loss reward (iter 756): 9.525406837463379
=== Iterazione IRL 757 ===
Loss reward (iter 757): 1.5163499116897583
=== Iterazione IRL 758 ===
Loss reward (iter 758): 0.32657909393310547
=== Iterazione IRL 759 ===
Loss reward (iter 759): -4.904993057250977
=== Iterazione IRL 760 ===
Loss reward (iter 760): -9.460296630859375
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -231     |
|    critic_loss     | 1.22e+05 |
|    ent_coef        | 10.9     |
|    ent_coef_loss   | 1.09     |
|    learning_rate   | 0.0003   |
|    n_updates       | 213099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -98.1    |
|    critic_loss     | 4.1e+05  |
|    ent_coef        | 10.7     |
|    ent_coef_loss   | 0.687    |
|    learning_rate   | 0.0003   |
|    n_updates       | 213499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -200     |
|    critic_loss     | 4.54e+05 |
|    ent_coef        | 10.2     |
|    ent_coef_loss   | 0.513    |
|    learning_rate   | 0.0003   |
|    n_updates       | 213899   |
---------------------------------
=== Iterazione IRL 761 ===
Loss reward (iter 761): 13.948575973510742
=== Iterazione IRL 762 ===
Loss reward (iter 762): 6.152695655822754
=== Iterazione IRL 763 ===
Loss reward (iter 763): 2.3384275436401367
=== Iterazione IRL 764 ===
Loss reward (iter 764): 2.494126558303833
=== Iterazione IRL 765 ===
Loss reward (iter 765): 1.5131314992904663
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -49.5    |
|    critic_loss     | 2.66e+05 |
|    ent_coef        | 9.68     |
|    ent_coef_loss   | -0.875   |
|    learning_rate   | 0.0003   |
|    n_updates       | 214499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 22.5     |
|    critic_loss     | 2.16e+05 |
|    ent_coef        | 9.49     |
|    ent_coef_loss   | 0.0569   |
|    learning_rate   | 0.0003   |
|    n_updates       | 214899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.03     |
|    critic_loss     | 3.53e+05 |
|    ent_coef        | 9.4      |
|    ent_coef_loss   | 0.0873   |
|    learning_rate   | 0.0003   |
|    n_updates       | 215299   |
---------------------------------
=== Iterazione IRL 766 ===
Loss reward (iter 766): 2.863579273223877
=== Iterazione IRL 767 ===
Loss reward (iter 767): 2.756831645965576
=== Iterazione IRL 768 ===
Loss reward (iter 768): 0.17990970611572266
=== Iterazione IRL 769 ===
Loss reward (iter 769): 2.4354028701782227
=== Iterazione IRL 770 ===
Loss reward (iter 770): 0.7638077735900879
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -25.9    |
|    critic_loss     | 1.4e+05  |
|    ent_coef        | 9.45     |
|    ent_coef_loss   | -0.999   |
|    learning_rate   | 0.0003   |
|    n_updates       | 215899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 40.8     |
|    critic_loss     | 1.65e+05 |
|    ent_coef        | 9.42     |
|    ent_coef_loss   | -0.149   |
|    learning_rate   | 0.0003   |
|    n_updates       | 216299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 46.9     |
|    critic_loss     | 1.5e+05  |
|    ent_coef        | 9.67     |
|    ent_coef_loss   | -0.933   |
|    learning_rate   | 0.0003   |
|    n_updates       | 216699   |
---------------------------------
=== Iterazione IRL 771 ===
Loss reward (iter 771): -2.6893091201782227
=== Iterazione IRL 772 ===
Loss reward (iter 772): 0.527553915977478
=== Iterazione IRL 773 ===
Loss reward (iter 773): 1.078324556350708
=== Iterazione IRL 774 ===
Loss reward (iter 774): 2.007150650024414
=== Iterazione IRL 775 ===
Loss reward (iter 775): 0.9947454929351807
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 201      |
|    critic_loss     | 3.55e+05 |
|    ent_coef        | 9.85     |
|    ent_coef_loss   | 0.0391   |
|    learning_rate   | 0.0003   |
|    n_updates       | 217299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 71.9     |
|    critic_loss     | 1.12e+05 |
|    ent_coef        | 10.3     |
|    ent_coef_loss   | -0.331   |
|    learning_rate   | 0.0003   |
|    n_updates       | 217699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 129      |
|    critic_loss     | 7.65e+05 |
|    ent_coef        | 10.6     |
|    ent_coef_loss   | -0.344   |
|    learning_rate   | 0.0003   |
|    n_updates       | 218099   |
---------------------------------
=== Iterazione IRL 776 ===
Loss reward (iter 776): -4.210822582244873
=== Iterazione IRL 777 ===
Loss reward (iter 777): 4.847412109375
=== Iterazione IRL 778 ===
Loss reward (iter 778): -3.616163969039917
=== Iterazione IRL 779 ===
Loss reward (iter 779): -4.061460494995117
=== Iterazione IRL 780 ===
Loss reward (iter 780): 1.3915141820907593
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 291      |
|    critic_loss     | 9.43e+04 |
|    ent_coef        | 10.5     |
|    ent_coef_loss   | 0.413    |
|    learning_rate   | 0.0003   |
|    n_updates       | 218699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 152      |
|    critic_loss     | 1.49e+05 |
|    ent_coef        | 10.5     |
|    ent_coef_loss   | 0.455    |
|    learning_rate   | 0.0003   |
|    n_updates       | 219099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 141      |
|    critic_loss     | 9.03e+05 |
|    ent_coef        | 10.8     |
|    ent_coef_loss   | 0.0185   |
|    learning_rate   | 0.0003   |
|    n_updates       | 219499   |
---------------------------------
=== Iterazione IRL 781 ===
Loss reward (iter 781): -2.0447781085968018
=== Iterazione IRL 782 ===
Loss reward (iter 782): -7.611202716827393
=== Iterazione IRL 783 ===
Loss reward (iter 783): -5.794951915740967
=== Iterazione IRL 784 ===
Loss reward (iter 784): -7.423681259155273
=== Iterazione IRL 785 ===
Loss reward (iter 785): -2.277740955352783
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 322      |
|    critic_loss     | 4.56e+05 |
|    ent_coef        | 10.3     |
|    ent_coef_loss   | 0.0214   |
|    learning_rate   | 0.0003   |
|    n_updates       | 220099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 322      |
|    critic_loss     | 1.61e+05 |
|    ent_coef        | 10.4     |
|    ent_coef_loss   | 0.253    |
|    learning_rate   | 0.0003   |
|    n_updates       | 220499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 497      |
|    critic_loss     | 1.2e+05  |
|    ent_coef        | 10.8     |
|    ent_coef_loss   | 0.00702  |
|    learning_rate   | 0.0003   |
|    n_updates       | 220899   |
---------------------------------
=== Iterazione IRL 786 ===
Loss reward (iter 786): -2.0513577461242676
=== Iterazione IRL 787 ===
Loss reward (iter 787): 2.5550408363342285
=== Iterazione IRL 788 ===
Loss reward (iter 788): -0.23374295234680176
=== Iterazione IRL 789 ===
Loss reward (iter 789): -2.8143560886383057
=== Iterazione IRL 790 ===
Loss reward (iter 790): -5.586206912994385
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 389      |
|    critic_loss     | 6.03e+05 |
|    ent_coef        | 10.9     |
|    ent_coef_loss   | -0.123   |
|    learning_rate   | 0.0003   |
|    n_updates       | 221499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 270      |
|    critic_loss     | 2.03e+05 |
|    ent_coef        | 10.7     |
|    ent_coef_loss   | 1.46     |
|    learning_rate   | 0.0003   |
|    n_updates       | 221899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 439      |
|    critic_loss     | 1.02e+05 |
|    ent_coef        | 10.6     |
|    ent_coef_loss   | -0.153   |
|    learning_rate   | 0.0003   |
|    n_updates       | 222299   |
---------------------------------
=== Iterazione IRL 791 ===
Loss reward (iter 791): -1.0654326677322388
=== Iterazione IRL 792 ===
Loss reward (iter 792): -1.5610281229019165
=== Iterazione IRL 793 ===
Loss reward (iter 793): -3.2617475986480713
=== Iterazione IRL 794 ===
Loss reward (iter 794): -5.6141180992126465
=== Iterazione IRL 795 ===
Loss reward (iter 795): 1.4802746772766113
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 502      |
|    critic_loss     | 2.84e+05 |
|    ent_coef        | 10.7     |
|    ent_coef_loss   | 0.534    |
|    learning_rate   | 0.0003   |
|    n_updates       | 222899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 380      |
|    critic_loss     | 3.52e+05 |
|    ent_coef        | 10.9     |
|    ent_coef_loss   | -0.148   |
|    learning_rate   | 0.0003   |
|    n_updates       | 223299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 648      |
|    critic_loss     | 1.61e+05 |
|    ent_coef        | 10.7     |
|    ent_coef_loss   | 0.136    |
|    learning_rate   | 0.0003   |
|    n_updates       | 223699   |
---------------------------------
=== Iterazione IRL 796 ===
Loss reward (iter 796): -1.3305107355117798
=== Iterazione IRL 797 ===
Loss reward (iter 797): 0.2246631383895874
=== Iterazione IRL 798 ===
Loss reward (iter 798): 3.6237072944641113
=== Iterazione IRL 799 ===
Loss reward (iter 799): 6.849956035614014
=== Iterazione IRL 800 ===
Loss reward (iter 800): 1.3500643968582153
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 535      |
|    critic_loss     | 5.06e+05 |
|    ent_coef        | 10.8     |
|    ent_coef_loss   | 0.683    |
|    learning_rate   | 0.0003   |
|    n_updates       | 224299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 494      |
|    critic_loss     | 1.53e+05 |
|    ent_coef        | 10.5     |
|    ent_coef_loss   | -0.493   |
|    learning_rate   | 0.0003   |
|    n_updates       | 224699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 598      |
|    critic_loss     | 3.9e+05  |
|    ent_coef        | 10.7     |
|    ent_coef_loss   | 1        |
|    learning_rate   | 0.0003   |
|    n_updates       | 225099   |
---------------------------------
=== Iterazione IRL 801 ===
Loss reward (iter 801): -1.3529818058013916
=== Iterazione IRL 802 ===
Loss reward (iter 802): -2.5009660720825195
=== Iterazione IRL 803 ===
Loss reward (iter 803): -1.5748870372772217
=== Iterazione IRL 804 ===
Loss reward (iter 804): -4.625964164733887
=== Iterazione IRL 805 ===
Loss reward (iter 805): 0.13348698616027832
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 518      |
|    critic_loss     | 3.58e+05 |
|    ent_coef        | 10.4     |
|    ent_coef_loss   | 0.309    |
|    learning_rate   | 0.0003   |
|    n_updates       | 225699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 690      |
|    critic_loss     | 1.82e+05 |
|    ent_coef        | 10.2     |
|    ent_coef_loss   | 0.772    |
|    learning_rate   | 0.0003   |
|    n_updates       | 226099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 676      |
|    critic_loss     | 1.26e+05 |
|    ent_coef        | 10.5     |
|    ent_coef_loss   | 0.437    |
|    learning_rate   | 0.0003   |
|    n_updates       | 226499   |
---------------------------------
=== Iterazione IRL 806 ===
Loss reward (iter 806): -0.17311108112335205
=== Iterazione IRL 807 ===
Loss reward (iter 807): -1.702862024307251
=== Iterazione IRL 808 ===
Loss reward (iter 808): -4.096609592437744
=== Iterazione IRL 809 ===
Loss reward (iter 809): -3.291489601135254
=== Iterazione IRL 810 ===
Loss reward (iter 810): 0.7492440938949585
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 758      |
|    critic_loss     | 2.78e+05 |
|    ent_coef        | 10.7     |
|    ent_coef_loss   | -0.179   |
|    learning_rate   | 0.0003   |
|    n_updates       | 227099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 653      |
|    critic_loss     | 1.67e+05 |
|    ent_coef        | 10.5     |
|    ent_coef_loss   | -1.13    |
|    learning_rate   | 0.0003   |
|    n_updates       | 227499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 731      |
|    critic_loss     | 5.17e+05 |
|    ent_coef        | 10.4     |
|    ent_coef_loss   | 0.0811   |
|    learning_rate   | 0.0003   |
|    n_updates       | 227899   |
---------------------------------
=== Iterazione IRL 811 ===
Loss reward (iter 811): 1.884411334991455
=== Iterazione IRL 812 ===
Loss reward (iter 812): -3.9882192611694336
=== Iterazione IRL 813 ===
Loss reward (iter 813): 0.3742560148239136
=== Iterazione IRL 814 ===
Loss reward (iter 814): -3.322474479675293
=== Iterazione IRL 815 ===
Loss reward (iter 815): -1.3541111946105957
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 732      |
|    critic_loss     | 1.42e+05 |
|    ent_coef        | 10.3     |
|    ent_coef_loss   | 0.145    |
|    learning_rate   | 0.0003   |
|    n_updates       | 228499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 629      |
|    critic_loss     | 1.08e+05 |
|    ent_coef        | 10.5     |
|    ent_coef_loss   | 0.969    |
|    learning_rate   | 0.0003   |
|    n_updates       | 228899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 882      |
|    critic_loss     | 4.27e+05 |
|    ent_coef        | 10.2     |
|    ent_coef_loss   | -0.515   |
|    learning_rate   | 0.0003   |
|    n_updates       | 229299   |
---------------------------------
=== Iterazione IRL 816 ===
Loss reward (iter 816): 4.574189186096191
=== Iterazione IRL 817 ===
Loss reward (iter 817): -0.9844383001327515
=== Iterazione IRL 818 ===
Loss reward (iter 818): -1.7500544786453247
=== Iterazione IRL 819 ===
Loss reward (iter 819): -3.635050058364868
=== Iterazione IRL 820 ===
Loss reward (iter 820): -3.302793264389038
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 823      |
|    critic_loss     | 1.75e+05 |
|    ent_coef        | 10.3     |
|    ent_coef_loss   | 0.116    |
|    learning_rate   | 0.0003   |
|    n_updates       | 229899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 772      |
|    critic_loss     | 5.72e+05 |
|    ent_coef        | 10.1     |
|    ent_coef_loss   | 0.221    |
|    learning_rate   | 0.0003   |
|    n_updates       | 230299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 692      |
|    critic_loss     | 1.92e+05 |
|    ent_coef        | 10.1     |
|    ent_coef_loss   | -0.0646  |
|    learning_rate   | 0.0003   |
|    n_updates       | 230699   |
---------------------------------
=== Iterazione IRL 821 ===
Loss reward (iter 821): 10.910204887390137
=== Iterazione IRL 822 ===
Loss reward (iter 822): 1.1311908960342407
=== Iterazione IRL 823 ===
Loss reward (iter 823): 0.8150736093521118
=== Iterazione IRL 824 ===
Loss reward (iter 824): 0.11177265644073486
=== Iterazione IRL 825 ===
Loss reward (iter 825): -2.6640429496765137
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 934      |
|    critic_loss     | 3.61e+05 |
|    ent_coef        | 10.3     |
|    ent_coef_loss   | 0.146    |
|    learning_rate   | 0.0003   |
|    n_updates       | 231299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 815      |
|    critic_loss     | 3.61e+05 |
|    ent_coef        | 10.5     |
|    ent_coef_loss   | 0.391    |
|    learning_rate   | 0.0003   |
|    n_updates       | 231699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 793      |
|    critic_loss     | 2.31e+05 |
|    ent_coef        | 10.7     |
|    ent_coef_loss   | 0.204    |
|    learning_rate   | 0.0003   |
|    n_updates       | 232099   |
---------------------------------
=== Iterazione IRL 826 ===
Loss reward (iter 826): -1.8832069635391235
=== Iterazione IRL 827 ===
Loss reward (iter 827): 1.4652329683303833
=== Iterazione IRL 828 ===
Loss reward (iter 828): -6.685925006866455
=== Iterazione IRL 829 ===
Loss reward (iter 829): -0.608198881149292
=== Iterazione IRL 830 ===
Loss reward (iter 830): 0.10722362995147705
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 933      |
|    critic_loss     | 1.5e+05  |
|    ent_coef        | 10.9     |
|    ent_coef_loss   | 0.0268   |
|    learning_rate   | 0.0003   |
|    n_updates       | 232699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 898      |
|    critic_loss     | 2.58e+05 |
|    ent_coef        | 11       |
|    ent_coef_loss   | 0.0431   |
|    learning_rate   | 0.0003   |
|    n_updates       | 233099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 896      |
|    critic_loss     | 6.96e+05 |
|    ent_coef        | 11.1     |
|    ent_coef_loss   | 0.0659   |
|    learning_rate   | 0.0003   |
|    n_updates       | 233499   |
---------------------------------
=== Iterazione IRL 831 ===
Loss reward (iter 831): -6.221903324127197
=== Iterazione IRL 832 ===
Loss reward (iter 832): -1.9179431200027466
=== Iterazione IRL 833 ===
Loss reward (iter 833): -3.057108163833618
=== Iterazione IRL 834 ===
Loss reward (iter 834): -5.336619853973389
=== Iterazione IRL 835 ===
Loss reward (iter 835): -6.584322452545166
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 928      |
|    critic_loss     | 1.47e+05 |
|    ent_coef        | 11       |
|    ent_coef_loss   | 0.994    |
|    learning_rate   | 0.0003   |
|    n_updates       | 234099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 920      |
|    critic_loss     | 2.64e+05 |
|    ent_coef        | 10.8     |
|    ent_coef_loss   | 0.246    |
|    learning_rate   | 0.0003   |
|    n_updates       | 234499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 917      |
|    critic_loss     | 4.37e+05 |
|    ent_coef        | 10.7     |
|    ent_coef_loss   | -0.34    |
|    learning_rate   | 0.0003   |
|    n_updates       | 234899   |
---------------------------------
=== Iterazione IRL 836 ===
Loss reward (iter 836): -3.9116663932800293
=== Iterazione IRL 837 ===
Loss reward (iter 837): -8.075078964233398
=== Iterazione IRL 838 ===
Loss reward (iter 838): -5.603849411010742
=== Iterazione IRL 839 ===
Loss reward (iter 839): -5.6404571533203125
=== Iterazione IRL 840 ===
Loss reward (iter 840): -3.69650936126709
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 999      |
|    critic_loss     | 1.64e+05 |
|    ent_coef        | 10.8     |
|    ent_coef_loss   | 0.081    |
|    learning_rate   | 0.0003   |
|    n_updates       | 235499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.04e+03 |
|    critic_loss     | 1.17e+05 |
|    ent_coef        | 11       |
|    ent_coef_loss   | 0.655    |
|    learning_rate   | 0.0003   |
|    n_updates       | 235899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 990      |
|    critic_loss     | 1.01e+05 |
|    ent_coef        | 10.8     |
|    ent_coef_loss   | 0.57     |
|    learning_rate   | 0.0003   |
|    n_updates       | 236299   |
---------------------------------
=== Iterazione IRL 841 ===
Loss reward (iter 841): -1.614141821861267
=== Iterazione IRL 842 ===
Loss reward (iter 842): -5.467810153961182
=== Iterazione IRL 843 ===
Loss reward (iter 843): -4.418001174926758
=== Iterazione IRL 844 ===
Loss reward (iter 844): -5.760960578918457
=== Iterazione IRL 845 ===
Loss reward (iter 845): -4.732776641845703
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.18e+03 |
|    critic_loss     | 8.08e+05 |
|    ent_coef        | 10.9     |
|    ent_coef_loss   | 0.468    |
|    learning_rate   | 0.0003   |
|    n_updates       | 236899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.09e+03 |
|    critic_loss     | 5.38e+04 |
|    ent_coef        | 10.9     |
|    ent_coef_loss   | 0.437    |
|    learning_rate   | 0.0003   |
|    n_updates       | 237299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.06e+03 |
|    critic_loss     | 9.52e+04 |
|    ent_coef        | 11.1     |
|    ent_coef_loss   | -0.133   |
|    learning_rate   | 0.0003   |
|    n_updates       | 237699   |
---------------------------------
=== Iterazione IRL 846 ===
Loss reward (iter 846): -1.5421074628829956
=== Iterazione IRL 847 ===
Loss reward (iter 847): 1.3522429466247559
=== Iterazione IRL 848 ===
Loss reward (iter 848): -0.4616607427597046
=== Iterazione IRL 849 ===
Loss reward (iter 849): -0.9688125848770142
=== Iterazione IRL 850 ===
Loss reward (iter 850): 0.6128082275390625
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.04e+03 |
|    critic_loss     | 1.76e+05 |
|    ent_coef        | 11.2     |
|    ent_coef_loss   | -0.308   |
|    learning_rate   | 0.0003   |
|    n_updates       | 238299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.17e+03 |
|    critic_loss     | 6.29e+04 |
|    ent_coef        | 12.1     |
|    ent_coef_loss   | 0.306    |
|    learning_rate   | 0.0003   |
|    n_updates       | 238699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.12e+03 |
|    critic_loss     | 1.78e+05 |
|    ent_coef        | 12       |
|    ent_coef_loss   | -0.0113  |
|    learning_rate   | 0.0003   |
|    n_updates       | 239099   |
---------------------------------
=== Iterazione IRL 851 ===
Loss reward (iter 851): -20.12327766418457
=== Iterazione IRL 852 ===
Loss reward (iter 852): -19.858266830444336
=== Iterazione IRL 853 ===
Loss reward (iter 853): -2.001336097717285
=== Iterazione IRL 854 ===
Loss reward (iter 854): -1.0463130474090576
=== Iterazione IRL 855 ===
Loss reward (iter 855): -4.426059722900391
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.15e+03 |
|    critic_loss     | 9.2e+04  |
|    ent_coef        | 11.6     |
|    ent_coef_loss   | 0.819    |
|    learning_rate   | 0.0003   |
|    n_updates       | 239699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.15e+03 |
|    critic_loss     | 1.57e+05 |
|    ent_coef        | 11.1     |
|    ent_coef_loss   | 0.018    |
|    learning_rate   | 0.0003   |
|    n_updates       | 240099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.37e+03 |
|    critic_loss     | 2.36e+05 |
|    ent_coef        | 10.9     |
|    ent_coef_loss   | -1.25    |
|    learning_rate   | 0.0003   |
|    n_updates       | 240499   |
---------------------------------
=== Iterazione IRL 856 ===
Loss reward (iter 856): -2.644141912460327
=== Iterazione IRL 857 ===
Loss reward (iter 857): -0.4008824825286865
=== Iterazione IRL 858 ===
Loss reward (iter 858): -6.835046291351318
=== Iterazione IRL 859 ===
Loss reward (iter 859): -2.7108068466186523
=== Iterazione IRL 860 ===
Loss reward (iter 860): -2.865621566772461
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.35e+03 |
|    critic_loss     | 2.41e+05 |
|    ent_coef        | 10.7     |
|    ent_coef_loss   | 0.276    |
|    learning_rate   | 0.0003   |
|    n_updates       | 241099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.28e+03 |
|    critic_loss     | 2.01e+05 |
|    ent_coef        | 10.6     |
|    ent_coef_loss   | -0.873   |
|    learning_rate   | 0.0003   |
|    n_updates       | 241499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.22e+03 |
|    critic_loss     | 4.4e+05  |
|    ent_coef        | 11       |
|    ent_coef_loss   | -0.22    |
|    learning_rate   | 0.0003   |
|    n_updates       | 241899   |
---------------------------------
=== Iterazione IRL 861 ===
Loss reward (iter 861): -2.783881187438965
=== Iterazione IRL 862 ===
Loss reward (iter 862): -8.680665016174316
=== Iterazione IRL 863 ===
Loss reward (iter 863): -5.739745140075684
=== Iterazione IRL 864 ===
Loss reward (iter 864): -13.523050308227539
=== Iterazione IRL 865 ===
Loss reward (iter 865): -11.987129211425781
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.47e+03 |
|    critic_loss     | 1.46e+05 |
|    ent_coef        | 10.6     |
|    ent_coef_loss   | -0.933   |
|    learning_rate   | 0.0003   |
|    n_updates       | 242499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.22e+03 |
|    critic_loss     | 1.81e+05 |
|    ent_coef        | 9.94     |
|    ent_coef_loss   | -0.0874  |
|    learning_rate   | 0.0003   |
|    n_updates       | 242899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.17e+03 |
|    critic_loss     | 5.1e+05  |
|    ent_coef        | 9.86     |
|    ent_coef_loss   | -0.349   |
|    learning_rate   | 0.0003   |
|    n_updates       | 243299   |
---------------------------------
=== Iterazione IRL 866 ===
Loss reward (iter 866): -4.832698822021484
=== Iterazione IRL 867 ===
Loss reward (iter 867): -5.247724533081055
=== Iterazione IRL 868 ===
Loss reward (iter 868): -7.622830390930176
=== Iterazione IRL 869 ===
Loss reward (iter 869): -2.661818265914917
=== Iterazione IRL 870 ===
Loss reward (iter 870): 0.2543759346008301
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.43e+03 |
|    critic_loss     | 4.81e+05 |
|    ent_coef        | 9.95     |
|    ent_coef_loss   | -0.564   |
|    learning_rate   | 0.0003   |
|    n_updates       | 243899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.38e+03 |
|    critic_loss     | 1.55e+05 |
|    ent_coef        | 10       |
|    ent_coef_loss   | 0.451    |
|    learning_rate   | 0.0003   |
|    n_updates       | 244299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.44e+03 |
|    critic_loss     | 1.28e+05 |
|    ent_coef        | 10.1     |
|    ent_coef_loss   | -0.479   |
|    learning_rate   | 0.0003   |
|    n_updates       | 244699   |
---------------------------------
=== Iterazione IRL 871 ===
Loss reward (iter 871): -1.0943679809570312
=== Iterazione IRL 872 ===
Loss reward (iter 872): -2.8357994556427
=== Iterazione IRL 873 ===
Loss reward (iter 873): 3.1351780891418457
=== Iterazione IRL 874 ===
Loss reward (iter 874): 1.520559549331665
=== Iterazione IRL 875 ===
Loss reward (iter 875): -2.175720453262329
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.35e+03 |
|    critic_loss     | 1.27e+05 |
|    ent_coef        | 9.94     |
|    ent_coef_loss   | -0.2     |
|    learning_rate   | 0.0003   |
|    n_updates       | 245299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.56e+03 |
|    critic_loss     | 1.98e+05 |
|    ent_coef        | 9.85     |
|    ent_coef_loss   | -0.316   |
|    learning_rate   | 0.0003   |
|    n_updates       | 245699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.65e+03 |
|    critic_loss     | 9.51e+04 |
|    ent_coef        | 9.91     |
|    ent_coef_loss   | -0.467   |
|    learning_rate   | 0.0003   |
|    n_updates       | 246099   |
---------------------------------
=== Iterazione IRL 876 ===
Loss reward (iter 876): -4.3807501792907715
=== Iterazione IRL 877 ===
Loss reward (iter 877): -11.975868225097656
=== Iterazione IRL 878 ===
Loss reward (iter 878): -3.2656617164611816
=== Iterazione IRL 879 ===
Loss reward (iter 879): -8.810853958129883
=== Iterazione IRL 880 ===
Loss reward (iter 880): -12.411312103271484
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.69e+03 |
|    critic_loss     | 4.12e+05 |
|    ent_coef        | 9.81     |
|    ent_coef_loss   | 0.214    |
|    learning_rate   | 0.0003   |
|    n_updates       | 246699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.48e+03 |
|    critic_loss     | 4.42e+05 |
|    ent_coef        | 9.66     |
|    ent_coef_loss   | -0.296   |
|    learning_rate   | 0.0003   |
|    n_updates       | 247099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.64e+03 |
|    critic_loss     | 4.56e+05 |
|    ent_coef        | 9.32     |
|    ent_coef_loss   | 0.213    |
|    learning_rate   | 0.0003   |
|    n_updates       | 247499   |
---------------------------------
=== Iterazione IRL 881 ===
Loss reward (iter 881): 0.8840556144714355
=== Iterazione IRL 882 ===
Loss reward (iter 882): -0.6301870346069336
=== Iterazione IRL 883 ===
Loss reward (iter 883): -1.4810025691986084
=== Iterazione IRL 884 ===
Loss reward (iter 884): 0.7176511287689209
=== Iterazione IRL 885 ===
Loss reward (iter 885): -1.574972152709961
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.75e+03 |
|    critic_loss     | 9.46e+04 |
|    ent_coef        | 9.46     |
|    ent_coef_loss   | 0.206    |
|    learning_rate   | 0.0003   |
|    n_updates       | 248099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.62e+03 |
|    critic_loss     | 1.43e+05 |
|    ent_coef        | 9.49     |
|    ent_coef_loss   | 0.387    |
|    learning_rate   | 0.0003   |
|    n_updates       | 248499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.7e+03  |
|    critic_loss     | 5.19e+05 |
|    ent_coef        | 9.45     |
|    ent_coef_loss   | 1.03     |
|    learning_rate   | 0.0003   |
|    n_updates       | 248899   |
---------------------------------
=== Iterazione IRL 886 ===
Loss reward (iter 886): -6.100608825683594
=== Iterazione IRL 887 ===
Loss reward (iter 887): -8.753826141357422
=== Iterazione IRL 888 ===
Loss reward (iter 888): -8.481245040893555
=== Iterazione IRL 889 ===
Loss reward (iter 889): -10.882567405700684
=== Iterazione IRL 890 ===
Loss reward (iter 890): -1.9927387237548828
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.71e+03 |
|    critic_loss     | 4.46e+05 |
|    ent_coef        | 9.29     |
|    ent_coef_loss   | -0.0607  |
|    learning_rate   | 0.0003   |
|    n_updates       | 249499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.7e+03  |
|    critic_loss     | 5.8e+05  |
|    ent_coef        | 9.43     |
|    ent_coef_loss   | 0.192    |
|    learning_rate   | 0.0003   |
|    n_updates       | 249899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.64e+03 |
|    critic_loss     | 1.08e+05 |
|    ent_coef        | 9.41     |
|    ent_coef_loss   | 0.976    |
|    learning_rate   | 0.0003   |
|    n_updates       | 250299   |
---------------------------------
=== Iterazione IRL 891 ===
Loss reward (iter 891): 4.491870880126953
=== Iterazione IRL 892 ===
Loss reward (iter 892): 2.989503860473633
=== Iterazione IRL 893 ===
Loss reward (iter 893): 1.4139714241027832
=== Iterazione IRL 894 ===
Loss reward (iter 894): 2.2721776962280273
=== Iterazione IRL 895 ===
Loss reward (iter 895): 0.12195539474487305
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.88e+03 |
|    critic_loss     | 1.36e+05 |
|    ent_coef        | 9.12     |
|    ent_coef_loss   | -0.783   |
|    learning_rate   | 0.0003   |
|    n_updates       | 250899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.6e+03  |
|    critic_loss     | 4.48e+05 |
|    ent_coef        | 9.1      |
|    ent_coef_loss   | 0.366    |
|    learning_rate   | 0.0003   |
|    n_updates       | 251299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.84e+03 |
|    critic_loss     | 4.18e+05 |
|    ent_coef        | 9.04     |
|    ent_coef_loss   | -0.318   |
|    learning_rate   | 0.0003   |
|    n_updates       | 251699   |
---------------------------------
=== Iterazione IRL 896 ===
Loss reward (iter 896): -3.2795357704162598
=== Iterazione IRL 897 ===
Loss reward (iter 897): -4.288356781005859
=== Iterazione IRL 898 ===
Loss reward (iter 898): -2.0969061851501465
=== Iterazione IRL 899 ===
Loss reward (iter 899): -2.4559266567230225
=== Iterazione IRL 900 ===
Loss reward (iter 900): -4.272623062133789
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.91e+03 |
|    critic_loss     | 2.57e+05 |
|    ent_coef        | 9.1      |
|    ent_coef_loss   | 0.276    |
|    learning_rate   | 0.0003   |
|    n_updates       | 252299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.76e+03 |
|    critic_loss     | 4.13e+05 |
|    ent_coef        | 8.98     |
|    ent_coef_loss   | 0.601    |
|    learning_rate   | 0.0003   |
|    n_updates       | 252699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.8e+03  |
|    critic_loss     | 9.34e+04 |
|    ent_coef        | 9.17     |
|    ent_coef_loss   | 0.596    |
|    learning_rate   | 0.0003   |
|    n_updates       | 253099   |
---------------------------------
=== Iterazione IRL 901 ===
Loss reward (iter 901): -1.9093313217163086
=== Iterazione IRL 902 ===
Loss reward (iter 902): -3.359076738357544
=== Iterazione IRL 903 ===
Loss reward (iter 903): 1.5169827938079834
=== Iterazione IRL 904 ===
Loss reward (iter 904): -1.7537739276885986
=== Iterazione IRL 905 ===
Loss reward (iter 905): 0.2132251262664795
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.9e+03  |
|    critic_loss     | 1.66e+05 |
|    ent_coef        | 9.07     |
|    ent_coef_loss   | 0.69     |
|    learning_rate   | 0.0003   |
|    n_updates       | 253699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.08e+03 |
|    critic_loss     | 1.79e+05 |
|    ent_coef        | 8.85     |
|    ent_coef_loss   | 0.595    |
|    learning_rate   | 0.0003   |
|    n_updates       | 254099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.07e+03 |
|    critic_loss     | 3.77e+05 |
|    ent_coef        | 8.92     |
|    ent_coef_loss   | -0.58    |
|    learning_rate   | 0.0003   |
|    n_updates       | 254499   |
---------------------------------
=== Iterazione IRL 906 ===
Loss reward (iter 906): 2.1897757053375244
=== Iterazione IRL 907 ===
Loss reward (iter 907): -0.9020593166351318
=== Iterazione IRL 908 ===
Loss reward (iter 908): 0.9677414894104004
=== Iterazione IRL 909 ===
Loss reward (iter 909): -3.2646663188934326
=== Iterazione IRL 910 ===
Loss reward (iter 910): -5.660922050476074
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.99e+03 |
|    critic_loss     | 2.52e+05 |
|    ent_coef        | 8.85     |
|    ent_coef_loss   | -0.361   |
|    learning_rate   | 0.0003   |
|    n_updates       | 255099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.13e+03 |
|    critic_loss     | 1.04e+05 |
|    ent_coef        | 8.92     |
|    ent_coef_loss   | 0.11     |
|    learning_rate   | 0.0003   |
|    n_updates       | 255499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.23e+03 |
|    critic_loss     | 1.33e+05 |
|    ent_coef        | 8.87     |
|    ent_coef_loss   | -0.0978  |
|    learning_rate   | 0.0003   |
|    n_updates       | 255899   |
---------------------------------
=== Iterazione IRL 911 ===
Loss reward (iter 911): 2.4926702976226807
=== Iterazione IRL 912 ===
Loss reward (iter 912): -2.9339547157287598
=== Iterazione IRL 913 ===
Loss reward (iter 913): -4.064055442810059
=== Iterazione IRL 914 ===
Loss reward (iter 914): -0.22322750091552734
=== Iterazione IRL 915 ===
Loss reward (iter 915): -4.291713714599609
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.06e+03 |
|    critic_loss     | 4.8e+05  |
|    ent_coef        | 8.71     |
|    ent_coef_loss   | -0.164   |
|    learning_rate   | 0.0003   |
|    n_updates       | 256499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.17e+03 |
|    critic_loss     | 1.7e+05  |
|    ent_coef        | 8.64     |
|    ent_coef_loss   | -0.607   |
|    learning_rate   | 0.0003   |
|    n_updates       | 256899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.05e+03 |
|    critic_loss     | 1.7e+05  |
|    ent_coef        | 8.74     |
|    ent_coef_loss   | 0.566    |
|    learning_rate   | 0.0003   |
|    n_updates       | 257299   |
---------------------------------
=== Iterazione IRL 916 ===
Loss reward (iter 916): -1.696929931640625
=== Iterazione IRL 917 ===
Loss reward (iter 917): 3.1120617389678955
=== Iterazione IRL 918 ===
Loss reward (iter 918): -0.8850939273834229
=== Iterazione IRL 919 ===
Loss reward (iter 919): -1.091315746307373
=== Iterazione IRL 920 ===
Loss reward (iter 920): -2.093086004257202
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.14e+03 |
|    critic_loss     | 1.49e+05 |
|    ent_coef        | 8.64     |
|    ent_coef_loss   | -0.401   |
|    learning_rate   | 0.0003   |
|    n_updates       | 257899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.35e+03 |
|    critic_loss     | 1.54e+05 |
|    ent_coef        | 8.76     |
|    ent_coef_loss   | -0.534   |
|    learning_rate   | 0.0003   |
|    n_updates       | 258299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.26e+03 |
|    critic_loss     | 3.98e+05 |
|    ent_coef        | 8.63     |
|    ent_coef_loss   | -0.0851  |
|    learning_rate   | 0.0003   |
|    n_updates       | 258699   |
---------------------------------
=== Iterazione IRL 921 ===
Loss reward (iter 921): -4.141846179962158
=== Iterazione IRL 922 ===
Loss reward (iter 922): -7.375943183898926
=== Iterazione IRL 923 ===
Loss reward (iter 923): -11.074728012084961
=== Iterazione IRL 924 ===
Loss reward (iter 924): -7.102041721343994
=== Iterazione IRL 925 ===
Loss reward (iter 925): -4.290861129760742
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.23e+03 |
|    critic_loss     | 4.54e+05 |
|    ent_coef        | 8.7      |
|    ent_coef_loss   | 0.329    |
|    learning_rate   | 0.0003   |
|    n_updates       | 259299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.2e+03  |
|    critic_loss     | 2.14e+05 |
|    ent_coef        | 8.61     |
|    ent_coef_loss   | 0.461    |
|    learning_rate   | 0.0003   |
|    n_updates       | 259699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.31e+03 |
|    critic_loss     | 2.83e+05 |
|    ent_coef        | 8.58     |
|    ent_coef_loss   | -0.59    |
|    learning_rate   | 0.0003   |
|    n_updates       | 260099   |
---------------------------------
=== Iterazione IRL 926 ===
Loss reward (iter 926): -9.287351608276367
=== Iterazione IRL 927 ===
Loss reward (iter 927): -6.846440315246582
=== Iterazione IRL 928 ===
Loss reward (iter 928): -3.447348117828369
=== Iterazione IRL 929 ===
Loss reward (iter 929): -6.25714111328125
=== Iterazione IRL 930 ===
Loss reward (iter 930): -5.099395275115967
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.23e+03 |
|    critic_loss     | 1.43e+05 |
|    ent_coef        | 8.66     |
|    ent_coef_loss   | -0.152   |
|    learning_rate   | 0.0003   |
|    n_updates       | 260699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.3e+03  |
|    critic_loss     | 3.28e+05 |
|    ent_coef        | 8.7      |
|    ent_coef_loss   | -0.126   |
|    learning_rate   | 0.0003   |
|    n_updates       | 261099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.29e+03 |
|    critic_loss     | 3.73e+05 |
|    ent_coef        | 8.78     |
|    ent_coef_loss   | -0.114   |
|    learning_rate   | 0.0003   |
|    n_updates       | 261499   |
---------------------------------
=== Iterazione IRL 931 ===
Loss reward (iter 931): -5.081673622131348
=== Iterazione IRL 932 ===
Loss reward (iter 932): -0.43344736099243164
=== Iterazione IRL 933 ===
Loss reward (iter 933): -6.572103023529053
=== Iterazione IRL 934 ===
Loss reward (iter 934): -11.59048080444336
=== Iterazione IRL 935 ===
Loss reward (iter 935): -10.496526718139648
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.25e+03 |
|    critic_loss     | 1.92e+05 |
|    ent_coef        | 8.79     |
|    ent_coef_loss   | 0.427    |
|    learning_rate   | 0.0003   |
|    n_updates       | 262099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.35e+03 |
|    critic_loss     | 3.47e+05 |
|    ent_coef        | 8.67     |
|    ent_coef_loss   | 0.521    |
|    learning_rate   | 0.0003   |
|    n_updates       | 262499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.45e+03 |
|    critic_loss     | 1.85e+05 |
|    ent_coef        | 8.75     |
|    ent_coef_loss   | 0.314    |
|    learning_rate   | 0.0003   |
|    n_updates       | 262899   |
---------------------------------
=== Iterazione IRL 936 ===
Loss reward (iter 936): -2.7540082931518555
=== Iterazione IRL 937 ===
Loss reward (iter 937): -7.740939617156982
=== Iterazione IRL 938 ===
Loss reward (iter 938): -4.310383319854736
=== Iterazione IRL 939 ===
Loss reward (iter 939): -5.946724891662598
=== Iterazione IRL 940 ===
Loss reward (iter 940): -5.8504438400268555
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.44e+03 |
|    critic_loss     | 2.13e+05 |
|    ent_coef        | 8.8      |
|    ent_coef_loss   | -0.309   |
|    learning_rate   | 0.0003   |
|    n_updates       | 263499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.48e+03 |
|    critic_loss     | 1.73e+05 |
|    ent_coef        | 8.62     |
|    ent_coef_loss   | 0.296    |
|    learning_rate   | 0.0003   |
|    n_updates       | 263899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.33e+03 |
|    critic_loss     | 2.88e+05 |
|    ent_coef        | 8.66     |
|    ent_coef_loss   | -0.291   |
|    learning_rate   | 0.0003   |
|    n_updates       | 264299   |
---------------------------------
=== Iterazione IRL 941 ===
Loss reward (iter 941): -2.8056461811065674
=== Iterazione IRL 942 ===
Loss reward (iter 942): -9.061354637145996
=== Iterazione IRL 943 ===
Loss reward (iter 943): -6.803483963012695
=== Iterazione IRL 944 ===
Loss reward (iter 944): -8.751145362854004
=== Iterazione IRL 945 ===
Loss reward (iter 945): -10.787506103515625
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.5e+03  |
|    critic_loss     | 3.9e+05  |
|    ent_coef        | 8.5      |
|    ent_coef_loss   | 0.217    |
|    learning_rate   | 0.0003   |
|    n_updates       | 264899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.44e+03 |
|    critic_loss     | 4.1e+05  |
|    ent_coef        | 8.53     |
|    ent_coef_loss   | -0.331   |
|    learning_rate   | 0.0003   |
|    n_updates       | 265299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.43e+03 |
|    critic_loss     | 1.47e+05 |
|    ent_coef        | 8.73     |
|    ent_coef_loss   | 0.0497   |
|    learning_rate   | 0.0003   |
|    n_updates       | 265699   |
---------------------------------
=== Iterazione IRL 946 ===
Loss reward (iter 946): -6.566884994506836
=== Iterazione IRL 947 ===
Loss reward (iter 947): -11.458185195922852
=== Iterazione IRL 948 ===
Loss reward (iter 948): -15.826935768127441
=== Iterazione IRL 949 ===
Loss reward (iter 949): -12.031179428100586
=== Iterazione IRL 950 ===
Loss reward (iter 950): -14.49601936340332
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.46e+03 |
|    critic_loss     | 1.1e+05  |
|    ent_coef        | 8.65     |
|    ent_coef_loss   | 0.737    |
|    learning_rate   | 0.0003   |
|    n_updates       | 266299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.38e+03 |
|    critic_loss     | 1.23e+05 |
|    ent_coef        | 8.41     |
|    ent_coef_loss   | 0.00896  |
|    learning_rate   | 0.0003   |
|    n_updates       | 266699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.56e+03 |
|    critic_loss     | 1.54e+05 |
|    ent_coef        | 8.3      |
|    ent_coef_loss   | -0.0683  |
|    learning_rate   | 0.0003   |
|    n_updates       | 267099   |
---------------------------------
=== Iterazione IRL 951 ===
Loss reward (iter 951): -6.237445831298828
=== Iterazione IRL 952 ===
Loss reward (iter 952): 1.026761531829834
=== Iterazione IRL 953 ===
Loss reward (iter 953): -8.94486141204834
=== Iterazione IRL 954 ===
Loss reward (iter 954): -4.957110404968262
=== Iterazione IRL 955 ===
Loss reward (iter 955): -12.720064163208008
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.51e+03 |
|    critic_loss     | 1.38e+05 |
|    ent_coef        | 8.24     |
|    ent_coef_loss   | 0.293    |
|    learning_rate   | 0.0003   |
|    n_updates       | 267699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.54e+03 |
|    critic_loss     | 4.72e+05 |
|    ent_coef        | 8.26     |
|    ent_coef_loss   | -0.66    |
|    learning_rate   | 0.0003   |
|    n_updates       | 268099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.44e+03 |
|    critic_loss     | 1.97e+05 |
|    ent_coef        | 8.36     |
|    ent_coef_loss   | 0.509    |
|    learning_rate   | 0.0003   |
|    n_updates       | 268499   |
---------------------------------
=== Iterazione IRL 956 ===
Loss reward (iter 956): 7.437166690826416
=== Iterazione IRL 957 ===
Loss reward (iter 957): -6.057634353637695
=== Iterazione IRL 958 ===
Loss reward (iter 958): 9.312182426452637
=== Iterazione IRL 959 ===
Loss reward (iter 959): -5.437662124633789
=== Iterazione IRL 960 ===
Loss reward (iter 960): -11.302029609680176
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.34e+03 |
|    critic_loss     | 3.13e+05 |
|    ent_coef        | 8.66     |
|    ent_coef_loss   | 0.0997   |
|    learning_rate   | 0.0003   |
|    n_updates       | 269099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.6e+03  |
|    critic_loss     | 2.13e+05 |
|    ent_coef        | 8.5      |
|    ent_coef_loss   | -0.343   |
|    learning_rate   | 0.0003   |
|    n_updates       | 269499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.44e+03 |
|    critic_loss     | 1.79e+05 |
|    ent_coef        | 8.48     |
|    ent_coef_loss   | 1.16     |
|    learning_rate   | 0.0003   |
|    n_updates       | 269899   |
---------------------------------
=== Iterazione IRL 961 ===
Loss reward (iter 961): -8.598047256469727
=== Iterazione IRL 962 ===
Loss reward (iter 962): -7.70142126083374
=== Iterazione IRL 963 ===
Loss reward (iter 963): -14.893143653869629
=== Iterazione IRL 964 ===
Loss reward (iter 964): -16.264820098876953
=== Iterazione IRL 965 ===
Loss reward (iter 965): -11.86331558227539
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.5e+03  |
|    critic_loss     | 7.9e+05  |
|    ent_coef        | 8.17     |
|    ent_coef_loss   | 0.216    |
|    learning_rate   | 0.0003   |
|    n_updates       | 270499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.69e+03 |
|    critic_loss     | 1.87e+05 |
|    ent_coef        | 8.13     |
|    ent_coef_loss   | 0.157    |
|    learning_rate   | 0.0003   |
|    n_updates       | 270899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.63e+03 |
|    critic_loss     | 7.71e+05 |
|    ent_coef        | 8.22     |
|    ent_coef_loss   | 0.135    |
|    learning_rate   | 0.0003   |
|    n_updates       | 271299   |
---------------------------------
=== Iterazione IRL 966 ===
Loss reward (iter 966): -3.370011329650879
=== Iterazione IRL 967 ===
Loss reward (iter 967): -10.979731559753418
=== Iterazione IRL 968 ===
Loss reward (iter 968): 0.7659852504730225
=== Iterazione IRL 969 ===
Loss reward (iter 969): -6.958017826080322
=== Iterazione IRL 970 ===
Loss reward (iter 970): -8.284612655639648
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.72e+03 |
|    critic_loss     | 1.39e+05 |
|    ent_coef        | 8.08     |
|    ent_coef_loss   | -0.264   |
|    learning_rate   | 0.0003   |
|    n_updates       | 271899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.66e+03 |
|    critic_loss     | 1.74e+05 |
|    ent_coef        | 8.24     |
|    ent_coef_loss   | 0.0434   |
|    learning_rate   | 0.0003   |
|    n_updates       | 272299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.6e+03  |
|    critic_loss     | 2.38e+05 |
|    ent_coef        | 8.1      |
|    ent_coef_loss   | -0.133   |
|    learning_rate   | 0.0003   |
|    n_updates       | 272699   |
---------------------------------
=== Iterazione IRL 971 ===
Loss reward (iter 971): 5.132057189941406
=== Iterazione IRL 972 ===
Loss reward (iter 972): 1.8389861583709717
=== Iterazione IRL 973 ===
Loss reward (iter 973): 19.021926879882812
=== Iterazione IRL 974 ===
Loss reward (iter 974): 5.197995185852051
=== Iterazione IRL 975 ===
Loss reward (iter 975): 1.5410895347595215
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.76e+03 |
|    critic_loss     | 1.19e+05 |
|    ent_coef        | 8        |
|    ent_coef_loss   | -0.905   |
|    learning_rate   | 0.0003   |
|    n_updates       | 273299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.78e+03 |
|    critic_loss     | 1.41e+05 |
|    ent_coef        | 8.24     |
|    ent_coef_loss   | -0.281   |
|    learning_rate   | 0.0003   |
|    n_updates       | 273699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.81e+03 |
|    critic_loss     | 4.59e+05 |
|    ent_coef        | 8.34     |
|    ent_coef_loss   | 0.545    |
|    learning_rate   | 0.0003   |
|    n_updates       | 274099   |
---------------------------------
=== Iterazione IRL 976 ===
Loss reward (iter 976): -5.988490104675293
=== Iterazione IRL 977 ===
Loss reward (iter 977): 14.911942481994629
=== Iterazione IRL 978 ===
Loss reward (iter 978): 14.060324668884277
=== Iterazione IRL 979 ===
Loss reward (iter 979): 1.5732700824737549
=== Iterazione IRL 980 ===
Loss reward (iter 980): 1.0276103019714355
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.85e+03 |
|    critic_loss     | 1.68e+05 |
|    ent_coef        | 8.17     |
|    ent_coef_loss   | -0.0245  |
|    learning_rate   | 0.0003   |
|    n_updates       | 274699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.73e+03 |
|    critic_loss     | 2.27e+05 |
|    ent_coef        | 8.14     |
|    ent_coef_loss   | -0.34    |
|    learning_rate   | 0.0003   |
|    n_updates       | 275099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.71e+03 |
|    critic_loss     | 1.21e+05 |
|    ent_coef        | 8.01     |
|    ent_coef_loss   | 0.0592   |
|    learning_rate   | 0.0003   |
|    n_updates       | 275499   |
---------------------------------
=== Iterazione IRL 981 ===
Loss reward (iter 981): -2.685648202896118
=== Iterazione IRL 982 ===
Loss reward (iter 982): -2.869946241378784
=== Iterazione IRL 983 ===
Loss reward (iter 983): -2.07958722114563
=== Iterazione IRL 984 ===
Loss reward (iter 984): -4.358403205871582
=== Iterazione IRL 985 ===
Loss reward (iter 985): -5.526678562164307
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.8e+03  |
|    critic_loss     | 1.49e+05 |
|    ent_coef        | 8.11     |
|    ent_coef_loss   | -0.19    |
|    learning_rate   | 0.0003   |
|    n_updates       | 276099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.8e+03  |
|    critic_loss     | 1.15e+05 |
|    ent_coef        | 8.04     |
|    ent_coef_loss   | -1.05    |
|    learning_rate   | 0.0003   |
|    n_updates       | 276499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.72e+03 |
|    critic_loss     | 9.72e+04 |
|    ent_coef        | 7.95     |
|    ent_coef_loss   | 0.459    |
|    learning_rate   | 0.0003   |
|    n_updates       | 276899   |
---------------------------------
=== Iterazione IRL 986 ===
Loss reward (iter 986): -4.464724540710449
=== Iterazione IRL 987 ===
Loss reward (iter 987): 1.9632585048675537
=== Iterazione IRL 988 ===
Loss reward (iter 988): 3.136885643005371
=== Iterazione IRL 989 ===
Loss reward (iter 989): 6.797564506530762
=== Iterazione IRL 990 ===
Loss reward (iter 990): 1.3563423156738281
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.89e+03 |
|    critic_loss     | 1.79e+05 |
|    ent_coef        | 8.21     |
|    ent_coef_loss   | -0.164   |
|    learning_rate   | 0.0003   |
|    n_updates       | 277499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.86e+03 |
|    critic_loss     | 1.72e+05 |
|    ent_coef        | 8.15     |
|    ent_coef_loss   | -0.935   |
|    learning_rate   | 0.0003   |
|    n_updates       | 277899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.7e+03  |
|    critic_loss     | 1.56e+05 |
|    ent_coef        | 8.01     |
|    ent_coef_loss   | -0.442   |
|    learning_rate   | 0.0003   |
|    n_updates       | 278299   |
---------------------------------
=== Iterazione IRL 991 ===
Loss reward (iter 991): -9.293495178222656
=== Iterazione IRL 992 ===
Loss reward (iter 992): -0.055710554122924805
=== Iterazione IRL 993 ===
Loss reward (iter 993): -3.449348211288452
=== Iterazione IRL 994 ===
Loss reward (iter 994): 1.0260541439056396
=== Iterazione IRL 995 ===
Loss reward (iter 995): -4.324784278869629
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.92e+03 |
|    critic_loss     | 1.99e+05 |
|    ent_coef        | 7.98     |
|    ent_coef_loss   | 0.0132   |
|    learning_rate   | 0.0003   |
|    n_updates       | 278899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.84e+03 |
|    critic_loss     | 4.64e+05 |
|    ent_coef        | 8.02     |
|    ent_coef_loss   | -0.202   |
|    learning_rate   | 0.0003   |
|    n_updates       | 279299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.89e+03 |
|    critic_loss     | 1.28e+05 |
|    ent_coef        | 8.26     |
|    ent_coef_loss   | -1.14    |
|    learning_rate   | 0.0003   |
|    n_updates       | 279699   |
---------------------------------
=== Iterazione IRL 996 ===
Loss reward (iter 996): -13.244831085205078
=== Iterazione IRL 997 ===
Loss reward (iter 997): -23.24848175048828
=== Iterazione IRL 998 ===
Loss reward (iter 998): -6.2961883544921875
=== Iterazione IRL 999 ===
Loss reward (iter 999): -22.61642074584961
=== Iterazione IRL 1000 ===
Loss reward (iter 1000): -2.2285497188568115
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.72e+03 |
|    critic_loss     | 2.18e+05 |
|    ent_coef        | 8.36     |
|    ent_coef_loss   | -0.566   |
|    learning_rate   | 0.0003   |
|    n_updates       | 280299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 130      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.96e+03 |
|    critic_loss     | 1.77e+05 |
|    ent_coef        | 8.62     |
|    ent_coef_loss   | 0.22     |
|    learning_rate   | 0.0003   |
|    n_updates       | 280699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.89e+03 |
|    critic_loss     | 1.46e+05 |
|    ent_coef        | 8.56     |
|    ent_coef_loss   | -0.175   |
|    learning_rate   | 0.0003   |
|    n_updates       | 281099   |
---------------------------------
=== Iterazione IRL 1001 ===
Loss reward (iter 1001): 8.91606330871582
=== Iterazione IRL 1002 ===
Loss reward (iter 1002): -4.011501789093018
=== Iterazione IRL 1003 ===
Loss reward (iter 1003): -12.26234245300293
=== Iterazione IRL 1004 ===
Loss reward (iter 1004): 1.5762009620666504
=== Iterazione IRL 1005 ===
Loss reward (iter 1005): -5.572606086730957
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.83e+03 |
|    critic_loss     | 2.2e+05  |
|    ent_coef        | 8.98     |
|    ent_coef_loss   | 0.425    |
|    learning_rate   | 0.0003   |
|    n_updates       | 281699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.91e+03 |
|    critic_loss     | 4.19e+05 |
|    ent_coef        | 8.81     |
|    ent_coef_loss   | -0.784   |
|    learning_rate   | 0.0003   |
|    n_updates       | 282099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.05e+03 |
|    critic_loss     | 9.88e+04 |
|    ent_coef        | 8.92     |
|    ent_coef_loss   | -0.0534  |
|    learning_rate   | 0.0003   |
|    n_updates       | 282499   |
---------------------------------
=== Iterazione IRL 1006 ===
Loss reward (iter 1006): -15.821527481079102
=== Iterazione IRL 1007 ===
Loss reward (iter 1007): -5.383602142333984
=== Iterazione IRL 1008 ===
Loss reward (iter 1008): -15.268519401550293
=== Iterazione IRL 1009 ===
Loss reward (iter 1009): -18.94110870361328
=== Iterazione IRL 1010 ===
Loss reward (iter 1010): -23.584259033203125
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.93e+03 |
|    critic_loss     | 1.1e+05  |
|    ent_coef        | 8.62     |
|    ent_coef_loss   | -0.308   |
|    learning_rate   | 0.0003   |
|    n_updates       | 283099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.91e+03 |
|    critic_loss     | 1.55e+05 |
|    ent_coef        | 8.35     |
|    ent_coef_loss   | -0.193   |
|    learning_rate   | 0.0003   |
|    n_updates       | 283499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.92e+03 |
|    critic_loss     | 8.28e+04 |
|    ent_coef        | 8.35     |
|    ent_coef_loss   | 0.0262   |
|    learning_rate   | 0.0003   |
|    n_updates       | 283899   |
---------------------------------
=== Iterazione IRL 1011 ===
Loss reward (iter 1011): 1.6867318153381348
=== Iterazione IRL 1012 ===
Loss reward (iter 1012): 8.565445899963379
=== Iterazione IRL 1013 ===
Loss reward (iter 1013): -3.651461124420166
=== Iterazione IRL 1014 ===
Loss reward (iter 1014): 7.088964462280273
=== Iterazione IRL 1015 ===
Loss reward (iter 1015): 9.016009330749512
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.13e+03 |
|    critic_loss     | 4.17e+05 |
|    ent_coef        | 8.66     |
|    ent_coef_loss   | 0.688    |
|    learning_rate   | 0.0003   |
|    n_updates       | 284499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 130      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.82e+03 |
|    critic_loss     | 9.06e+04 |
|    ent_coef        | 8.91     |
|    ent_coef_loss   | -0.593   |
|    learning_rate   | 0.0003   |
|    n_updates       | 284899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.95e+03 |
|    critic_loss     | 7.84e+04 |
|    ent_coef        | 8.87     |
|    ent_coef_loss   | -0.0236  |
|    learning_rate   | 0.0003   |
|    n_updates       | 285299   |
---------------------------------
=== Iterazione IRL 1016 ===
Loss reward (iter 1016): 10.207096099853516
=== Iterazione IRL 1017 ===
Loss reward (iter 1017): -4.842000961303711
=== Iterazione IRL 1018 ===
Loss reward (iter 1018): -2.026167154312134
=== Iterazione IRL 1019 ===
Loss reward (iter 1019): 0.6235308647155762
=== Iterazione IRL 1020 ===
Loss reward (iter 1020): -3.5101253986358643
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.88e+03 |
|    critic_loss     | 7.22e+04 |
|    ent_coef        | 8.98     |
|    ent_coef_loss   | 0.146    |
|    learning_rate   | 0.0003   |
|    n_updates       | 285899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.94e+03 |
|    critic_loss     | 1.25e+05 |
|    ent_coef        | 9.03     |
|    ent_coef_loss   | 0.255    |
|    learning_rate   | 0.0003   |
|    n_updates       | 286299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.01e+03 |
|    critic_loss     | 9.12e+04 |
|    ent_coef        | 8.81     |
|    ent_coef_loss   | -0.754   |
|    learning_rate   | 0.0003   |
|    n_updates       | 286699   |
---------------------------------
=== Iterazione IRL 1021 ===
Loss reward (iter 1021): 2.570197105407715
=== Iterazione IRL 1022 ===
Loss reward (iter 1022): -5.643424034118652
=== Iterazione IRL 1023 ===
Loss reward (iter 1023): 1.939377784729004
=== Iterazione IRL 1024 ===
Loss reward (iter 1024): 1.436715841293335
=== Iterazione IRL 1025 ===
Loss reward (iter 1025): -1.352018117904663
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.93e+03 |
|    critic_loss     | 1.89e+05 |
|    ent_coef        | 8.63     |
|    ent_coef_loss   | -0.701   |
|    learning_rate   | 0.0003   |
|    n_updates       | 287299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.94e+03 |
|    critic_loss     | 1.53e+05 |
|    ent_coef        | 8.5      |
|    ent_coef_loss   | 0.213    |
|    learning_rate   | 0.0003   |
|    n_updates       | 287699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.93e+03 |
|    critic_loss     | 1.52e+05 |
|    ent_coef        | 8.77     |
|    ent_coef_loss   | -0.0872  |
|    learning_rate   | 0.0003   |
|    n_updates       | 288099   |
---------------------------------
=== Iterazione IRL 1026 ===
Loss reward (iter 1026): -0.4218616485595703
=== Iterazione IRL 1027 ===
Loss reward (iter 1027): -5.9688239097595215
=== Iterazione IRL 1028 ===
Loss reward (iter 1028): -1.1569676399230957
=== Iterazione IRL 1029 ===
Loss reward (iter 1029): 1.6264731884002686
=== Iterazione IRL 1030 ===
Loss reward (iter 1030): -7.634953498840332
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.03e+03 |
|    critic_loss     | 1.63e+05 |
|    ent_coef        | 8.73     |
|    ent_coef_loss   | -0.151   |
|    learning_rate   | 0.0003   |
|    n_updates       | 288699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.16e+03 |
|    critic_loss     | 1.54e+05 |
|    ent_coef        | 8.71     |
|    ent_coef_loss   | -0.016   |
|    learning_rate   | 0.0003   |
|    n_updates       | 289099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.94e+03 |
|    critic_loss     | 1.98e+05 |
|    ent_coef        | 8.83     |
|    ent_coef_loss   | -0.117   |
|    learning_rate   | 0.0003   |
|    n_updates       | 289499   |
---------------------------------
=== Iterazione IRL 1031 ===
Loss reward (iter 1031): 2.9219136238098145
=== Iterazione IRL 1032 ===
Loss reward (iter 1032): -3.8720879554748535
=== Iterazione IRL 1033 ===
Loss reward (iter 1033): -8.5968656539917
=== Iterazione IRL 1034 ===
Loss reward (iter 1034): 5.4942731857299805
=== Iterazione IRL 1035 ===
Loss reward (iter 1035): -0.7282938957214355
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.03e+03 |
|    critic_loss     | 7.32e+04 |
|    ent_coef        | 8.71     |
|    ent_coef_loss   | -0.483   |
|    learning_rate   | 0.0003   |
|    n_updates       | 290099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.14e+03 |
|    critic_loss     | 3.06e+05 |
|    ent_coef        | 8.84     |
|    ent_coef_loss   | 0.449    |
|    learning_rate   | 0.0003   |
|    n_updates       | 290499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.03e+03 |
|    critic_loss     | 1.98e+05 |
|    ent_coef        | 8.73     |
|    ent_coef_loss   | -0.23    |
|    learning_rate   | 0.0003   |
|    n_updates       | 290899   |
---------------------------------
=== Iterazione IRL 1036 ===
Loss reward (iter 1036): -0.27780723571777344
=== Iterazione IRL 1037 ===
Loss reward (iter 1037): 2.0549938678741455
=== Iterazione IRL 1038 ===
Loss reward (iter 1038): -1.3448076248168945
=== Iterazione IRL 1039 ===
Loss reward (iter 1039): 8.15207290649414
=== Iterazione IRL 1040 ===
Loss reward (iter 1040): 0.8950519561767578
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.1e+03  |
|    critic_loss     | 8.21e+04 |
|    ent_coef        | 8.9      |
|    ent_coef_loss   | -0.105   |
|    learning_rate   | 0.0003   |
|    n_updates       | 291499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 130      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.08e+03 |
|    critic_loss     | 8.05e+04 |
|    ent_coef        | 9.08     |
|    ent_coef_loss   | -0.343   |
|    learning_rate   | 0.0003   |
|    n_updates       | 291899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.21e+03 |
|    critic_loss     | 7.45e+04 |
|    ent_coef        | 8.83     |
|    ent_coef_loss   | -0.749   |
|    learning_rate   | 0.0003   |
|    n_updates       | 292299   |
---------------------------------
=== Iterazione IRL 1041 ===
Loss reward (iter 1041): -0.014642953872680664
=== Iterazione IRL 1042 ===
Loss reward (iter 1042): -5.195634841918945
=== Iterazione IRL 1043 ===
Loss reward (iter 1043): 0.6407022476196289
=== Iterazione IRL 1044 ===
Loss reward (iter 1044): -4.797518730163574
=== Iterazione IRL 1045 ===
Loss reward (iter 1045): -6.211787700653076
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.17e+03 |
|    critic_loss     | 1.88e+05 |
|    ent_coef        | 8.87     |
|    ent_coef_loss   | 0.159    |
|    learning_rate   | 0.0003   |
|    n_updates       | 292899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.3e+03  |
|    critic_loss     | 1.69e+05 |
|    ent_coef        | 8.86     |
|    ent_coef_loss   | -0.139   |
|    learning_rate   | 0.0003   |
|    n_updates       | 293299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.2e+03  |
|    critic_loss     | 1.12e+05 |
|    ent_coef        | 8.72     |
|    ent_coef_loss   | 0.124    |
|    learning_rate   | 0.0003   |
|    n_updates       | 293699   |
---------------------------------
=== Iterazione IRL 1046 ===
Loss reward (iter 1046): -2.3790903091430664
=== Iterazione IRL 1047 ===
Loss reward (iter 1047): 2.9128029346466064
=== Iterazione IRL 1048 ===
Loss reward (iter 1048): 7.16910982131958
=== Iterazione IRL 1049 ===
Loss reward (iter 1049): 8.388553619384766
=== Iterazione IRL 1050 ===
Loss reward (iter 1050): -3.118246078491211
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.39e+03 |
|    critic_loss     | 1.71e+05 |
|    ent_coef        | 8.69     |
|    ent_coef_loss   | -0.806   |
|    learning_rate   | 0.0003   |
|    n_updates       | 294299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.19e+03 |
|    critic_loss     | 8.86e+04 |
|    ent_coef        | 9.04     |
|    ent_coef_loss   | 0.754    |
|    learning_rate   | 0.0003   |
|    n_updates       | 294699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.11e+03 |
|    critic_loss     | 1.32e+05 |
|    ent_coef        | 8.95     |
|    ent_coef_loss   | -0.468   |
|    learning_rate   | 0.0003   |
|    n_updates       | 295099   |
---------------------------------
=== Iterazione IRL 1051 ===
Loss reward (iter 1051): 0.3585236072540283
=== Iterazione IRL 1052 ===
Loss reward (iter 1052): -4.73940372467041
=== Iterazione IRL 1053 ===
Loss reward (iter 1053): 6.000874042510986
=== Iterazione IRL 1054 ===
Loss reward (iter 1054): 0.15902924537658691
=== Iterazione IRL 1055 ===
Loss reward (iter 1055): -3.9455013275146484
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.38e+03 |
|    critic_loss     | 1.2e+05  |
|    ent_coef        | 8.99     |
|    ent_coef_loss   | -0.239   |
|    learning_rate   | 0.0003   |
|    n_updates       | 295699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.28e+03 |
|    critic_loss     | 9.81e+04 |
|    ent_coef        | 8.66     |
|    ent_coef_loss   | -0.0111  |
|    learning_rate   | 0.0003   |
|    n_updates       | 296099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.27e+03 |
|    critic_loss     | 9.49e+04 |
|    ent_coef        | 8.62     |
|    ent_coef_loss   | 0.121    |
|    learning_rate   | 0.0003   |
|    n_updates       | 296499   |
---------------------------------
=== Iterazione IRL 1056 ===
Loss reward (iter 1056): 4.971625804901123
=== Iterazione IRL 1057 ===
Loss reward (iter 1057): -4.936907768249512
=== Iterazione IRL 1058 ===
Loss reward (iter 1058): -2.551809549331665
=== Iterazione IRL 1059 ===
Loss reward (iter 1059): -2.861741542816162
=== Iterazione IRL 1060 ===
Loss reward (iter 1060): -5.722829818725586
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.37e+03 |
|    critic_loss     | 1.84e+05 |
|    ent_coef        | 8.67     |
|    ent_coef_loss   | 0.0768   |
|    learning_rate   | 0.0003   |
|    n_updates       | 297099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 130      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.27e+03 |
|    critic_loss     | 9.39e+04 |
|    ent_coef        | 8.72     |
|    ent_coef_loss   | -0.456   |
|    learning_rate   | 0.0003   |
|    n_updates       | 297499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.27e+03 |
|    critic_loss     | 2.18e+05 |
|    ent_coef        | 8.7      |
|    ent_coef_loss   | 0.00687  |
|    learning_rate   | 0.0003   |
|    n_updates       | 297899   |
---------------------------------
=== Iterazione IRL 1061 ===
Loss reward (iter 1061): -12.462571144104004
=== Iterazione IRL 1062 ===
Loss reward (iter 1062): -1.9216814041137695
=== Iterazione IRL 1063 ===
Loss reward (iter 1063): -7.536357879638672
=== Iterazione IRL 1064 ===
Loss reward (iter 1064): -8.53160285949707
=== Iterazione IRL 1065 ===
Loss reward (iter 1065): -13.640875816345215
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.34e+03 |
|    critic_loss     | 6.74e+05 |
|    ent_coef        | 8.79     |
|    ent_coef_loss   | -0.319   |
|    learning_rate   | 0.0003   |
|    n_updates       | 298499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.29e+03 |
|    critic_loss     | 1.82e+05 |
|    ent_coef        | 8.64     |
|    ent_coef_loss   | -0.296   |
|    learning_rate   | 0.0003   |
|    n_updates       | 298899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.38e+03 |
|    critic_loss     | 1.48e+05 |
|    ent_coef        | 8.79     |
|    ent_coef_loss   | -0.00172 |
|    learning_rate   | 0.0003   |
|    n_updates       | 299299   |
---------------------------------
=== Iterazione IRL 1066 ===
Loss reward (iter 1066): 6.505415916442871
=== Iterazione IRL 1067 ===
Loss reward (iter 1067): 9.912652969360352
=== Iterazione IRL 1068 ===
Loss reward (iter 1068): 8.314803123474121
=== Iterazione IRL 1069 ===
Loss reward (iter 1069): 11.009458541870117
=== Iterazione IRL 1070 ===
Loss reward (iter 1070): 4.331768035888672
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.29e+03 |
|    critic_loss     | 1.49e+05 |
|    ent_coef        | 8.77     |
|    ent_coef_loss   | 0.564    |
|    learning_rate   | 0.0003   |
|    n_updates       | 299899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.37e+03 |
|    critic_loss     | 4.78e+05 |
|    ent_coef        | 8.5      |
|    ent_coef_loss   | -0.827   |
|    learning_rate   | 0.0003   |
|    n_updates       | 300299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.47e+03 |
|    critic_loss     | 2.2e+05  |
|    ent_coef        | 8.38     |
|    ent_coef_loss   | 0.254    |
|    learning_rate   | 0.0003   |
|    n_updates       | 300699   |
---------------------------------
=== Iterazione IRL 1071 ===
Loss reward (iter 1071): 0.7327187061309814
=== Iterazione IRL 1072 ===
Loss reward (iter 1072): 10.215373039245605
=== Iterazione IRL 1073 ===
Loss reward (iter 1073): 2.2614338397979736
=== Iterazione IRL 1074 ===
Loss reward (iter 1074): -6.254754066467285
=== Iterazione IRL 1075 ===
Loss reward (iter 1075): -2.677500009536743
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.47e+03 |
|    critic_loss     | 1.07e+05 |
|    ent_coef        | 8.5      |
|    ent_coef_loss   | -0.346   |
|    learning_rate   | 0.0003   |
|    n_updates       | 301299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.35e+03 |
|    critic_loss     | 3.74e+05 |
|    ent_coef        | 8.52     |
|    ent_coef_loss   | 0.262    |
|    learning_rate   | 0.0003   |
|    n_updates       | 301699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.5e+03  |
|    critic_loss     | 3.82e+05 |
|    ent_coef        | 8.74     |
|    ent_coef_loss   | -0.467   |
|    learning_rate   | 0.0003   |
|    n_updates       | 302099   |
---------------------------------
=== Iterazione IRL 1076 ===
Loss reward (iter 1076): 5.333978652954102
=== Iterazione IRL 1077 ===
Loss reward (iter 1077): 0.09535932540893555
=== Iterazione IRL 1078 ===
Loss reward (iter 1078): -1.6937251091003418
=== Iterazione IRL 1079 ===
Loss reward (iter 1079): -2.0396745204925537
=== Iterazione IRL 1080 ===
Loss reward (iter 1080): -1.9942591190338135
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.36e+03 |
|    critic_loss     | 1.91e+05 |
|    ent_coef        | 8.65     |
|    ent_coef_loss   | -0.632   |
|    learning_rate   | 0.0003   |
|    n_updates       | 302699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.38e+03 |
|    critic_loss     | 6.77e+04 |
|    ent_coef        | 8.47     |
|    ent_coef_loss   | -0.0604  |
|    learning_rate   | 0.0003   |
|    n_updates       | 303099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.52e+03 |
|    critic_loss     | 1.26e+05 |
|    ent_coef        | 8.53     |
|    ent_coef_loss   | -0.166   |
|    learning_rate   | 0.0003   |
|    n_updates       | 303499   |
---------------------------------
=== Iterazione IRL 1081 ===
Loss reward (iter 1081): 0.1486949920654297
=== Iterazione IRL 1082 ===
Loss reward (iter 1082): 0.2559700012207031
=== Iterazione IRL 1083 ===
Loss reward (iter 1083): 3.532413959503174
=== Iterazione IRL 1084 ===
Loss reward (iter 1084): -2.104990243911743
=== Iterazione IRL 1085 ===
Loss reward (iter 1085): -1.865894079208374
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.39e+03 |
|    critic_loss     | 1.23e+05 |
|    ent_coef        | 8.43     |
|    ent_coef_loss   | 0.603    |
|    learning_rate   | 0.0003   |
|    n_updates       | 304099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.46e+03 |
|    critic_loss     | 1.27e+05 |
|    ent_coef        | 8.45     |
|    ent_coef_loss   | -0.413   |
|    learning_rate   | 0.0003   |
|    n_updates       | 304499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.45e+03 |
|    critic_loss     | 1.39e+05 |
|    ent_coef        | 8.24     |
|    ent_coef_loss   | 0.842    |
|    learning_rate   | 0.0003   |
|    n_updates       | 304899   |
---------------------------------
=== Iterazione IRL 1086 ===
Loss reward (iter 1086): -0.10910725593566895
=== Iterazione IRL 1087 ===
Loss reward (iter 1087): -0.1659224033355713
=== Iterazione IRL 1088 ===
Loss reward (iter 1088): -1.6929845809936523
=== Iterazione IRL 1089 ===
Loss reward (iter 1089): -0.5329897403717041
=== Iterazione IRL 1090 ===
Loss reward (iter 1090): -0.9123439788818359
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.46e+03 |
|    critic_loss     | 1.5e+05  |
|    ent_coef        | 8.34     |
|    ent_coef_loss   | -0.137   |
|    learning_rate   | 0.0003   |
|    n_updates       | 305499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.3e+03  |
|    critic_loss     | 1.09e+05 |
|    ent_coef        | 8.22     |
|    ent_coef_loss   | 0.0219   |
|    learning_rate   | 0.0003   |
|    n_updates       | 305899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.34e+03 |
|    critic_loss     | 1.17e+05 |
|    ent_coef        | 8.01     |
|    ent_coef_loss   | 0.513    |
|    learning_rate   | 0.0003   |
|    n_updates       | 306299   |
---------------------------------
=== Iterazione IRL 1091 ===
Loss reward (iter 1091): -3.4766647815704346
=== Iterazione IRL 1092 ===
Loss reward (iter 1092): -9.845848083496094
=== Iterazione IRL 1093 ===
Loss reward (iter 1093): -0.2765076160430908
=== Iterazione IRL 1094 ===
Loss reward (iter 1094): -8.013975143432617
=== Iterazione IRL 1095 ===
Loss reward (iter 1095): -2.163555860519409
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.47e+03 |
|    critic_loss     | 8.98e+04 |
|    ent_coef        | 7.96     |
|    ent_coef_loss   | -0.173   |
|    learning_rate   | 0.0003   |
|    n_updates       | 306899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.43e+03 |
|    critic_loss     | 1.05e+05 |
|    ent_coef        | 7.89     |
|    ent_coef_loss   | -0.0568  |
|    learning_rate   | 0.0003   |
|    n_updates       | 307299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.4e+03  |
|    critic_loss     | 1.6e+05  |
|    ent_coef        | 8.08     |
|    ent_coef_loss   | -0.415   |
|    learning_rate   | 0.0003   |
|    n_updates       | 307699   |
---------------------------------
=== Iterazione IRL 1096 ===
Loss reward (iter 1096): -1.737868309020996
=== Iterazione IRL 1097 ===
Loss reward (iter 1097): -4.135387420654297
=== Iterazione IRL 1098 ===
Loss reward (iter 1098): -6.846772193908691
=== Iterazione IRL 1099 ===
Loss reward (iter 1099): -1.413827657699585
=== Iterazione IRL 1100 ===
Loss reward (iter 1100): -2.788600444793701
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.64e+03 |
|    critic_loss     | 2.22e+05 |
|    ent_coef        | 8.13     |
|    ent_coef_loss   | 0.574    |
|    learning_rate   | 0.0003   |
|    n_updates       | 308299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.5e+03  |
|    critic_loss     | 1.17e+05 |
|    ent_coef        | 8.05     |
|    ent_coef_loss   | 0.23     |
|    learning_rate   | 0.0003   |
|    n_updates       | 308699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.35e+03 |
|    critic_loss     | 5.79e+05 |
|    ent_coef        | 8.08     |
|    ent_coef_loss   | -0.232   |
|    learning_rate   | 0.0003   |
|    n_updates       | 309099   |
---------------------------------
=== Iterazione IRL 1101 ===
Loss reward (iter 1101): -2.4483983516693115
=== Iterazione IRL 1102 ===
Loss reward (iter 1102): -0.7876174449920654
=== Iterazione IRL 1103 ===
Loss reward (iter 1103): 0.49173617362976074
=== Iterazione IRL 1104 ===
Loss reward (iter 1104): -2.3158860206604004
=== Iterazione IRL 1105 ===
Loss reward (iter 1105): -3.062436580657959
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.44e+03 |
|    critic_loss     | 3.9e+05  |
|    ent_coef        | 8.16     |
|    ent_coef_loss   | 0.155    |
|    learning_rate   | 0.0003   |
|    n_updates       | 309699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.37e+03 |
|    critic_loss     | 2.15e+05 |
|    ent_coef        | 7.99     |
|    ent_coef_loss   | -0.14    |
|    learning_rate   | 0.0003   |
|    n_updates       | 310099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.53e+03 |
|    critic_loss     | 1.77e+05 |
|    ent_coef        | 8.08     |
|    ent_coef_loss   | -0.562   |
|    learning_rate   | 0.0003   |
|    n_updates       | 310499   |
---------------------------------
=== Iterazione IRL 1106 ===
Loss reward (iter 1106): -3.291226387023926
=== Iterazione IRL 1107 ===
Loss reward (iter 1107): -6.667198181152344
=== Iterazione IRL 1108 ===
Loss reward (iter 1108): -3.1493678092956543
=== Iterazione IRL 1109 ===
Loss reward (iter 1109): 7.470248222351074
=== Iterazione IRL 1110 ===
Loss reward (iter 1110): -1.1025111675262451
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.4e+03  |
|    critic_loss     | 9.83e+04 |
|    ent_coef        | 7.96     |
|    ent_coef_loss   | 0.371    |
|    learning_rate   | 0.0003   |
|    n_updates       | 311099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.45e+03 |
|    critic_loss     | 2.45e+05 |
|    ent_coef        | 8.12     |
|    ent_coef_loss   | 0.754    |
|    learning_rate   | 0.0003   |
|    n_updates       | 311499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.35e+03 |
|    critic_loss     | 1.56e+05 |
|    ent_coef        | 8.12     |
|    ent_coef_loss   | -0.138   |
|    learning_rate   | 0.0003   |
|    n_updates       | 311899   |
---------------------------------
=== Iterazione IRL 1111 ===
Loss reward (iter 1111): 1.5347380638122559
=== Iterazione IRL 1112 ===
Loss reward (iter 1112): 2.713273048400879
=== Iterazione IRL 1113 ===
Loss reward (iter 1113): -5.456050872802734
=== Iterazione IRL 1114 ===
Loss reward (iter 1114): -1.0171611309051514
=== Iterazione IRL 1115 ===
Loss reward (iter 1115): -2.336031198501587
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.31e+03 |
|    critic_loss     | 1.54e+05 |
|    ent_coef        | 7.99     |
|    ent_coef_loss   | -0.467   |
|    learning_rate   | 0.0003   |
|    n_updates       | 312499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.55e+03 |
|    critic_loss     | 1.88e+05 |
|    ent_coef        | 7.9      |
|    ent_coef_loss   | 0.955    |
|    learning_rate   | 0.0003   |
|    n_updates       | 312899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.4e+03  |
|    critic_loss     | 4.96e+05 |
|    ent_coef        | 7.92     |
|    ent_coef_loss   | 0.647    |
|    learning_rate   | 0.0003   |
|    n_updates       | 313299   |
---------------------------------
=== Iterazione IRL 1116 ===
Loss reward (iter 1116): -2.1446051597595215
=== Iterazione IRL 1117 ===
Loss reward (iter 1117): -2.962751865386963
=== Iterazione IRL 1118 ===
Loss reward (iter 1118): -1.4722638130187988
=== Iterazione IRL 1119 ===
Loss reward (iter 1119): -6.5994648933410645
=== Iterazione IRL 1120 ===
Loss reward (iter 1120): -3.8566267490386963
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.52e+03 |
|    critic_loss     | 1.72e+05 |
|    ent_coef        | 7.97     |
|    ent_coef_loss   | 0.263    |
|    learning_rate   | 0.0003   |
|    n_updates       | 313899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.56e+03 |
|    critic_loss     | 1.17e+05 |
|    ent_coef        | 7.72     |
|    ent_coef_loss   | -0.169   |
|    learning_rate   | 0.0003   |
|    n_updates       | 314299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.4e+03  |
|    critic_loss     | 2.01e+05 |
|    ent_coef        | 7.53     |
|    ent_coef_loss   | -0.994   |
|    learning_rate   | 0.0003   |
|    n_updates       | 314699   |
---------------------------------
=== Iterazione IRL 1121 ===
Loss reward (iter 1121): -5.603960037231445
=== Iterazione IRL 1122 ===
Loss reward (iter 1122): -5.572024345397949
=== Iterazione IRL 1123 ===
Loss reward (iter 1123): -0.3280303478240967
=== Iterazione IRL 1124 ===
Loss reward (iter 1124): -3.656388759613037
=== Iterazione IRL 1125 ===
Loss reward (iter 1125): -0.46750426292419434
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.49e+03 |
|    critic_loss     | 1.36e+05 |
|    ent_coef        | 7.66     |
|    ent_coef_loss   | -0.011   |
|    learning_rate   | 0.0003   |
|    n_updates       | 315299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.42e+03 |
|    critic_loss     | 3.68e+05 |
|    ent_coef        | 7.75     |
|    ent_coef_loss   | -0.662   |
|    learning_rate   | 0.0003   |
|    n_updates       | 315699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.47e+03 |
|    critic_loss     | 1.18e+05 |
|    ent_coef        | 7.79     |
|    ent_coef_loss   | 0.402    |
|    learning_rate   | 0.0003   |
|    n_updates       | 316099   |
---------------------------------
=== Iterazione IRL 1126 ===
Loss reward (iter 1126): -2.681403636932373
=== Iterazione IRL 1127 ===
Loss reward (iter 1127): -4.15595006942749
=== Iterazione IRL 1128 ===
Loss reward (iter 1128): -6.357231140136719
=== Iterazione IRL 1129 ===
Loss reward (iter 1129): -2.960855007171631
=== Iterazione IRL 1130 ===
Loss reward (iter 1130): -6.275634765625
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.42e+03 |
|    critic_loss     | 1.32e+05 |
|    ent_coef        | 8.06     |
|    ent_coef_loss   | 0.573    |
|    learning_rate   | 0.0003   |
|    n_updates       | 316699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.35e+03 |
|    critic_loss     | 3.33e+05 |
|    ent_coef        | 7.96     |
|    ent_coef_loss   | 0.092    |
|    learning_rate   | 0.0003   |
|    n_updates       | 317099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.48e+03 |
|    critic_loss     | 6.95e+04 |
|    ent_coef        | 7.88     |
|    ent_coef_loss   | -1.03    |
|    learning_rate   | 0.0003   |
|    n_updates       | 317499   |
---------------------------------
=== Iterazione IRL 1131 ===
Loss reward (iter 1131): -6.180783748626709
=== Iterazione IRL 1132 ===
Loss reward (iter 1132): -9.768893241882324
=== Iterazione IRL 1133 ===
Loss reward (iter 1133): -11.730640411376953
=== Iterazione IRL 1134 ===
Loss reward (iter 1134): -15.245708465576172
=== Iterazione IRL 1135 ===
Loss reward (iter 1135): -8.601241111755371
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.38e+03 |
|    critic_loss     | 4.38e+05 |
|    ent_coef        | 7.94     |
|    ent_coef_loss   | -0.661   |
|    learning_rate   | 0.0003   |
|    n_updates       | 318099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.46e+03 |
|    critic_loss     | 8.71e+04 |
|    ent_coef        | 7.94     |
|    ent_coef_loss   | -0.14    |
|    learning_rate   | 0.0003   |
|    n_updates       | 318499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.31e+03 |
|    critic_loss     | 1.03e+05 |
|    ent_coef        | 7.89     |
|    ent_coef_loss   | -0.458   |
|    learning_rate   | 0.0003   |
|    n_updates       | 318899   |
---------------------------------
=== Iterazione IRL 1136 ===
Loss reward (iter 1136): -5.028805732727051
=== Iterazione IRL 1137 ===
Loss reward (iter 1137): -8.424544334411621
=== Iterazione IRL 1138 ===
Loss reward (iter 1138): -8.547189712524414
=== Iterazione IRL 1139 ===
Loss reward (iter 1139): -11.168431282043457
=== Iterazione IRL 1140 ===
Loss reward (iter 1140): -10.805105209350586
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.37e+03 |
|    critic_loss     | 4.13e+05 |
|    ent_coef        | 7.89     |
|    ent_coef_loss   | 0.323    |
|    learning_rate   | 0.0003   |
|    n_updates       | 319499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.33e+03 |
|    critic_loss     | 1.81e+05 |
|    ent_coef        | 8.01     |
|    ent_coef_loss   | 0.228    |
|    learning_rate   | 0.0003   |
|    n_updates       | 319899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.26e+03 |
|    critic_loss     | 2.88e+05 |
|    ent_coef        | 7.99     |
|    ent_coef_loss   | 0.271    |
|    learning_rate   | 0.0003   |
|    n_updates       | 320299   |
---------------------------------
=== Iterazione IRL 1141 ===
Loss reward (iter 1141): 2.6612608432769775
=== Iterazione IRL 1142 ===
Loss reward (iter 1142): -5.614304542541504
=== Iterazione IRL 1143 ===
Loss reward (iter 1143): 11.015079498291016
=== Iterazione IRL 1144 ===
Loss reward (iter 1144): -2.9792821407318115
=== Iterazione IRL 1145 ===
Loss reward (iter 1145): -3.827829360961914
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.36e+03 |
|    critic_loss     | 1.57e+05 |
|    ent_coef        | 7.84     |
|    ent_coef_loss   | 0.507    |
|    learning_rate   | 0.0003   |
|    n_updates       | 320899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.36e+03 |
|    critic_loss     | 2.8e+05  |
|    ent_coef        | 7.85     |
|    ent_coef_loss   | -0.263   |
|    learning_rate   | 0.0003   |
|    n_updates       | 321299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.37e+03 |
|    critic_loss     | 2.25e+05 |
|    ent_coef        | 7.81     |
|    ent_coef_loss   | -0.16    |
|    learning_rate   | 0.0003   |
|    n_updates       | 321699   |
---------------------------------
=== Iterazione IRL 1146 ===
Loss reward (iter 1146): -5.057547092437744
=== Iterazione IRL 1147 ===
Loss reward (iter 1147): -2.9443397521972656
=== Iterazione IRL 1148 ===
Loss reward (iter 1148): -4.2396955490112305
=== Iterazione IRL 1149 ===
Loss reward (iter 1149): -3.224036693572998
=== Iterazione IRL 1150 ===
Loss reward (iter 1150): -1.7441074848175049
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.46e+03 |
|    critic_loss     | 1.93e+05 |
|    ent_coef        | 7.78     |
|    ent_coef_loss   | -0.276   |
|    learning_rate   | 0.0003   |
|    n_updates       | 322299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 130      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.39e+03 |
|    critic_loss     | 2.25e+05 |
|    ent_coef        | 7.66     |
|    ent_coef_loss   | -0.758   |
|    learning_rate   | 0.0003   |
|    n_updates       | 322699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.47e+03 |
|    critic_loss     | 1.19e+05 |
|    ent_coef        | 7.64     |
|    ent_coef_loss   | 0.0585   |
|    learning_rate   | 0.0003   |
|    n_updates       | 323099   |
---------------------------------
=== Iterazione IRL 1151 ===
Loss reward (iter 1151): -7.38209342956543
=== Iterazione IRL 1152 ===
Loss reward (iter 1152): -6.604416370391846
=== Iterazione IRL 1153 ===
Loss reward (iter 1153): -6.404839515686035
=== Iterazione IRL 1154 ===
Loss reward (iter 1154): -8.767992973327637
=== Iterazione IRL 1155 ===
Loss reward (iter 1155): -9.065131187438965
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.39e+03 |
|    critic_loss     | 3.36e+05 |
|    ent_coef        | 7.78     |
|    ent_coef_loss   | 0.684    |
|    learning_rate   | 0.0003   |
|    n_updates       | 323699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 130      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.23e+03 |
|    critic_loss     | 1.45e+05 |
|    ent_coef        | 7.87     |
|    ent_coef_loss   | 0.506    |
|    learning_rate   | 0.0003   |
|    n_updates       | 324099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.22e+03 |
|    critic_loss     | 9.46e+05 |
|    ent_coef        | 7.69     |
|    ent_coef_loss   | -0.0139  |
|    learning_rate   | 0.0003   |
|    n_updates       | 324499   |
---------------------------------
=== Iterazione IRL 1156 ===
Loss reward (iter 1156): 6.25609016418457
=== Iterazione IRL 1157 ===
Loss reward (iter 1157): -8.806513786315918
=== Iterazione IRL 1158 ===
Loss reward (iter 1158): 5.24582576751709
=== Iterazione IRL 1159 ===
Loss reward (iter 1159): 3.4752697944641113
=== Iterazione IRL 1160 ===
Loss reward (iter 1160): -11.17408275604248
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.36e+03 |
|    critic_loss     | 2.48e+05 |
|    ent_coef        | 7.71     |
|    ent_coef_loss   | 0.00243  |
|    learning_rate   | 0.0003   |
|    n_updates       | 325099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 130      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.5e+03  |
|    critic_loss     | 1.95e+05 |
|    ent_coef        | 7.46     |
|    ent_coef_loss   | -0.462   |
|    learning_rate   | 0.0003   |
|    n_updates       | 325499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.26e+03 |
|    critic_loss     | 4.33e+05 |
|    ent_coef        | 7.35     |
|    ent_coef_loss   | -0.516   |
|    learning_rate   | 0.0003   |
|    n_updates       | 325899   |
---------------------------------
=== Iterazione IRL 1161 ===
Loss reward (iter 1161): 19.254892349243164
=== Iterazione IRL 1162 ===
Loss reward (iter 1162): -7.3691630363464355
=== Iterazione IRL 1163 ===
Loss reward (iter 1163): -8.000038146972656
=== Iterazione IRL 1164 ===
Loss reward (iter 1164): -3.33701491355896
=== Iterazione IRL 1165 ===
Loss reward (iter 1165): -9.670263290405273
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.34e+03 |
|    critic_loss     | 1.25e+05 |
|    ent_coef        | 7.43     |
|    ent_coef_loss   | 0.346    |
|    learning_rate   | 0.0003   |
|    n_updates       | 326499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 130      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.37e+03 |
|    critic_loss     | 1.37e+05 |
|    ent_coef        | 7.36     |
|    ent_coef_loss   | -0.115   |
|    learning_rate   | 0.0003   |
|    n_updates       | 326899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.3e+03  |
|    critic_loss     | 9.26e+04 |
|    ent_coef        | 7.43     |
|    ent_coef_loss   | 0.146    |
|    learning_rate   | 0.0003   |
|    n_updates       | 327299   |
---------------------------------
=== Iterazione IRL 1166 ===
Loss reward (iter 1166): 2.182537794113159
=== Iterazione IRL 1167 ===
Loss reward (iter 1167): -4.800069808959961
=== Iterazione IRL 1168 ===
Loss reward (iter 1168): -2.5760610103607178
=== Iterazione IRL 1169 ===
Loss reward (iter 1169): -7.615517616271973
=== Iterazione IRL 1170 ===
Loss reward (iter 1170): -7.433239936828613
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.31e+03 |
|    critic_loss     | 4.48e+05 |
|    ent_coef        | 7.49     |
|    ent_coef_loss   | 0.00291  |
|    learning_rate   | 0.0003   |
|    n_updates       | 327899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.31e+03 |
|    critic_loss     | 1.54e+05 |
|    ent_coef        | 7.42     |
|    ent_coef_loss   | 0.588    |
|    learning_rate   | 0.0003   |
|    n_updates       | 328299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.29e+03 |
|    critic_loss     | 2.86e+05 |
|    ent_coef        | 7.42     |
|    ent_coef_loss   | -1.05    |
|    learning_rate   | 0.0003   |
|    n_updates       | 328699   |
---------------------------------
=== Iterazione IRL 1171 ===
Loss reward (iter 1171): -4.451987266540527
=== Iterazione IRL 1172 ===
Loss reward (iter 1172): -5.690866470336914
=== Iterazione IRL 1173 ===
Loss reward (iter 1173): -5.48375940322876
=== Iterazione IRL 1174 ===
Loss reward (iter 1174): -10.114445686340332
=== Iterazione IRL 1175 ===
Loss reward (iter 1175): -10.273765563964844
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.06e+03 |
|    critic_loss     | 2.86e+05 |
|    ent_coef        | 7.48     |
|    ent_coef_loss   | -0.255   |
|    learning_rate   | 0.0003   |
|    n_updates       | 329299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.3e+03  |
|    critic_loss     | 2.25e+05 |
|    ent_coef        | 7.45     |
|    ent_coef_loss   | 0.088    |
|    learning_rate   | 0.0003   |
|    n_updates       | 329699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.24e+03 |
|    critic_loss     | 9.7e+04  |
|    ent_coef        | 7.38     |
|    ent_coef_loss   | 0.277    |
|    learning_rate   | 0.0003   |
|    n_updates       | 330099   |
---------------------------------
=== Iterazione IRL 1176 ===
Loss reward (iter 1176): -10.926667213439941
=== Iterazione IRL 1177 ===
Loss reward (iter 1177): -6.019077777862549
=== Iterazione IRL 1178 ===
Loss reward (iter 1178): 2.565213441848755
=== Iterazione IRL 1179 ===
Loss reward (iter 1179): -1.639726161956787
=== Iterazione IRL 1180 ===
Loss reward (iter 1180): -4.279626846313477
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.28e+03 |
|    critic_loss     | 2.73e+05 |
|    ent_coef        | 7.42     |
|    ent_coef_loss   | 0.295    |
|    learning_rate   | 0.0003   |
|    n_updates       | 330699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.42e+03 |
|    critic_loss     | 3.01e+05 |
|    ent_coef        | 7.54     |
|    ent_coef_loss   | -0.257   |
|    learning_rate   | 0.0003   |
|    n_updates       | 331099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.27e+03 |
|    critic_loss     | 8.12e+04 |
|    ent_coef        | 7.38     |
|    ent_coef_loss   | 0.00881  |
|    learning_rate   | 0.0003   |
|    n_updates       | 331499   |
---------------------------------
=== Iterazione IRL 1181 ===
Loss reward (iter 1181): -8.398209571838379
=== Iterazione IRL 1182 ===
Loss reward (iter 1182): -8.900986671447754
=== Iterazione IRL 1183 ===
Loss reward (iter 1183): -8.487506866455078
=== Iterazione IRL 1184 ===
Loss reward (iter 1184): -7.202578067779541
=== Iterazione IRL 1185 ===
Loss reward (iter 1185): -15.513748168945312
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.32e+03 |
|    critic_loss     | 2.25e+05 |
|    ent_coef        | 7.32     |
|    ent_coef_loss   | 0.432    |
|    learning_rate   | 0.0003   |
|    n_updates       | 332099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.37e+03 |
|    critic_loss     | 9.99e+04 |
|    ent_coef        | 7.36     |
|    ent_coef_loss   | -0.174   |
|    learning_rate   | 0.0003   |
|    n_updates       | 332499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.37e+03 |
|    critic_loss     | 5.33e+05 |
|    ent_coef        | 7.33     |
|    ent_coef_loss   | 0.712    |
|    learning_rate   | 0.0003   |
|    n_updates       | 332899   |
---------------------------------
=== Iterazione IRL 1186 ===
Loss reward (iter 1186): -5.025298595428467
=== Iterazione IRL 1187 ===
Loss reward (iter 1187): -4.783449172973633
=== Iterazione IRL 1188 ===
Loss reward (iter 1188): -6.6633477210998535
=== Iterazione IRL 1189 ===
Loss reward (iter 1189): -5.742630958557129
=== Iterazione IRL 1190 ===
Loss reward (iter 1190): -5.9655351638793945
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.37e+03 |
|    critic_loss     | 1.26e+05 |
|    ent_coef        | 7.34     |
|    ent_coef_loss   | -0.69    |
|    learning_rate   | 0.0003   |
|    n_updates       | 333499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.39e+03 |
|    critic_loss     | 3.35e+05 |
|    ent_coef        | 7.25     |
|    ent_coef_loss   | 0.0695   |
|    learning_rate   | 0.0003   |
|    n_updates       | 333899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.36e+03 |
|    critic_loss     | 2.09e+05 |
|    ent_coef        | 7.18     |
|    ent_coef_loss   | -0.326   |
|    learning_rate   | 0.0003   |
|    n_updates       | 334299   |
---------------------------------
=== Iterazione IRL 1191 ===
Loss reward (iter 1191): -14.4558687210083
=== Iterazione IRL 1192 ===
Loss reward (iter 1192): -14.351273536682129
=== Iterazione IRL 1193 ===
Loss reward (iter 1193): -11.335550308227539
=== Iterazione IRL 1194 ===
Loss reward (iter 1194): -18.5427303314209
=== Iterazione IRL 1195 ===
Loss reward (iter 1195): -12.551514625549316
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.44e+03 |
|    critic_loss     | 8.44e+04 |
|    ent_coef        | 7.25     |
|    ent_coef_loss   | 0.288    |
|    learning_rate   | 0.0003   |
|    n_updates       | 334899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.3e+03  |
|    critic_loss     | 4.27e+05 |
|    ent_coef        | 7.19     |
|    ent_coef_loss   | -0.551   |
|    learning_rate   | 0.0003   |
|    n_updates       | 335299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.32e+03 |
|    critic_loss     | 9.13e+04 |
|    ent_coef        | 7.43     |
|    ent_coef_loss   | 0.51     |
|    learning_rate   | 0.0003   |
|    n_updates       | 335699   |
---------------------------------
=== Iterazione IRL 1196 ===
Loss reward (iter 1196): 3.159604787826538
=== Iterazione IRL 1197 ===
Loss reward (iter 1197): -2.602421522140503
=== Iterazione IRL 1198 ===
Loss reward (iter 1198): -9.319286346435547
=== Iterazione IRL 1199 ===
Loss reward (iter 1199): -7.238529205322266
=== Iterazione IRL 1200 ===
Loss reward (iter 1200): -9.782937049865723
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.36e+03 |
|    critic_loss     | 1.57e+05 |
|    ent_coef        | 7.45     |
|    ent_coef_loss   | 0.289    |
|    learning_rate   | 0.0003   |
|    n_updates       | 336299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.34e+03 |
|    critic_loss     | 1.07e+05 |
|    ent_coef        | 7.53     |
|    ent_coef_loss   | -0.548   |
|    learning_rate   | 0.0003   |
|    n_updates       | 336699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.4e+03  |
|    critic_loss     | 3.47e+05 |
|    ent_coef        | 7.52     |
|    ent_coef_loss   | 0.557    |
|    learning_rate   | 0.0003   |
|    n_updates       | 337099   |
---------------------------------
=== Iterazione IRL 1201 ===
Loss reward (iter 1201): -5.519506931304932
=== Iterazione IRL 1202 ===
Loss reward (iter 1202): -7.961361408233643
=== Iterazione IRL 1203 ===
Loss reward (iter 1203): -9.548242568969727
=== Iterazione IRL 1204 ===
Loss reward (iter 1204): -15.465353965759277
=== Iterazione IRL 1205 ===
Loss reward (iter 1205): -13.744895935058594
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.27e+03 |
|    critic_loss     | 4.53e+05 |
|    ent_coef        | 7.53     |
|    ent_coef_loss   | 0.924    |
|    learning_rate   | 0.0003   |
|    n_updates       | 337699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.29e+03 |
|    critic_loss     | 1.61e+05 |
|    ent_coef        | 7.57     |
|    ent_coef_loss   | 0.198    |
|    learning_rate   | 0.0003   |
|    n_updates       | 338099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.4e+03  |
|    critic_loss     | 2.12e+05 |
|    ent_coef        | 7.76     |
|    ent_coef_loss   | -1.12    |
|    learning_rate   | 0.0003   |
|    n_updates       | 338499   |
---------------------------------
=== Iterazione IRL 1206 ===
Loss reward (iter 1206): -7.995059013366699
=== Iterazione IRL 1207 ===
Loss reward (iter 1207): 2.494776725769043
=== Iterazione IRL 1208 ===
Loss reward (iter 1208): -4.021352291107178
=== Iterazione IRL 1209 ===
Loss reward (iter 1209): -4.026190757751465
=== Iterazione IRL 1210 ===
Loss reward (iter 1210): -1.7258517742156982
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.26e+03 |
|    critic_loss     | 1.15e+05 |
|    ent_coef        | 7.7      |
|    ent_coef_loss   | -0.0816  |
|    learning_rate   | 0.0003   |
|    n_updates       | 339099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.43e+03 |
|    critic_loss     | 9.97e+04 |
|    ent_coef        | 7.9      |
|    ent_coef_loss   | -0.487   |
|    learning_rate   | 0.0003   |
|    n_updates       | 339499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.34e+03 |
|    critic_loss     | 9.81e+04 |
|    ent_coef        | 7.89     |
|    ent_coef_loss   | 0.0848   |
|    learning_rate   | 0.0003   |
|    n_updates       | 339899   |
---------------------------------
=== Iterazione IRL 1211 ===
Loss reward (iter 1211): 9.300394058227539
=== Iterazione IRL 1212 ===
Loss reward (iter 1212): -4.058363914489746
=== Iterazione IRL 1213 ===
Loss reward (iter 1213): -6.9860429763793945
=== Iterazione IRL 1214 ===
Loss reward (iter 1214): -10.512359619140625
=== Iterazione IRL 1215 ===
Loss reward (iter 1215): -8.229990005493164
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.37e+03 |
|    critic_loss     | 6.79e+04 |
|    ent_coef        | 7.86     |
|    ent_coef_loss   | -0.0264  |
|    learning_rate   | 0.0003   |
|    n_updates       | 340499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.39e+03 |
|    critic_loss     | 3.78e+05 |
|    ent_coef        | 8.15     |
|    ent_coef_loss   | 0.177    |
|    learning_rate   | 0.0003   |
|    n_updates       | 340899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.33e+03 |
|    critic_loss     | 9.7e+04  |
|    ent_coef        | 8.19     |
|    ent_coef_loss   | 0.942    |
|    learning_rate   | 0.0003   |
|    n_updates       | 341299   |
---------------------------------
=== Iterazione IRL 1216 ===
Loss reward (iter 1216): -2.8536276817321777
=== Iterazione IRL 1217 ===
Loss reward (iter 1217): 1.942152500152588
=== Iterazione IRL 1218 ===
Loss reward (iter 1218): 0.16553711891174316
=== Iterazione IRL 1219 ===
Loss reward (iter 1219): -2.7630631923675537
=== Iterazione IRL 1220 ===
Loss reward (iter 1220): -7.225398063659668
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.31e+03 |
|    critic_loss     | 2.46e+05 |
|    ent_coef        | 8.02     |
|    ent_coef_loss   | -0.101   |
|    learning_rate   | 0.0003   |
|    n_updates       | 341899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.3e+03  |
|    critic_loss     | 1.34e+05 |
|    ent_coef        | 7.82     |
|    ent_coef_loss   | 0.118    |
|    learning_rate   | 0.0003   |
|    n_updates       | 342299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.49e+03 |
|    critic_loss     | 1.28e+05 |
|    ent_coef        | 7.98     |
|    ent_coef_loss   | -0.52    |
|    learning_rate   | 0.0003   |
|    n_updates       | 342699   |
---------------------------------
=== Iterazione IRL 1221 ===
Loss reward (iter 1221): -10.732331275939941
=== Iterazione IRL 1222 ===
Loss reward (iter 1222): -1.6652820110321045
=== Iterazione IRL 1223 ===
Loss reward (iter 1223): -10.908126831054688
=== Iterazione IRL 1224 ===
Loss reward (iter 1224): 4.261701583862305
=== Iterazione IRL 1225 ===
Loss reward (iter 1225): -8.637630462646484
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.33e+03 |
|    critic_loss     | 1.72e+05 |
|    ent_coef        | 8.05     |
|    ent_coef_loss   | -0.0326  |
|    learning_rate   | 0.0003   |
|    n_updates       | 343299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.44e+03 |
|    critic_loss     | 7.34e+04 |
|    ent_coef        | 8.06     |
|    ent_coef_loss   | -0.118   |
|    learning_rate   | 0.0003   |
|    n_updates       | 343699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.47e+03 |
|    critic_loss     | 4.25e+05 |
|    ent_coef        | 8.13     |
|    ent_coef_loss   | 0.0297   |
|    learning_rate   | 0.0003   |
|    n_updates       | 344099   |
---------------------------------
=== Iterazione IRL 1226 ===
Loss reward (iter 1226): 0.9316208362579346
=== Iterazione IRL 1227 ===
Loss reward (iter 1227): -2.1283979415893555
=== Iterazione IRL 1228 ===
Loss reward (iter 1228): -0.9932754039764404
=== Iterazione IRL 1229 ===
Loss reward (iter 1229): -8.269790649414062
=== Iterazione IRL 1230 ===
Loss reward (iter 1230): -0.66412353515625
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.44e+03 |
|    critic_loss     | 1.59e+05 |
|    ent_coef        | 8.19     |
|    ent_coef_loss   | -0.784   |
|    learning_rate   | 0.0003   |
|    n_updates       | 344699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.53e+03 |
|    critic_loss     | 2.32e+05 |
|    ent_coef        | 8.5      |
|    ent_coef_loss   | -0.0199  |
|    learning_rate   | 0.0003   |
|    n_updates       | 345099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.39e+03 |
|    critic_loss     | 4.4e+05  |
|    ent_coef        | 8.69     |
|    ent_coef_loss   | 0.314    |
|    learning_rate   | 0.0003   |
|    n_updates       | 345499   |
---------------------------------
=== Iterazione IRL 1231 ===
Loss reward (iter 1231): -1.895939826965332
=== Iterazione IRL 1232 ===
Loss reward (iter 1232): 4.822243690490723
=== Iterazione IRL 1233 ===
Loss reward (iter 1233): 5.914859294891357
=== Iterazione IRL 1234 ===
Loss reward (iter 1234): -1.5099570751190186
=== Iterazione IRL 1235 ===
Loss reward (iter 1235): -3.808429718017578
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.43e+03 |
|    critic_loss     | 4e+05    |
|    ent_coef        | 8.33     |
|    ent_coef_loss   | -0.476   |
|    learning_rate   | 0.0003   |
|    n_updates       | 346099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.4e+03  |
|    critic_loss     | 7.44e+04 |
|    ent_coef        | 8.42     |
|    ent_coef_loss   | 0.0105   |
|    learning_rate   | 0.0003   |
|    n_updates       | 346499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.34e+03 |
|    critic_loss     | 1.11e+05 |
|    ent_coef        | 8.32     |
|    ent_coef_loss   | -0.609   |
|    learning_rate   | 0.0003   |
|    n_updates       | 346899   |
---------------------------------
=== Iterazione IRL 1236 ===
Loss reward (iter 1236): 8.039243698120117
=== Iterazione IRL 1237 ===
Loss reward (iter 1237): 1.8079066276550293
=== Iterazione IRL 1238 ===
Loss reward (iter 1238): -1.7622272968292236
=== Iterazione IRL 1239 ===
Loss reward (iter 1239): 13.65191650390625
=== Iterazione IRL 1240 ===
Loss reward (iter 1240): 5.828213214874268
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.27e+03 |
|    critic_loss     | 1.88e+05 |
|    ent_coef        | 8.33     |
|    ent_coef_loss   | -0.321   |
|    learning_rate   | 0.0003   |
|    n_updates       | 347499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.3e+03  |
|    critic_loss     | 3.31e+05 |
|    ent_coef        | 8.21     |
|    ent_coef_loss   | 0.479    |
|    learning_rate   | 0.0003   |
|    n_updates       | 347899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.46e+03 |
|    critic_loss     | 5.79e+04 |
|    ent_coef        | 8.46     |
|    ent_coef_loss   | 0.376    |
|    learning_rate   | 0.0003   |
|    n_updates       | 348299   |
---------------------------------
=== Iterazione IRL 1241 ===
Loss reward (iter 1241): 4.726436614990234
=== Iterazione IRL 1242 ===
Loss reward (iter 1242): -0.04220414161682129
=== Iterazione IRL 1243 ===
Loss reward (iter 1243): 0.3183121681213379
=== Iterazione IRL 1244 ===
Loss reward (iter 1244): -6.243840217590332
=== Iterazione IRL 1245 ===
Loss reward (iter 1245): 0.22983527183532715
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.46e+03 |
|    critic_loss     | 3.19e+05 |
|    ent_coef        | 8.37     |
|    ent_coef_loss   | -0.465   |
|    learning_rate   | 0.0003   |
|    n_updates       | 348899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.5e+03  |
|    critic_loss     | 2.73e+05 |
|    ent_coef        | 8.26     |
|    ent_coef_loss   | -0.838   |
|    learning_rate   | 0.0003   |
|    n_updates       | 349299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.69e+03 |
|    critic_loss     | 7.18e+04 |
|    ent_coef        | 8.19     |
|    ent_coef_loss   | 0.143    |
|    learning_rate   | 0.0003   |
|    n_updates       | 349699   |
---------------------------------
=== Iterazione IRL 1246 ===
Loss reward (iter 1246): 14.226234436035156
=== Iterazione IRL 1247 ===
Loss reward (iter 1247): 1.8785340785980225
=== Iterazione IRL 1248 ===
Loss reward (iter 1248): -1.5384819507598877
=== Iterazione IRL 1249 ===
Loss reward (iter 1249): 0.6532049179077148
=== Iterazione IRL 1250 ===
Loss reward (iter 1250): 2.2684803009033203
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.51e+03 |
|    critic_loss     | 9.94e+04 |
|    ent_coef        | 8.11     |
|    ent_coef_loss   | 0.173    |
|    learning_rate   | 0.0003   |
|    n_updates       | 350299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.34e+03 |
|    critic_loss     | 1.19e+05 |
|    ent_coef        | 8.17     |
|    ent_coef_loss   | 0.602    |
|    learning_rate   | 0.0003   |
|    n_updates       | 350699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.36e+03 |
|    critic_loss     | 9.77e+04 |
|    ent_coef        | 8.28     |
|    ent_coef_loss   | -0.25    |
|    learning_rate   | 0.0003   |
|    n_updates       | 351099   |
---------------------------------
=== Iterazione IRL 1251 ===
Loss reward (iter 1251): 7.268665313720703
=== Iterazione IRL 1252 ===
Loss reward (iter 1252): -4.304027557373047
=== Iterazione IRL 1253 ===
Loss reward (iter 1253): -0.06933999061584473
=== Iterazione IRL 1254 ===
Loss reward (iter 1254): -2.998934507369995
=== Iterazione IRL 1255 ===
Loss reward (iter 1255): -2.659031629562378
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.38e+03 |
|    critic_loss     | 8.03e+04 |
|    ent_coef        | 8.23     |
|    ent_coef_loss   | 0.143    |
|    learning_rate   | 0.0003   |
|    n_updates       | 351699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.53e+03 |
|    critic_loss     | 9.89e+04 |
|    ent_coef        | 8.15     |
|    ent_coef_loss   | -0.364   |
|    learning_rate   | 0.0003   |
|    n_updates       | 352099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.47e+03 |
|    critic_loss     | 1.49e+05 |
|    ent_coef        | 8.2      |
|    ent_coef_loss   | 0.545    |
|    learning_rate   | 0.0003   |
|    n_updates       | 352499   |
---------------------------------
=== Iterazione IRL 1256 ===
Loss reward (iter 1256): -1.5938260555267334
=== Iterazione IRL 1257 ===
Loss reward (iter 1257): -2.65138578414917
=== Iterazione IRL 1258 ===
Loss reward (iter 1258): -5.311834335327148
=== Iterazione IRL 1259 ===
Loss reward (iter 1259): -2.386946678161621
=== Iterazione IRL 1260 ===
Loss reward (iter 1260): -4.280780792236328
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.27e+03 |
|    critic_loss     | 2.14e+05 |
|    ent_coef        | 8.14     |
|    ent_coef_loss   | -0.0292  |
|    learning_rate   | 0.0003   |
|    n_updates       | 353099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.39e+03 |
|    critic_loss     | 1.97e+05 |
|    ent_coef        | 8.21     |
|    ent_coef_loss   | -0.233   |
|    learning_rate   | 0.0003   |
|    n_updates       | 353499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.44e+03 |
|    critic_loss     | 1.58e+05 |
|    ent_coef        | 8.01     |
|    ent_coef_loss   | -0.211   |
|    learning_rate   | 0.0003   |
|    n_updates       | 353899   |
---------------------------------
=== Iterazione IRL 1261 ===
Loss reward (iter 1261): -10.119781494140625
=== Iterazione IRL 1262 ===
Loss reward (iter 1262): -18.452598571777344
=== Iterazione IRL 1263 ===
Loss reward (iter 1263): -8.986818313598633
=== Iterazione IRL 1264 ===
Loss reward (iter 1264): -0.9057538509368896
=== Iterazione IRL 1265 ===
Loss reward (iter 1265): -10.925873756408691
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.46e+03 |
|    critic_loss     | 1.34e+05 |
|    ent_coef        | 8.06     |
|    ent_coef_loss   | 0.493    |
|    learning_rate   | 0.0003   |
|    n_updates       | 354499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.57e+03 |
|    critic_loss     | 3.76e+05 |
|    ent_coef        | 8.09     |
|    ent_coef_loss   | -0.353   |
|    learning_rate   | 0.0003   |
|    n_updates       | 354899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.45e+03 |
|    critic_loss     | 1.29e+05 |
|    ent_coef        | 8.13     |
|    ent_coef_loss   | -0.652   |
|    learning_rate   | 0.0003   |
|    n_updates       | 355299   |
---------------------------------
=== Iterazione IRL 1266 ===
Loss reward (iter 1266): 2.4466309547424316
=== Iterazione IRL 1267 ===
Loss reward (iter 1267): 3.7297556400299072
=== Iterazione IRL 1268 ===
Loss reward (iter 1268): -2.360517740249634
=== Iterazione IRL 1269 ===
Loss reward (iter 1269): -5.873163223266602
=== Iterazione IRL 1270 ===
Loss reward (iter 1270): -3.1210222244262695
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.36e+03 |
|    critic_loss     | 5.52e+05 |
|    ent_coef        | 7.74     |
|    ent_coef_loss   | 0.0852   |
|    learning_rate   | 0.0003   |
|    n_updates       | 355899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.45e+03 |
|    critic_loss     | 4.5e+05  |
|    ent_coef        | 7.77     |
|    ent_coef_loss   | -0.299   |
|    learning_rate   | 0.0003   |
|    n_updates       | 356299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.45e+03 |
|    critic_loss     | 1.89e+05 |
|    ent_coef        | 8.01     |
|    ent_coef_loss   | 0.344    |
|    learning_rate   | 0.0003   |
|    n_updates       | 356699   |
---------------------------------
=== Iterazione IRL 1271 ===
Loss reward (iter 1271): 0.4470651149749756
=== Iterazione IRL 1272 ===
Loss reward (iter 1272): -5.602260589599609
=== Iterazione IRL 1273 ===
Loss reward (iter 1273): -4.220817565917969
=== Iterazione IRL 1274 ===
Loss reward (iter 1274): -3.2501957416534424
=== Iterazione IRL 1275 ===
Loss reward (iter 1275): -7.346989154815674
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.35e+03 |
|    critic_loss     | 1.66e+05 |
|    ent_coef        | 8.09     |
|    ent_coef_loss   | -0.128   |
|    learning_rate   | 0.0003   |
|    n_updates       | 357299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.37e+03 |
|    critic_loss     | 2.51e+05 |
|    ent_coef        | 7.88     |
|    ent_coef_loss   | 0.521    |
|    learning_rate   | 0.0003   |
|    n_updates       | 357699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.39e+03 |
|    critic_loss     | 1.11e+05 |
|    ent_coef        | 7.88     |
|    ent_coef_loss   | 0.32     |
|    learning_rate   | 0.0003   |
|    n_updates       | 358099   |
---------------------------------
=== Iterazione IRL 1276 ===
Loss reward (iter 1276): 0.2868006229400635
=== Iterazione IRL 1277 ===
Loss reward (iter 1277): -5.116369247436523
=== Iterazione IRL 1278 ===
Loss reward (iter 1278): -7.0344953536987305
=== Iterazione IRL 1279 ===
Loss reward (iter 1279): -1.9529964923858643
=== Iterazione IRL 1280 ===
Loss reward (iter 1280): -0.6544990539550781
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.46e+03 |
|    critic_loss     | 8.38e+04 |
|    ent_coef        | 7.81     |
|    ent_coef_loss   | -0.779   |
|    learning_rate   | 0.0003   |
|    n_updates       | 358699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.44e+03 |
|    critic_loss     | 4.27e+05 |
|    ent_coef        | 7.98     |
|    ent_coef_loss   | -0.521   |
|    learning_rate   | 0.0003   |
|    n_updates       | 359099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.21e+03 |
|    critic_loss     | 1.71e+05 |
|    ent_coef        | 8.27     |
|    ent_coef_loss   | 0.00488  |
|    learning_rate   | 0.0003   |
|    n_updates       | 359499   |
---------------------------------
=== Iterazione IRL 1281 ===
Loss reward (iter 1281): -3.431544542312622
=== Iterazione IRL 1282 ===
Loss reward (iter 1282): 4.639233589172363
=== Iterazione IRL 1283 ===
Loss reward (iter 1283): -2.505802869796753
=== Iterazione IRL 1284 ===
Loss reward (iter 1284): -0.9244532585144043
=== Iterazione IRL 1285 ===
Loss reward (iter 1285): -2.8633275032043457
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.54e+03 |
|    critic_loss     | 1.46e+05 |
|    ent_coef        | 7.94     |
|    ent_coef_loss   | 0.0238   |
|    learning_rate   | 0.0003   |
|    n_updates       | 360099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.44e+03 |
|    critic_loss     | 8.74e+04 |
|    ent_coef        | 7.95     |
|    ent_coef_loss   | 0.129    |
|    learning_rate   | 0.0003   |
|    n_updates       | 360499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.37e+03 |
|    critic_loss     | 3e+05    |
|    ent_coef        | 7.77     |
|    ent_coef_loss   | 0.352    |
|    learning_rate   | 0.0003   |
|    n_updates       | 360899   |
---------------------------------
=== Iterazione IRL 1286 ===
Loss reward (iter 1286): -2.5035605430603027
=== Iterazione IRL 1287 ===
Loss reward (iter 1287): -2.4432263374328613
=== Iterazione IRL 1288 ===
Loss reward (iter 1288): -3.2335383892059326
=== Iterazione IRL 1289 ===
Loss reward (iter 1289): -4.096152305603027
=== Iterazione IRL 1290 ===
Loss reward (iter 1290): -4.962508678436279
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.32e+03 |
|    critic_loss     | 7.65e+04 |
|    ent_coef        | 7.77     |
|    ent_coef_loss   | 0.391    |
|    learning_rate   | 0.0003   |
|    n_updates       | 361499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.45e+03 |
|    critic_loss     | 1.56e+05 |
|    ent_coef        | 7.8      |
|    ent_coef_loss   | -0.00735 |
|    learning_rate   | 0.0003   |
|    n_updates       | 361899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.33e+03 |
|    critic_loss     | 1.66e+05 |
|    ent_coef        | 7.87     |
|    ent_coef_loss   | 0.728    |
|    learning_rate   | 0.0003   |
|    n_updates       | 362299   |
---------------------------------
=== Iterazione IRL 1291 ===
Loss reward (iter 1291): 1.1010091304779053
=== Iterazione IRL 1292 ===
Loss reward (iter 1292): 14.549054145812988
=== Iterazione IRL 1293 ===
Loss reward (iter 1293): 4.836007118225098
=== Iterazione IRL 1294 ===
Loss reward (iter 1294): -3.2120776176452637
=== Iterazione IRL 1295 ===
Loss reward (iter 1295): -3.208556890487671
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.4e+03  |
|    critic_loss     | 1.32e+05 |
|    ent_coef        | 7.81     |
|    ent_coef_loss   | 0.282    |
|    learning_rate   | 0.0003   |
|    n_updates       | 362899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.58e+03 |
|    critic_loss     | 4.23e+05 |
|    ent_coef        | 7.7      |
|    ent_coef_loss   | -0.43    |
|    learning_rate   | 0.0003   |
|    n_updates       | 363299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.5e+03  |
|    critic_loss     | 2.53e+05 |
|    ent_coef        | 7.58     |
|    ent_coef_loss   | 0.118    |
|    learning_rate   | 0.0003   |
|    n_updates       | 363699   |
---------------------------------
=== Iterazione IRL 1296 ===
Loss reward (iter 1296): 1.8826310634613037
=== Iterazione IRL 1297 ===
Loss reward (iter 1297): 2.7555580139160156
=== Iterazione IRL 1298 ===
Loss reward (iter 1298): 9.20755386352539
=== Iterazione IRL 1299 ===
Loss reward (iter 1299): 6.052251815795898
=== Iterazione IRL 1300 ===
Loss reward (iter 1300): 0.005009889602661133
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.52e+03 |
|    critic_loss     | 1.6e+05  |
|    ent_coef        | 7.65     |
|    ent_coef_loss   | 0.49     |
|    learning_rate   | 0.0003   |
|    n_updates       | 364299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.33e+03 |
|    critic_loss     | 3.9e+05  |
|    ent_coef        | 7.73     |
|    ent_coef_loss   | -0.745   |
|    learning_rate   | 0.0003   |
|    n_updates       | 364699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.41e+03 |
|    critic_loss     | 5.55e+04 |
|    ent_coef        | 7.85     |
|    ent_coef_loss   | 0.531    |
|    learning_rate   | 0.0003   |
|    n_updates       | 365099   |
---------------------------------
=== Iterazione IRL 1301 ===
Loss reward (iter 1301): 1.1280982494354248
=== Iterazione IRL 1302 ===
Loss reward (iter 1302): 2.11749005317688
=== Iterazione IRL 1303 ===
Loss reward (iter 1303): -5.581267833709717
=== Iterazione IRL 1304 ===
Loss reward (iter 1304): -4.057361125946045
=== Iterazione IRL 1305 ===
Loss reward (iter 1305): -5.16928243637085
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.38e+03 |
|    critic_loss     | 3.01e+05 |
|    ent_coef        | 7.87     |
|    ent_coef_loss   | 0.344    |
|    learning_rate   | 0.0003   |
|    n_updates       | 365699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.39e+03 |
|    critic_loss     | 1.39e+05 |
|    ent_coef        | 8.04     |
|    ent_coef_loss   | 0.532    |
|    learning_rate   | 0.0003   |
|    n_updates       | 366099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.45e+03 |
|    critic_loss     | 8.78e+04 |
|    ent_coef        | 8.02     |
|    ent_coef_loss   | 0.311    |
|    learning_rate   | 0.0003   |
|    n_updates       | 366499   |
---------------------------------
=== Iterazione IRL 1306 ===
Loss reward (iter 1306): -7.825384140014648
=== Iterazione IRL 1307 ===
Loss reward (iter 1307): 0.2284398078918457
=== Iterazione IRL 1308 ===
Loss reward (iter 1308): -5.916910171508789
=== Iterazione IRL 1309 ===
Loss reward (iter 1309): -4.9963860511779785
=== Iterazione IRL 1310 ===
Loss reward (iter 1310): 2.004772663116455
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.4e+03  |
|    critic_loss     | 1.63e+05 |
|    ent_coef        | 7.92     |
|    ent_coef_loss   | -0.296   |
|    learning_rate   | 0.0003   |
|    n_updates       | 367099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 130      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.45e+03 |
|    critic_loss     | 1.57e+05 |
|    ent_coef        | 7.94     |
|    ent_coef_loss   | -0.0246  |
|    learning_rate   | 0.0003   |
|    n_updates       | 367499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.3e+03  |
|    critic_loss     | 1.6e+05  |
|    ent_coef        | 7.93     |
|    ent_coef_loss   | -0.337   |
|    learning_rate   | 0.0003   |
|    n_updates       | 367899   |
---------------------------------
=== Iterazione IRL 1311 ===
Loss reward (iter 1311): 3.2543203830718994
=== Iterazione IRL 1312 ===
Loss reward (iter 1312): -1.5874183177947998
=== Iterazione IRL 1313 ===
Loss reward (iter 1313): -0.9110503196716309
=== Iterazione IRL 1314 ===
Loss reward (iter 1314): -5.260903835296631
=== Iterazione IRL 1315 ===
Loss reward (iter 1315): -5.386406898498535
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.6e+03  |
|    critic_loss     | 4.18e+05 |
|    ent_coef        | 7.95     |
|    ent_coef_loss   | -0.508   |
|    learning_rate   | 0.0003   |
|    n_updates       | 368499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.39e+03 |
|    critic_loss     | 1.51e+05 |
|    ent_coef        | 7.9      |
|    ent_coef_loss   | 0.1      |
|    learning_rate   | 0.0003   |
|    n_updates       | 368899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.39e+03 |
|    critic_loss     | 2.32e+05 |
|    ent_coef        | 7.96     |
|    ent_coef_loss   | -0.434   |
|    learning_rate   | 0.0003   |
|    n_updates       | 369299   |
---------------------------------
=== Iterazione IRL 1316 ===
Loss reward (iter 1316): -0.299774169921875
=== Iterazione IRL 1317 ===
Loss reward (iter 1317): -3.8946831226348877
=== Iterazione IRL 1318 ===
Loss reward (iter 1318): -0.030279874801635742
=== Iterazione IRL 1319 ===
Loss reward (iter 1319): 0.7930400371551514
=== Iterazione IRL 1320 ===
Loss reward (iter 1320): 0.6161754131317139
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.24e+03 |
|    critic_loss     | 2.09e+05 |
|    ent_coef        | 8.03     |
|    ent_coef_loss   | 0.642    |
|    learning_rate   | 0.0003   |
|    n_updates       | 369899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.51e+03 |
|    critic_loss     | 2.3e+05  |
|    ent_coef        | 8.11     |
|    ent_coef_loss   | -0.159   |
|    learning_rate   | 0.0003   |
|    n_updates       | 370299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.37e+03 |
|    critic_loss     | 1.38e+05 |
|    ent_coef        | 8.05     |
|    ent_coef_loss   | -0.195   |
|    learning_rate   | 0.0003   |
|    n_updates       | 370699   |
---------------------------------
=== Iterazione IRL 1321 ===
Loss reward (iter 1321): 7.175106048583984
=== Iterazione IRL 1322 ===
Loss reward (iter 1322): 0.4732823371887207
=== Iterazione IRL 1323 ===
Loss reward (iter 1323): 1.725921630859375
=== Iterazione IRL 1324 ===
Loss reward (iter 1324): -1.259214162826538
=== Iterazione IRL 1325 ===
Loss reward (iter 1325): -3.0996487140655518
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.53e+03 |
|    critic_loss     | 1.15e+05 |
|    ent_coef        | 8.04     |
|    ent_coef_loss   | -0.699   |
|    learning_rate   | 0.0003   |
|    n_updates       | 371299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.35e+03 |
|    critic_loss     | 2.77e+05 |
|    ent_coef        | 8.19     |
|    ent_coef_loss   | -0.326   |
|    learning_rate   | 0.0003   |
|    n_updates       | 371699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.44e+03 |
|    critic_loss     | 2.7e+05  |
|    ent_coef        | 8.26     |
|    ent_coef_loss   | -0.444   |
|    learning_rate   | 0.0003   |
|    n_updates       | 372099   |
---------------------------------
=== Iterazione IRL 1326 ===
Loss reward (iter 1326): -3.4862658977508545
=== Iterazione IRL 1327 ===
Loss reward (iter 1327): -0.8958649635314941
=== Iterazione IRL 1328 ===
Loss reward (iter 1328): 2.713771104812622
=== Iterazione IRL 1329 ===
Loss reward (iter 1329): -1.0290305614471436
=== Iterazione IRL 1330 ===
Loss reward (iter 1330): -0.9949321746826172
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.39e+03 |
|    critic_loss     | 1.54e+05 |
|    ent_coef        | 8.35     |
|    ent_coef_loss   | -0.266   |
|    learning_rate   | 0.0003   |
|    n_updates       | 372699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.35e+03 |
|    critic_loss     | 1.54e+05 |
|    ent_coef        | 8.38     |
|    ent_coef_loss   | -0.134   |
|    learning_rate   | 0.0003   |
|    n_updates       | 373099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.33e+03 |
|    critic_loss     | 5.05e+05 |
|    ent_coef        | 8.26     |
|    ent_coef_loss   | 0.304    |
|    learning_rate   | 0.0003   |
|    n_updates       | 373499   |
---------------------------------
=== Iterazione IRL 1331 ===
Loss reward (iter 1331): -1.7976505756378174
=== Iterazione IRL 1332 ===
Loss reward (iter 1332): 1.253075361251831
=== Iterazione IRL 1333 ===
Loss reward (iter 1333): -3.9626283645629883
=== Iterazione IRL 1334 ===
Loss reward (iter 1334): 1.2466776371002197
=== Iterazione IRL 1335 ===
Loss reward (iter 1335): -1.2653875350952148
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.35e+03 |
|    critic_loss     | 9.64e+04 |
|    ent_coef        | 8.11     |
|    ent_coef_loss   | -0.121   |
|    learning_rate   | 0.0003   |
|    n_updates       | 374099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.49e+03 |
|    critic_loss     | 1.14e+05 |
|    ent_coef        | 8.07     |
|    ent_coef_loss   | 0.11     |
|    learning_rate   | 0.0003   |
|    n_updates       | 374499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.47e+03 |
|    critic_loss     | 8.85e+04 |
|    ent_coef        | 8.1      |
|    ent_coef_loss   | 0.538    |
|    learning_rate   | 0.0003   |
|    n_updates       | 374899   |
---------------------------------
=== Iterazione IRL 1336 ===
Loss reward (iter 1336): -1.9309563636779785
=== Iterazione IRL 1337 ===
Loss reward (iter 1337): -4.011768341064453
=== Iterazione IRL 1338 ===
Loss reward (iter 1338): -2.728135585784912
=== Iterazione IRL 1339 ===
Loss reward (iter 1339): -4.435454368591309
=== Iterazione IRL 1340 ===
Loss reward (iter 1340): 1.6527273654937744
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.48e+03 |
|    critic_loss     | 8.47e+04 |
|    ent_coef        | 8.03     |
|    ent_coef_loss   | 0.584    |
|    learning_rate   | 0.0003   |
|    n_updates       | 375499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.53e+03 |
|    critic_loss     | 5.38e+04 |
|    ent_coef        | 8.07     |
|    ent_coef_loss   | 0.0768   |
|    learning_rate   | 0.0003   |
|    n_updates       | 375899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.47e+03 |
|    critic_loss     | 1.26e+05 |
|    ent_coef        | 7.93     |
|    ent_coef_loss   | -0.085   |
|    learning_rate   | 0.0003   |
|    n_updates       | 376299   |
---------------------------------
=== Iterazione IRL 1341 ===
Loss reward (iter 1341): 0.5483477115631104
=== Iterazione IRL 1342 ===
Loss reward (iter 1342): 2.4536643028259277
=== Iterazione IRL 1343 ===
Loss reward (iter 1343): 3.13981032371521
=== Iterazione IRL 1344 ===
Loss reward (iter 1344): 3.687497854232788
=== Iterazione IRL 1345 ===
Loss reward (iter 1345): -1.978543758392334
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.27e+03 |
|    critic_loss     | 1.58e+05 |
|    ent_coef        | 8.09     |
|    ent_coef_loss   | 0.163    |
|    learning_rate   | 0.0003   |
|    n_updates       | 376899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.33e+03 |
|    critic_loss     | 1.43e+05 |
|    ent_coef        | 8.22     |
|    ent_coef_loss   | -0.313   |
|    learning_rate   | 0.0003   |
|    n_updates       | 377299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.46e+03 |
|    critic_loss     | 3.87e+05 |
|    ent_coef        | 8.21     |
|    ent_coef_loss   | 0.045    |
|    learning_rate   | 0.0003   |
|    n_updates       | 377699   |
---------------------------------
=== Iterazione IRL 1346 ===
Loss reward (iter 1346): -3.0448317527770996
=== Iterazione IRL 1347 ===
Loss reward (iter 1347): -0.9944794178009033
=== Iterazione IRL 1348 ===
Loss reward (iter 1348): 1.7558560371398926
=== Iterazione IRL 1349 ===
Loss reward (iter 1349): 3.817859649658203
=== Iterazione IRL 1350 ===
Loss reward (iter 1350): 3.2976489067077637
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.28e+03 |
|    critic_loss     | 1.99e+05 |
|    ent_coef        | 8.07     |
|    ent_coef_loss   | -0.2     |
|    learning_rate   | 0.0003   |
|    n_updates       | 378299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.35e+03 |
|    critic_loss     | 1.85e+05 |
|    ent_coef        | 8.18     |
|    ent_coef_loss   | 0.192    |
|    learning_rate   | 0.0003   |
|    n_updates       | 378699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.5e+03  |
|    critic_loss     | 1.67e+05 |
|    ent_coef        | 8.23     |
|    ent_coef_loss   | 0.282    |
|    learning_rate   | 0.0003   |
|    n_updates       | 379099   |
---------------------------------
=== Iterazione IRL 1351 ===
Loss reward (iter 1351): 5.2324066162109375
=== Iterazione IRL 1352 ===
Loss reward (iter 1352): -1.0669841766357422
=== Iterazione IRL 1353 ===
Loss reward (iter 1353): -2.2193703651428223
=== Iterazione IRL 1354 ===
Loss reward (iter 1354): 1.7494609355926514
=== Iterazione IRL 1355 ===
Loss reward (iter 1355): -0.15796971321105957
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.61e+03 |
|    critic_loss     | 1.26e+05 |
|    ent_coef        | 8.04     |
|    ent_coef_loss   | 0.299    |
|    learning_rate   | 0.0003   |
|    n_updates       | 379699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 130      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.45e+03 |
|    critic_loss     | 1.77e+05 |
|    ent_coef        | 7.95     |
|    ent_coef_loss   | 0.0407   |
|    learning_rate   | 0.0003   |
|    n_updates       | 380099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.31e+03 |
|    critic_loss     | 2.89e+05 |
|    ent_coef        | 8.1      |
|    ent_coef_loss   | -0.274   |
|    learning_rate   | 0.0003   |
|    n_updates       | 380499   |
---------------------------------
=== Iterazione IRL 1356 ===
Loss reward (iter 1356): -0.011637210845947266
=== Iterazione IRL 1357 ===
Loss reward (iter 1357): -6.8879780769348145
=== Iterazione IRL 1358 ===
Loss reward (iter 1358): -4.811640739440918
=== Iterazione IRL 1359 ===
Loss reward (iter 1359): 2.6193127632141113
=== Iterazione IRL 1360 ===
Loss reward (iter 1360): 3.5536913871765137
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.39e+03 |
|    critic_loss     | 2.2e+05  |
|    ent_coef        | 8.37     |
|    ent_coef_loss   | -0.0494  |
|    learning_rate   | 0.0003   |
|    n_updates       | 381099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 130      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.44e+03 |
|    critic_loss     | 1.4e+05  |
|    ent_coef        | 8.29     |
|    ent_coef_loss   | 1.01     |
|    learning_rate   | 0.0003   |
|    n_updates       | 381499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.31e+03 |
|    critic_loss     | 3.53e+05 |
|    ent_coef        | 8.17     |
|    ent_coef_loss   | -0.525   |
|    learning_rate   | 0.0003   |
|    n_updates       | 381899   |
---------------------------------
=== Iterazione IRL 1361 ===
Loss reward (iter 1361): 5.918994903564453
=== Iterazione IRL 1362 ===
Loss reward (iter 1362): -0.755455493927002
=== Iterazione IRL 1363 ===
Loss reward (iter 1363): 3.633218288421631
=== Iterazione IRL 1364 ===
Loss reward (iter 1364): -0.5330946445465088
=== Iterazione IRL 1365 ===
Loss reward (iter 1365): 1.6300239562988281
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.46e+03 |
|    critic_loss     | 3.16e+05 |
|    ent_coef        | 8.16     |
|    ent_coef_loss   | 0.0495   |
|    learning_rate   | 0.0003   |
|    n_updates       | 382499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.35e+03 |
|    critic_loss     | 1.25e+05 |
|    ent_coef        | 8.16     |
|    ent_coef_loss   | 0.0843   |
|    learning_rate   | 0.0003   |
|    n_updates       | 382899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.49e+03 |
|    critic_loss     | 1.38e+05 |
|    ent_coef        | 8.28     |
|    ent_coef_loss   | 0.0888   |
|    learning_rate   | 0.0003   |
|    n_updates       | 383299   |
---------------------------------
=== Iterazione IRL 1366 ===
Loss reward (iter 1366): 0.4060704708099365
=== Iterazione IRL 1367 ===
Loss reward (iter 1367): -3.8879191875457764
=== Iterazione IRL 1368 ===
Loss reward (iter 1368): -4.570169448852539
=== Iterazione IRL 1369 ===
Loss reward (iter 1369): -0.27132296562194824
=== Iterazione IRL 1370 ===
Loss reward (iter 1370): -2.833434581756592
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.38e+03 |
|    critic_loss     | 4.34e+05 |
|    ent_coef        | 8.2      |
|    ent_coef_loss   | -0.764   |
|    learning_rate   | 0.0003   |
|    n_updates       | 383899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.37e+03 |
|    critic_loss     | 1.43e+05 |
|    ent_coef        | 8.24     |
|    ent_coef_loss   | 0.419    |
|    learning_rate   | 0.0003   |
|    n_updates       | 384299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.53e+03 |
|    critic_loss     | 2.28e+05 |
|    ent_coef        | 8.28     |
|    ent_coef_loss   | 0.229    |
|    learning_rate   | 0.0003   |
|    n_updates       | 384699   |
---------------------------------
=== Iterazione IRL 1371 ===
Loss reward (iter 1371): 0.575566291809082
=== Iterazione IRL 1372 ===
Loss reward (iter 1372): -4.05675745010376
=== Iterazione IRL 1373 ===
Loss reward (iter 1373): -6.837785720825195
=== Iterazione IRL 1374 ===
Loss reward (iter 1374): -4.785304069519043
=== Iterazione IRL 1375 ===
Loss reward (iter 1375): -10.185202598571777
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.45e+03 |
|    critic_loss     | 8.31e+04 |
|    ent_coef        | 8.34     |
|    ent_coef_loss   | -0.21    |
|    learning_rate   | 0.0003   |
|    n_updates       | 385299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.3e+03  |
|    critic_loss     | 1.7e+05  |
|    ent_coef        | 8.31     |
|    ent_coef_loss   | -1.18    |
|    learning_rate   | 0.0003   |
|    n_updates       | 385699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.32e+03 |
|    critic_loss     | 1.65e+05 |
|    ent_coef        | 8.15     |
|    ent_coef_loss   | -0.224   |
|    learning_rate   | 0.0003   |
|    n_updates       | 386099   |
---------------------------------
=== Iterazione IRL 1376 ===
Loss reward (iter 1376): -10.549560546875
=== Iterazione IRL 1377 ===
Loss reward (iter 1377): -4.308234214782715
=== Iterazione IRL 1378 ===
Loss reward (iter 1378): -10.893747329711914
=== Iterazione IRL 1379 ===
Loss reward (iter 1379): -12.829996109008789
=== Iterazione IRL 1380 ===
Loss reward (iter 1380): -10.464982986450195
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.26e+03 |
|    critic_loss     | 2.66e+05 |
|    ent_coef        | 8.17     |
|    ent_coef_loss   | 0.373    |
|    learning_rate   | 0.0003   |
|    n_updates       | 386699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.35e+03 |
|    critic_loss     | 7.87e+04 |
|    ent_coef        | 8.18     |
|    ent_coef_loss   | 0.582    |
|    learning_rate   | 0.0003   |
|    n_updates       | 387099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.26e+03 |
|    critic_loss     | 1.54e+05 |
|    ent_coef        | 8.16     |
|    ent_coef_loss   | 0.532    |
|    learning_rate   | 0.0003   |
|    n_updates       | 387499   |
---------------------------------
=== Iterazione IRL 1381 ===
Loss reward (iter 1381): -5.909783363342285
=== Iterazione IRL 1382 ===
Loss reward (iter 1382): -1.6756598949432373
=== Iterazione IRL 1383 ===
Loss reward (iter 1383): 4.472098350524902
=== Iterazione IRL 1384 ===
Loss reward (iter 1384): -4.371891975402832
=== Iterazione IRL 1385 ===
Loss reward (iter 1385): -7.1910295486450195
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.44e+03 |
|    critic_loss     | 1.76e+05 |
|    ent_coef        | 7.96     |
|    ent_coef_loss   | -0.194   |
|    learning_rate   | 0.0003   |
|    n_updates       | 388099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.25e+03 |
|    critic_loss     | 1.47e+05 |
|    ent_coef        | 8.16     |
|    ent_coef_loss   | 0.286    |
|    learning_rate   | 0.0003   |
|    n_updates       | 388499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.24e+03 |
|    critic_loss     | 1.3e+05  |
|    ent_coef        | 8.01     |
|    ent_coef_loss   | -0.112   |
|    learning_rate   | 0.0003   |
|    n_updates       | 388899   |
---------------------------------
=== Iterazione IRL 1386 ===
Loss reward (iter 1386): -3.408146858215332
=== Iterazione IRL 1387 ===
Loss reward (iter 1387): -8.041655540466309
=== Iterazione IRL 1388 ===
Loss reward (iter 1388): -7.197175025939941
=== Iterazione IRL 1389 ===
Loss reward (iter 1389): -11.85671329498291
=== Iterazione IRL 1390 ===
Loss reward (iter 1390): -5.168330192565918
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.22e+03 |
|    critic_loss     | 1.47e+05 |
|    ent_coef        | 7.83     |
|    ent_coef_loss   | -0.0576  |
|    learning_rate   | 0.0003   |
|    n_updates       | 389499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.2e+03  |
|    critic_loss     | 2.51e+05 |
|    ent_coef        | 7.87     |
|    ent_coef_loss   | -0.257   |
|    learning_rate   | 0.0003   |
|    n_updates       | 389899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.08e+03 |
|    critic_loss     | 7.81e+04 |
|    ent_coef        | 8.01     |
|    ent_coef_loss   | 0.363    |
|    learning_rate   | 0.0003   |
|    n_updates       | 390299   |
---------------------------------
=== Iterazione IRL 1391 ===
Loss reward (iter 1391): -9.573028564453125
=== Iterazione IRL 1392 ===
Loss reward (iter 1392): -9.49098014831543
=== Iterazione IRL 1393 ===
Loss reward (iter 1393): -11.12621784210205
=== Iterazione IRL 1394 ===
Loss reward (iter 1394): -14.013381004333496
=== Iterazione IRL 1395 ===
Loss reward (iter 1395): -12.278894424438477
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.28e+03 |
|    critic_loss     | 7.64e+04 |
|    ent_coef        | 8.21     |
|    ent_coef_loss   | -0.116   |
|    learning_rate   | 0.0003   |
|    n_updates       | 390899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.21e+03 |
|    critic_loss     | 1.81e+05 |
|    ent_coef        | 8.14     |
|    ent_coef_loss   | 0.498    |
|    learning_rate   | 0.0003   |
|    n_updates       | 391299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.25e+03 |
|    critic_loss     | 1.38e+05 |
|    ent_coef        | 8.3      |
|    ent_coef_loss   | 0.0757   |
|    learning_rate   | 0.0003   |
|    n_updates       | 391699   |
---------------------------------
=== Iterazione IRL 1396 ===
Loss reward (iter 1396): -7.01385498046875
=== Iterazione IRL 1397 ===
Loss reward (iter 1397): -0.16231703758239746
=== Iterazione IRL 1398 ===
Loss reward (iter 1398): -9.747003555297852
=== Iterazione IRL 1399 ===
Loss reward (iter 1399): -12.209177017211914
=== Iterazione IRL 1400 ===
Loss reward (iter 1400): -7.392398834228516
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.25e+03 |
|    critic_loss     | 1.06e+05 |
|    ent_coef        | 8.02     |
|    ent_coef_loss   | -0.039   |
|    learning_rate   | 0.0003   |
|    n_updates       | 392299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.24e+03 |
|    critic_loss     | 1.05e+05 |
|    ent_coef        | 8        |
|    ent_coef_loss   | -0.0202  |
|    learning_rate   | 0.0003   |
|    n_updates       | 392699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.23e+03 |
|    critic_loss     | 1.37e+05 |
|    ent_coef        | 8.07     |
|    ent_coef_loss   | 0.662    |
|    learning_rate   | 0.0003   |
|    n_updates       | 393099   |
---------------------------------
=== Iterazione IRL 1401 ===
Loss reward (iter 1401): 4.099373817443848
=== Iterazione IRL 1402 ===
Loss reward (iter 1402): -6.032541275024414
=== Iterazione IRL 1403 ===
Loss reward (iter 1403): -7.407296180725098
=== Iterazione IRL 1404 ===
Loss reward (iter 1404): -9.638162612915039
=== Iterazione IRL 1405 ===
Loss reward (iter 1405): -12.793123245239258
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.3e+03  |
|    critic_loss     | 1.02e+05 |
|    ent_coef        | 8.03     |
|    ent_coef_loss   | 0.323    |
|    learning_rate   | 0.0003   |
|    n_updates       | 393699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.29e+03 |
|    critic_loss     | 1.44e+05 |
|    ent_coef        | 7.95     |
|    ent_coef_loss   | -0.296   |
|    learning_rate   | 0.0003   |
|    n_updates       | 394099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.3e+03  |
|    critic_loss     | 3e+05    |
|    ent_coef        | 7.78     |
|    ent_coef_loss   | -0.259   |
|    learning_rate   | 0.0003   |
|    n_updates       | 394499   |
---------------------------------
=== Iterazione IRL 1406 ===
Loss reward (iter 1406): 6.519023895263672
=== Iterazione IRL 1407 ===
Loss reward (iter 1407): 20.64064598083496
=== Iterazione IRL 1408 ===
Loss reward (iter 1408): 8.099699020385742
=== Iterazione IRL 1409 ===
Loss reward (iter 1409): 2.3781278133392334
=== Iterazione IRL 1410 ===
Loss reward (iter 1410): 2.0895278453826904
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.33e+03 |
|    critic_loss     | 4.13e+05 |
|    ent_coef        | 7.87     |
|    ent_coef_loss   | 0.183    |
|    learning_rate   | 0.0003   |
|    n_updates       | 395099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.26e+03 |
|    critic_loss     | 1.12e+05 |
|    ent_coef        | 7.84     |
|    ent_coef_loss   | 0.146    |
|    learning_rate   | 0.0003   |
|    n_updates       | 395499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.21e+03 |
|    critic_loss     | 1.66e+05 |
|    ent_coef        | 7.96     |
|    ent_coef_loss   | 0.301    |
|    learning_rate   | 0.0003   |
|    n_updates       | 395899   |
---------------------------------
=== Iterazione IRL 1411 ===
Loss reward (iter 1411): -4.144031047821045
=== Iterazione IRL 1412 ===
Loss reward (iter 1412): -9.488201141357422
=== Iterazione IRL 1413 ===
Loss reward (iter 1413): -6.002352237701416
=== Iterazione IRL 1414 ===
Loss reward (iter 1414): -10.830307960510254
=== Iterazione IRL 1415 ===
Loss reward (iter 1415): -5.876806259155273
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.2e+03  |
|    critic_loss     | 3.1e+05  |
|    ent_coef        | 7.72     |
|    ent_coef_loss   | -0.391   |
|    learning_rate   | 0.0003   |
|    n_updates       | 396499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.11e+03 |
|    critic_loss     | 1.72e+05 |
|    ent_coef        | 7.9      |
|    ent_coef_loss   | -0.41    |
|    learning_rate   | 0.0003   |
|    n_updates       | 396899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.19e+03 |
|    critic_loss     | 1.39e+05 |
|    ent_coef        | 7.89     |
|    ent_coef_loss   | -0.402   |
|    learning_rate   | 0.0003   |
|    n_updates       | 397299   |
---------------------------------
=== Iterazione IRL 1416 ===
Loss reward (iter 1416): -4.194650650024414
=== Iterazione IRL 1417 ===
Loss reward (iter 1417): -2.9374260902404785
=== Iterazione IRL 1418 ===
Loss reward (iter 1418): -6.322698593139648
=== Iterazione IRL 1419 ===
Loss reward (iter 1419): -6.211790084838867
=== Iterazione IRL 1420 ===
Loss reward (iter 1420): -5.979819297790527
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.18e+03 |
|    critic_loss     | 6.37e+04 |
|    ent_coef        | 7.93     |
|    ent_coef_loss   | -0.223   |
|    learning_rate   | 0.0003   |
|    n_updates       | 397899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.09e+03 |
|    critic_loss     | 1.42e+05 |
|    ent_coef        | 8.16     |
|    ent_coef_loss   | 0.0874   |
|    learning_rate   | 0.0003   |
|    n_updates       | 398299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.22e+03 |
|    critic_loss     | 1.62e+05 |
|    ent_coef        | 8.01     |
|    ent_coef_loss   | -0.0459  |
|    learning_rate   | 0.0003   |
|    n_updates       | 398699   |
---------------------------------
=== Iterazione IRL 1421 ===
Loss reward (iter 1421): -5.3746771812438965
=== Iterazione IRL 1422 ===
Loss reward (iter 1422): 16.01336097717285
=== Iterazione IRL 1423 ===
Loss reward (iter 1423): -5.574563026428223
=== Iterazione IRL 1424 ===
Loss reward (iter 1424): -0.9011006355285645
=== Iterazione IRL 1425 ===
Loss reward (iter 1425): 1.8353323936462402
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.98e+03 |
|    critic_loss     | 2.74e+05 |
|    ent_coef        | 8        |
|    ent_coef_loss   | 0.157    |
|    learning_rate   | 0.0003   |
|    n_updates       | 399299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.1e+03  |
|    critic_loss     | 1.96e+05 |
|    ent_coef        | 7.84     |
|    ent_coef_loss   | 0.261    |
|    learning_rate   | 0.0003   |
|    n_updates       | 399699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.31e+03 |
|    critic_loss     | 7.3e+04  |
|    ent_coef        | 7.91     |
|    ent_coef_loss   | 0.0109   |
|    learning_rate   | 0.0003   |
|    n_updates       | 400099   |
---------------------------------
=== Iterazione IRL 1426 ===
Loss reward (iter 1426): -4.492734909057617
=== Iterazione IRL 1427 ===
Loss reward (iter 1427): -9.315718650817871
=== Iterazione IRL 1428 ===
Loss reward (iter 1428): -1.3814942836761475
=== Iterazione IRL 1429 ===
Loss reward (iter 1429): -6.369549751281738
=== Iterazione IRL 1430 ===
Loss reward (iter 1430): -4.307768821716309
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.13e+03 |
|    critic_loss     | 9.87e+04 |
|    ent_coef        | 7.85     |
|    ent_coef_loss   | -1.12    |
|    learning_rate   | 0.0003   |
|    n_updates       | 400699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.09e+03 |
|    critic_loss     | 1.22e+05 |
|    ent_coef        | 7.88     |
|    ent_coef_loss   | -0.494   |
|    learning_rate   | 0.0003   |
|    n_updates       | 401099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.12e+03 |
|    critic_loss     | 8.97e+04 |
|    ent_coef        | 7.94     |
|    ent_coef_loss   | -0.242   |
|    learning_rate   | 0.0003   |
|    n_updates       | 401499   |
---------------------------------
=== Iterazione IRL 1431 ===
Loss reward (iter 1431): 5.580790996551514
=== Iterazione IRL 1432 ===
Loss reward (iter 1432): -1.343442440032959
=== Iterazione IRL 1433 ===
Loss reward (iter 1433): 9.546354293823242
=== Iterazione IRL 1434 ===
Loss reward (iter 1434): -0.6724317073822021
=== Iterazione IRL 1435 ===
Loss reward (iter 1435): -2.872340202331543
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.15e+03 |
|    critic_loss     | 1.89e+05 |
|    ent_coef        | 8.08     |
|    ent_coef_loss   | -0.232   |
|    learning_rate   | 0.0003   |
|    n_updates       | 402099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.13e+03 |
|    critic_loss     | 3.58e+05 |
|    ent_coef        | 8.04     |
|    ent_coef_loss   | 0.498    |
|    learning_rate   | 0.0003   |
|    n_updates       | 402499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.1e+03  |
|    critic_loss     | 2.87e+05 |
|    ent_coef        | 8.11     |
|    ent_coef_loss   | -0.353   |
|    learning_rate   | 0.0003   |
|    n_updates       | 402899   |
---------------------------------
=== Iterazione IRL 1436 ===
Loss reward (iter 1436): -4.7737345695495605
=== Iterazione IRL 1437 ===
Loss reward (iter 1437): -2.4242286682128906
=== Iterazione IRL 1438 ===
Loss reward (iter 1438): -6.21510124206543
=== Iterazione IRL 1439 ===
Loss reward (iter 1439): -5.235841751098633
=== Iterazione IRL 1440 ===
Loss reward (iter 1440): -8.256695747375488
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.88e+03 |
|    critic_loss     | 1.31e+05 |
|    ent_coef        | 7.84     |
|    ent_coef_loss   | 0.289    |
|    learning_rate   | 0.0003   |
|    n_updates       | 403499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.23e+03 |
|    critic_loss     | 7.85e+04 |
|    ent_coef        | 7.88     |
|    ent_coef_loss   | -0.282   |
|    learning_rate   | 0.0003   |
|    n_updates       | 403899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.05e+03 |
|    critic_loss     | 8.87e+04 |
|    ent_coef        | 8.04     |
|    ent_coef_loss   | 0.476    |
|    learning_rate   | 0.0003   |
|    n_updates       | 404299   |
---------------------------------
=== Iterazione IRL 1441 ===
Loss reward (iter 1441): -1.9255106449127197
=== Iterazione IRL 1442 ===
Loss reward (iter 1442): -5.0800628662109375
=== Iterazione IRL 1443 ===
Loss reward (iter 1443): 9.699874877929688
=== Iterazione IRL 1444 ===
Loss reward (iter 1444): -0.7327642440795898
=== Iterazione IRL 1445 ===
Loss reward (iter 1445): -1.9971799850463867
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.96e+03 |
|    critic_loss     | 1.17e+05 |
|    ent_coef        | 7.89     |
|    ent_coef_loss   | 0.398    |
|    learning_rate   | 0.0003   |
|    n_updates       | 404899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.11e+03 |
|    critic_loss     | 3.48e+05 |
|    ent_coef        | 7.88     |
|    ent_coef_loss   | 0.403    |
|    learning_rate   | 0.0003   |
|    n_updates       | 405299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.94e+03 |
|    critic_loss     | 1.64e+05 |
|    ent_coef        | 7.8      |
|    ent_coef_loss   | -0.492   |
|    learning_rate   | 0.0003   |
|    n_updates       | 405699   |
---------------------------------
=== Iterazione IRL 1446 ===
Loss reward (iter 1446): 5.7160773277282715
=== Iterazione IRL 1447 ===
Loss reward (iter 1447): -2.9605507850646973
=== Iterazione IRL 1448 ===
Loss reward (iter 1448): -0.9129102230072021
=== Iterazione IRL 1449 ===
Loss reward (iter 1449): 0.4341597557067871
=== Iterazione IRL 1450 ===
Loss reward (iter 1450): 0.07714176177978516
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.1e+03  |
|    critic_loss     | 5.61e+05 |
|    ent_coef        | 7.95     |
|    ent_coef_loss   | -0.309   |
|    learning_rate   | 0.0003   |
|    n_updates       | 406299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.97e+03 |
|    critic_loss     | 4.22e+05 |
|    ent_coef        | 7.95     |
|    ent_coef_loss   | 0.153    |
|    learning_rate   | 0.0003   |
|    n_updates       | 406699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.87e+03 |
|    critic_loss     | 1.35e+05 |
|    ent_coef        | 8.1      |
|    ent_coef_loss   | 0.347    |
|    learning_rate   | 0.0003   |
|    n_updates       | 407099   |
---------------------------------
=== Iterazione IRL 1451 ===
Loss reward (iter 1451): -2.6341664791107178
=== Iterazione IRL 1452 ===
Loss reward (iter 1452): -5.148196220397949
=== Iterazione IRL 1453 ===
Loss reward (iter 1453): -1.3302714824676514
=== Iterazione IRL 1454 ===
Loss reward (iter 1454): -3.343482255935669
=== Iterazione IRL 1455 ===
Loss reward (iter 1455): -7.489810943603516
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.98e+03 |
|    critic_loss     | 1.01e+05 |
|    ent_coef        | 8.01     |
|    ent_coef_loss   | 0.333    |
|    learning_rate   | 0.0003   |
|    n_updates       | 407699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.94e+03 |
|    critic_loss     | 1.81e+05 |
|    ent_coef        | 8.03     |
|    ent_coef_loss   | 0.173    |
|    learning_rate   | 0.0003   |
|    n_updates       | 408099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.85e+03 |
|    critic_loss     | 2.55e+05 |
|    ent_coef        | 8.07     |
|    ent_coef_loss   | -0.0898  |
|    learning_rate   | 0.0003   |
|    n_updates       | 408499   |
---------------------------------
=== Iterazione IRL 1456 ===
Loss reward (iter 1456): -7.094091415405273
=== Iterazione IRL 1457 ===
Loss reward (iter 1457): -6.462510108947754
=== Iterazione IRL 1458 ===
Loss reward (iter 1458): -4.700980186462402
=== Iterazione IRL 1459 ===
Loss reward (iter 1459): 5.983258247375488
=== Iterazione IRL 1460 ===
Loss reward (iter 1460): -4.972134590148926
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.99e+03 |
|    critic_loss     | 1.1e+05  |
|    ent_coef        | 7.94     |
|    ent_coef_loss   | 0.116    |
|    learning_rate   | 0.0003   |
|    n_updates       | 409099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.82e+03 |
|    critic_loss     | 6.43e+04 |
|    ent_coef        | 7.95     |
|    ent_coef_loss   | 0.305    |
|    learning_rate   | 0.0003   |
|    n_updates       | 409499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.92e+03 |
|    critic_loss     | 1.88e+05 |
|    ent_coef        | 7.88     |
|    ent_coef_loss   | 0.214    |
|    learning_rate   | 0.0003   |
|    n_updates       | 409899   |
---------------------------------
=== Iterazione IRL 1461 ===
Loss reward (iter 1461): 4.215047836303711
=== Iterazione IRL 1462 ===
Loss reward (iter 1462): 5.544684410095215
=== Iterazione IRL 1463 ===
Loss reward (iter 1463): 3.087812900543213
=== Iterazione IRL 1464 ===
Loss reward (iter 1464): 2.550851345062256
=== Iterazione IRL 1465 ===
Loss reward (iter 1465): -3.5056097507476807
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.92e+03 |
|    critic_loss     | 1.04e+05 |
|    ent_coef        | 7.86     |
|    ent_coef_loss   | -0.304   |
|    learning_rate   | 0.0003   |
|    n_updates       | 410499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 3.06e+03 |
|    critic_loss     | 9.14e+04 |
|    ent_coef        | 8.16     |
|    ent_coef_loss   | -0.152   |
|    learning_rate   | 0.0003   |
|    n_updates       | 410899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.99e+03 |
|    critic_loss     | 7.82e+04 |
|    ent_coef        | 8.12     |
|    ent_coef_loss   | 0.422    |
|    learning_rate   | 0.0003   |
|    n_updates       | 411299   |
---------------------------------
=== Iterazione IRL 1466 ===
Loss reward (iter 1466): -0.8139832019805908
=== Iterazione IRL 1467 ===
Loss reward (iter 1467): 1.7962291240692139
=== Iterazione IRL 1468 ===
Loss reward (iter 1468): -2.2604870796203613
=== Iterazione IRL 1469 ===
Loss reward (iter 1469): -0.7985305786132812
=== Iterazione IRL 1470 ===
Loss reward (iter 1470): 1.4974992275238037
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.95e+03 |
|    critic_loss     | 1.34e+05 |
|    ent_coef        | 8.22     |
|    ent_coef_loss   | 0.617    |
|    learning_rate   | 0.0003   |
|    n_updates       | 411899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.87e+03 |
|    critic_loss     | 2.48e+05 |
|    ent_coef        | 8.06     |
|    ent_coef_loss   | 0.527    |
|    learning_rate   | 0.0003   |
|    n_updates       | 412299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.82e+03 |
|    critic_loss     | 6.84e+05 |
|    ent_coef        | 8.17     |
|    ent_coef_loss   | -0.613   |
|    learning_rate   | 0.0003   |
|    n_updates       | 412699   |
---------------------------------
=== Iterazione IRL 1471 ===
Loss reward (iter 1471): -0.27155494689941406
=== Iterazione IRL 1472 ===
Loss reward (iter 1472): 4.2055559158325195
=== Iterazione IRL 1473 ===
Loss reward (iter 1473): -1.457798957824707
=== Iterazione IRL 1474 ===
Loss reward (iter 1474): 0.7541654109954834
=== Iterazione IRL 1475 ===
Loss reward (iter 1475): 1.6925632953643799
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.87e+03 |
|    critic_loss     | 1.33e+05 |
|    ent_coef        | 8.06     |
|    ent_coef_loss   | -0.306   |
|    learning_rate   | 0.0003   |
|    n_updates       | 413299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.87e+03 |
|    critic_loss     | 1.13e+05 |
|    ent_coef        | 8.06     |
|    ent_coef_loss   | 0.243    |
|    learning_rate   | 0.0003   |
|    n_updates       | 413699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.99e+03 |
|    critic_loss     | 2.18e+05 |
|    ent_coef        | 8.06     |
|    ent_coef_loss   | 0.085    |
|    learning_rate   | 0.0003   |
|    n_updates       | 414099   |
---------------------------------
=== Iterazione IRL 1476 ===
Loss reward (iter 1476): -4.389312744140625
=== Iterazione IRL 1477 ===
Loss reward (iter 1477): 0.502922534942627
=== Iterazione IRL 1478 ===
Loss reward (iter 1478): -0.6541550159454346
=== Iterazione IRL 1479 ===
Loss reward (iter 1479): -3.7905964851379395
=== Iterazione IRL 1480 ===
Loss reward (iter 1480): -1.4957611560821533
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.03e+03 |
|    critic_loss     | 2.95e+05 |
|    ent_coef        | 8.11     |
|    ent_coef_loss   | -0.931   |
|    learning_rate   | 0.0003   |
|    n_updates       | 414699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.91e+03 |
|    critic_loss     | 1.78e+05 |
|    ent_coef        | 7.98     |
|    ent_coef_loss   | 0.696    |
|    learning_rate   | 0.0003   |
|    n_updates       | 415099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.78e+03 |
|    critic_loss     | 2.33e+05 |
|    ent_coef        | 7.99     |
|    ent_coef_loss   | -0.12    |
|    learning_rate   | 0.0003   |
|    n_updates       | 415499   |
---------------------------------
=== Iterazione IRL 1481 ===
Loss reward (iter 1481): -5.043745517730713
=== Iterazione IRL 1482 ===
Loss reward (iter 1482): -4.6219329833984375
=== Iterazione IRL 1483 ===
Loss reward (iter 1483): -7.48444938659668
=== Iterazione IRL 1484 ===
Loss reward (iter 1484): -3.70105242729187
=== Iterazione IRL 1485 ===
Loss reward (iter 1485): -1.50840425491333
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.84e+03 |
|    critic_loss     | 1.56e+05 |
|    ent_coef        | 8.27     |
|    ent_coef_loss   | 0.317    |
|    learning_rate   | 0.0003   |
|    n_updates       | 416099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.75e+03 |
|    critic_loss     | 2.47e+05 |
|    ent_coef        | 8.23     |
|    ent_coef_loss   | 0.835    |
|    learning_rate   | 0.0003   |
|    n_updates       | 416499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.81e+03 |
|    critic_loss     | 2.4e+05  |
|    ent_coef        | 8.2      |
|    ent_coef_loss   | -0.506   |
|    learning_rate   | 0.0003   |
|    n_updates       | 416899   |
---------------------------------
=== Iterazione IRL 1486 ===
Loss reward (iter 1486): -5.798861503601074
=== Iterazione IRL 1487 ===
Loss reward (iter 1487): 1.5538239479064941
=== Iterazione IRL 1488 ===
Loss reward (iter 1488): -3.9299163818359375
=== Iterazione IRL 1489 ===
Loss reward (iter 1489): -0.7977569103240967
=== Iterazione IRL 1490 ===
Loss reward (iter 1490): -1.59531831741333
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.69e+03 |
|    critic_loss     | 2.64e+05 |
|    ent_coef        | 8.2      |
|    ent_coef_loss   | 0.551    |
|    learning_rate   | 0.0003   |
|    n_updates       | 417499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.74e+03 |
|    critic_loss     | 7.92e+04 |
|    ent_coef        | 8.1      |
|    ent_coef_loss   | 0.163    |
|    learning_rate   | 0.0003   |
|    n_updates       | 417899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.86e+03 |
|    critic_loss     | 7.79e+04 |
|    ent_coef        | 8.12     |
|    ent_coef_loss   | 0.629    |
|    learning_rate   | 0.0003   |
|    n_updates       | 418299   |
---------------------------------
=== Iterazione IRL 1491 ===
Loss reward (iter 1491): -0.4600343704223633
=== Iterazione IRL 1492 ===
Loss reward (iter 1492): -3.380922317504883
=== Iterazione IRL 1493 ===
Loss reward (iter 1493): 3.0128846168518066
=== Iterazione IRL 1494 ===
Loss reward (iter 1494): -3.1034834384918213
=== Iterazione IRL 1495 ===
Loss reward (iter 1495): -1.8812024593353271
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.84e+03 |
|    critic_loss     | 1.99e+05 |
|    ent_coef        | 8.07     |
|    ent_coef_loss   | 0.00225  |
|    learning_rate   | 0.0003   |
|    n_updates       | 418899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.73e+03 |
|    critic_loss     | 8.25e+04 |
|    ent_coef        | 8        |
|    ent_coef_loss   | 0.525    |
|    learning_rate   | 0.0003   |
|    n_updates       | 419299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.96e+03 |
|    critic_loss     | 1.6e+05  |
|    ent_coef        | 8.05     |
|    ent_coef_loss   | -0.419   |
|    learning_rate   | 0.0003   |
|    n_updates       | 419699   |
---------------------------------
=== Iterazione IRL 1496 ===
Loss reward (iter 1496): -2.444838523864746
=== Iterazione IRL 1497 ===
Loss reward (iter 1497): -4.453719139099121
=== Iterazione IRL 1498 ===
Loss reward (iter 1498): -3.324136734008789
=== Iterazione IRL 1499 ===
Loss reward (iter 1499): -1.126511573791504
Modello SAC salvato.
