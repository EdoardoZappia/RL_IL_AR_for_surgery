Inizio training IRL
Using device: cuda
Using cuda device
=== Iterazione IRL 0 ===
Loss reward (iter 0): 6.314342498779297
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 146      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.97    |
|    critic_loss     | 0.0532   |
|    ent_coef        | 0.915    |
|    ent_coef_loss   | -0.29    |
|    learning_rate   | 0.0003   |
|    n_updates       | 299      |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 129      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -9.68    |
|    critic_loss     | 0.0206   |
|    ent_coef        | 0.812    |
|    ent_coef_loss   | -0.675   |
|    learning_rate   | 0.0003   |
|    n_updates       | 699      |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 124      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -13.4    |
|    critic_loss     | 0.0331   |
|    ent_coef        | 0.721    |
|    ent_coef_loss   | -1.07    |
|    learning_rate   | 0.0003   |
|    n_updates       | 1099     |
---------------------------------
=== Iterazione IRL 1 ===
Loss reward (iter 1): 5.839219570159912
=== Iterazione IRL 2 ===
Loss reward (iter 2): 4.698318004608154
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -17.3    |
|    critic_loss     | 0.637    |
|    ent_coef        | 0.66     |
|    ent_coef_loss   | -1.3     |
|    learning_rate   | 0.0003   |
|    n_updates       | 1399     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -24.5    |
|    critic_loss     | 1.12     |
|    ent_coef        | 0.591    |
|    ent_coef_loss   | -1.35    |
|    learning_rate   | 0.0003   |
|    n_updates       | 1799     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -33.3    |
|    critic_loss     | 1.01     |
|    ent_coef        | 0.538    |
|    ent_coef_loss   | -1.18    |
|    learning_rate   | 0.0003   |
|    n_updates       | 2199     |
---------------------------------
=== Iterazione IRL 3 ===
Loss reward (iter 3): 7.7014031410217285
=== Iterazione IRL 4 ===
Loss reward (iter 4): 6.111110210418701
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -38.6    |
|    critic_loss     | 1.56     |
|    ent_coef        | 0.503    |
|    ent_coef_loss   | -1.21    |
|    learning_rate   | 0.0003   |
|    n_updates       | 2499     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -48.9    |
|    critic_loss     | 1.52     |
|    ent_coef        | 0.475    |
|    ent_coef_loss   | -0.526   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2899     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -60.9    |
|    critic_loss     | 1.99     |
|    ent_coef        | 0.463    |
|    ent_coef_loss   | -0.303   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3299     |
---------------------------------
=== Iterazione IRL 5 ===
Loss reward (iter 5): 13.036528587341309
=== Iterazione IRL 6 ===
Loss reward (iter 6): 9.858864784240723
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -70.5    |
|    critic_loss     | 3.12     |
|    ent_coef        | 0.458    |
|    ent_coef_loss   | 0.0425   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3599     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -82      |
|    critic_loss     | 3.46     |
|    ent_coef        | 0.455    |
|    ent_coef_loss   | -0.0487  |
|    learning_rate   | 0.0003   |
|    n_updates       | 3999     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -92.2    |
|    critic_loss     | 4.79     |
|    ent_coef        | 0.456    |
|    ent_coef_loss   | 0.0789   |
|    learning_rate   | 0.0003   |
|    n_updates       | 4399     |
---------------------------------
=== Iterazione IRL 7 ===
Loss reward (iter 7): 7.509456157684326
=== Iterazione IRL 8 ===
Loss reward (iter 8): 6.346936225891113
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -100     |
|    critic_loss     | 6.58     |
|    ent_coef        | 0.457    |
|    ent_coef_loss   | 0.199    |
|    learning_rate   | 0.0003   |
|    n_updates       | 4699     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -111     |
|    critic_loss     | 7.04     |
|    ent_coef        | 0.455    |
|    ent_coef_loss   | -0.111   |
|    learning_rate   | 0.0003   |
|    n_updates       | 5099     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -121     |
|    critic_loss     | 6.7      |
|    ent_coef        | 0.445    |
|    ent_coef_loss   | -0.15    |
|    learning_rate   | 0.0003   |
|    n_updates       | 5499     |
---------------------------------
=== Iterazione IRL 9 ===
Loss reward (iter 9): 6.562253475189209
=== Iterazione IRL 10 ===
Loss reward (iter 10): 5.860077857971191
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -128     |
|    critic_loss     | 7.16     |
|    ent_coef        | 0.422    |
|    ent_coef_loss   | -0.264   |
|    learning_rate   | 0.0003   |
|    n_updates       | 5799     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -136     |
|    critic_loss     | 7.17     |
|    ent_coef        | 0.398    |
|    ent_coef_loss   | -0.177   |
|    learning_rate   | 0.0003   |
|    n_updates       | 6199     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -147     |
|    critic_loss     | 7.53     |
|    ent_coef        | 0.38     |
|    ent_coef_loss   | -0.273   |
|    learning_rate   | 0.0003   |
|    n_updates       | 6599     |
---------------------------------
=== Iterazione IRL 11 ===
Loss reward (iter 11): 5.942981243133545
=== Iterazione IRL 12 ===
Loss reward (iter 12): 5.1601762771606445
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -154     |
|    critic_loss     | 6.18     |
|    ent_coef        | 0.37     |
|    ent_coef_loss   | -0.262   |
|    learning_rate   | 0.0003   |
|    n_updates       | 6899     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -159     |
|    critic_loss     | 5.62     |
|    ent_coef        | 0.341    |
|    ent_coef_loss   | -0.368   |
|    learning_rate   | 0.0003   |
|    n_updates       | 7299     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -166     |
|    critic_loss     | 7.98     |
|    ent_coef        | 0.321    |
|    ent_coef_loss   | 0.157    |
|    learning_rate   | 0.0003   |
|    n_updates       | 7699     |
---------------------------------
=== Iterazione IRL 13 ===
Loss reward (iter 13): 4.285220623016357
=== Iterazione IRL 14 ===
Loss reward (iter 14): 3.661416530609131
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 149      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -171     |
|    critic_loss     | 7.5      |
|    ent_coef        | 0.323    |
|    ent_coef_loss   | 0.185    |
|    learning_rate   | 0.0003   |
|    n_updates       | 7999     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 130      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -177     |
|    critic_loss     | 8.42     |
|    ent_coef        | 0.335    |
|    ent_coef_loss   | 0.0428   |
|    learning_rate   | 0.0003   |
|    n_updates       | 8399     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 124      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -185     |
|    critic_loss     | 8.67     |
|    ent_coef        | 0.359    |
|    ent_coef_loss   | 0.0441   |
|    learning_rate   | 0.0003   |
|    n_updates       | 8799     |
---------------------------------
=== Iterazione IRL 15 ===
Loss reward (iter 15): 13.186776161193848
=== Iterazione IRL 16 ===
Loss reward (iter 16): 9.725687980651855
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 149      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -189     |
|    critic_loss     | 7.23     |
|    ent_coef        | 0.371    |
|    ent_coef_loss   | 0.149    |
|    learning_rate   | 0.0003   |
|    n_updates       | 9099     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 130      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -198     |
|    critic_loss     | 7.31     |
|    ent_coef        | 0.378    |
|    ent_coef_loss   | 0.408    |
|    learning_rate   | 0.0003   |
|    n_updates       | 9499     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -203     |
|    critic_loss     | 8.15     |
|    ent_coef        | 0.384    |
|    ent_coef_loss   | -0.334   |
|    learning_rate   | 0.0003   |
|    n_updates       | 9899     |
---------------------------------
=== Iterazione IRL 17 ===
Loss reward (iter 17): 7.936034202575684
=== Iterazione IRL 18 ===
Loss reward (iter 18): 7.0279412269592285
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -207     |
|    critic_loss     | 10.1     |
|    ent_coef        | 0.388    |
|    ent_coef_loss   | -0.148   |
|    learning_rate   | 0.0003   |
|    n_updates       | 10199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -215     |
|    critic_loss     | 11.8     |
|    ent_coef        | 0.398    |
|    ent_coef_loss   | -0.311   |
|    learning_rate   | 0.0003   |
|    n_updates       | 10599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -219     |
|    critic_loss     | 10.9     |
|    ent_coef        | 0.407    |
|    ent_coef_loss   | 0.0824   |
|    learning_rate   | 0.0003   |
|    n_updates       | 10999    |
---------------------------------
=== Iterazione IRL 19 ===
Loss reward (iter 19): 7.509923934936523
=== Iterazione IRL 20 ===
Loss reward (iter 20): 7.195712089538574
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -226     |
|    critic_loss     | 11.9     |
|    ent_coef        | 0.406    |
|    ent_coef_loss   | 0.247    |
|    learning_rate   | 0.0003   |
|    n_updates       | 11299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -230     |
|    critic_loss     | 12       |
|    ent_coef        | 0.407    |
|    ent_coef_loss   | -0.0837  |
|    learning_rate   | 0.0003   |
|    n_updates       | 11699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -231     |
|    critic_loss     | 15.5     |
|    ent_coef        | 0.394    |
|    ent_coef_loss   | 0.0727   |
|    learning_rate   | 0.0003   |
|    n_updates       | 12099    |
---------------------------------
=== Iterazione IRL 21 ===
Loss reward (iter 21): 7.002375602722168
=== Iterazione IRL 22 ===
Loss reward (iter 22): 6.732464790344238
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -239     |
|    critic_loss     | 10.2     |
|    ent_coef        | 0.383    |
|    ent_coef_loss   | -0.12    |
|    learning_rate   | 0.0003   |
|    n_updates       | 12399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 130      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -244     |
|    critic_loss     | 14.3     |
|    ent_coef        | 0.374    |
|    ent_coef_loss   | 0.348    |
|    learning_rate   | 0.0003   |
|    n_updates       | 12799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -248     |
|    critic_loss     | 15.9     |
|    ent_coef        | 0.364    |
|    ent_coef_loss   | -0.0846  |
|    learning_rate   | 0.0003   |
|    n_updates       | 13199    |
---------------------------------
=== Iterazione IRL 23 ===
Loss reward (iter 23): 5.935916423797607
=== Iterazione IRL 24 ===
Loss reward (iter 24): 5.776923656463623
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -249     |
|    critic_loss     | 19.3     |
|    ent_coef        | 0.35     |
|    ent_coef_loss   | -0.107   |
|    learning_rate   | 0.0003   |
|    n_updates       | 13499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -251     |
|    critic_loss     | 17.6     |
|    ent_coef        | 0.327    |
|    ent_coef_loss   | -0.324   |
|    learning_rate   | 0.0003   |
|    n_updates       | 13899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -255     |
|    critic_loss     | 19.3     |
|    ent_coef        | 0.314    |
|    ent_coef_loss   | -0.0116  |
|    learning_rate   | 0.0003   |
|    n_updates       | 14299    |
---------------------------------
=== Iterazione IRL 25 ===
Loss reward (iter 25): 5.81273078918457
=== Iterazione IRL 26 ===
Loss reward (iter 26): 5.189230442047119
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -257     |
|    critic_loss     | 21       |
|    ent_coef        | 0.305    |
|    ent_coef_loss   | 0.293    |
|    learning_rate   | 0.0003   |
|    n_updates       | 14599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -258     |
|    critic_loss     | 18.6     |
|    ent_coef        | 0.291    |
|    ent_coef_loss   | 0.0694   |
|    learning_rate   | 0.0003   |
|    n_updates       | 14999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -260     |
|    critic_loss     | 18.8     |
|    ent_coef        | 0.282    |
|    ent_coef_loss   | -0.0166  |
|    learning_rate   | 0.0003   |
|    n_updates       | 15399    |
---------------------------------
=== Iterazione IRL 27 ===
Loss reward (iter 27): 6.160836219787598
=== Iterazione IRL 28 ===
Loss reward (iter 28): 5.728244781494141
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -257     |
|    critic_loss     | 18.8     |
|    ent_coef        | 0.276    |
|    ent_coef_loss   | -0.193   |
|    learning_rate   | 0.0003   |
|    n_updates       | 15699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -264     |
|    critic_loss     | 22.5     |
|    ent_coef        | 0.266    |
|    ent_coef_loss   | -0.24    |
|    learning_rate   | 0.0003   |
|    n_updates       | 16099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -263     |
|    critic_loss     | 17.7     |
|    ent_coef        | 0.251    |
|    ent_coef_loss   | 0.183    |
|    learning_rate   | 0.0003   |
|    n_updates       | 16499    |
---------------------------------
=== Iterazione IRL 29 ===
Loss reward (iter 29): 5.736189842224121
=== Iterazione IRL 30 ===
Loss reward (iter 30): 5.488247871398926
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -263     |
|    critic_loss     | 17.3     |
|    ent_coef        | 0.246    |
|    ent_coef_loss   | 0.161    |
|    learning_rate   | 0.0003   |
|    n_updates       | 16799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -265     |
|    critic_loss     | 18.1     |
|    ent_coef        | 0.248    |
|    ent_coef_loss   | 0.124    |
|    learning_rate   | 0.0003   |
|    n_updates       | 17199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -264     |
|    critic_loss     | 18.2     |
|    ent_coef        | 0.251    |
|    ent_coef_loss   | -0.303   |
|    learning_rate   | 0.0003   |
|    n_updates       | 17599    |
---------------------------------
=== Iterazione IRL 31 ===
Loss reward (iter 31): 5.627608299255371
=== Iterazione IRL 32 ===
Loss reward (iter 32): 6.403136253356934
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -266     |
|    critic_loss     | 19       |
|    ent_coef        | 0.248    |
|    ent_coef_loss   | 0.149    |
|    learning_rate   | 0.0003   |
|    n_updates       | 17899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -264     |
|    critic_loss     | 28.9     |
|    ent_coef        | 0.247    |
|    ent_coef_loss   | -0.125   |
|    learning_rate   | 0.0003   |
|    n_updates       | 18299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -267     |
|    critic_loss     | 22.8     |
|    ent_coef        | 0.237    |
|    ent_coef_loss   | 0.617    |
|    learning_rate   | 0.0003   |
|    n_updates       | 18699    |
---------------------------------
=== Iterazione IRL 33 ===
Loss reward (iter 33): 5.204841136932373
=== Iterazione IRL 34 ===
Loss reward (iter 34): 5.100156307220459
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -268     |
|    critic_loss     | 26.2     |
|    ent_coef        | 0.238    |
|    ent_coef_loss   | 0.0851   |
|    learning_rate   | 0.0003   |
|    n_updates       | 18999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -267     |
|    critic_loss     | 30       |
|    ent_coef        | 0.24     |
|    ent_coef_loss   | 0.222    |
|    learning_rate   | 0.0003   |
|    n_updates       | 19399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -262     |
|    critic_loss     | 20.1     |
|    ent_coef        | 0.232    |
|    ent_coef_loss   | -0.242   |
|    learning_rate   | 0.0003   |
|    n_updates       | 19799    |
---------------------------------
=== Iterazione IRL 35 ===
Loss reward (iter 35): 4.9492669105529785
=== Iterazione IRL 36 ===
Loss reward (iter 36): 5.623006343841553
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -263     |
|    critic_loss     | 25.7     |
|    ent_coef        | 0.221    |
|    ent_coef_loss   | -0.0796  |
|    learning_rate   | 0.0003   |
|    n_updates       | 20099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -263     |
|    critic_loss     | 21.2     |
|    ent_coef        | 0.212    |
|    ent_coef_loss   | 0.0976   |
|    learning_rate   | 0.0003   |
|    n_updates       | 20499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -261     |
|    critic_loss     | 25.9     |
|    ent_coef        | 0.212    |
|    ent_coef_loss   | 0.417    |
|    learning_rate   | 0.0003   |
|    n_updates       | 20899    |
---------------------------------
=== Iterazione IRL 37 ===
Loss reward (iter 37): 5.602071762084961
=== Iterazione IRL 38 ===
Loss reward (iter 38): 5.069453239440918
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -260     |
|    critic_loss     | 31.3     |
|    ent_coef        | 0.21     |
|    ent_coef_loss   | 0.133    |
|    learning_rate   | 0.0003   |
|    n_updates       | 21199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -259     |
|    critic_loss     | 27.8     |
|    ent_coef        | 0.203    |
|    ent_coef_loss   | -0.263   |
|    learning_rate   | 0.0003   |
|    n_updates       | 21599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -254     |
|    critic_loss     | 31.2     |
|    ent_coef        | 0.191    |
|    ent_coef_loss   | -0.212   |
|    learning_rate   | 0.0003   |
|    n_updates       | 21999    |
---------------------------------
=== Iterazione IRL 39 ===
Loss reward (iter 39): 5.97772216796875
=== Iterazione IRL 40 ===
Loss reward (iter 40): 5.519187927246094
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -252     |
|    critic_loss     | 32.4     |
|    ent_coef        | 0.177    |
|    ent_coef_loss   | -1.09    |
|    learning_rate   | 0.0003   |
|    n_updates       | 22299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -256     |
|    critic_loss     | 25.9     |
|    ent_coef        | 0.17     |
|    ent_coef_loss   | -0.17    |
|    learning_rate   | 0.0003   |
|    n_updates       | 22699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -255     |
|    critic_loss     | 28.5     |
|    ent_coef        | 0.163    |
|    ent_coef_loss   | 0.609    |
|    learning_rate   | 0.0003   |
|    n_updates       | 23099    |
---------------------------------
=== Iterazione IRL 41 ===
Loss reward (iter 41): 5.650537967681885
=== Iterazione IRL 42 ===
Loss reward (iter 42): 5.619344711303711
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -248     |
|    critic_loss     | 25.2     |
|    ent_coef        | 0.163    |
|    ent_coef_loss   | -0.0943  |
|    learning_rate   | 0.0003   |
|    n_updates       | 23399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -244     |
|    critic_loss     | 23.3     |
|    ent_coef        | 0.162    |
|    ent_coef_loss   | -0.129   |
|    learning_rate   | 0.0003   |
|    n_updates       | 23799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -243     |
|    critic_loss     | 26.4     |
|    ent_coef        | 0.168    |
|    ent_coef_loss   | 0.325    |
|    learning_rate   | 0.0003   |
|    n_updates       | 24199    |
---------------------------------
=== Iterazione IRL 43 ===
Loss reward (iter 43): 6.096591949462891
=== Iterazione IRL 44 ===
Loss reward (iter 44): 5.1925835609436035
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -242     |
|    critic_loss     | 23.7     |
|    ent_coef        | 0.172    |
|    ent_coef_loss   | 0.409    |
|    learning_rate   | 0.0003   |
|    n_updates       | 24499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -240     |
|    critic_loss     | 28.4     |
|    ent_coef        | 0.177    |
|    ent_coef_loss   | -0.662   |
|    learning_rate   | 0.0003   |
|    n_updates       | 24899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -236     |
|    critic_loss     | 22.8     |
|    ent_coef        | 0.171    |
|    ent_coef_loss   | -0.356   |
|    learning_rate   | 0.0003   |
|    n_updates       | 25299    |
---------------------------------
=== Iterazione IRL 45 ===
Loss reward (iter 45): 5.427712440490723
=== Iterazione IRL 46 ===
Loss reward (iter 46): 5.1742329597473145
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -237     |
|    critic_loss     | 23.9     |
|    ent_coef        | 0.167    |
|    ent_coef_loss   | 0.0572   |
|    learning_rate   | 0.0003   |
|    n_updates       | 25599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -231     |
|    critic_loss     | 29.9     |
|    ent_coef        | 0.166    |
|    ent_coef_loss   | -0.41    |
|    learning_rate   | 0.0003   |
|    n_updates       | 25999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -228     |
|    critic_loss     | 20.7     |
|    ent_coef        | 0.171    |
|    ent_coef_loss   | -0.104   |
|    learning_rate   | 0.0003   |
|    n_updates       | 26399    |
---------------------------------
=== Iterazione IRL 47 ===
Loss reward (iter 47): 5.621572017669678
=== Iterazione IRL 48 ===
Loss reward (iter 48): 4.867007255554199
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -227     |
|    critic_loss     | 23.5     |
|    ent_coef        | 0.17     |
|    ent_coef_loss   | -0.145   |
|    learning_rate   | 0.0003   |
|    n_updates       | 26699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -226     |
|    critic_loss     | 22.8     |
|    ent_coef        | 0.175    |
|    ent_coef_loss   | -0.0968  |
|    learning_rate   | 0.0003   |
|    n_updates       | 27099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -226     |
|    critic_loss     | 20.6     |
|    ent_coef        | 0.177    |
|    ent_coef_loss   | -0.475   |
|    learning_rate   | 0.0003   |
|    n_updates       | 27499    |
---------------------------------
=== Iterazione IRL 49 ===
Loss reward (iter 49): 5.403672695159912
=== Iterazione IRL 50 ===
Loss reward (iter 50): 4.960251331329346
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -218     |
|    critic_loss     | 24.7     |
|    ent_coef        | 0.178    |
|    ent_coef_loss   | 0.0384   |
|    learning_rate   | 0.0003   |
|    n_updates       | 27799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -220     |
|    critic_loss     | 20.1     |
|    ent_coef        | 0.175    |
|    ent_coef_loss   | -0.628   |
|    learning_rate   | 0.0003   |
|    n_updates       | 28199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -217     |
|    critic_loss     | 22.5     |
|    ent_coef        | 0.17     |
|    ent_coef_loss   | -0.141   |
|    learning_rate   | 0.0003   |
|    n_updates       | 28599    |
---------------------------------
=== Iterazione IRL 51 ===
Loss reward (iter 51): 5.194229602813721
=== Iterazione IRL 52 ===
Loss reward (iter 52): 3.5857040882110596
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -215     |
|    critic_loss     | 23.4     |
|    ent_coef        | 0.174    |
|    ent_coef_loss   | -0.127   |
|    learning_rate   | 0.0003   |
|    n_updates       | 28899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -213     |
|    critic_loss     | 23.2     |
|    ent_coef        | 0.172    |
|    ent_coef_loss   | -0.461   |
|    learning_rate   | 0.0003   |
|    n_updates       | 29299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -212     |
|    critic_loss     | 20.5     |
|    ent_coef        | 0.172    |
|    ent_coef_loss   | 0.629    |
|    learning_rate   | 0.0003   |
|    n_updates       | 29699    |
---------------------------------
=== Iterazione IRL 53 ===
Loss reward (iter 53): 5.402973175048828
=== Iterazione IRL 54 ===
Loss reward (iter 54): 4.975463390350342
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -209     |
|    critic_loss     | 25.1     |
|    ent_coef        | 0.175    |
|    ent_coef_loss   | 0.321    |
|    learning_rate   | 0.0003   |
|    n_updates       | 29999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -207     |
|    critic_loss     | 20       |
|    ent_coef        | 0.175    |
|    ent_coef_loss   | 0.474    |
|    learning_rate   | 0.0003   |
|    n_updates       | 30399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -205     |
|    critic_loss     | 23.7     |
|    ent_coef        | 0.179    |
|    ent_coef_loss   | 0.0877   |
|    learning_rate   | 0.0003   |
|    n_updates       | 30799    |
---------------------------------
=== Iterazione IRL 55 ===
Loss reward (iter 55): 6.68359375
=== Iterazione IRL 56 ===
Loss reward (iter 56): 6.257510662078857
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -204     |
|    critic_loss     | 20.1     |
|    ent_coef        | 0.177    |
|    ent_coef_loss   | 1.15     |
|    learning_rate   | 0.0003   |
|    n_updates       | 31099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -202     |
|    critic_loss     | 21.6     |
|    ent_coef        | 0.178    |
|    ent_coef_loss   | -0.192   |
|    learning_rate   | 0.0003   |
|    n_updates       | 31499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -199     |
|    critic_loss     | 18.4     |
|    ent_coef        | 0.178    |
|    ent_coef_loss   | -0.0708  |
|    learning_rate   | 0.0003   |
|    n_updates       | 31899    |
---------------------------------
=== Iterazione IRL 57 ===
Loss reward (iter 57): 5.860710620880127
=== Iterazione IRL 58 ===
Loss reward (iter 58): 5.993989944458008
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -198     |
|    critic_loss     | 21.2     |
|    ent_coef        | 0.181    |
|    ent_coef_loss   | 0.226    |
|    learning_rate   | 0.0003   |
|    n_updates       | 32199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -198     |
|    critic_loss     | 17.8     |
|    ent_coef        | 0.174    |
|    ent_coef_loss   | 0.206    |
|    learning_rate   | 0.0003   |
|    n_updates       | 32599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -195     |
|    critic_loss     | 19.8     |
|    ent_coef        | 0.174    |
|    ent_coef_loss   | 0.289    |
|    learning_rate   | 0.0003   |
|    n_updates       | 32999    |
---------------------------------
=== Iterazione IRL 59 ===
Loss reward (iter 59): 6.203893184661865
=== Iterazione IRL 60 ===
Loss reward (iter 60): 5.735613822937012
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -197     |
|    critic_loss     | 19.6     |
|    ent_coef        | 0.177    |
|    ent_coef_loss   | -0.403   |
|    learning_rate   | 0.0003   |
|    n_updates       | 33299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -195     |
|    critic_loss     | 18.8     |
|    ent_coef        | 0.172    |
|    ent_coef_loss   | -0.405   |
|    learning_rate   | 0.0003   |
|    n_updates       | 33699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -193     |
|    critic_loss     | 18.4     |
|    ent_coef        | 0.171    |
|    ent_coef_loss   | 0.748    |
|    learning_rate   | 0.0003   |
|    n_updates       | 34099    |
---------------------------------
=== Iterazione IRL 61 ===
Loss reward (iter 61): 6.495933532714844
=== Iterazione IRL 62 ===
Loss reward (iter 62): 6.099587440490723
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -191     |
|    critic_loss     | 17.5     |
|    ent_coef        | 0.177    |
|    ent_coef_loss   | -0.571   |
|    learning_rate   | 0.0003   |
|    n_updates       | 34399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -192     |
|    critic_loss     | 20       |
|    ent_coef        | 0.177    |
|    ent_coef_loss   | -0.546   |
|    learning_rate   | 0.0003   |
|    n_updates       | 34799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -187     |
|    critic_loss     | 19.8     |
|    ent_coef        | 0.18     |
|    ent_coef_loss   | -0.69    |
|    learning_rate   | 0.0003   |
|    n_updates       | 35199    |
---------------------------------
=== Iterazione IRL 63 ===
Loss reward (iter 63): 6.254092693328857
=== Iterazione IRL 64 ===
Loss reward (iter 64): 5.95192813873291
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -187     |
|    critic_loss     | 17.6     |
|    ent_coef        | 0.18     |
|    ent_coef_loss   | 0.154    |
|    learning_rate   | 0.0003   |
|    n_updates       | 35499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -189     |
|    critic_loss     | 18.6     |
|    ent_coef        | 0.183    |
|    ent_coef_loss   | -0.82    |
|    learning_rate   | 0.0003   |
|    n_updates       | 35899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -185     |
|    critic_loss     | 17.5     |
|    ent_coef        | 0.185    |
|    ent_coef_loss   | -0.466   |
|    learning_rate   | 0.0003   |
|    n_updates       | 36299    |
---------------------------------
=== Iterazione IRL 65 ===
Loss reward (iter 65): 6.269488334655762
=== Iterazione IRL 66 ===
Loss reward (iter 66): 6.175303936004639
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -185     |
|    critic_loss     | 20       |
|    ent_coef        | 0.186    |
|    ent_coef_loss   | -0.558   |
|    learning_rate   | 0.0003   |
|    n_updates       | 36599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -183     |
|    critic_loss     | 17.9     |
|    ent_coef        | 0.195    |
|    ent_coef_loss   | -0.213   |
|    learning_rate   | 0.0003   |
|    n_updates       | 36999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -187     |
|    critic_loss     | 17.7     |
|    ent_coef        | 0.196    |
|    ent_coef_loss   | -0.215   |
|    learning_rate   | 0.0003   |
|    n_updates       | 37399    |
---------------------------------
=== Iterazione IRL 67 ===
Loss reward (iter 67): 6.075106143951416
=== Iterazione IRL 68 ===
Loss reward (iter 68): 6.396065711975098
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -183     |
|    critic_loss     | 19.2     |
|    ent_coef        | 0.204    |
|    ent_coef_loss   | 0.46     |
|    learning_rate   | 0.0003   |
|    n_updates       | 37699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -179     |
|    critic_loss     | 19.1     |
|    ent_coef        | 0.208    |
|    ent_coef_loss   | -0.289   |
|    learning_rate   | 0.0003   |
|    n_updates       | 38099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -183     |
|    critic_loss     | 16.6     |
|    ent_coef        | 0.208    |
|    ent_coef_loss   | 0.207    |
|    learning_rate   | 0.0003   |
|    n_updates       | 38499    |
---------------------------------
=== Iterazione IRL 69 ===
Loss reward (iter 69): 6.26340389251709
=== Iterazione IRL 70 ===
Loss reward (iter 70): 6.489111423492432
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -180     |
|    critic_loss     | 16.7     |
|    ent_coef        | 0.212    |
|    ent_coef_loss   | -0.177   |
|    learning_rate   | 0.0003   |
|    n_updates       | 38799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -180     |
|    critic_loss     | 18.9     |
|    ent_coef        | 0.212    |
|    ent_coef_loss   | -0.33    |
|    learning_rate   | 0.0003   |
|    n_updates       | 39199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -182     |
|    critic_loss     | 18.1     |
|    ent_coef        | 0.217    |
|    ent_coef_loss   | 0.388    |
|    learning_rate   | 0.0003   |
|    n_updates       | 39599    |
---------------------------------
=== Iterazione IRL 71 ===
Loss reward (iter 71): 6.349263668060303
=== Iterazione IRL 72 ===
Loss reward (iter 72): 6.2023773193359375
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -180     |
|    critic_loss     | 18.1     |
|    ent_coef        | 0.219    |
|    ent_coef_loss   | 0.223    |
|    learning_rate   | 0.0003   |
|    n_updates       | 39899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -181     |
|    critic_loss     | 16.4     |
|    ent_coef        | 0.218    |
|    ent_coef_loss   | -0.192   |
|    learning_rate   | 0.0003   |
|    n_updates       | 40299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -180     |
|    critic_loss     | 14.4     |
|    ent_coef        | 0.214    |
|    ent_coef_loss   | -0.237   |
|    learning_rate   | 0.0003   |
|    n_updates       | 40699    |
---------------------------------
=== Iterazione IRL 73 ===
Loss reward (iter 73): 5.961170196533203
=== Iterazione IRL 74 ===
Loss reward (iter 74): 5.798587799072266
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -180     |
|    critic_loss     | 18.4     |
|    ent_coef        | 0.213    |
|    ent_coef_loss   | 0.133    |
|    learning_rate   | 0.0003   |
|    n_updates       | 40999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -181     |
|    critic_loss     | 17.8     |
|    ent_coef        | 0.204    |
|    ent_coef_loss   | -0.507   |
|    learning_rate   | 0.0003   |
|    n_updates       | 41399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -176     |
|    critic_loss     | 15.2     |
|    ent_coef        | 0.206    |
|    ent_coef_loss   | -0.612   |
|    learning_rate   | 0.0003   |
|    n_updates       | 41799    |
---------------------------------
=== Iterazione IRL 75 ===
Loss reward (iter 75): 6.072688579559326
=== Iterazione IRL 76 ===
Loss reward (iter 76): 5.984574794769287
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -177     |
|    critic_loss     | 20.4     |
|    ent_coef        | 0.203    |
|    ent_coef_loss   | 0.145    |
|    learning_rate   | 0.0003   |
|    n_updates       | 42099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -176     |
|    critic_loss     | 16       |
|    ent_coef        | 0.203    |
|    ent_coef_loss   | 0.644    |
|    learning_rate   | 0.0003   |
|    n_updates       | 42499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -176     |
|    critic_loss     | 19.7     |
|    ent_coef        | 0.206    |
|    ent_coef_loss   | -0.051   |
|    learning_rate   | 0.0003   |
|    n_updates       | 42899    |
---------------------------------
=== Iterazione IRL 77 ===
Loss reward (iter 77): 5.990987300872803
=== Iterazione IRL 78 ===
Loss reward (iter 78): 6.041218280792236
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -174     |
|    critic_loss     | 15.7     |
|    ent_coef        | 0.205    |
|    ent_coef_loss   | -0.364   |
|    learning_rate   | 0.0003   |
|    n_updates       | 43199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -178     |
|    critic_loss     | 15       |
|    ent_coef        | 0.207    |
|    ent_coef_loss   | 0.217    |
|    learning_rate   | 0.0003   |
|    n_updates       | 43599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -179     |
|    critic_loss     | 15.6     |
|    ent_coef        | 0.209    |
|    ent_coef_loss   | 0.52     |
|    learning_rate   | 0.0003   |
|    n_updates       | 43999    |
---------------------------------
=== Iterazione IRL 79 ===
Loss reward (iter 79): 6.0275397300720215
=== Iterazione IRL 80 ===
Loss reward (iter 80): 6.030489444732666
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -177     |
|    critic_loss     | 19.1     |
|    ent_coef        | 0.208    |
|    ent_coef_loss   | -0.18    |
|    learning_rate   | 0.0003   |
|    n_updates       | 44299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -178     |
|    critic_loss     | 14.6     |
|    ent_coef        | 0.209    |
|    ent_coef_loss   | -0.0702  |
|    learning_rate   | 0.0003   |
|    n_updates       | 44699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -175     |
|    critic_loss     | 17.3     |
|    ent_coef        | 0.213    |
|    ent_coef_loss   | 0.504    |
|    learning_rate   | 0.0003   |
|    n_updates       | 45099    |
---------------------------------
=== Iterazione IRL 81 ===
Loss reward (iter 81): 6.1962080001831055
=== Iterazione IRL 82 ===
Loss reward (iter 82): 5.8486223220825195
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -174     |
|    critic_loss     | 16.2     |
|    ent_coef        | 0.214    |
|    ent_coef_loss   | 0.0621   |
|    learning_rate   | 0.0003   |
|    n_updates       | 45399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -176     |
|    critic_loss     | 16.1     |
|    ent_coef        | 0.214    |
|    ent_coef_loss   | 0.0832   |
|    learning_rate   | 0.0003   |
|    n_updates       | 45799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -177     |
|    critic_loss     | 16.9     |
|    ent_coef        | 0.214    |
|    ent_coef_loss   | -0.0754  |
|    learning_rate   | 0.0003   |
|    n_updates       | 46199    |
---------------------------------
=== Iterazione IRL 83 ===
Loss reward (iter 83): 6.08122444152832
=== Iterazione IRL 84 ===
Loss reward (iter 84): 5.994711875915527
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -178     |
|    critic_loss     | 14.9     |
|    ent_coef        | 0.213    |
|    ent_coef_loss   | -0.063   |
|    learning_rate   | 0.0003   |
|    n_updates       | 46499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -178     |
|    critic_loss     | 15       |
|    ent_coef        | 0.214    |
|    ent_coef_loss   | 0.543    |
|    learning_rate   | 0.0003   |
|    n_updates       | 46899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -179     |
|    critic_loss     | 13.4     |
|    ent_coef        | 0.218    |
|    ent_coef_loss   | -0.115   |
|    learning_rate   | 0.0003   |
|    n_updates       | 47299    |
---------------------------------
=== Iterazione IRL 85 ===
Loss reward (iter 85): 5.896019458770752
=== Iterazione IRL 86 ===
Loss reward (iter 86): 5.868806838989258
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -182     |
|    critic_loss     | 15.8     |
|    ent_coef        | 0.218    |
|    ent_coef_loss   | 0.0327   |
|    learning_rate   | 0.0003   |
|    n_updates       | 47599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -173     |
|    critic_loss     | 16.9     |
|    ent_coef        | 0.222    |
|    ent_coef_loss   | 0.804    |
|    learning_rate   | 0.0003   |
|    n_updates       | 47999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -177     |
|    critic_loss     | 18.1     |
|    ent_coef        | 0.217    |
|    ent_coef_loss   | 0.231    |
|    learning_rate   | 0.0003   |
|    n_updates       | 48399    |
---------------------------------
=== Iterazione IRL 87 ===
Loss reward (iter 87): 5.944556713104248
=== Iterazione IRL 88 ===
Loss reward (iter 88): 5.909415245056152
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -179     |
|    critic_loss     | 16.3     |
|    ent_coef        | 0.218    |
|    ent_coef_loss   | 0.182    |
|    learning_rate   | 0.0003   |
|    n_updates       | 48699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -177     |
|    critic_loss     | 14.9     |
|    ent_coef        | 0.217    |
|    ent_coef_loss   | -0.904   |
|    learning_rate   | 0.0003   |
|    n_updates       | 49099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -179     |
|    critic_loss     | 18       |
|    ent_coef        | 0.215    |
|    ent_coef_loss   | 0.205    |
|    learning_rate   | 0.0003   |
|    n_updates       | 49499    |
---------------------------------
=== Iterazione IRL 89 ===
Loss reward (iter 89): 5.7712531089782715
=== Iterazione IRL 90 ===
Loss reward (iter 90): 5.8544511795043945
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -176     |
|    critic_loss     | 15.6     |
|    ent_coef        | 0.215    |
|    ent_coef_loss   | 0.784    |
|    learning_rate   | 0.0003   |
|    n_updates       | 49799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -175     |
|    critic_loss     | 15.4     |
|    ent_coef        | 0.213    |
|    ent_coef_loss   | 0.255    |
|    learning_rate   | 0.0003   |
|    n_updates       | 50199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -180     |
|    critic_loss     | 16       |
|    ent_coef        | 0.211    |
|    ent_coef_loss   | 0.2      |
|    learning_rate   | 0.0003   |
|    n_updates       | 50599    |
---------------------------------
=== Iterazione IRL 91 ===
Loss reward (iter 91): 5.758744239807129
=== Iterazione IRL 92 ===
Loss reward (iter 92): 5.996888160705566
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -180     |
|    critic_loss     | 15.1     |
|    ent_coef        | 0.213    |
|    ent_coef_loss   | 0.367    |
|    learning_rate   | 0.0003   |
|    n_updates       | 50899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -178     |
|    critic_loss     | 19.1     |
|    ent_coef        | 0.21     |
|    ent_coef_loss   | -0.164   |
|    learning_rate   | 0.0003   |
|    n_updates       | 51299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -176     |
|    critic_loss     | 16.3     |
|    ent_coef        | 0.209    |
|    ent_coef_loss   | -0.505   |
|    learning_rate   | 0.0003   |
|    n_updates       | 51699    |
---------------------------------
=== Iterazione IRL 93 ===
Loss reward (iter 93): 5.9104743003845215
=== Iterazione IRL 94 ===
Loss reward (iter 94): 5.695109844207764
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -183     |
|    critic_loss     | 15.1     |
|    ent_coef        | 0.211    |
|    ent_coef_loss   | 0.327    |
|    learning_rate   | 0.0003   |
|    n_updates       | 51999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -180     |
|    critic_loss     | 16.2     |
|    ent_coef        | 0.21     |
|    ent_coef_loss   | 0.203    |
|    learning_rate   | 0.0003   |
|    n_updates       | 52399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -180     |
|    critic_loss     | 19.5     |
|    ent_coef        | 0.209    |
|    ent_coef_loss   | -0.211   |
|    learning_rate   | 0.0003   |
|    n_updates       | 52799    |
---------------------------------
=== Iterazione IRL 95 ===
Loss reward (iter 95): 5.800398349761963
=== Iterazione IRL 96 ===
Loss reward (iter 96): 5.614380359649658
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -177     |
|    critic_loss     | 17.6     |
|    ent_coef        | 0.204    |
|    ent_coef_loss   | 0.399    |
|    learning_rate   | 0.0003   |
|    n_updates       | 53099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -182     |
|    critic_loss     | 17.3     |
|    ent_coef        | 0.201    |
|    ent_coef_loss   | 0.291    |
|    learning_rate   | 0.0003   |
|    n_updates       | 53499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -184     |
|    critic_loss     | 15       |
|    ent_coef        | 0.2      |
|    ent_coef_loss   | -0.269   |
|    learning_rate   | 0.0003   |
|    n_updates       | 53899    |
---------------------------------
=== Iterazione IRL 97 ===
Loss reward (iter 97): 5.726663589477539
=== Iterazione IRL 98 ===
Loss reward (iter 98): 5.778295993804932
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -180     |
|    critic_loss     | 16.7     |
|    ent_coef        | 0.199    |
|    ent_coef_loss   | -0.131   |
|    learning_rate   | 0.0003   |
|    n_updates       | 54199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -181     |
|    critic_loss     | 16.3     |
|    ent_coef        | 0.197    |
|    ent_coef_loss   | 0.385    |
|    learning_rate   | 0.0003   |
|    n_updates       | 54599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -178     |
|    critic_loss     | 20.4     |
|    ent_coef        | 0.197    |
|    ent_coef_loss   | -0.268   |
|    learning_rate   | 0.0003   |
|    n_updates       | 54999    |
---------------------------------
=== Iterazione IRL 99 ===
Loss reward (iter 99): 5.539597988128662
=== Iterazione IRL 100 ===
Loss reward (iter 100): 5.61815071105957
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -180     |
|    critic_loss     | 18.7     |
|    ent_coef        | 0.19     |
|    ent_coef_loss   | 0.736    |
|    learning_rate   | 0.0003   |
|    n_updates       | 55299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -178     |
|    critic_loss     | 15.8     |
|    ent_coef        | 0.186    |
|    ent_coef_loss   | -0.274   |
|    learning_rate   | 0.0003   |
|    n_updates       | 55699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -181     |
|    critic_loss     | 17.6     |
|    ent_coef        | 0.184    |
|    ent_coef_loss   | -0.39    |
|    learning_rate   | 0.0003   |
|    n_updates       | 56099    |
---------------------------------
=== Iterazione IRL 101 ===
Loss reward (iter 101): 5.659247398376465
=== Iterazione IRL 102 ===
Loss reward (iter 102): 5.703838348388672
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -177     |
|    critic_loss     | 20.2     |
|    ent_coef        | 0.184    |
|    ent_coef_loss   | 0.353    |
|    learning_rate   | 0.0003   |
|    n_updates       | 56399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -177     |
|    critic_loss     | 19.9     |
|    ent_coef        | 0.182    |
|    ent_coef_loss   | -0.0485  |
|    learning_rate   | 0.0003   |
|    n_updates       | 56799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -181     |
|    critic_loss     | 18.5     |
|    ent_coef        | 0.178    |
|    ent_coef_loss   | -0.31    |
|    learning_rate   | 0.0003   |
|    n_updates       | 57199    |
---------------------------------
=== Iterazione IRL 103 ===
Loss reward (iter 103): 5.455639839172363
=== Iterazione IRL 104 ===
Loss reward (iter 104): 5.589791297912598
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -175     |
|    critic_loss     | 19.2     |
|    ent_coef        | 0.174    |
|    ent_coef_loss   | -0.333   |
|    learning_rate   | 0.0003   |
|    n_updates       | 57499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -178     |
|    critic_loss     | 17.5     |
|    ent_coef        | 0.171    |
|    ent_coef_loss   | -0.398   |
|    learning_rate   | 0.0003   |
|    n_updates       | 57899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -178     |
|    critic_loss     | 18.6     |
|    ent_coef        | 0.17     |
|    ent_coef_loss   | -0.138   |
|    learning_rate   | 0.0003   |
|    n_updates       | 58299    |
---------------------------------
=== Iterazione IRL 105 ===
Loss reward (iter 105): 5.777136325836182
=== Iterazione IRL 106 ===
Loss reward (iter 106): 5.513418674468994
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -177     |
|    critic_loss     | 18.9     |
|    ent_coef        | 0.169    |
|    ent_coef_loss   | 0.277    |
|    learning_rate   | 0.0003   |
|    n_updates       | 58599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -176     |
|    critic_loss     | 23.2     |
|    ent_coef        | 0.167    |
|    ent_coef_loss   | 0.118    |
|    learning_rate   | 0.0003   |
|    n_updates       | 58999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -179     |
|    critic_loss     | 16.6     |
|    ent_coef        | 0.163    |
|    ent_coef_loss   | -0.27    |
|    learning_rate   | 0.0003   |
|    n_updates       | 59399    |
---------------------------------
=== Iterazione IRL 107 ===
Loss reward (iter 107): 5.411787033081055
=== Iterazione IRL 108 ===
Loss reward (iter 108): 5.3736796379089355
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -176     |
|    critic_loss     | 21.9     |
|    ent_coef        | 0.158    |
|    ent_coef_loss   | 0.435    |
|    learning_rate   | 0.0003   |
|    n_updates       | 59699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -174     |
|    critic_loss     | 18.7     |
|    ent_coef        | 0.16     |
|    ent_coef_loss   | -0.0711  |
|    learning_rate   | 0.0003   |
|    n_updates       | 60099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -176     |
|    critic_loss     | 20.8     |
|    ent_coef        | 0.164    |
|    ent_coef_loss   | 0.688    |
|    learning_rate   | 0.0003   |
|    n_updates       | 60499    |
---------------------------------
=== Iterazione IRL 109 ===
Loss reward (iter 109): 5.698586463928223
=== Iterazione IRL 110 ===
Loss reward (iter 110): 5.311237812042236
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -175     |
|    critic_loss     | 19.6     |
|    ent_coef        | 0.164    |
|    ent_coef_loss   | -0.345   |
|    learning_rate   | 0.0003   |
|    n_updates       | 60799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -172     |
|    critic_loss     | 20.5     |
|    ent_coef        | 0.164    |
|    ent_coef_loss   | 0.515    |
|    learning_rate   | 0.0003   |
|    n_updates       | 61199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -171     |
|    critic_loss     | 21.9     |
|    ent_coef        | 0.164    |
|    ent_coef_loss   | 0.283    |
|    learning_rate   | 0.0003   |
|    n_updates       | 61599    |
---------------------------------
=== Iterazione IRL 111 ===
Loss reward (iter 111): 5.277900218963623
=== Iterazione IRL 112 ===
Loss reward (iter 112): 5.32241678237915
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -169     |
|    critic_loss     | 17.6     |
|    ent_coef        | 0.161    |
|    ent_coef_loss   | -0.344   |
|    learning_rate   | 0.0003   |
|    n_updates       | 61899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -171     |
|    critic_loss     | 22.6     |
|    ent_coef        | 0.158    |
|    ent_coef_loss   | -0.223   |
|    learning_rate   | 0.0003   |
|    n_updates       | 62299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -170     |
|    critic_loss     | 21.9     |
|    ent_coef        | 0.162    |
|    ent_coef_loss   | 0.533    |
|    learning_rate   | 0.0003   |
|    n_updates       | 62699    |
---------------------------------
=== Iterazione IRL 113 ===
Loss reward (iter 113): 4.915912628173828
=== Iterazione IRL 114 ===
Loss reward (iter 114): 5.322197437286377
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -171     |
|    critic_loss     | 22.8     |
|    ent_coef        | 0.161    |
|    ent_coef_loss   | 0.109    |
|    learning_rate   | 0.0003   |
|    n_updates       | 62999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -172     |
|    critic_loss     | 18.8     |
|    ent_coef        | 0.163    |
|    ent_coef_loss   | 0.297    |
|    learning_rate   | 0.0003   |
|    n_updates       | 63399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -170     |
|    critic_loss     | 23.1     |
|    ent_coef        | 0.166    |
|    ent_coef_loss   | 1.04     |
|    learning_rate   | 0.0003   |
|    n_updates       | 63799    |
---------------------------------
=== Iterazione IRL 115 ===
Loss reward (iter 115): 5.231077671051025
=== Iterazione IRL 116 ===
Loss reward (iter 116): 5.165449619293213
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -168     |
|    critic_loss     | 21.4     |
|    ent_coef        | 0.167    |
|    ent_coef_loss   | -0.0871  |
|    learning_rate   | 0.0003   |
|    n_updates       | 64099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -167     |
|    critic_loss     | 21.7     |
|    ent_coef        | 0.171    |
|    ent_coef_loss   | 0.0739   |
|    learning_rate   | 0.0003   |
|    n_updates       | 64499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -166     |
|    critic_loss     | 23.6     |
|    ent_coef        | 0.169    |
|    ent_coef_loss   | -0.0563  |
|    learning_rate   | 0.0003   |
|    n_updates       | 64899    |
---------------------------------
=== Iterazione IRL 117 ===
Loss reward (iter 117): 5.144289493560791
=== Iterazione IRL 118 ===
Loss reward (iter 118): 5.100633144378662
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -163     |
|    critic_loss     | 23.5     |
|    ent_coef        | 0.169    |
|    ent_coef_loss   | 0.276    |
|    learning_rate   | 0.0003   |
|    n_updates       | 65199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -162     |
|    critic_loss     | 23.6     |
|    ent_coef        | 0.175    |
|    ent_coef_loss   | -0.154   |
|    learning_rate   | 0.0003   |
|    n_updates       | 65599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -161     |
|    critic_loss     | 25.5     |
|    ent_coef        | 0.176    |
|    ent_coef_loss   | -0.0887  |
|    learning_rate   | 0.0003   |
|    n_updates       | 65999    |
---------------------------------
=== Iterazione IRL 119 ===
Loss reward (iter 119): 4.956385612487793
=== Iterazione IRL 120 ===
Loss reward (iter 120): 4.829514980316162
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -159     |
|    critic_loss     | 27       |
|    ent_coef        | 0.187    |
|    ent_coef_loss   | 0.533    |
|    learning_rate   | 0.0003   |
|    n_updates       | 66299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -160     |
|    critic_loss     | 26.4     |
|    ent_coef        | 0.202    |
|    ent_coef_loss   | 0.16     |
|    learning_rate   | 0.0003   |
|    n_updates       | 66699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -156     |
|    critic_loss     | 30.9     |
|    ent_coef        | 0.217    |
|    ent_coef_loss   | -0.0226  |
|    learning_rate   | 0.0003   |
|    n_updates       | 67099    |
---------------------------------
=== Iterazione IRL 121 ===
Loss reward (iter 121): 4.196188449859619
=== Iterazione IRL 122 ===
Loss reward (iter 122): 3.8948049545288086
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -155     |
|    critic_loss     | 27.7     |
|    ent_coef        | 0.225    |
|    ent_coef_loss   | 0.25     |
|    learning_rate   | 0.0003   |
|    n_updates       | 67399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -152     |
|    critic_loss     | 24.6     |
|    ent_coef        | 0.232    |
|    ent_coef_loss   | -0.173   |
|    learning_rate   | 0.0003   |
|    n_updates       | 67799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -153     |
|    critic_loss     | 22.1     |
|    ent_coef        | 0.228    |
|    ent_coef_loss   | -0.0782  |
|    learning_rate   | 0.0003   |
|    n_updates       | 68199    |
---------------------------------
=== Iterazione IRL 123 ===
Loss reward (iter 123): 4.167766571044922
=== Iterazione IRL 124 ===
Loss reward (iter 124): 2.968385696411133
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -150     |
|    critic_loss     | 28       |
|    ent_coef        | 0.223    |
|    ent_coef_loss   | -0.146   |
|    learning_rate   | 0.0003   |
|    n_updates       | 68499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -148     |
|    critic_loss     | 23.4     |
|    ent_coef        | 0.203    |
|    ent_coef_loss   | 0.277    |
|    learning_rate   | 0.0003   |
|    n_updates       | 68899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -147     |
|    critic_loss     | 24.1     |
|    ent_coef        | 0.194    |
|    ent_coef_loss   | -0.0337  |
|    learning_rate   | 0.0003   |
|    n_updates       | 69299    |
---------------------------------
=== Iterazione IRL 125 ===
Loss reward (iter 125): 6.260375022888184
=== Iterazione IRL 126 ===
Loss reward (iter 126): 5.127498626708984
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -146     |
|    critic_loss     | 24       |
|    ent_coef        | 0.188    |
|    ent_coef_loss   | -0.166   |
|    learning_rate   | 0.0003   |
|    n_updates       | 69599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -143     |
|    critic_loss     | 21.2     |
|    ent_coef        | 0.182    |
|    ent_coef_loss   | -0.0727  |
|    learning_rate   | 0.0003   |
|    n_updates       | 69999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -143     |
|    critic_loss     | 23.1     |
|    ent_coef        | 0.181    |
|    ent_coef_loss   | -0.758   |
|    learning_rate   | 0.0003   |
|    n_updates       | 70399    |
---------------------------------
=== Iterazione IRL 127 ===
Loss reward (iter 127): 5.69105863571167
=== Iterazione IRL 128 ===
Loss reward (iter 128): 5.5433807373046875
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -138     |
|    critic_loss     | 23.8     |
|    ent_coef        | 0.184    |
|    ent_coef_loss   | 0.59     |
|    learning_rate   | 0.0003   |
|    n_updates       | 70699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -139     |
|    critic_loss     | 20       |
|    ent_coef        | 0.186    |
|    ent_coef_loss   | 0.338    |
|    learning_rate   | 0.0003   |
|    n_updates       | 71099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -132     |
|    critic_loss     | 24.8     |
|    ent_coef        | 0.187    |
|    ent_coef_loss   | 0.0527   |
|    learning_rate   | 0.0003   |
|    n_updates       | 71499    |
---------------------------------
=== Iterazione IRL 129 ===
Loss reward (iter 129): 4.792994022369385
=== Iterazione IRL 130 ===
Loss reward (iter 130): 4.736621379852295
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -134     |
|    critic_loss     | 24.7     |
|    ent_coef        | 0.187    |
|    ent_coef_loss   | -0.0247  |
|    learning_rate   | 0.0003   |
|    n_updates       | 71799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -131     |
|    critic_loss     | 25.1     |
|    ent_coef        | 0.181    |
|    ent_coef_loss   | 0.192    |
|    learning_rate   | 0.0003   |
|    n_updates       | 72199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -131     |
|    critic_loss     | 23.4     |
|    ent_coef        | 0.176    |
|    ent_coef_loss   | 0.224    |
|    learning_rate   | 0.0003   |
|    n_updates       | 72599    |
---------------------------------
=== Iterazione IRL 131 ===
Loss reward (iter 131): 4.4672322273254395
=== Iterazione IRL 132 ===
Loss reward (iter 132): 4.686683654785156
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -128     |
|    critic_loss     | 21.2     |
|    ent_coef        | 0.174    |
|    ent_coef_loss   | 0.407    |
|    learning_rate   | 0.0003   |
|    n_updates       | 72899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -131     |
|    critic_loss     | 23.9     |
|    ent_coef        | 0.172    |
|    ent_coef_loss   | -0.317   |
|    learning_rate   | 0.0003   |
|    n_updates       | 73299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -126     |
|    critic_loss     | 24.5     |
|    ent_coef        | 0.173    |
|    ent_coef_loss   | -0.318   |
|    learning_rate   | 0.0003   |
|    n_updates       | 73699    |
---------------------------------
=== Iterazione IRL 133 ===
Loss reward (iter 133): 4.452980041503906
=== Iterazione IRL 134 ===
Loss reward (iter 134): 4.492786884307861
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -125     |
|    critic_loss     | 25.2     |
|    ent_coef        | 0.174    |
|    ent_coef_loss   | -0.0373  |
|    learning_rate   | 0.0003   |
|    n_updates       | 73999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -124     |
|    critic_loss     | 21.7     |
|    ent_coef        | 0.171    |
|    ent_coef_loss   | -0.371   |
|    learning_rate   | 0.0003   |
|    n_updates       | 74399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -120     |
|    critic_loss     | 27.9     |
|    ent_coef        | 0.172    |
|    ent_coef_loss   | -0.499   |
|    learning_rate   | 0.0003   |
|    n_updates       | 74799    |
---------------------------------
=== Iterazione IRL 135 ===
Loss reward (iter 135): 4.849638938903809
=== Iterazione IRL 136 ===
Loss reward (iter 136): 4.159185409545898
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -116     |
|    critic_loss     | 30       |
|    ent_coef        | 0.172    |
|    ent_coef_loss   | 0.192    |
|    learning_rate   | 0.0003   |
|    n_updates       | 75099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -119     |
|    critic_loss     | 24       |
|    ent_coef        | 0.171    |
|    ent_coef_loss   | -0.0995  |
|    learning_rate   | 0.0003   |
|    n_updates       | 75499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -113     |
|    critic_loss     | 18.3     |
|    ent_coef        | 0.165    |
|    ent_coef_loss   | 0.114    |
|    learning_rate   | 0.0003   |
|    n_updates       | 75899    |
---------------------------------
=== Iterazione IRL 137 ===
Loss reward (iter 137): 4.109521865844727
=== Iterazione IRL 138 ===
Loss reward (iter 138): 4.238080978393555
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -110     |
|    critic_loss     | 25.1     |
|    ent_coef        | 0.162    |
|    ent_coef_loss   | -0.113   |
|    learning_rate   | 0.0003   |
|    n_updates       | 76199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -111     |
|    critic_loss     | 29.7     |
|    ent_coef        | 0.165    |
|    ent_coef_loss   | 0.033    |
|    learning_rate   | 0.0003   |
|    n_updates       | 76599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -110     |
|    critic_loss     | 27.6     |
|    ent_coef        | 0.167    |
|    ent_coef_loss   | 0.634    |
|    learning_rate   | 0.0003   |
|    n_updates       | 76999    |
---------------------------------
=== Iterazione IRL 139 ===
Loss reward (iter 139): 3.6093947887420654
=== Iterazione IRL 140 ===
Loss reward (iter 140): 4.443358421325684
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -110     |
|    critic_loss     | 24.9     |
|    ent_coef        | 0.168    |
|    ent_coef_loss   | 0.392    |
|    learning_rate   | 0.0003   |
|    n_updates       | 77299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -110     |
|    critic_loss     | 20.8     |
|    ent_coef        | 0.167    |
|    ent_coef_loss   | -0.178   |
|    learning_rate   | 0.0003   |
|    n_updates       | 77699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -104     |
|    critic_loss     | 27.4     |
|    ent_coef        | 0.162    |
|    ent_coef_loss   | -0.0984  |
|    learning_rate   | 0.0003   |
|    n_updates       | 78099    |
---------------------------------
=== Iterazione IRL 141 ===
Loss reward (iter 141): 4.466376304626465
=== Iterazione IRL 142 ===
Loss reward (iter 142): 4.214005470275879
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -106     |
|    critic_loss     | 26.3     |
|    ent_coef        | 0.161    |
|    ent_coef_loss   | 0.499    |
|    learning_rate   | 0.0003   |
|    n_updates       | 78399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -100     |
|    critic_loss     | 26.4     |
|    ent_coef        | 0.156    |
|    ent_coef_loss   | -0.422   |
|    learning_rate   | 0.0003   |
|    n_updates       | 78799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -99.9    |
|    critic_loss     | 30.9     |
|    ent_coef        | 0.151    |
|    ent_coef_loss   | -0.544   |
|    learning_rate   | 0.0003   |
|    n_updates       | 79199    |
---------------------------------
=== Iterazione IRL 143 ===
Loss reward (iter 143): 4.946468830108643
=== Iterazione IRL 144 ===
Loss reward (iter 144): 4.372979640960693
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -96.7    |
|    critic_loss     | 35.7     |
|    ent_coef        | 0.15     |
|    ent_coef_loss   | 0.0221   |
|    learning_rate   | 0.0003   |
|    n_updates       | 79499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -98.4    |
|    critic_loss     | 30.5     |
|    ent_coef        | 0.151    |
|    ent_coef_loss   | 0.0195   |
|    learning_rate   | 0.0003   |
|    n_updates       | 79899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -94.9    |
|    critic_loss     | 29.9     |
|    ent_coef        | 0.148    |
|    ent_coef_loss   | 0.271    |
|    learning_rate   | 0.0003   |
|    n_updates       | 80299    |
---------------------------------
=== Iterazione IRL 145 ===
Loss reward (iter 145): 3.9240851402282715
=== Iterazione IRL 146 ===
Loss reward (iter 146): 3.983883857727051
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -94.3    |
|    critic_loss     | 30.8     |
|    ent_coef        | 0.148    |
|    ent_coef_loss   | -0.0418  |
|    learning_rate   | 0.0003   |
|    n_updates       | 80599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -87.9    |
|    critic_loss     | 28       |
|    ent_coef        | 0.148    |
|    ent_coef_loss   | 0.14     |
|    learning_rate   | 0.0003   |
|    n_updates       | 80999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -89.9    |
|    critic_loss     | 32.8     |
|    ent_coef        | 0.149    |
|    ent_coef_loss   | -0.298   |
|    learning_rate   | 0.0003   |
|    n_updates       | 81399    |
---------------------------------
=== Iterazione IRL 147 ===
Loss reward (iter 147): 4.5029144287109375
=== Iterazione IRL 148 ===
Loss reward (iter 148): 4.164660453796387
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -89.1    |
|    critic_loss     | 38.8     |
|    ent_coef        | 0.144    |
|    ent_coef_loss   | 0.257    |
|    learning_rate   | 0.0003   |
|    n_updates       | 81699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -81      |
|    critic_loss     | 38.8     |
|    ent_coef        | 0.146    |
|    ent_coef_loss   | 0.64     |
|    learning_rate   | 0.0003   |
|    n_updates       | 82099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -80.7    |
|    critic_loss     | 37.9     |
|    ent_coef        | 0.145    |
|    ent_coef_loss   | -0.546   |
|    learning_rate   | 0.0003   |
|    n_updates       | 82499    |
---------------------------------
=== Iterazione IRL 149 ===
Loss reward (iter 149): 4.5500617027282715
=== Iterazione IRL 150 ===
Loss reward (iter 150): 4.918778896331787
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -78.9    |
|    critic_loss     | 33       |
|    ent_coef        | 0.143    |
|    ent_coef_loss   | 0.232    |
|    learning_rate   | 0.0003   |
|    n_updates       | 82799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -74.3    |
|    critic_loss     | 35.4     |
|    ent_coef        | 0.146    |
|    ent_coef_loss   | 0.128    |
|    learning_rate   | 0.0003   |
|    n_updates       | 83199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -75.4    |
|    critic_loss     | 45.8     |
|    ent_coef        | 0.143    |
|    ent_coef_loss   | -0.185   |
|    learning_rate   | 0.0003   |
|    n_updates       | 83599    |
---------------------------------
=== Iterazione IRL 151 ===
Loss reward (iter 151): 4.584944725036621
=== Iterazione IRL 152 ===
Loss reward (iter 152): 4.852469444274902
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -72.2    |
|    critic_loss     | 37.6     |
|    ent_coef        | 0.143    |
|    ent_coef_loss   | -0.19    |
|    learning_rate   | 0.0003   |
|    n_updates       | 83899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -70.1    |
|    critic_loss     | 58.6     |
|    ent_coef        | 0.142    |
|    ent_coef_loss   | -0.332   |
|    learning_rate   | 0.0003   |
|    n_updates       | 84299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -68      |
|    critic_loss     | 34.8     |
|    ent_coef        | 0.146    |
|    ent_coef_loss   | -0.272   |
|    learning_rate   | 0.0003   |
|    n_updates       | 84699    |
---------------------------------
=== Iterazione IRL 153 ===
Loss reward (iter 153): 4.376235008239746
=== Iterazione IRL 154 ===
Loss reward (iter 154): 4.220542907714844
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -63.8    |
|    critic_loss     | 50.5     |
|    ent_coef        | 0.142    |
|    ent_coef_loss   | 0.282    |
|    learning_rate   | 0.0003   |
|    n_updates       | 84999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -60.3    |
|    critic_loss     | 37       |
|    ent_coef        | 0.142    |
|    ent_coef_loss   | 0.679    |
|    learning_rate   | 0.0003   |
|    n_updates       | 85399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -59.3    |
|    critic_loss     | 48.3     |
|    ent_coef        | 0.148    |
|    ent_coef_loss   | 0.0743   |
|    learning_rate   | 0.0003   |
|    n_updates       | 85799    |
---------------------------------
=== Iterazione IRL 155 ===
Loss reward (iter 155): 4.073382377624512
=== Iterazione IRL 156 ===
Loss reward (iter 156): 4.72093391418457
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -59.5    |
|    critic_loss     | 40.9     |
|    ent_coef        | 0.149    |
|    ent_coef_loss   | -0.225   |
|    learning_rate   | 0.0003   |
|    n_updates       | 86099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -52.7    |
|    critic_loss     | 38.1     |
|    ent_coef        | 0.149    |
|    ent_coef_loss   | 0.1      |
|    learning_rate   | 0.0003   |
|    n_updates       | 86499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -50.4    |
|    critic_loss     | 38.1     |
|    ent_coef        | 0.149    |
|    ent_coef_loss   | 0.38     |
|    learning_rate   | 0.0003   |
|    n_updates       | 86899    |
---------------------------------
=== Iterazione IRL 157 ===
Loss reward (iter 157): 4.448431968688965
=== Iterazione IRL 158 ===
Loss reward (iter 158): 4.746959686279297
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -52.5    |
|    critic_loss     | 42.1     |
|    ent_coef        | 0.148    |
|    ent_coef_loss   | 0.077    |
|    learning_rate   | 0.0003   |
|    n_updates       | 87199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -44.4    |
|    critic_loss     | 48.5     |
|    ent_coef        | 0.148    |
|    ent_coef_loss   | -0.466   |
|    learning_rate   | 0.0003   |
|    n_updates       | 87599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -42.7    |
|    critic_loss     | 43.1     |
|    ent_coef        | 0.149    |
|    ent_coef_loss   | 0.689    |
|    learning_rate   | 0.0003   |
|    n_updates       | 87999    |
---------------------------------
=== Iterazione IRL 159 ===
Loss reward (iter 159): 4.565579414367676
=== Iterazione IRL 160 ===
Loss reward (iter 160): 5.025885105133057
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -37.1    |
|    critic_loss     | 42.5     |
|    ent_coef        | 0.147    |
|    ent_coef_loss   | -0.448   |
|    learning_rate   | 0.0003   |
|    n_updates       | 88299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -43.5    |
|    critic_loss     | 38.8     |
|    ent_coef        | 0.153    |
|    ent_coef_loss   | 0.103    |
|    learning_rate   | 0.0003   |
|    n_updates       | 88699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -34.8    |
|    critic_loss     | 51.6     |
|    ent_coef        | 0.157    |
|    ent_coef_loss   | 0.0748   |
|    learning_rate   | 0.0003   |
|    n_updates       | 89099    |
---------------------------------
=== Iterazione IRL 161 ===
Loss reward (iter 161): 4.066476821899414
=== Iterazione IRL 162 ===
Loss reward (iter 162): 4.475142955780029
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -34.4    |
|    critic_loss     | 50.5     |
|    ent_coef        | 0.156    |
|    ent_coef_loss   | -0.0736  |
|    learning_rate   | 0.0003   |
|    n_updates       | 89399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -28      |
|    critic_loss     | 56.5     |
|    ent_coef        | 0.16     |
|    ent_coef_loss   | -0.117   |
|    learning_rate   | 0.0003   |
|    n_updates       | 89799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -31.4    |
|    critic_loss     | 54.6     |
|    ent_coef        | 0.158    |
|    ent_coef_loss   | -0.268   |
|    learning_rate   | 0.0003   |
|    n_updates       | 90199    |
---------------------------------
=== Iterazione IRL 163 ===
Loss reward (iter 163): 3.848817825317383
=== Iterazione IRL 164 ===
Loss reward (iter 164): 4.099125385284424
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -25.8    |
|    critic_loss     | 49.9     |
|    ent_coef        | 0.154    |
|    ent_coef_loss   | -0.266   |
|    learning_rate   | 0.0003   |
|    n_updates       | 90499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -18.3    |
|    critic_loss     | 52.4     |
|    ent_coef        | 0.154    |
|    ent_coef_loss   | 0.947    |
|    learning_rate   | 0.0003   |
|    n_updates       | 90899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -19.7    |
|    critic_loss     | 53.7     |
|    ent_coef        | 0.149    |
|    ent_coef_loss   | 0.413    |
|    learning_rate   | 0.0003   |
|    n_updates       | 91299    |
---------------------------------
=== Iterazione IRL 165 ===
Loss reward (iter 165): 3.8466949462890625
=== Iterazione IRL 166 ===
Loss reward (iter 166): 3.97274112701416
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -17.8    |
|    critic_loss     | 44.4     |
|    ent_coef        | 0.149    |
|    ent_coef_loss   | -0.773   |
|    learning_rate   | 0.0003   |
|    n_updates       | 91599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.9    |
|    critic_loss     | 58.3     |
|    ent_coef        | 0.144    |
|    ent_coef_loss   | 0.301    |
|    learning_rate   | 0.0003   |
|    n_updates       | 91999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -11.7    |
|    critic_loss     | 55.4     |
|    ent_coef        | 0.14     |
|    ent_coef_loss   | -0.16    |
|    learning_rate   | 0.0003   |
|    n_updates       | 92399    |
---------------------------------
=== Iterazione IRL 167 ===
Loss reward (iter 167): 3.4668099880218506
=== Iterazione IRL 168 ===
Loss reward (iter 168): 3.720381021499634
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -9.72    |
|    critic_loss     | 46.5     |
|    ent_coef        | 0.137    |
|    ent_coef_loss   | 0.148    |
|    learning_rate   | 0.0003   |
|    n_updates       | 92699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.91    |
|    critic_loss     | 64.6     |
|    ent_coef        | 0.136    |
|    ent_coef_loss   | -0.156   |
|    learning_rate   | 0.0003   |
|    n_updates       | 93099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -2.57    |
|    critic_loss     | 58.4     |
|    ent_coef        | 0.133    |
|    ent_coef_loss   | 0.109    |
|    learning_rate   | 0.0003   |
|    n_updates       | 93499    |
---------------------------------
=== Iterazione IRL 169 ===
Loss reward (iter 169): 3.76141357421875
=== Iterazione IRL 170 ===
Loss reward (iter 170): 4.173057556152344
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.8      |
|    critic_loss     | 52.4     |
|    ent_coef        | 0.133    |
|    ent_coef_loss   | 0.101    |
|    learning_rate   | 0.0003   |
|    n_updates       | 93799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.03     |
|    critic_loss     | 41.3     |
|    ent_coef        | 0.134    |
|    ent_coef_loss   | -0.358   |
|    learning_rate   | 0.0003   |
|    n_updates       | 94199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 3.14     |
|    critic_loss     | 60       |
|    ent_coef        | 0.133    |
|    ent_coef_loss   | 0.219    |
|    learning_rate   | 0.0003   |
|    n_updates       | 94599    |
---------------------------------
=== Iterazione IRL 171 ===
Loss reward (iter 171): 3.9868569374084473
=== Iterazione IRL 172 ===
Loss reward (iter 172): 5.038080215454102
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 7.59     |
|    critic_loss     | 62       |
|    ent_coef        | 0.136    |
|    ent_coef_loss   | -0.177   |
|    learning_rate   | 0.0003   |
|    n_updates       | 94899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 9.89     |
|    critic_loss     | 65.4     |
|    ent_coef        | 0.142    |
|    ent_coef_loss   | 0.285    |
|    learning_rate   | 0.0003   |
|    n_updates       | 95299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 13       |
|    critic_loss     | 56.9     |
|    ent_coef        | 0.142    |
|    ent_coef_loss   | 0.315    |
|    learning_rate   | 0.0003   |
|    n_updates       | 95699    |
---------------------------------
=== Iterazione IRL 173 ===
Loss reward (iter 173): 4.38737678527832
=== Iterazione IRL 174 ===
Loss reward (iter 174): 4.2052226066589355
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 16.7     |
|    critic_loss     | 55       |
|    ent_coef        | 0.144    |
|    ent_coef_loss   | 1.5      |
|    learning_rate   | 0.0003   |
|    n_updates       | 95999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 18.4     |
|    critic_loss     | 86.2     |
|    ent_coef        | 0.143    |
|    ent_coef_loss   | 0.043    |
|    learning_rate   | 0.0003   |
|    n_updates       | 96399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 20.2     |
|    critic_loss     | 61.7     |
|    ent_coef        | 0.143    |
|    ent_coef_loss   | 0.184    |
|    learning_rate   | 0.0003   |
|    n_updates       | 96799    |
---------------------------------
=== Iterazione IRL 175 ===
Loss reward (iter 175): 4.0857062339782715
=== Iterazione IRL 176 ===
Loss reward (iter 176): 3.384885787963867
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 24.2     |
|    critic_loss     | 69.1     |
|    ent_coef        | 0.142    |
|    ent_coef_loss   | 0.154    |
|    learning_rate   | 0.0003   |
|    n_updates       | 97099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 25.2     |
|    critic_loss     | 57.6     |
|    ent_coef        | 0.14     |
|    ent_coef_loss   | -0.0941  |
|    learning_rate   | 0.0003   |
|    n_updates       | 97499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 33.4     |
|    critic_loss     | 85.4     |
|    ent_coef        | 0.143    |
|    ent_coef_loss   | 0.488    |
|    learning_rate   | 0.0003   |
|    n_updates       | 97899    |
---------------------------------
=== Iterazione IRL 177 ===
Loss reward (iter 177): 3.478818655014038
=== Iterazione IRL 178 ===
Loss reward (iter 178): 3.5560147762298584
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 32.4     |
|    critic_loss     | 68       |
|    ent_coef        | 0.144    |
|    ent_coef_loss   | 0.707    |
|    learning_rate   | 0.0003   |
|    n_updates       | 98199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 33       |
|    critic_loss     | 63.9     |
|    ent_coef        | 0.152    |
|    ent_coef_loss   | -0.3     |
|    learning_rate   | 0.0003   |
|    n_updates       | 98599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 43.2     |
|    critic_loss     | 72.3     |
|    ent_coef        | 0.15     |
|    ent_coef_loss   | 0.189    |
|    learning_rate   | 0.0003   |
|    n_updates       | 98999    |
---------------------------------
=== Iterazione IRL 179 ===
Loss reward (iter 179): 3.1430015563964844
=== Iterazione IRL 180 ===
Loss reward (iter 180): 2.092071294784546
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 37.9     |
|    critic_loss     | 59       |
|    ent_coef        | 0.148    |
|    ent_coef_loss   | -0.215   |
|    learning_rate   | 0.0003   |
|    n_updates       | 99299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 44.6     |
|    critic_loss     | 64.6     |
|    ent_coef        | 0.155    |
|    ent_coef_loss   | -0.0304  |
|    learning_rate   | 0.0003   |
|    n_updates       | 99699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 45.4     |
|    critic_loss     | 87.8     |
|    ent_coef        | 0.155    |
|    ent_coef_loss   | 0.204    |
|    learning_rate   | 0.0003   |
|    n_updates       | 100099   |
---------------------------------
=== Iterazione IRL 181 ===
Loss reward (iter 181): 3.7527809143066406
=== Iterazione IRL 182 ===
Loss reward (iter 182): 2.9246766567230225
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 51.3     |
|    critic_loss     | 75.5     |
|    ent_coef        | 0.153    |
|    ent_coef_loss   | -0.597   |
|    learning_rate   | 0.0003   |
|    n_updates       | 100399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 53.6     |
|    critic_loss     | 93.7     |
|    ent_coef        | 0.154    |
|    ent_coef_loss   | -0.836   |
|    learning_rate   | 0.0003   |
|    n_updates       | 100799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 55.2     |
|    critic_loss     | 82.9     |
|    ent_coef        | 0.157    |
|    ent_coef_loss   | -0.118   |
|    learning_rate   | 0.0003   |
|    n_updates       | 101199   |
---------------------------------
=== Iterazione IRL 183 ===
Loss reward (iter 183): 3.5240719318389893
=== Iterazione IRL 184 ===
Loss reward (iter 184): 3.803095817565918
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 58       |
|    critic_loss     | 86.7     |
|    ent_coef        | 0.161    |
|    ent_coef_loss   | -0.0227  |
|    learning_rate   | 0.0003   |
|    n_updates       | 101499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 61.4     |
|    critic_loss     | 74.5     |
|    ent_coef        | 0.168    |
|    ent_coef_loss   | 0.0444   |
|    learning_rate   | 0.0003   |
|    n_updates       | 101899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 64       |
|    critic_loss     | 88.6     |
|    ent_coef        | 0.171    |
|    ent_coef_loss   | 0.26     |
|    learning_rate   | 0.0003   |
|    n_updates       | 102299   |
---------------------------------
=== Iterazione IRL 185 ===
Loss reward (iter 185): 4.65542459487915
=== Iterazione IRL 186 ===
Loss reward (iter 186): 4.0689287185668945
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 70       |
|    critic_loss     | 96.7     |
|    ent_coef        | 0.181    |
|    ent_coef_loss   | 0.226    |
|    learning_rate   | 0.0003   |
|    n_updates       | 102599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 70.9     |
|    critic_loss     | 55.5     |
|    ent_coef        | 0.191    |
|    ent_coef_loss   | -0.299   |
|    learning_rate   | 0.0003   |
|    n_updates       | 102999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 77.7     |
|    critic_loss     | 84.1     |
|    ent_coef        | 0.197    |
|    ent_coef_loss   | 0.149    |
|    learning_rate   | 0.0003   |
|    n_updates       | 103399   |
---------------------------------
=== Iterazione IRL 187 ===
Loss reward (iter 187): 4.794946670532227
=== Iterazione IRL 188 ===
Loss reward (iter 188): 4.584983825683594
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 73.9     |
|    critic_loss     | 89.2     |
|    ent_coef        | 0.198    |
|    ent_coef_loss   | 0.0568   |
|    learning_rate   | 0.0003   |
|    n_updates       | 103699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 82.5     |
|    critic_loss     | 92.8     |
|    ent_coef        | 0.195    |
|    ent_coef_loss   | -0.519   |
|    learning_rate   | 0.0003   |
|    n_updates       | 104099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 84.9     |
|    critic_loss     | 87.3     |
|    ent_coef        | 0.198    |
|    ent_coef_loss   | -0.07    |
|    learning_rate   | 0.0003   |
|    n_updates       | 104499   |
---------------------------------
=== Iterazione IRL 189 ===
Loss reward (iter 189): 5.7813801765441895
=== Iterazione IRL 190 ===
Loss reward (iter 190): 4.284175872802734
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 89.5     |
|    critic_loss     | 78.5     |
|    ent_coef        | 0.196    |
|    ent_coef_loss   | -0.228   |
|    learning_rate   | 0.0003   |
|    n_updates       | 104799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 87.5     |
|    critic_loss     | 68.5     |
|    ent_coef        | 0.193    |
|    ent_coef_loss   | -0.0686  |
|    learning_rate   | 0.0003   |
|    n_updates       | 105199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 92.4     |
|    critic_loss     | 87.2     |
|    ent_coef        | 0.19     |
|    ent_coef_loss   | 0.175    |
|    learning_rate   | 0.0003   |
|    n_updates       | 105599   |
---------------------------------
=== Iterazione IRL 191 ===
Loss reward (iter 191): 3.829336404800415
=== Iterazione IRL 192 ===
Loss reward (iter 192): 4.828679084777832
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 83.1     |
|    critic_loss     | 71.2     |
|    ent_coef        | 0.193    |
|    ent_coef_loss   | -0.106   |
|    learning_rate   | 0.0003   |
|    n_updates       | 105899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 93       |
|    critic_loss     | 85.9     |
|    ent_coef        | 0.193    |
|    ent_coef_loss   | -0.312   |
|    learning_rate   | 0.0003   |
|    n_updates       | 106299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 105      |
|    critic_loss     | 86.2     |
|    ent_coef        | 0.185    |
|    ent_coef_loss   | -0.0265  |
|    learning_rate   | 0.0003   |
|    n_updates       | 106699   |
---------------------------------
=== Iterazione IRL 193 ===
Loss reward (iter 193): 4.934508323669434
=== Iterazione IRL 194 ===
Loss reward (iter 194): 4.351486682891846
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 94.2     |
|    critic_loss     | 98       |
|    ent_coef        | 0.188    |
|    ent_coef_loss   | -0.451   |
|    learning_rate   | 0.0003   |
|    n_updates       | 106999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 102      |
|    critic_loss     | 90       |
|    ent_coef        | 0.187    |
|    ent_coef_loss   | -0.463   |
|    learning_rate   | 0.0003   |
|    n_updates       | 107399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 100      |
|    critic_loss     | 84.2     |
|    ent_coef        | 0.179    |
|    ent_coef_loss   | -0.673   |
|    learning_rate   | 0.0003   |
|    n_updates       | 107799   |
---------------------------------
=== Iterazione IRL 195 ===
Loss reward (iter 195): 4.9509806632995605
=== Iterazione IRL 196 ===
Loss reward (iter 196): 4.659030914306641
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 108      |
|    critic_loss     | 91.1     |
|    ent_coef        | 0.176    |
|    ent_coef_loss   | 0.0416   |
|    learning_rate   | 0.0003   |
|    n_updates       | 108099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 104      |
|    critic_loss     | 85.1     |
|    ent_coef        | 0.171    |
|    ent_coef_loss   | -0.163   |
|    learning_rate   | 0.0003   |
|    n_updates       | 108499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 115      |
|    critic_loss     | 84.4     |
|    ent_coef        | 0.163    |
|    ent_coef_loss   | 0.644    |
|    learning_rate   | 0.0003   |
|    n_updates       | 108899   |
---------------------------------
=== Iterazione IRL 197 ===
Loss reward (iter 197): 5.151291370391846
=== Iterazione IRL 198 ===
Loss reward (iter 198): 4.721601963043213
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 113      |
|    critic_loss     | 92.8     |
|    ent_coef        | 0.161    |
|    ent_coef_loss   | -0.497   |
|    learning_rate   | 0.0003   |
|    n_updates       | 109199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 116      |
|    critic_loss     | 89.9     |
|    ent_coef        | 0.164    |
|    ent_coef_loss   | -0.343   |
|    learning_rate   | 0.0003   |
|    n_updates       | 109599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 118      |
|    critic_loss     | 93.5     |
|    ent_coef        | 0.165    |
|    ent_coef_loss   | 0.319    |
|    learning_rate   | 0.0003   |
|    n_updates       | 109999   |
---------------------------------
=== Iterazione IRL 199 ===
Loss reward (iter 199): 4.730985164642334
=== Iterazione IRL 200 ===
Loss reward (iter 200): 4.587039947509766
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 128      |
|    critic_loss     | 98.3     |
|    ent_coef        | 0.159    |
|    ent_coef_loss   | -0.125   |
|    learning_rate   | 0.0003   |
|    n_updates       | 110299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 127      |
|    critic_loss     | 102      |
|    ent_coef        | 0.155    |
|    ent_coef_loss   | -0.315   |
|    learning_rate   | 0.0003   |
|    n_updates       | 110699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 128      |
|    critic_loss     | 92.3     |
|    ent_coef        | 0.152    |
|    ent_coef_loss   | -0.464   |
|    learning_rate   | 0.0003   |
|    n_updates       | 111099   |
---------------------------------
=== Iterazione IRL 201 ===
Loss reward (iter 201): 3.902815103530884
=== Iterazione IRL 202 ===
Loss reward (iter 202): 3.999401092529297
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 136      |
|    critic_loss     | 93.9     |
|    ent_coef        | 0.152    |
|    ent_coef_loss   | -0.172   |
|    learning_rate   | 0.0003   |
|    n_updates       | 111399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 131      |
|    critic_loss     | 124      |
|    ent_coef        | 0.156    |
|    ent_coef_loss   | -0.141   |
|    learning_rate   | 0.0003   |
|    n_updates       | 111799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 138      |
|    critic_loss     | 112      |
|    ent_coef        | 0.158    |
|    ent_coef_loss   | 0.53     |
|    learning_rate   | 0.0003   |
|    n_updates       | 112199   |
---------------------------------
=== Iterazione IRL 203 ===
Loss reward (iter 203): 4.645108699798584
=== Iterazione IRL 204 ===
Loss reward (iter 204): 3.487534523010254
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 157      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 140      |
|    critic_loss     | 83.4     |
|    ent_coef        | 0.159    |
|    ent_coef_loss   | -0.343   |
|    learning_rate   | 0.0003   |
|    n_updates       | 112499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 137      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 144      |
|    critic_loss     | 120      |
|    ent_coef        | 0.159    |
|    ent_coef_loss   | 0.0882   |
|    learning_rate   | 0.0003   |
|    n_updates       | 112899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 131      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 148      |
|    critic_loss     | 92.6     |
|    ent_coef        | 0.161    |
|    ent_coef_loss   | 0.0315   |
|    learning_rate   | 0.0003   |
|    n_updates       | 113299   |
---------------------------------
=== Iterazione IRL 205 ===
Loss reward (iter 205): 4.743315696716309
=== Iterazione IRL 206 ===
Loss reward (iter 206): 4.0407938957214355
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 157      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 145      |
|    critic_loss     | 90.6     |
|    ent_coef        | 0.158    |
|    ent_coef_loss   | -0.374   |
|    learning_rate   | 0.0003   |
|    n_updates       | 113599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 137      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 150      |
|    critic_loss     | 99.8     |
|    ent_coef        | 0.165    |
|    ent_coef_loss   | -0.265   |
|    learning_rate   | 0.0003   |
|    n_updates       | 113999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 131      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 154      |
|    critic_loss     | 87.8     |
|    ent_coef        | 0.166    |
|    ent_coef_loss   | -0.527   |
|    learning_rate   | 0.0003   |
|    n_updates       | 114399   |
---------------------------------
=== Iterazione IRL 207 ===
Loss reward (iter 207): 4.032885551452637
=== Iterazione IRL 208 ===
Loss reward (iter 208): 4.018738269805908
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 157      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 160      |
|    critic_loss     | 102      |
|    ent_coef        | 0.167    |
|    ent_coef_loss   | 0.594    |
|    learning_rate   | 0.0003   |
|    n_updates       | 114699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 137      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 159      |
|    critic_loss     | 84.5     |
|    ent_coef        | 0.164    |
|    ent_coef_loss   | -0.388   |
|    learning_rate   | 0.0003   |
|    n_updates       | 115099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 131      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 164      |
|    critic_loss     | 90.5     |
|    ent_coef        | 0.169    |
|    ent_coef_loss   | -0.0238  |
|    learning_rate   | 0.0003   |
|    n_updates       | 115499   |
---------------------------------
=== Iterazione IRL 209 ===
Loss reward (iter 209): 4.6671142578125
=== Iterazione IRL 210 ===
Loss reward (iter 210): 4.156665802001953
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 171      |
|    critic_loss     | 121      |
|    ent_coef        | 0.167    |
|    ent_coef_loss   | -0.189   |
|    learning_rate   | 0.0003   |
|    n_updates       | 115799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 161      |
|    critic_loss     | 92.1     |
|    ent_coef        | 0.16     |
|    ent_coef_loss   | -0.4     |
|    learning_rate   | 0.0003   |
|    n_updates       | 116199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 131      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 184      |
|    critic_loss     | 82.3     |
|    ent_coef        | 0.162    |
|    ent_coef_loss   | -0.084   |
|    learning_rate   | 0.0003   |
|    n_updates       | 116599   |
---------------------------------
=== Iterazione IRL 211 ===
Loss reward (iter 211): 3.9310107231140137
=== Iterazione IRL 212 ===
Loss reward (iter 212): 4.056814193725586
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 157      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 172      |
|    critic_loss     | 89.5     |
|    ent_coef        | 0.16     |
|    ent_coef_loss   | -0.236   |
|    learning_rate   | 0.0003   |
|    n_updates       | 116899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 137      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 181      |
|    critic_loss     | 90.3     |
|    ent_coef        | 0.162    |
|    ent_coef_loss   | 0.0457   |
|    learning_rate   | 0.0003   |
|    n_updates       | 117299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 131      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 192      |
|    critic_loss     | 119      |
|    ent_coef        | 0.171    |
|    ent_coef_loss   | 0.493    |
|    learning_rate   | 0.0003   |
|    n_updates       | 117699   |
---------------------------------
=== Iterazione IRL 213 ===
Loss reward (iter 213): 3.449260950088501
=== Iterazione IRL 214 ===
Loss reward (iter 214): 2.468911647796631
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 157      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 187      |
|    critic_loss     | 109      |
|    ent_coef        | 0.178    |
|    ent_coef_loss   | 0.118    |
|    learning_rate   | 0.0003   |
|    n_updates       | 117999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 194      |
|    critic_loss     | 120      |
|    ent_coef        | 0.172    |
|    ent_coef_loss   | 0.237    |
|    learning_rate   | 0.0003   |
|    n_updates       | 118399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 131      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 190      |
|    critic_loss     | 96.4     |
|    ent_coef        | 0.171    |
|    ent_coef_loss   | -0.252   |
|    learning_rate   | 0.0003   |
|    n_updates       | 118799   |
---------------------------------
=== Iterazione IRL 215 ===
Loss reward (iter 215): 2.4284632205963135
=== Iterazione IRL 216 ===
Loss reward (iter 216): 2.995537042617798
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 197      |
|    critic_loss     | 103      |
|    ent_coef        | 0.171    |
|    ent_coef_loss   | 0.311    |
|    learning_rate   | 0.0003   |
|    n_updates       | 119099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 200      |
|    critic_loss     | 142      |
|    ent_coef        | 0.169    |
|    ent_coef_loss   | 0.2      |
|    learning_rate   | 0.0003   |
|    n_updates       | 119499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 131      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 203      |
|    critic_loss     | 161      |
|    ent_coef        | 0.17     |
|    ent_coef_loss   | -0.26    |
|    learning_rate   | 0.0003   |
|    n_updates       | 119899   |
---------------------------------
=== Iterazione IRL 217 ===
Loss reward (iter 217): 2.745880603790283
=== Iterazione IRL 218 ===
Loss reward (iter 218): 2.2686986923217773
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 157      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 204      |
|    critic_loss     | 117      |
|    ent_coef        | 0.168    |
|    ent_coef_loss   | 0.468    |
|    learning_rate   | 0.0003   |
|    n_updates       | 120199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 214      |
|    critic_loss     | 111      |
|    ent_coef        | 0.173    |
|    ent_coef_loss   | 0.514    |
|    learning_rate   | 0.0003   |
|    n_updates       | 120599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 131      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 206      |
|    critic_loss     | 87.2     |
|    ent_coef        | 0.172    |
|    ent_coef_loss   | -0.413   |
|    learning_rate   | 0.0003   |
|    n_updates       | 120999   |
---------------------------------
=== Iterazione IRL 219 ===
Loss reward (iter 219): 4.447535514831543
=== Iterazione IRL 220 ===
Loss reward (iter 220): 4.086925983428955
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 217      |
|    critic_loss     | 133      |
|    ent_coef        | 0.173    |
|    ent_coef_loss   | 0.153    |
|    learning_rate   | 0.0003   |
|    n_updates       | 121299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 216      |
|    critic_loss     | 104      |
|    ent_coef        | 0.176    |
|    ent_coef_loss   | -0.323   |
|    learning_rate   | 0.0003   |
|    n_updates       | 121699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 131      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 224      |
|    critic_loss     | 121      |
|    ent_coef        | 0.178    |
|    ent_coef_loss   | -0.561   |
|    learning_rate   | 0.0003   |
|    n_updates       | 122099   |
---------------------------------
=== Iterazione IRL 221 ===
Loss reward (iter 221): 2.753427028656006
=== Iterazione IRL 222 ===
Loss reward (iter 222): 2.192814350128174
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 218      |
|    critic_loss     | 86.3     |
|    ent_coef        | 0.18     |
|    ent_coef_loss   | -0.024   |
|    learning_rate   | 0.0003   |
|    n_updates       | 122399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 220      |
|    critic_loss     | 96       |
|    ent_coef        | 0.187    |
|    ent_coef_loss   | -0.175   |
|    learning_rate   | 0.0003   |
|    n_updates       | 122799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 131      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 227      |
|    critic_loss     | 119      |
|    ent_coef        | 0.188    |
|    ent_coef_loss   | 0.387    |
|    learning_rate   | 0.0003   |
|    n_updates       | 123199   |
---------------------------------
=== Iterazione IRL 223 ===
Loss reward (iter 223): 2.6494832038879395
=== Iterazione IRL 224 ===
Loss reward (iter 224): 2.6009767055511475
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 233      |
|    critic_loss     | 120      |
|    ent_coef        | 0.191    |
|    ent_coef_loss   | 0.24     |
|    learning_rate   | 0.0003   |
|    n_updates       | 123499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 231      |
|    critic_loss     | 115      |
|    ent_coef        | 0.195    |
|    ent_coef_loss   | 0.164    |
|    learning_rate   | 0.0003   |
|    n_updates       | 123899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 131      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 232      |
|    critic_loss     | 120      |
|    ent_coef        | 0.19     |
|    ent_coef_loss   | -0.414   |
|    learning_rate   | 0.0003   |
|    n_updates       | 124299   |
---------------------------------
=== Iterazione IRL 225 ===
Loss reward (iter 225): 3.0627825260162354
=== Iterazione IRL 226 ===
Loss reward (iter 226): 3.318204402923584
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 239      |
|    critic_loss     | 105      |
|    ent_coef        | 0.188    |
|    ent_coef_loss   | 0.315    |
|    learning_rate   | 0.0003   |
|    n_updates       | 124599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 245      |
|    critic_loss     | 106      |
|    ent_coef        | 0.187    |
|    ent_coef_loss   | 0.0605   |
|    learning_rate   | 0.0003   |
|    n_updates       | 124999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 248      |
|    critic_loss     | 138      |
|    ent_coef        | 0.188    |
|    ent_coef_loss   | 0.0874   |
|    learning_rate   | 0.0003   |
|    n_updates       | 125399   |
---------------------------------
=== Iterazione IRL 227 ===
Loss reward (iter 227): 3.0818064212799072
=== Iterazione IRL 228 ===
Loss reward (iter 228): 3.103652000427246
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 251      |
|    critic_loss     | 138      |
|    ent_coef        | 0.187    |
|    ent_coef_loss   | 0.33     |
|    learning_rate   | 0.0003   |
|    n_updates       | 125699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 251      |
|    critic_loss     | 112      |
|    ent_coef        | 0.184    |
|    ent_coef_loss   | -0.0645  |
|    learning_rate   | 0.0003   |
|    n_updates       | 126099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 254      |
|    critic_loss     | 103      |
|    ent_coef        | 0.183    |
|    ent_coef_loss   | 0.47     |
|    learning_rate   | 0.0003   |
|    n_updates       | 126499   |
---------------------------------
=== Iterazione IRL 229 ===
Loss reward (iter 229): 3.039829969406128
=== Iterazione IRL 230 ===
Loss reward (iter 230): 2.6330676078796387
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 261      |
|    critic_loss     | 140      |
|    ent_coef        | 0.182    |
|    ent_coef_loss   | -0.207   |
|    learning_rate   | 0.0003   |
|    n_updates       | 126799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 261      |
|    critic_loss     | 110      |
|    ent_coef        | 0.181    |
|    ent_coef_loss   | 0.366    |
|    learning_rate   | 0.0003   |
|    n_updates       | 127199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 262      |
|    critic_loss     | 101      |
|    ent_coef        | 0.183    |
|    ent_coef_loss   | 0.603    |
|    learning_rate   | 0.0003   |
|    n_updates       | 127599   |
---------------------------------
=== Iterazione IRL 231 ===
Loss reward (iter 231): 2.1842551231384277
=== Iterazione IRL 232 ===
Loss reward (iter 232): 3.6692962646484375
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 260      |
|    critic_loss     | 127      |
|    ent_coef        | 0.189    |
|    ent_coef_loss   | 0.83     |
|    learning_rate   | 0.0003   |
|    n_updates       | 127899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 265      |
|    critic_loss     | 98.2     |
|    ent_coef        | 0.19     |
|    ent_coef_loss   | 0.0637   |
|    learning_rate   | 0.0003   |
|    n_updates       | 128299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 274      |
|    critic_loss     | 123      |
|    ent_coef        | 0.192    |
|    ent_coef_loss   | -0.0365  |
|    learning_rate   | 0.0003   |
|    n_updates       | 128699   |
---------------------------------
=== Iterazione IRL 233 ===
Loss reward (iter 233): 2.581430435180664
=== Iterazione IRL 234 ===
Loss reward (iter 234): 2.874967575073242
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 276      |
|    critic_loss     | 114      |
|    ent_coef        | 0.197    |
|    ent_coef_loss   | -0.236   |
|    learning_rate   | 0.0003   |
|    n_updates       | 128999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 277      |
|    critic_loss     | 130      |
|    ent_coef        | 0.198    |
|    ent_coef_loss   | -0.146   |
|    learning_rate   | 0.0003   |
|    n_updates       | 129399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 283      |
|    critic_loss     | 107      |
|    ent_coef        | 0.202    |
|    ent_coef_loss   | 0.0475   |
|    learning_rate   | 0.0003   |
|    n_updates       | 129799   |
---------------------------------
=== Iterazione IRL 235 ===
Loss reward (iter 235): 3.3066484928131104
=== Iterazione IRL 236 ===
Loss reward (iter 236): 3.0910518169403076
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 287      |
|    critic_loss     | 145      |
|    ent_coef        | 0.205    |
|    ent_coef_loss   | 0.372    |
|    learning_rate   | 0.0003   |
|    n_updates       | 130099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 286      |
|    critic_loss     | 128      |
|    ent_coef        | 0.207    |
|    ent_coef_loss   | 0.22     |
|    learning_rate   | 0.0003   |
|    n_updates       | 130499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 284      |
|    critic_loss     | 109      |
|    ent_coef        | 0.205    |
|    ent_coef_loss   | -0.901   |
|    learning_rate   | 0.0003   |
|    n_updates       | 130899   |
---------------------------------
=== Iterazione IRL 237 ===
Loss reward (iter 237): 2.78037428855896
=== Iterazione IRL 238 ===
Loss reward (iter 238): 2.5545337200164795
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 297      |
|    critic_loss     | 105      |
|    ent_coef        | 0.207    |
|    ent_coef_loss   | -0.746   |
|    learning_rate   | 0.0003   |
|    n_updates       | 131199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 298      |
|    critic_loss     | 157      |
|    ent_coef        | 0.207    |
|    ent_coef_loss   | 0.468    |
|    learning_rate   | 0.0003   |
|    n_updates       | 131599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 302      |
|    critic_loss     | 116      |
|    ent_coef        | 0.211    |
|    ent_coef_loss   | -0.104   |
|    learning_rate   | 0.0003   |
|    n_updates       | 131999   |
---------------------------------
=== Iterazione IRL 239 ===
Loss reward (iter 239): 2.2042815685272217
=== Iterazione IRL 240 ===
Loss reward (iter 240): 2.5935239791870117
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 301      |
|    critic_loss     | 121      |
|    ent_coef        | 0.212    |
|    ent_coef_loss   | 0.317    |
|    learning_rate   | 0.0003   |
|    n_updates       | 132299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 307      |
|    critic_loss     | 120      |
|    ent_coef        | 0.217    |
|    ent_coef_loss   | -0.00343 |
|    learning_rate   | 0.0003   |
|    n_updates       | 132699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 301      |
|    critic_loss     | 145      |
|    ent_coef        | 0.222    |
|    ent_coef_loss   | 0.046    |
|    learning_rate   | 0.0003   |
|    n_updates       | 133099   |
---------------------------------
=== Iterazione IRL 241 ===
Loss reward (iter 241): 3.7354767322540283
=== Iterazione IRL 242 ===
Loss reward (iter 242): 2.9557077884674072
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 309      |
|    critic_loss     | 167      |
|    ent_coef        | 0.216    |
|    ent_coef_loss   | 0.0159   |
|    learning_rate   | 0.0003   |
|    n_updates       | 133399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 312      |
|    critic_loss     | 163      |
|    ent_coef        | 0.207    |
|    ent_coef_loss   | -0.18    |
|    learning_rate   | 0.0003   |
|    n_updates       | 133799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 319      |
|    critic_loss     | 142      |
|    ent_coef        | 0.199    |
|    ent_coef_loss   | 0.324    |
|    learning_rate   | 0.0003   |
|    n_updates       | 134199   |
---------------------------------
=== Iterazione IRL 243 ===
Loss reward (iter 243): 3.3983569145202637
=== Iterazione IRL 244 ===
Loss reward (iter 244): 3.185771942138672
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 320      |
|    critic_loss     | 189      |
|    ent_coef        | 0.2      |
|    ent_coef_loss   | -0.0788  |
|    learning_rate   | 0.0003   |
|    n_updates       | 134499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 328      |
|    critic_loss     | 155      |
|    ent_coef        | 0.195    |
|    ent_coef_loss   | 0.136    |
|    learning_rate   | 0.0003   |
|    n_updates       | 134899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 328      |
|    critic_loss     | 146      |
|    ent_coef        | 0.197    |
|    ent_coef_loss   | 0.134    |
|    learning_rate   | 0.0003   |
|    n_updates       | 135299   |
---------------------------------
=== Iterazione IRL 245 ===
Loss reward (iter 245): 3.585672378540039
=== Iterazione IRL 246 ===
Loss reward (iter 246): 3.3361921310424805
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 332      |
|    critic_loss     | 129      |
|    ent_coef        | 0.193    |
|    ent_coef_loss   | 0.213    |
|    learning_rate   | 0.0003   |
|    n_updates       | 135599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 336      |
|    critic_loss     | 180      |
|    ent_coef        | 0.192    |
|    ent_coef_loss   | 0.291    |
|    learning_rate   | 0.0003   |
|    n_updates       | 135999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 340      |
|    critic_loss     | 156      |
|    ent_coef        | 0.195    |
|    ent_coef_loss   | 0.0616   |
|    learning_rate   | 0.0003   |
|    n_updates       | 136399   |
---------------------------------
=== Iterazione IRL 247 ===
Loss reward (iter 247): 1.1545822620391846
=== Iterazione IRL 248 ===
Loss reward (iter 248): 3.144590139389038
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 341      |
|    critic_loss     | 183      |
|    ent_coef        | 0.199    |
|    ent_coef_loss   | 0.0849   |
|    learning_rate   | 0.0003   |
|    n_updates       | 136699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 349      |
|    critic_loss     | 292      |
|    ent_coef        | 0.202    |
|    ent_coef_loss   | 1.18     |
|    learning_rate   | 0.0003   |
|    n_updates       | 137099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 352      |
|    critic_loss     | 222      |
|    ent_coef        | 0.208    |
|    ent_coef_loss   | -0.282   |
|    learning_rate   | 0.0003   |
|    n_updates       | 137499   |
---------------------------------
=== Iterazione IRL 249 ===
Loss reward (iter 249): 2.114414930343628
=== Iterazione IRL 250 ===
Loss reward (iter 250): 1.876029133796692
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 353      |
|    critic_loss     | 151      |
|    ent_coef        | 0.202    |
|    ent_coef_loss   | 0.456    |
|    learning_rate   | 0.0003   |
|    n_updates       | 137799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 358      |
|    critic_loss     | 210      |
|    ent_coef        | 0.209    |
|    ent_coef_loss   | 0.351    |
|    learning_rate   | 0.0003   |
|    n_updates       | 138199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 362      |
|    critic_loss     | 160      |
|    ent_coef        | 0.212    |
|    ent_coef_loss   | -0.139   |
|    learning_rate   | 0.0003   |
|    n_updates       | 138599   |
---------------------------------
=== Iterazione IRL 251 ===
Loss reward (iter 251): 1.0395009517669678
=== Iterazione IRL 252 ===
Loss reward (iter 252): 2.68000864982605
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 358      |
|    critic_loss     | 192      |
|    ent_coef        | 0.215    |
|    ent_coef_loss   | -0.826   |
|    learning_rate   | 0.0003   |
|    n_updates       | 138899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 368      |
|    critic_loss     | 169      |
|    ent_coef        | 0.222    |
|    ent_coef_loss   | -0.0175  |
|    learning_rate   | 0.0003   |
|    n_updates       | 139299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 376      |
|    critic_loss     | 164      |
|    ent_coef        | 0.221    |
|    ent_coef_loss   | 0.401    |
|    learning_rate   | 0.0003   |
|    n_updates       | 139699   |
---------------------------------
=== Iterazione IRL 253 ===
Loss reward (iter 253): 3.871610641479492
=== Iterazione IRL 254 ===
Loss reward (iter 254): 3.1912598609924316
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 373      |
|    critic_loss     | 180      |
|    ent_coef        | 0.218    |
|    ent_coef_loss   | 0.129    |
|    learning_rate   | 0.0003   |
|    n_updates       | 139999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 380      |
|    critic_loss     | 168      |
|    ent_coef        | 0.212    |
|    ent_coef_loss   | -0.558   |
|    learning_rate   | 0.0003   |
|    n_updates       | 140399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 382      |
|    critic_loss     | 151      |
|    ent_coef        | 0.216    |
|    ent_coef_loss   | 0.0111   |
|    learning_rate   | 0.0003   |
|    n_updates       | 140799   |
---------------------------------
=== Iterazione IRL 255 ===
Loss reward (iter 255): 1.3829890489578247
=== Iterazione IRL 256 ===
Loss reward (iter 256): 1.6048558950424194
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 391      |
|    critic_loss     | 190      |
|    ent_coef        | 0.218    |
|    ent_coef_loss   | 0.246    |
|    learning_rate   | 0.0003   |
|    n_updates       | 141099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 394      |
|    critic_loss     | 201      |
|    ent_coef        | 0.215    |
|    ent_coef_loss   | -0.604   |
|    learning_rate   | 0.0003   |
|    n_updates       | 141499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 394      |
|    critic_loss     | 206      |
|    ent_coef        | 0.217    |
|    ent_coef_loss   | 0.395    |
|    learning_rate   | 0.0003   |
|    n_updates       | 141899   |
---------------------------------
=== Iterazione IRL 257 ===
Loss reward (iter 257): 1.8835651874542236
=== Iterazione IRL 258 ===
Loss reward (iter 258): 3.7756142616271973
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 395      |
|    critic_loss     | 241      |
|    ent_coef        | 0.214    |
|    ent_coef_loss   | 0.505    |
|    learning_rate   | 0.0003   |
|    n_updates       | 142199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 394      |
|    critic_loss     | 172      |
|    ent_coef        | 0.222    |
|    ent_coef_loss   | 0.695    |
|    learning_rate   | 0.0003   |
|    n_updates       | 142599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 401      |
|    critic_loss     | 159      |
|    ent_coef        | 0.222    |
|    ent_coef_loss   | -0.53    |
|    learning_rate   | 0.0003   |
|    n_updates       | 142999   |
---------------------------------
=== Iterazione IRL 259 ===
Loss reward (iter 259): 3.768468141555786
=== Iterazione IRL 260 ===
Loss reward (iter 260): 5.683897018432617
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 404      |
|    critic_loss     | 228      |
|    ent_coef        | 0.224    |
|    ent_coef_loss   | -0.526   |
|    learning_rate   | 0.0003   |
|    n_updates       | 143299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 409      |
|    critic_loss     | 200      |
|    ent_coef        | 0.227    |
|    ent_coef_loss   | -0.0572  |
|    learning_rate   | 0.0003   |
|    n_updates       | 143699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 414      |
|    critic_loss     | 212      |
|    ent_coef        | 0.241    |
|    ent_coef_loss   | -0.0865  |
|    learning_rate   | 0.0003   |
|    n_updates       | 144099   |
---------------------------------
=== Iterazione IRL 261 ===
Loss reward (iter 261): 4.588554859161377
=== Iterazione IRL 262 ===
Loss reward (iter 262): 4.78618049621582
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 422      |
|    critic_loss     | 286      |
|    ent_coef        | 0.244    |
|    ent_coef_loss   | -0.223   |
|    learning_rate   | 0.0003   |
|    n_updates       | 144399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 419      |
|    critic_loss     | 189      |
|    ent_coef        | 0.245    |
|    ent_coef_loss   | -0.0961  |
|    learning_rate   | 0.0003   |
|    n_updates       | 144799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 430      |
|    critic_loss     | 216      |
|    ent_coef        | 0.24     |
|    ent_coef_loss   | -0.0522  |
|    learning_rate   | 0.0003   |
|    n_updates       | 145199   |
---------------------------------
=== Iterazione IRL 263 ===
Loss reward (iter 263): 3.505070924758911
=== Iterazione IRL 264 ===
Loss reward (iter 264): 3.3366260528564453
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 428      |
|    critic_loss     | 246      |
|    ent_coef        | 0.237    |
|    ent_coef_loss   | -0.328   |
|    learning_rate   | 0.0003   |
|    n_updates       | 145499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 438      |
|    critic_loss     | 285      |
|    ent_coef        | 0.236    |
|    ent_coef_loss   | -0.0385  |
|    learning_rate   | 0.0003   |
|    n_updates       | 145899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 436      |
|    critic_loss     | 265      |
|    ent_coef        | 0.236    |
|    ent_coef_loss   | 0.115    |
|    learning_rate   | 0.0003   |
|    n_updates       | 146299   |
---------------------------------
=== Iterazione IRL 265 ===
Loss reward (iter 265): 3.886367082595825
=== Iterazione IRL 266 ===
Loss reward (iter 266): 4.167194843292236
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 430      |
|    critic_loss     | 221      |
|    ent_coef        | 0.236    |
|    ent_coef_loss   | 0.277    |
|    learning_rate   | 0.0003   |
|    n_updates       | 146599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 438      |
|    critic_loss     | 228      |
|    ent_coef        | 0.233    |
|    ent_coef_loss   | -0.761   |
|    learning_rate   | 0.0003   |
|    n_updates       | 146999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 443      |
|    critic_loss     | 244      |
|    ent_coef        | 0.225    |
|    ent_coef_loss   | -0.473   |
|    learning_rate   | 0.0003   |
|    n_updates       | 147399   |
---------------------------------
=== Iterazione IRL 267 ===
Loss reward (iter 267): 3.2383952140808105
=== Iterazione IRL 268 ===
Loss reward (iter 268): 3.718674659729004
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 447      |
|    critic_loss     | 218      |
|    ent_coef        | 0.227    |
|    ent_coef_loss   | 0.0862   |
|    learning_rate   | 0.0003   |
|    n_updates       | 147699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 457      |
|    critic_loss     | 305      |
|    ent_coef        | 0.228    |
|    ent_coef_loss   | 0.389    |
|    learning_rate   | 0.0003   |
|    n_updates       | 148099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 456      |
|    critic_loss     | 321      |
|    ent_coef        | 0.243    |
|    ent_coef_loss   | -0.151   |
|    learning_rate   | 0.0003   |
|    n_updates       | 148499   |
---------------------------------
=== Iterazione IRL 269 ===
Loss reward (iter 269): 3.089292049407959
=== Iterazione IRL 270 ===
Loss reward (iter 270): 2.9840946197509766
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 465      |
|    critic_loss     | 298      |
|    ent_coef        | 0.24     |
|    ent_coef_loss   | -0.255   |
|    learning_rate   | 0.0003   |
|    n_updates       | 148799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 471      |
|    critic_loss     | 281      |
|    ent_coef        | 0.238    |
|    ent_coef_loss   | -0.329   |
|    learning_rate   | 0.0003   |
|    n_updates       | 149199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 474      |
|    critic_loss     | 269      |
|    ent_coef        | 0.237    |
|    ent_coef_loss   | 0.107    |
|    learning_rate   | 0.0003   |
|    n_updates       | 149599   |
---------------------------------
=== Iterazione IRL 271 ===
Loss reward (iter 271): 3.4506497383117676
=== Iterazione IRL 272 ===
Loss reward (iter 272): 3.278707981109619
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 476      |
|    critic_loss     | 284      |
|    ent_coef        | 0.235    |
|    ent_coef_loss   | -0.188   |
|    learning_rate   | 0.0003   |
|    n_updates       | 149899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 484      |
|    critic_loss     | 236      |
|    ent_coef        | 0.236    |
|    ent_coef_loss   | -0.124   |
|    learning_rate   | 0.0003   |
|    n_updates       | 150299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 486      |
|    critic_loss     | 244      |
|    ent_coef        | 0.236    |
|    ent_coef_loss   | -0.272   |
|    learning_rate   | 0.0003   |
|    n_updates       | 150699   |
---------------------------------
=== Iterazione IRL 273 ===
Loss reward (iter 273): 2.962414264678955
=== Iterazione IRL 274 ===
Loss reward (iter 274): 4.796194553375244
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 489      |
|    critic_loss     | 276      |
|    ent_coef        | 0.24     |
|    ent_coef_loss   | 0.227    |
|    learning_rate   | 0.0003   |
|    n_updates       | 150999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 492      |
|    critic_loss     | 272      |
|    ent_coef        | 0.248    |
|    ent_coef_loss   | 0.156    |
|    learning_rate   | 0.0003   |
|    n_updates       | 151399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 496      |
|    critic_loss     | 232      |
|    ent_coef        | 0.255    |
|    ent_coef_loss   | -0.155   |
|    learning_rate   | 0.0003   |
|    n_updates       | 151799   |
---------------------------------
=== Iterazione IRL 275 ===
Loss reward (iter 275): 3.2784149646759033
=== Iterazione IRL 276 ===
Loss reward (iter 276): 2.335918664932251
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 504      |
|    critic_loss     | 304      |
|    ent_coef        | 0.254    |
|    ent_coef_loss   | 0.452    |
|    learning_rate   | 0.0003   |
|    n_updates       | 152099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 502      |
|    critic_loss     | 290      |
|    ent_coef        | 0.259    |
|    ent_coef_loss   | 0.192    |
|    learning_rate   | 0.0003   |
|    n_updates       | 152499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 518      |
|    critic_loss     | 292      |
|    ent_coef        | 0.269    |
|    ent_coef_loss   | 0.0838   |
|    learning_rate   | 0.0003   |
|    n_updates       | 152899   |
---------------------------------
=== Iterazione IRL 277 ===
Loss reward (iter 277): 2.3708934783935547
=== Iterazione IRL 278 ===
Loss reward (iter 278): 3.6424455642700195
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 512      |
|    critic_loss     | 233      |
|    ent_coef        | 0.268    |
|    ent_coef_loss   | 0.122    |
|    learning_rate   | 0.0003   |
|    n_updates       | 153199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 518      |
|    critic_loss     | 283      |
|    ent_coef        | 0.273    |
|    ent_coef_loss   | 0.362    |
|    learning_rate   | 0.0003   |
|    n_updates       | 153599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 518      |
|    critic_loss     | 281      |
|    ent_coef        | 0.27     |
|    ent_coef_loss   | 0.714    |
|    learning_rate   | 0.0003   |
|    n_updates       | 153999   |
---------------------------------
=== Iterazione IRL 279 ===
Loss reward (iter 279): 3.0642194747924805
=== Iterazione IRL 280 ===
Loss reward (iter 280): 3.654022216796875
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 514      |
|    critic_loss     | 272      |
|    ent_coef        | 0.277    |
|    ent_coef_loss   | -0.0495  |
|    learning_rate   | 0.0003   |
|    n_updates       | 154299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 523      |
|    critic_loss     | 252      |
|    ent_coef        | 0.281    |
|    ent_coef_loss   | -0.114   |
|    learning_rate   | 0.0003   |
|    n_updates       | 154699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 526      |
|    critic_loss     | 246      |
|    ent_coef        | 0.276    |
|    ent_coef_loss   | 0.0597   |
|    learning_rate   | 0.0003   |
|    n_updates       | 155099   |
---------------------------------
=== Iterazione IRL 281 ===
Loss reward (iter 281): 3.1184635162353516
=== Iterazione IRL 282 ===
Loss reward (iter 282): 3.1228880882263184
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 534      |
|    critic_loss     | 285      |
|    ent_coef        | 0.274    |
|    ent_coef_loss   | -0.159   |
|    learning_rate   | 0.0003   |
|    n_updates       | 155399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 534      |
|    critic_loss     | 242      |
|    ent_coef        | 0.273    |
|    ent_coef_loss   | 0.217    |
|    learning_rate   | 0.0003   |
|    n_updates       | 155799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 531      |
|    critic_loss     | 281      |
|    ent_coef        | 0.274    |
|    ent_coef_loss   | -0.0372  |
|    learning_rate   | 0.0003   |
|    n_updates       | 156199   |
---------------------------------
=== Iterazione IRL 283 ===
Loss reward (iter 283): 4.329606533050537
=== Iterazione IRL 284 ===
Loss reward (iter 284): 3.078400135040283
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 542      |
|    critic_loss     | 326      |
|    ent_coef        | 0.282    |
|    ent_coef_loss   | -0.133   |
|    learning_rate   | 0.0003   |
|    n_updates       | 156499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 535      |
|    critic_loss     | 243      |
|    ent_coef        | 0.271    |
|    ent_coef_loss   | -0.479   |
|    learning_rate   | 0.0003   |
|    n_updates       | 156899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 546      |
|    critic_loss     | 308      |
|    ent_coef        | 0.263    |
|    ent_coef_loss   | -0.279   |
|    learning_rate   | 0.0003   |
|    n_updates       | 157299   |
---------------------------------
=== Iterazione IRL 285 ===
Loss reward (iter 285): 2.713573932647705
=== Iterazione IRL 286 ===
Loss reward (iter 286): 2.9163658618927
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 552      |
|    critic_loss     | 283      |
|    ent_coef        | 0.261    |
|    ent_coef_loss   | 0.0335   |
|    learning_rate   | 0.0003   |
|    n_updates       | 157599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 543      |
|    critic_loss     | 306      |
|    ent_coef        | 0.256    |
|    ent_coef_loss   | -0.362   |
|    learning_rate   | 0.0003   |
|    n_updates       | 157999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 554      |
|    critic_loss     | 329      |
|    ent_coef        | 0.26     |
|    ent_coef_loss   | -0.271   |
|    learning_rate   | 0.0003   |
|    n_updates       | 158399   |
---------------------------------
=== Iterazione IRL 287 ===
Loss reward (iter 287): 2.063359498977661
=== Iterazione IRL 288 ===
Loss reward (iter 288): 1.4098455905914307
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 558      |
|    critic_loss     | 336      |
|    ent_coef        | 0.26     |
|    ent_coef_loss   | 0.0953   |
|    learning_rate   | 0.0003   |
|    n_updates       | 158699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 554      |
|    critic_loss     | 265      |
|    ent_coef        | 0.263    |
|    ent_coef_loss   | -0.191   |
|    learning_rate   | 0.0003   |
|    n_updates       | 159099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 561      |
|    critic_loss     | 229      |
|    ent_coef        | 0.273    |
|    ent_coef_loss   | -0.0128  |
|    learning_rate   | 0.0003   |
|    n_updates       | 159499   |
---------------------------------
=== Iterazione IRL 289 ===
Loss reward (iter 289): 1.8563655614852905
=== Iterazione IRL 290 ===
Loss reward (iter 290): 3.668560028076172
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 568      |
|    critic_loss     | 345      |
|    ent_coef        | 0.269    |
|    ent_coef_loss   | -0.113   |
|    learning_rate   | 0.0003   |
|    n_updates       | 159799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 574      |
|    critic_loss     | 333      |
|    ent_coef        | 0.266    |
|    ent_coef_loss   | -0.309   |
|    learning_rate   | 0.0003   |
|    n_updates       | 160199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 579      |
|    critic_loss     | 288      |
|    ent_coef        | 0.268    |
|    ent_coef_loss   | 0.424    |
|    learning_rate   | 0.0003   |
|    n_updates       | 160599   |
---------------------------------
=== Iterazione IRL 291 ===
Loss reward (iter 291): 3.289977550506592
=== Iterazione IRL 292 ===
Loss reward (iter 292): 1.9485480785369873
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 587      |
|    critic_loss     | 374      |
|    ent_coef        | 0.273    |
|    ent_coef_loss   | 0.167    |
|    learning_rate   | 0.0003   |
|    n_updates       | 160899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 584      |
|    critic_loss     | 300      |
|    ent_coef        | 0.273    |
|    ent_coef_loss   | 0.0101   |
|    learning_rate   | 0.0003   |
|    n_updates       | 161299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 586      |
|    critic_loss     | 315      |
|    ent_coef        | 0.277    |
|    ent_coef_loss   | 0.25     |
|    learning_rate   | 0.0003   |
|    n_updates       | 161699   |
---------------------------------
=== Iterazione IRL 293 ===
Loss reward (iter 293): 4.185562610626221
=== Iterazione IRL 294 ===
Loss reward (iter 294): 4.952876091003418
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 596      |
|    critic_loss     | 360      |
|    ent_coef        | 0.29     |
|    ent_coef_loss   | 0.383    |
|    learning_rate   | 0.0003   |
|    n_updates       | 161999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 593      |
|    critic_loss     | 482      |
|    ent_coef        | 0.289    |
|    ent_coef_loss   | 0.0188   |
|    learning_rate   | 0.0003   |
|    n_updates       | 162399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 600      |
|    critic_loss     | 369      |
|    ent_coef        | 0.296    |
|    ent_coef_loss   | -0.144   |
|    learning_rate   | 0.0003   |
|    n_updates       | 162799   |
---------------------------------
=== Iterazione IRL 295 ===
Loss reward (iter 295): 3.8559350967407227
=== Iterazione IRL 296 ===
Loss reward (iter 296): 4.744750499725342
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 597      |
|    critic_loss     | 315      |
|    ent_coef        | 0.294    |
|    ent_coef_loss   | -0.32    |
|    learning_rate   | 0.0003   |
|    n_updates       | 163099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 595      |
|    critic_loss     | 304      |
|    ent_coef        | 0.302    |
|    ent_coef_loss   | 0.199    |
|    learning_rate   | 0.0003   |
|    n_updates       | 163499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 603      |
|    critic_loss     | 393      |
|    ent_coef        | 0.3      |
|    ent_coef_loss   | -0.129   |
|    learning_rate   | 0.0003   |
|    n_updates       | 163899   |
---------------------------------
=== Iterazione IRL 297 ===
Loss reward (iter 297): 4.248449325561523
=== Iterazione IRL 298 ===
Loss reward (iter 298): 3.5252997875213623
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 620      |
|    critic_loss     | 362      |
|    ent_coef        | 0.292    |
|    ent_coef_loss   | 0.341    |
|    learning_rate   | 0.0003   |
|    n_updates       | 164199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 611      |
|    critic_loss     | 312      |
|    ent_coef        | 0.287    |
|    ent_coef_loss   | -0.15    |
|    learning_rate   | 0.0003   |
|    n_updates       | 164599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 615      |
|    critic_loss     | 369      |
|    ent_coef        | 0.286    |
|    ent_coef_loss   | 0.349    |
|    learning_rate   | 0.0003   |
|    n_updates       | 164999   |
---------------------------------
=== Iterazione IRL 299 ===
Loss reward (iter 299): 3.4742238521575928
=== Iterazione IRL 300 ===
Loss reward (iter 300): 2.1558849811553955
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 617      |
|    critic_loss     | 357      |
|    ent_coef        | 0.28     |
|    ent_coef_loss   | 0.406    |
|    learning_rate   | 0.0003   |
|    n_updates       | 165299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 610      |
|    critic_loss     | 592      |
|    ent_coef        | 0.291    |
|    ent_coef_loss   | -0.148   |
|    learning_rate   | 0.0003   |
|    n_updates       | 165699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 623      |
|    critic_loss     | 307      |
|    ent_coef        | 0.289    |
|    ent_coef_loss   | 0.209    |
|    learning_rate   | 0.0003   |
|    n_updates       | 166099   |
---------------------------------
=== Iterazione IRL 301 ===
Loss reward (iter 301): 3.7499899864196777
=== Iterazione IRL 302 ===
Loss reward (iter 302): 3.5543735027313232
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 631      |
|    critic_loss     | 273      |
|    ent_coef        | 0.295    |
|    ent_coef_loss   | -0.151   |
|    learning_rate   | 0.0003   |
|    n_updates       | 166399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 630      |
|    critic_loss     | 319      |
|    ent_coef        | 0.302    |
|    ent_coef_loss   | 0.0457   |
|    learning_rate   | 0.0003   |
|    n_updates       | 166799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 642      |
|    critic_loss     | 376      |
|    ent_coef        | 0.303    |
|    ent_coef_loss   | 0.359    |
|    learning_rate   | 0.0003   |
|    n_updates       | 167199   |
---------------------------------
=== Iterazione IRL 303 ===
Loss reward (iter 303): 3.6427412033081055
=== Iterazione IRL 304 ===
Loss reward (iter 304): 3.3893322944641113
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 635      |
|    critic_loss     | 321      |
|    ent_coef        | 0.309    |
|    ent_coef_loss   | -0.137   |
|    learning_rate   | 0.0003   |
|    n_updates       | 167499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 640      |
|    critic_loss     | 400      |
|    ent_coef        | 0.318    |
|    ent_coef_loss   | -0.252   |
|    learning_rate   | 0.0003   |
|    n_updates       | 167899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 638      |
|    critic_loss     | 342      |
|    ent_coef        | 0.322    |
|    ent_coef_loss   | 0.13     |
|    learning_rate   | 0.0003   |
|    n_updates       | 168299   |
---------------------------------
=== Iterazione IRL 305 ===
Loss reward (iter 305): 1.3708367347717285
=== Iterazione IRL 306 ===
Loss reward (iter 306): 2.9819343090057373
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 648      |
|    critic_loss     | 453      |
|    ent_coef        | 0.326    |
|    ent_coef_loss   | 0.122    |
|    learning_rate   | 0.0003   |
|    n_updates       | 168599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 649      |
|    critic_loss     | 332      |
|    ent_coef        | 0.319    |
|    ent_coef_loss   | -0.0117  |
|    learning_rate   | 0.0003   |
|    n_updates       | 168999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 647      |
|    critic_loss     | 395      |
|    ent_coef        | 0.308    |
|    ent_coef_loss   | -0.401   |
|    learning_rate   | 0.0003   |
|    n_updates       | 169399   |
---------------------------------
=== Iterazione IRL 307 ===
Loss reward (iter 307): 3.7111358642578125
=== Iterazione IRL 308 ===
Loss reward (iter 308): 1.8504977226257324
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 659      |
|    critic_loss     | 468      |
|    ent_coef        | 0.312    |
|    ent_coef_loss   | 0.752    |
|    learning_rate   | 0.0003   |
|    n_updates       | 169699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 651      |
|    critic_loss     | 373      |
|    ent_coef        | 0.32     |
|    ent_coef_loss   | 0.255    |
|    learning_rate   | 0.0003   |
|    n_updates       | 170099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 669      |
|    critic_loss     | 340      |
|    ent_coef        | 0.321    |
|    ent_coef_loss   | 0.119    |
|    learning_rate   | 0.0003   |
|    n_updates       | 170499   |
---------------------------------
=== Iterazione IRL 309 ===
Loss reward (iter 309): 2.0199570655822754
=== Iterazione IRL 310 ===
Loss reward (iter 310): 2.541347026824951
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 658      |
|    critic_loss     | 323      |
|    ent_coef        | 0.33     |
|    ent_coef_loss   | 0.278    |
|    learning_rate   | 0.0003   |
|    n_updates       | 170799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 669      |
|    critic_loss     | 393      |
|    ent_coef        | 0.33     |
|    ent_coef_loss   | 0.121    |
|    learning_rate   | 0.0003   |
|    n_updates       | 171199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 671      |
|    critic_loss     | 368      |
|    ent_coef        | 0.326    |
|    ent_coef_loss   | 0.0747   |
|    learning_rate   | 0.0003   |
|    n_updates       | 171599   |
---------------------------------
=== Iterazione IRL 311 ===
Loss reward (iter 311): 3.130553960800171
=== Iterazione IRL 312 ===
Loss reward (iter 312): 3.23510479927063
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 677      |
|    critic_loss     | 414      |
|    ent_coef        | 0.319    |
|    ent_coef_loss   | -0.17    |
|    learning_rate   | 0.0003   |
|    n_updates       | 171899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 677      |
|    critic_loss     | 385      |
|    ent_coef        | 0.319    |
|    ent_coef_loss   | 0.097    |
|    learning_rate   | 0.0003   |
|    n_updates       | 172299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 678      |
|    critic_loss     | 372      |
|    ent_coef        | 0.322    |
|    ent_coef_loss   | 0.168    |
|    learning_rate   | 0.0003   |
|    n_updates       | 172699   |
---------------------------------
=== Iterazione IRL 313 ===
Loss reward (iter 313): 2.433579921722412
=== Iterazione IRL 314 ===
Loss reward (iter 314): 2.5832631587982178
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 671      |
|    critic_loss     | 297      |
|    ent_coef        | 0.313    |
|    ent_coef_loss   | 0.0642   |
|    learning_rate   | 0.0003   |
|    n_updates       | 172999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 678      |
|    critic_loss     | 391      |
|    ent_coef        | 0.316    |
|    ent_coef_loss   | -0.378   |
|    learning_rate   | 0.0003   |
|    n_updates       | 173399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 686      |
|    critic_loss     | 413      |
|    ent_coef        | 0.322    |
|    ent_coef_loss   | -0.182   |
|    learning_rate   | 0.0003   |
|    n_updates       | 173799   |
---------------------------------
=== Iterazione IRL 315 ===
Loss reward (iter 315): 3.7616353034973145
=== Iterazione IRL 316 ===
Loss reward (iter 316): 5.825434684753418
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 682      |
|    critic_loss     | 346      |
|    ent_coef        | 0.329    |
|    ent_coef_loss   | -0.0443  |
|    learning_rate   | 0.0003   |
|    n_updates       | 174099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 690      |
|    critic_loss     | 480      |
|    ent_coef        | 0.334    |
|    ent_coef_loss   | -0.218   |
|    learning_rate   | 0.0003   |
|    n_updates       | 174499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 699      |
|    critic_loss     | 340      |
|    ent_coef        | 0.331    |
|    ent_coef_loss   | -0.103   |
|    learning_rate   | 0.0003   |
|    n_updates       | 174899   |
---------------------------------
=== Iterazione IRL 317 ===
Loss reward (iter 317): 4.541725158691406
=== Iterazione IRL 318 ===
Loss reward (iter 318): 4.251008033752441
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 698      |
|    critic_loss     | 318      |
|    ent_coef        | 0.336    |
|    ent_coef_loss   | -0.0927  |
|    learning_rate   | 0.0003   |
|    n_updates       | 175199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 701      |
|    critic_loss     | 341      |
|    ent_coef        | 0.344    |
|    ent_coef_loss   | 0.273    |
|    learning_rate   | 0.0003   |
|    n_updates       | 175599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 131      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 705      |
|    critic_loss     | 379      |
|    ent_coef        | 0.349    |
|    ent_coef_loss   | 0.0113   |
|    learning_rate   | 0.0003   |
|    n_updates       | 175999   |
---------------------------------
=== Iterazione IRL 319 ===
Loss reward (iter 319): 4.313991069793701
=== Iterazione IRL 320 ===
Loss reward (iter 320): 4.403324127197266
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 696      |
|    critic_loss     | 291      |
|    ent_coef        | 0.355    |
|    ent_coef_loss   | -0.233   |
|    learning_rate   | 0.0003   |
|    n_updates       | 176299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 708      |
|    critic_loss     | 362      |
|    ent_coef        | 0.346    |
|    ent_coef_loss   | 0.4      |
|    learning_rate   | 0.0003   |
|    n_updates       | 176699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 714      |
|    critic_loss     | 317      |
|    ent_coef        | 0.342    |
|    ent_coef_loss   | 0.215    |
|    learning_rate   | 0.0003   |
|    n_updates       | 177099   |
---------------------------------
=== Iterazione IRL 321 ===
Loss reward (iter 321): 4.7417497634887695
=== Iterazione IRL 322 ===
Loss reward (iter 322): 4.757023811340332
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 707      |
|    critic_loss     | 300      |
|    ent_coef        | 0.347    |
|    ent_coef_loss   | -0.345   |
|    learning_rate   | 0.0003   |
|    n_updates       | 177399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 717      |
|    critic_loss     | 302      |
|    ent_coef        | 0.355    |
|    ent_coef_loss   | 0.0441   |
|    learning_rate   | 0.0003   |
|    n_updates       | 177799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 715      |
|    critic_loss     | 408      |
|    ent_coef        | 0.364    |
|    ent_coef_loss   | -0.152   |
|    learning_rate   | 0.0003   |
|    n_updates       | 178199   |
---------------------------------
=== Iterazione IRL 323 ===
Loss reward (iter 323): 3.9926066398620605
=== Iterazione IRL 324 ===
Loss reward (iter 324): 4.010116100311279
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 712      |
|    critic_loss     | 337      |
|    ent_coef        | 0.358    |
|    ent_coef_loss   | -0.292   |
|    learning_rate   | 0.0003   |
|    n_updates       | 178499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 712      |
|    critic_loss     | 252      |
|    ent_coef        | 0.35     |
|    ent_coef_loss   | -0.465   |
|    learning_rate   | 0.0003   |
|    n_updates       | 178899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 717      |
|    critic_loss     | 340      |
|    ent_coef        | 0.354    |
|    ent_coef_loss   | -0.161   |
|    learning_rate   | 0.0003   |
|    n_updates       | 179299   |
---------------------------------
=== Iterazione IRL 325 ===
Loss reward (iter 325): 4.240498065948486
=== Iterazione IRL 326 ===
Loss reward (iter 326): 5.102254390716553
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 723      |
|    critic_loss     | 305      |
|    ent_coef        | 0.348    |
|    ent_coef_loss   | 0.274    |
|    learning_rate   | 0.0003   |
|    n_updates       | 179599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 728      |
|    critic_loss     | 329      |
|    ent_coef        | 0.347    |
|    ent_coef_loss   | -0.1     |
|    learning_rate   | 0.0003   |
|    n_updates       | 179999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 736      |
|    critic_loss     | 375      |
|    ent_coef        | 0.347    |
|    ent_coef_loss   | -0.0519  |
|    learning_rate   | 0.0003   |
|    n_updates       | 180399   |
---------------------------------
=== Iterazione IRL 327 ===
Loss reward (iter 327): 4.28219747543335
=== Iterazione IRL 328 ===
Loss reward (iter 328): 4.579038619995117
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 727      |
|    critic_loss     | 348      |
|    ent_coef        | 0.338    |
|    ent_coef_loss   | 0.187    |
|    learning_rate   | 0.0003   |
|    n_updates       | 180699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 727      |
|    critic_loss     | 313      |
|    ent_coef        | 0.34     |
|    ent_coef_loss   | 0.0813   |
|    learning_rate   | 0.0003   |
|    n_updates       | 181099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 747      |
|    critic_loss     | 383      |
|    ent_coef        | 0.328    |
|    ent_coef_loss   | -0.0345  |
|    learning_rate   | 0.0003   |
|    n_updates       | 181499   |
---------------------------------
=== Iterazione IRL 329 ===
Loss reward (iter 329): 2.8782167434692383
=== Iterazione IRL 330 ===
Loss reward (iter 330): 4.424567699432373
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 737      |
|    critic_loss     | 390      |
|    ent_coef        | 0.325    |
|    ent_coef_loss   | 0.439    |
|    learning_rate   | 0.0003   |
|    n_updates       | 181799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 744      |
|    critic_loss     | 277      |
|    ent_coef        | 0.326    |
|    ent_coef_loss   | 0.089    |
|    learning_rate   | 0.0003   |
|    n_updates       | 182199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 751      |
|    critic_loss     | 478      |
|    ent_coef        | 0.323    |
|    ent_coef_loss   | -0.169   |
|    learning_rate   | 0.0003   |
|    n_updates       | 182599   |
---------------------------------
=== Iterazione IRL 331 ===
Loss reward (iter 331): 3.7721478939056396
=== Iterazione IRL 332 ===
Loss reward (iter 332): 3.8388278484344482
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 742      |
|    critic_loss     | 346      |
|    ent_coef        | 0.319    |
|    ent_coef_loss   | -0.457   |
|    learning_rate   | 0.0003   |
|    n_updates       | 182899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 754      |
|    critic_loss     | 340      |
|    ent_coef        | 0.312    |
|    ent_coef_loss   | 0.0891   |
|    learning_rate   | 0.0003   |
|    n_updates       | 183299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 746      |
|    critic_loss     | 380      |
|    ent_coef        | 0.313    |
|    ent_coef_loss   | 0.234    |
|    learning_rate   | 0.0003   |
|    n_updates       | 183699   |
---------------------------------
=== Iterazione IRL 333 ===
Loss reward (iter 333): 4.75116491317749
=== Iterazione IRL 334 ===
Loss reward (iter 334): 4.528581619262695
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 755      |
|    critic_loss     | 355      |
|    ent_coef        | 0.321    |
|    ent_coef_loss   | 0.0954   |
|    learning_rate   | 0.0003   |
|    n_updates       | 183999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 758      |
|    critic_loss     | 306      |
|    ent_coef        | 0.329    |
|    ent_coef_loss   | -0.0124  |
|    learning_rate   | 0.0003   |
|    n_updates       | 184399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 751      |
|    critic_loss     | 412      |
|    ent_coef        | 0.326    |
|    ent_coef_loss   | -0.0207  |
|    learning_rate   | 0.0003   |
|    n_updates       | 184799   |
---------------------------------
=== Iterazione IRL 335 ===
Loss reward (iter 335): 4.784881591796875
=== Iterazione IRL 336 ===
Loss reward (iter 336): 4.053341865539551
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 754      |
|    critic_loss     | 461      |
|    ent_coef        | 0.332    |
|    ent_coef_loss   | 0.224    |
|    learning_rate   | 0.0003   |
|    n_updates       | 185099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 761      |
|    critic_loss     | 350      |
|    ent_coef        | 0.329    |
|    ent_coef_loss   | -0.23    |
|    learning_rate   | 0.0003   |
|    n_updates       | 185499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 760      |
|    critic_loss     | 349      |
|    ent_coef        | 0.325    |
|    ent_coef_loss   | -0.207   |
|    learning_rate   | 0.0003   |
|    n_updates       | 185899   |
---------------------------------
=== Iterazione IRL 337 ===
Loss reward (iter 337): 3.621922016143799
=== Iterazione IRL 338 ===
Loss reward (iter 338): 3.7473835945129395
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 766      |
|    critic_loss     | 387      |
|    ent_coef        | 0.33     |
|    ent_coef_loss   | -0.15    |
|    learning_rate   | 0.0003   |
|    n_updates       | 186199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 768      |
|    critic_loss     | 477      |
|    ent_coef        | 0.328    |
|    ent_coef_loss   | 0.136    |
|    learning_rate   | 0.0003   |
|    n_updates       | 186599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 770      |
|    critic_loss     | 484      |
|    ent_coef        | 0.319    |
|    ent_coef_loss   | 0.218    |
|    learning_rate   | 0.0003   |
|    n_updates       | 186999   |
---------------------------------
=== Iterazione IRL 339 ===
Loss reward (iter 339): 3.2161974906921387
=== Iterazione IRL 340 ===
Loss reward (iter 340): 2.9869558811187744
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 762      |
|    critic_loss     | 321      |
|    ent_coef        | 0.323    |
|    ent_coef_loss   | -0.364   |
|    learning_rate   | 0.0003   |
|    n_updates       | 187299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 764      |
|    critic_loss     | 327      |
|    ent_coef        | 0.321    |
|    ent_coef_loss   | -0.113   |
|    learning_rate   | 0.0003   |
|    n_updates       | 187699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 770      |
|    critic_loss     | 388      |
|    ent_coef        | 0.319    |
|    ent_coef_loss   | 0.0773   |
|    learning_rate   | 0.0003   |
|    n_updates       | 188099   |
---------------------------------
=== Iterazione IRL 341 ===
Loss reward (iter 341): 3.2119274139404297
=== Iterazione IRL 342 ===
Loss reward (iter 342): 2.991464614868164
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 777      |
|    critic_loss     | 305      |
|    ent_coef        | 0.317    |
|    ent_coef_loss   | -0.0546  |
|    learning_rate   | 0.0003   |
|    n_updates       | 188399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 774      |
|    critic_loss     | 399      |
|    ent_coef        | 0.308    |
|    ent_coef_loss   | 0.157    |
|    learning_rate   | 0.0003   |
|    n_updates       | 188799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 784      |
|    critic_loss     | 353      |
|    ent_coef        | 0.306    |
|    ent_coef_loss   | 0.0259   |
|    learning_rate   | 0.0003   |
|    n_updates       | 189199   |
---------------------------------
=== Iterazione IRL 343 ===
Loss reward (iter 343): 3.2100443840026855
=== Iterazione IRL 344 ===
Loss reward (iter 344): 3.221752166748047
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 791      |
|    critic_loss     | 439      |
|    ent_coef        | 0.299    |
|    ent_coef_loss   | 0.0411   |
|    learning_rate   | 0.0003   |
|    n_updates       | 189499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 797      |
|    critic_loss     | 454      |
|    ent_coef        | 0.3      |
|    ent_coef_loss   | -0.108   |
|    learning_rate   | 0.0003   |
|    n_updates       | 189899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 795      |
|    critic_loss     | 493      |
|    ent_coef        | 0.296    |
|    ent_coef_loss   | 0.0679   |
|    learning_rate   | 0.0003   |
|    n_updates       | 190299   |
---------------------------------
=== Iterazione IRL 345 ===
Loss reward (iter 345): 2.9724721908569336
=== Iterazione IRL 346 ===
Loss reward (iter 346): 3.448728561401367
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 784      |
|    critic_loss     | 491      |
|    ent_coef        | 0.292    |
|    ent_coef_loss   | 0.054    |
|    learning_rate   | 0.0003   |
|    n_updates       | 190599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 802      |
|    critic_loss     | 371      |
|    ent_coef        | 0.294    |
|    ent_coef_loss   | -0.0804  |
|    learning_rate   | 0.0003   |
|    n_updates       | 190999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 789      |
|    critic_loss     | 560      |
|    ent_coef        | 0.302    |
|    ent_coef_loss   | 0.0763   |
|    learning_rate   | 0.0003   |
|    n_updates       | 191399   |
---------------------------------
=== Iterazione IRL 347 ===
Loss reward (iter 347): 3.7373507022857666
=== Iterazione IRL 348 ===
Loss reward (iter 348): 4.321097373962402
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 809      |
|    critic_loss     | 439      |
|    ent_coef        | 0.304    |
|    ent_coef_loss   | -0.382   |
|    learning_rate   | 0.0003   |
|    n_updates       | 191699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 802      |
|    critic_loss     | 447      |
|    ent_coef        | 0.301    |
|    ent_coef_loss   | -0.0551  |
|    learning_rate   | 0.0003   |
|    n_updates       | 192099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 806      |
|    critic_loss     | 407      |
|    ent_coef        | 0.302    |
|    ent_coef_loss   | -0.195   |
|    learning_rate   | 0.0003   |
|    n_updates       | 192499   |
---------------------------------
=== Iterazione IRL 349 ===
Loss reward (iter 349): 3.5824031829833984
=== Iterazione IRL 350 ===
Loss reward (iter 350): 4.122827529907227
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 817      |
|    critic_loss     | 460      |
|    ent_coef        | 0.304    |
|    ent_coef_loss   | -0.0708  |
|    learning_rate   | 0.0003   |
|    n_updates       | 192799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 809      |
|    critic_loss     | 396      |
|    ent_coef        | 0.306    |
|    ent_coef_loss   | -0.348   |
|    learning_rate   | 0.0003   |
|    n_updates       | 193199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 815      |
|    critic_loss     | 466      |
|    ent_coef        | 0.3      |
|    ent_coef_loss   | -0.271   |
|    learning_rate   | 0.0003   |
|    n_updates       | 193599   |
---------------------------------
=== Iterazione IRL 351 ===
Loss reward (iter 351): 3.4037561416625977
=== Iterazione IRL 352 ===
Loss reward (iter 352): 3.1835737228393555
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 812      |
|    critic_loss     | 487      |
|    ent_coef        | 0.308    |
|    ent_coef_loss   | 0.197    |
|    learning_rate   | 0.0003   |
|    n_updates       | 193899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 829      |
|    critic_loss     | 407      |
|    ent_coef        | 0.305    |
|    ent_coef_loss   | -0.213   |
|    learning_rate   | 0.0003   |
|    n_updates       | 194299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 824      |
|    critic_loss     | 466      |
|    ent_coef        | 0.303    |
|    ent_coef_loss   | -0.237   |
|    learning_rate   | 0.0003   |
|    n_updates       | 194699   |
---------------------------------
=== Iterazione IRL 353 ===
Loss reward (iter 353): 4.264251708984375
=== Iterazione IRL 354 ===
Loss reward (iter 354): 3.675206422805786
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 820      |
|    critic_loss     | 521      |
|    ent_coef        | 0.3      |
|    ent_coef_loss   | 0.0483   |
|    learning_rate   | 0.0003   |
|    n_updates       | 194999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 825      |
|    critic_loss     | 369      |
|    ent_coef        | 0.303    |
|    ent_coef_loss   | -0.088   |
|    learning_rate   | 0.0003   |
|    n_updates       | 195399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 830      |
|    critic_loss     | 468      |
|    ent_coef        | 0.304    |
|    ent_coef_loss   | 0.237    |
|    learning_rate   | 0.0003   |
|    n_updates       | 195799   |
---------------------------------
=== Iterazione IRL 355 ===
Loss reward (iter 355): 3.273616075515747
=== Iterazione IRL 356 ===
Loss reward (iter 356): 2.723255157470703
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 827      |
|    critic_loss     | 423      |
|    ent_coef        | 0.308    |
|    ent_coef_loss   | -0.0846  |
|    learning_rate   | 0.0003   |
|    n_updates       | 196099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 837      |
|    critic_loss     | 472      |
|    ent_coef        | 0.315    |
|    ent_coef_loss   | 0.176    |
|    learning_rate   | 0.0003   |
|    n_updates       | 196499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 837      |
|    critic_loss     | 407      |
|    ent_coef        | 0.321    |
|    ent_coef_loss   | -0.152   |
|    learning_rate   | 0.0003   |
|    n_updates       | 196899   |
---------------------------------
=== Iterazione IRL 357 ===
Loss reward (iter 357): 3.3612890243530273
=== Iterazione IRL 358 ===
Loss reward (iter 358): 4.327187538146973
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 825      |
|    critic_loss     | 379      |
|    ent_coef        | 0.323    |
|    ent_coef_loss   | -0.247   |
|    learning_rate   | 0.0003   |
|    n_updates       | 197199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 850      |
|    critic_loss     | 436      |
|    ent_coef        | 0.328    |
|    ent_coef_loss   | -0.0885  |
|    learning_rate   | 0.0003   |
|    n_updates       | 197599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 854      |
|    critic_loss     | 531      |
|    ent_coef        | 0.324    |
|    ent_coef_loss   | 0.0584   |
|    learning_rate   | 0.0003   |
|    n_updates       | 197999   |
---------------------------------
=== Iterazione IRL 359 ===
Loss reward (iter 359): 3.463778495788574
=== Iterazione IRL 360 ===
Loss reward (iter 360): 3.7051849365234375
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 844      |
|    critic_loss     | 553      |
|    ent_coef        | 0.318    |
|    ent_coef_loss   | 0.22     |
|    learning_rate   | 0.0003   |
|    n_updates       | 198299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 860      |
|    critic_loss     | 568      |
|    ent_coef        | 0.325    |
|    ent_coef_loss   | -0.186   |
|    learning_rate   | 0.0003   |
|    n_updates       | 198699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 860      |
|    critic_loss     | 616      |
|    ent_coef        | 0.335    |
|    ent_coef_loss   | 0.229    |
|    learning_rate   | 0.0003   |
|    n_updates       | 199099   |
---------------------------------
=== Iterazione IRL 361 ===
Loss reward (iter 361): 3.1171951293945312
=== Iterazione IRL 362 ===
Loss reward (iter 362): 2.193714141845703
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 863      |
|    critic_loss     | 608      |
|    ent_coef        | 0.339    |
|    ent_coef_loss   | 0.0246   |
|    learning_rate   | 0.0003   |
|    n_updates       | 199399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 848      |
|    critic_loss     | 428      |
|    ent_coef        | 0.342    |
|    ent_coef_loss   | 0.258    |
|    learning_rate   | 0.0003   |
|    n_updates       | 199799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 866      |
|    critic_loss     | 617      |
|    ent_coef        | 0.342    |
|    ent_coef_loss   | 0.178    |
|    learning_rate   | 0.0003   |
|    n_updates       | 200199   |
---------------------------------
=== Iterazione IRL 363 ===
Loss reward (iter 363): 4.085511207580566
=== Iterazione IRL 364 ===
Loss reward (iter 364): 2.188868522644043
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 860      |
|    critic_loss     | 393      |
|    ent_coef        | 0.341    |
|    ent_coef_loss   | 0.12     |
|    learning_rate   | 0.0003   |
|    n_updates       | 200499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 849      |
|    critic_loss     | 468      |
|    ent_coef        | 0.346    |
|    ent_coef_loss   | -0.0216  |
|    learning_rate   | 0.0003   |
|    n_updates       | 200899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 862      |
|    critic_loss     | 670      |
|    ent_coef        | 0.337    |
|    ent_coef_loss   | -0.074   |
|    learning_rate   | 0.0003   |
|    n_updates       | 201299   |
---------------------------------
=== Iterazione IRL 365 ===
Loss reward (iter 365): 3.027863025665283
=== Iterazione IRL 366 ===
Loss reward (iter 366): 3.662569046020508
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 870      |
|    critic_loss     | 568      |
|    ent_coef        | 0.334    |
|    ent_coef_loss   | 0.0456   |
|    learning_rate   | 0.0003   |
|    n_updates       | 201599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 873      |
|    critic_loss     | 388      |
|    ent_coef        | 0.337    |
|    ent_coef_loss   | 0.204    |
|    learning_rate   | 0.0003   |
|    n_updates       | 201999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 868      |
|    critic_loss     | 578      |
|    ent_coef        | 0.33     |
|    ent_coef_loss   | 0.321    |
|    learning_rate   | 0.0003   |
|    n_updates       | 202399   |
---------------------------------
=== Iterazione IRL 367 ===
Loss reward (iter 367): 2.184863328933716
=== Iterazione IRL 368 ===
Loss reward (iter 368): 1.920507788658142
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 878      |
|    critic_loss     | 489      |
|    ent_coef        | 0.331    |
|    ent_coef_loss   | 0.315    |
|    learning_rate   | 0.0003   |
|    n_updates       | 202699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 878      |
|    critic_loss     | 548      |
|    ent_coef        | 0.33     |
|    ent_coef_loss   | -0.0424  |
|    learning_rate   | 0.0003   |
|    n_updates       | 203099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 889      |
|    critic_loss     | 561      |
|    ent_coef        | 0.324    |
|    ent_coef_loss   | 0.308    |
|    learning_rate   | 0.0003   |
|    n_updates       | 203499   |
---------------------------------
=== Iterazione IRL 369 ===
Loss reward (iter 369): 3.608938694000244
=== Iterazione IRL 370 ===
Loss reward (iter 370): 1.8644758462905884
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 895      |
|    critic_loss     | 526      |
|    ent_coef        | 0.325    |
|    ent_coef_loss   | 0.0827   |
|    learning_rate   | 0.0003   |
|    n_updates       | 203799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 892      |
|    critic_loss     | 617      |
|    ent_coef        | 0.324    |
|    ent_coef_loss   | -0.269   |
|    learning_rate   | 0.0003   |
|    n_updates       | 204199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 902      |
|    critic_loss     | 636      |
|    ent_coef        | 0.318    |
|    ent_coef_loss   | -0.257   |
|    learning_rate   | 0.0003   |
|    n_updates       | 204599   |
---------------------------------
=== Iterazione IRL 371 ===
Loss reward (iter 371): 4.125612258911133
=== Iterazione IRL 372 ===
Loss reward (iter 372): 4.585101127624512
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 888      |
|    critic_loss     | 618      |
|    ent_coef        | 0.325    |
|    ent_coef_loss   | 0.274    |
|    learning_rate   | 0.0003   |
|    n_updates       | 204899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 889      |
|    critic_loss     | 483      |
|    ent_coef        | 0.327    |
|    ent_coef_loss   | -0.435   |
|    learning_rate   | 0.0003   |
|    n_updates       | 205299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 889      |
|    critic_loss     | 529      |
|    ent_coef        | 0.321    |
|    ent_coef_loss   | -0.325   |
|    learning_rate   | 0.0003   |
|    n_updates       | 205699   |
---------------------------------
=== Iterazione IRL 373 ===
Loss reward (iter 373): 3.3048291206359863
=== Iterazione IRL 374 ===
Loss reward (iter 374): 5.2958173751831055
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 890      |
|    critic_loss     | 626      |
|    ent_coef        | 0.32     |
|    ent_coef_loss   | -0.0862  |
|    learning_rate   | 0.0003   |
|    n_updates       | 205999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 908      |
|    critic_loss     | 479      |
|    ent_coef        | 0.324    |
|    ent_coef_loss   | -0.101   |
|    learning_rate   | 0.0003   |
|    n_updates       | 206399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 901      |
|    critic_loss     | 440      |
|    ent_coef        | 0.319    |
|    ent_coef_loss   | -0.0716  |
|    learning_rate   | 0.0003   |
|    n_updates       | 206799   |
---------------------------------
=== Iterazione IRL 375 ===
Loss reward (iter 375): 4.173513412475586
=== Iterazione IRL 376 ===
Loss reward (iter 376): 4.150053024291992
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 909      |
|    critic_loss     | 538      |
|    ent_coef        | 0.309    |
|    ent_coef_loss   | 0.154    |
|    learning_rate   | 0.0003   |
|    n_updates       | 207099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 929      |
|    critic_loss     | 713      |
|    ent_coef        | 0.312    |
|    ent_coef_loss   | 0.307    |
|    learning_rate   | 0.0003   |
|    n_updates       | 207499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 920      |
|    critic_loss     | 427      |
|    ent_coef        | 0.311    |
|    ent_coef_loss   | -0.357   |
|    learning_rate   | 0.0003   |
|    n_updates       | 207899   |
---------------------------------
=== Iterazione IRL 377 ===
Loss reward (iter 377): 4.317897796630859
=== Iterazione IRL 378 ===
Loss reward (iter 378): 4.136959075927734
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 934      |
|    critic_loss     | 594      |
|    ent_coef        | 0.319    |
|    ent_coef_loss   | 0.354    |
|    learning_rate   | 0.0003   |
|    n_updates       | 208199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 923      |
|    critic_loss     | 570      |
|    ent_coef        | 0.318    |
|    ent_coef_loss   | -0.422   |
|    learning_rate   | 0.0003   |
|    n_updates       | 208599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 920      |
|    critic_loss     | 531      |
|    ent_coef        | 0.316    |
|    ent_coef_loss   | 0.0282   |
|    learning_rate   | 0.0003   |
|    n_updates       | 208999   |
---------------------------------
=== Iterazione IRL 379 ===
Loss reward (iter 379): 4.014797687530518
=== Iterazione IRL 380 ===
Loss reward (iter 380): 3.4347543716430664
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 924      |
|    critic_loss     | 571      |
|    ent_coef        | 0.315    |
|    ent_coef_loss   | -0.097   |
|    learning_rate   | 0.0003   |
|    n_updates       | 209299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 921      |
|    critic_loss     | 563      |
|    ent_coef        | 0.316    |
|    ent_coef_loss   | -0.0837  |
|    learning_rate   | 0.0003   |
|    n_updates       | 209699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 944      |
|    critic_loss     | 588      |
|    ent_coef        | 0.318    |
|    ent_coef_loss   | -0.0454  |
|    learning_rate   | 0.0003   |
|    n_updates       | 210099   |
---------------------------------
=== Iterazione IRL 381 ===
Loss reward (iter 381): 4.053299903869629
=== Iterazione IRL 382 ===
Loss reward (iter 382): 5.689664363861084
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 935      |
|    critic_loss     | 617      |
|    ent_coef        | 0.319    |
|    ent_coef_loss   | -0.0487  |
|    learning_rate   | 0.0003   |
|    n_updates       | 210399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 934      |
|    critic_loss     | 637      |
|    ent_coef        | 0.314    |
|    ent_coef_loss   | 0.178    |
|    learning_rate   | 0.0003   |
|    n_updates       | 210799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 131      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 939      |
|    critic_loss     | 596      |
|    ent_coef        | 0.309    |
|    ent_coef_loss   | 0.0154   |
|    learning_rate   | 0.0003   |
|    n_updates       | 211199   |
---------------------------------
=== Iterazione IRL 383 ===
Loss reward (iter 383): 5.8492112159729
=== Iterazione IRL 384 ===
Loss reward (iter 384): 5.954561710357666
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 926      |
|    critic_loss     | 465      |
|    ent_coef        | 0.31     |
|    ent_coef_loss   | 0.319    |
|    learning_rate   | 0.0003   |
|    n_updates       | 211499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 936      |
|    critic_loss     | 581      |
|    ent_coef        | 0.31     |
|    ent_coef_loss   | 0.0348   |
|    learning_rate   | 0.0003   |
|    n_updates       | 211899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 131      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 934      |
|    critic_loss     | 492      |
|    ent_coef        | 0.313    |
|    ent_coef_loss   | -0.0678  |
|    learning_rate   | 0.0003   |
|    n_updates       | 212299   |
---------------------------------
=== Iterazione IRL 385 ===
Loss reward (iter 385): 3.829831600189209
=== Iterazione IRL 386 ===
Loss reward (iter 386): 3.510230302810669
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 969      |
|    critic_loss     | 530      |
|    ent_coef        | 0.306    |
|    ent_coef_loss   | 0.38     |
|    learning_rate   | 0.0003   |
|    n_updates       | 212599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 951      |
|    critic_loss     | 586      |
|    ent_coef        | 0.324    |
|    ent_coef_loss   | -0.0564  |
|    learning_rate   | 0.0003   |
|    n_updates       | 212999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 955      |
|    critic_loss     | 533      |
|    ent_coef        | 0.323    |
|    ent_coef_loss   | -0.0462  |
|    learning_rate   | 0.0003   |
|    n_updates       | 213399   |
---------------------------------
=== Iterazione IRL 387 ===
Loss reward (iter 387): 3.6681995391845703
=== Iterazione IRL 388 ===
Loss reward (iter 388): 3.3535540103912354
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 948      |
|    critic_loss     | 703      |
|    ent_coef        | 0.321    |
|    ent_coef_loss   | -0.202   |
|    learning_rate   | 0.0003   |
|    n_updates       | 213699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 960      |
|    critic_loss     | 681      |
|    ent_coef        | 0.318    |
|    ent_coef_loss   | -0.277   |
|    learning_rate   | 0.0003   |
|    n_updates       | 214099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 962      |
|    critic_loss     | 653      |
|    ent_coef        | 0.322    |
|    ent_coef_loss   | 0.311    |
|    learning_rate   | 0.0003   |
|    n_updates       | 214499   |
---------------------------------
=== Iterazione IRL 389 ===
Loss reward (iter 389): 4.498918056488037
=== Iterazione IRL 390 ===
Loss reward (iter 390): 3.7041056156158447
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 950      |
|    critic_loss     | 592      |
|    ent_coef        | 0.324    |
|    ent_coef_loss   | 0.382    |
|    learning_rate   | 0.0003   |
|    n_updates       | 214799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 954      |
|    critic_loss     | 493      |
|    ent_coef        | 0.32     |
|    ent_coef_loss   | 0.0447   |
|    learning_rate   | 0.0003   |
|    n_updates       | 215199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 969      |
|    critic_loss     | 657      |
|    ent_coef        | 0.325    |
|    ent_coef_loss   | -0.0358  |
|    learning_rate   | 0.0003   |
|    n_updates       | 215599   |
---------------------------------
=== Iterazione IRL 391 ===
Loss reward (iter 391): 2.7111620903015137
=== Iterazione IRL 392 ===
Loss reward (iter 392): 2.2856764793395996
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 953      |
|    critic_loss     | 542      |
|    ent_coef        | 0.331    |
|    ent_coef_loss   | -0.202   |
|    learning_rate   | 0.0003   |
|    n_updates       | 215899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 980      |
|    critic_loss     | 704      |
|    ent_coef        | 0.33     |
|    ent_coef_loss   | 0.23     |
|    learning_rate   | 0.0003   |
|    n_updates       | 216299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 974      |
|    critic_loss     | 676      |
|    ent_coef        | 0.321    |
|    ent_coef_loss   | -0.636   |
|    learning_rate   | 0.0003   |
|    n_updates       | 216699   |
---------------------------------
=== Iterazione IRL 393 ===
Loss reward (iter 393): 3.3139195442199707
=== Iterazione IRL 394 ===
Loss reward (iter 394): 3.7163283824920654
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 965      |
|    critic_loss     | 637      |
|    ent_coef        | 0.318    |
|    ent_coef_loss   | 0.349    |
|    learning_rate   | 0.0003   |
|    n_updates       | 216999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 988      |
|    critic_loss     | 611      |
|    ent_coef        | 0.319    |
|    ent_coef_loss   | 0.363    |
|    learning_rate   | 0.0003   |
|    n_updates       | 217399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 986      |
|    critic_loss     | 604      |
|    ent_coef        | 0.328    |
|    ent_coef_loss   | -0.0927  |
|    learning_rate   | 0.0003   |
|    n_updates       | 217799   |
---------------------------------
=== Iterazione IRL 395 ===
Loss reward (iter 395): 2.9647185802459717
=== Iterazione IRL 396 ===
Loss reward (iter 396): 4.016582489013672
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 978      |
|    critic_loss     | 624      |
|    ent_coef        | 0.323    |
|    ent_coef_loss   | 0.0701   |
|    learning_rate   | 0.0003   |
|    n_updates       | 218099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 990      |
|    critic_loss     | 533      |
|    ent_coef        | 0.315    |
|    ent_coef_loss   | 0.257    |
|    learning_rate   | 0.0003   |
|    n_updates       | 218499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 990      |
|    critic_loss     | 626      |
|    ent_coef        | 0.314    |
|    ent_coef_loss   | 0.142    |
|    learning_rate   | 0.0003   |
|    n_updates       | 218899   |
---------------------------------
=== Iterazione IRL 397 ===
Loss reward (iter 397): 2.60697340965271
=== Iterazione IRL 398 ===
Loss reward (iter 398): 2.671049118041992
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 995      |
|    critic_loss     | 695      |
|    ent_coef        | 0.311    |
|    ent_coef_loss   | -0.1     |
|    learning_rate   | 0.0003   |
|    n_updates       | 219199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 996      |
|    critic_loss     | 498      |
|    ent_coef        | 0.315    |
|    ent_coef_loss   | -0.328   |
|    learning_rate   | 0.0003   |
|    n_updates       | 219599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.01e+03 |
|    critic_loss     | 665      |
|    ent_coef        | 0.312    |
|    ent_coef_loss   | 0.308    |
|    learning_rate   | 0.0003   |
|    n_updates       | 219999   |
---------------------------------
=== Iterazione IRL 399 ===
Loss reward (iter 399): 3.840894937515259
=== Iterazione IRL 400 ===
Loss reward (iter 400): 1.9926409721374512
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1e+03    |
|    critic_loss     | 703      |
|    ent_coef        | 0.316    |
|    ent_coef_loss   | -0.207   |
|    learning_rate   | 0.0003   |
|    n_updates       | 220299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 991      |
|    critic_loss     | 628      |
|    ent_coef        | 0.319    |
|    ent_coef_loss   | -0.304   |
|    learning_rate   | 0.0003   |
|    n_updates       | 220699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1e+03    |
|    critic_loss     | 685      |
|    ent_coef        | 0.318    |
|    ent_coef_loss   | -0.221   |
|    learning_rate   | 0.0003   |
|    n_updates       | 221099   |
---------------------------------
=== Iterazione IRL 401 ===
Loss reward (iter 401): 3.4258737564086914
=== Iterazione IRL 402 ===
Loss reward (iter 402): 1.589343786239624
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.01e+03 |
|    critic_loss     | 662      |
|    ent_coef        | 0.313    |
|    ent_coef_loss   | 0.223    |
|    learning_rate   | 0.0003   |
|    n_updates       | 221399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.01e+03 |
|    critic_loss     | 862      |
|    ent_coef        | 0.317    |
|    ent_coef_loss   | 0.372    |
|    learning_rate   | 0.0003   |
|    n_updates       | 221799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.02e+03 |
|    critic_loss     | 559      |
|    ent_coef        | 0.317    |
|    ent_coef_loss   | -0.385   |
|    learning_rate   | 0.0003   |
|    n_updates       | 222199   |
---------------------------------
=== Iterazione IRL 403 ===
Loss reward (iter 403): 3.5905635356903076
=== Iterazione IRL 404 ===
Loss reward (iter 404): 3.025155782699585
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.01e+03 |
|    critic_loss     | 573      |
|    ent_coef        | 0.313    |
|    ent_coef_loss   | -0.465   |
|    learning_rate   | 0.0003   |
|    n_updates       | 222499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.02e+03 |
|    critic_loss     | 615      |
|    ent_coef        | 0.304    |
|    ent_coef_loss   | -0.415   |
|    learning_rate   | 0.0003   |
|    n_updates       | 222899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.03e+03 |
|    critic_loss     | 810      |
|    ent_coef        | 0.296    |
|    ent_coef_loss   | 0.502    |
|    learning_rate   | 0.0003   |
|    n_updates       | 223299   |
---------------------------------
=== Iterazione IRL 405 ===
Loss reward (iter 405): 2.5358848571777344
=== Iterazione IRL 406 ===
Loss reward (iter 406): 1.4713151454925537
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.03e+03 |
|    critic_loss     | 790      |
|    ent_coef        | 0.3      |
|    ent_coef_loss   | -0.234   |
|    learning_rate   | 0.0003   |
|    n_updates       | 223599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.02e+03 |
|    critic_loss     | 691      |
|    ent_coef        | 0.3      |
|    ent_coef_loss   | 0.294    |
|    learning_rate   | 0.0003   |
|    n_updates       | 223999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.04e+03 |
|    critic_loss     | 821      |
|    ent_coef        | 0.299    |
|    ent_coef_loss   | 0.0643   |
|    learning_rate   | 0.0003   |
|    n_updates       | 224399   |
---------------------------------
=== Iterazione IRL 407 ===
Loss reward (iter 407): 3.1804816722869873
=== Iterazione IRL 408 ===
Loss reward (iter 408): 2.5672268867492676
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.03e+03 |
|    critic_loss     | 830      |
|    ent_coef        | 0.296    |
|    ent_coef_loss   | -0.356   |
|    learning_rate   | 0.0003   |
|    n_updates       | 224699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.04e+03 |
|    critic_loss     | 569      |
|    ent_coef        | 0.294    |
|    ent_coef_loss   | 0.162    |
|    learning_rate   | 0.0003   |
|    n_updates       | 225099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.05e+03 |
|    critic_loss     | 761      |
|    ent_coef        | 0.301    |
|    ent_coef_loss   | 0.39     |
|    learning_rate   | 0.0003   |
|    n_updates       | 225499   |
---------------------------------
=== Iterazione IRL 409 ===
Loss reward (iter 409): 3.039513111114502
=== Iterazione IRL 410 ===
Loss reward (iter 410): 3.854592800140381
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.05e+03 |
|    critic_loss     | 705      |
|    ent_coef        | 0.299    |
|    ent_coef_loss   | -0.0632  |
|    learning_rate   | 0.0003   |
|    n_updates       | 225799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.05e+03 |
|    critic_loss     | 674      |
|    ent_coef        | 0.302    |
|    ent_coef_loss   | -0.124   |
|    learning_rate   | 0.0003   |
|    n_updates       | 226199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.05e+03 |
|    critic_loss     | 1.15e+03 |
|    ent_coef        | 0.3      |
|    ent_coef_loss   | -0.0673  |
|    learning_rate   | 0.0003   |
|    n_updates       | 226599   |
---------------------------------
=== Iterazione IRL 411 ===
Loss reward (iter 411): 3.5287575721740723
=== Iterazione IRL 412 ===
Loss reward (iter 412): 3.3295068740844727
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.06e+03 |
|    critic_loss     | 771      |
|    ent_coef        | 0.299    |
|    ent_coef_loss   | 0.149    |
|    learning_rate   | 0.0003   |
|    n_updates       | 226899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.07e+03 |
|    critic_loss     | 786      |
|    ent_coef        | 0.304    |
|    ent_coef_loss   | 0.149    |
|    learning_rate   | 0.0003   |
|    n_updates       | 227299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.06e+03 |
|    critic_loss     | 644      |
|    ent_coef        | 0.311    |
|    ent_coef_loss   | 0.231    |
|    learning_rate   | 0.0003   |
|    n_updates       | 227699   |
---------------------------------
=== Iterazione IRL 413 ===
Loss reward (iter 413): 3.3011093139648438
=== Iterazione IRL 414 ===
Loss reward (iter 414): 2.5214695930480957
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.05e+03 |
|    critic_loss     | 770      |
|    ent_coef        | 0.312    |
|    ent_coef_loss   | -0.0165  |
|    learning_rate   | 0.0003   |
|    n_updates       | 227999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.07e+03 |
|    critic_loss     | 822      |
|    ent_coef        | 0.308    |
|    ent_coef_loss   | -0.0262  |
|    learning_rate   | 0.0003   |
|    n_updates       | 228399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.05e+03 |
|    critic_loss     | 738      |
|    ent_coef        | 0.308    |
|    ent_coef_loss   | 0.184    |
|    learning_rate   | 0.0003   |
|    n_updates       | 228799   |
---------------------------------
=== Iterazione IRL 415 ===
Loss reward (iter 415): 3.458495616912842
=== Iterazione IRL 416 ===
Loss reward (iter 416): 3.0189647674560547
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.07e+03 |
|    critic_loss     | 677      |
|    ent_coef        | 0.308    |
|    ent_coef_loss   | -0.0613  |
|    learning_rate   | 0.0003   |
|    n_updates       | 229099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.08e+03 |
|    critic_loss     | 653      |
|    ent_coef        | 0.31     |
|    ent_coef_loss   | 0.273    |
|    learning_rate   | 0.0003   |
|    n_updates       | 229499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.07e+03 |
|    critic_loss     | 758      |
|    ent_coef        | 0.322    |
|    ent_coef_loss   | 0.111    |
|    learning_rate   | 0.0003   |
|    n_updates       | 229899   |
---------------------------------
=== Iterazione IRL 417 ===
Loss reward (iter 417): 2.5374553203582764
=== Iterazione IRL 418 ===
Loss reward (iter 418): 3.074948787689209
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.08e+03 |
|    critic_loss     | 676      |
|    ent_coef        | 0.328    |
|    ent_coef_loss   | -0.204   |
|    learning_rate   | 0.0003   |
|    n_updates       | 230199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.07e+03 |
|    critic_loss     | 687      |
|    ent_coef        | 0.338    |
|    ent_coef_loss   | -0.298   |
|    learning_rate   | 0.0003   |
|    n_updates       | 230599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.08e+03 |
|    critic_loss     | 866      |
|    ent_coef        | 0.328    |
|    ent_coef_loss   | -0.0239  |
|    learning_rate   | 0.0003   |
|    n_updates       | 230999   |
---------------------------------
=== Iterazione IRL 419 ===
Loss reward (iter 419): 3.4262588024139404
=== Iterazione IRL 420 ===
Loss reward (iter 420): 2.646181344985962
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.08e+03 |
|    critic_loss     | 888      |
|    ent_coef        | 0.322    |
|    ent_coef_loss   | -0.179   |
|    learning_rate   | 0.0003   |
|    n_updates       | 231299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.1e+03  |
|    critic_loss     | 753      |
|    ent_coef        | 0.314    |
|    ent_coef_loss   | -0.134   |
|    learning_rate   | 0.0003   |
|    n_updates       | 231699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.1e+03  |
|    critic_loss     | 752      |
|    ent_coef        | 0.319    |
|    ent_coef_loss   | 0.311    |
|    learning_rate   | 0.0003   |
|    n_updates       | 232099   |
---------------------------------
=== Iterazione IRL 421 ===
Loss reward (iter 421): 3.1676340103149414
=== Iterazione IRL 422 ===
Loss reward (iter 422): 3.092700719833374
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.09e+03 |
|    critic_loss     | 629      |
|    ent_coef        | 0.327    |
|    ent_coef_loss   | -0.151   |
|    learning_rate   | 0.0003   |
|    n_updates       | 232399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.09e+03 |
|    critic_loss     | 721      |
|    ent_coef        | 0.332    |
|    ent_coef_loss   | 0.0655   |
|    learning_rate   | 0.0003   |
|    n_updates       | 232799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.11e+03 |
|    critic_loss     | 707      |
|    ent_coef        | 0.336    |
|    ent_coef_loss   | -0.312   |
|    learning_rate   | 0.0003   |
|    n_updates       | 233199   |
---------------------------------
=== Iterazione IRL 423 ===
Loss reward (iter 423): 1.816334843635559
=== Iterazione IRL 424 ===
Loss reward (iter 424): 2.2768802642822266
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.09e+03 |
|    critic_loss     | 738      |
|    ent_coef        | 0.339    |
|    ent_coef_loss   | -0.0852  |
|    learning_rate   | 0.0003   |
|    n_updates       | 233499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.12e+03 |
|    critic_loss     | 775      |
|    ent_coef        | 0.336    |
|    ent_coef_loss   | 0.244    |
|    learning_rate   | 0.0003   |
|    n_updates       | 233899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.11e+03 |
|    critic_loss     | 638      |
|    ent_coef        | 0.335    |
|    ent_coef_loss   | 0.0485   |
|    learning_rate   | 0.0003   |
|    n_updates       | 234299   |
---------------------------------
=== Iterazione IRL 425 ===
Loss reward (iter 425): 6.166884899139404
=== Iterazione IRL 426 ===
Loss reward (iter 426): 5.551403999328613
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.11e+03 |
|    critic_loss     | 890      |
|    ent_coef        | 0.344    |
|    ent_coef_loss   | -0.0181  |
|    learning_rate   | 0.0003   |
|    n_updates       | 234599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.1e+03  |
|    critic_loss     | 916      |
|    ent_coef        | 0.339    |
|    ent_coef_loss   | 0.00923  |
|    learning_rate   | 0.0003   |
|    n_updates       | 234999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.1e+03  |
|    critic_loss     | 712      |
|    ent_coef        | 0.331    |
|    ent_coef_loss   | 0.107    |
|    learning_rate   | 0.0003   |
|    n_updates       | 235399   |
---------------------------------
=== Iterazione IRL 427 ===
Loss reward (iter 427): 5.105203151702881
=== Iterazione IRL 428 ===
Loss reward (iter 428): 4.392977714538574
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.12e+03 |
|    critic_loss     | 665      |
|    ent_coef        | 0.33     |
|    ent_coef_loss   | -0.23    |
|    learning_rate   | 0.0003   |
|    n_updates       | 235699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.12e+03 |
|    critic_loss     | 696      |
|    ent_coef        | 0.325    |
|    ent_coef_loss   | 0.109    |
|    learning_rate   | 0.0003   |
|    n_updates       | 236099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.13e+03 |
|    critic_loss     | 1.06e+03 |
|    ent_coef        | 0.334    |
|    ent_coef_loss   | 0.215    |
|    learning_rate   | 0.0003   |
|    n_updates       | 236499   |
---------------------------------
=== Iterazione IRL 429 ===
Loss reward (iter 429): 3.6210036277770996
=== Iterazione IRL 430 ===
Loss reward (iter 430): 3.2325780391693115
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.13e+03 |
|    critic_loss     | 724      |
|    ent_coef        | 0.329    |
|    ent_coef_loss   | -0.0969  |
|    learning_rate   | 0.0003   |
|    n_updates       | 236799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.13e+03 |
|    critic_loss     | 763      |
|    ent_coef        | 0.33     |
|    ent_coef_loss   | 0.282    |
|    learning_rate   | 0.0003   |
|    n_updates       | 237199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.13e+03 |
|    critic_loss     | 683      |
|    ent_coef        | 0.335    |
|    ent_coef_loss   | -0.115   |
|    learning_rate   | 0.0003   |
|    n_updates       | 237599   |
---------------------------------
=== Iterazione IRL 431 ===
Loss reward (iter 431): 3.5590083599090576
=== Iterazione IRL 432 ===
Loss reward (iter 432): 1.2996909618377686
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.14e+03 |
|    critic_loss     | 731      |
|    ent_coef        | 0.343    |
|    ent_coef_loss   | 0.345    |
|    learning_rate   | 0.0003   |
|    n_updates       | 237899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.14e+03 |
|    critic_loss     | 710      |
|    ent_coef        | 0.341    |
|    ent_coef_loss   | -0.041   |
|    learning_rate   | 0.0003   |
|    n_updates       | 238299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.13e+03 |
|    critic_loss     | 834      |
|    ent_coef        | 0.34     |
|    ent_coef_loss   | 0.403    |
|    learning_rate   | 0.0003   |
|    n_updates       | 238699   |
---------------------------------
=== Iterazione IRL 433 ===
Loss reward (iter 433): 2.4060256481170654
=== Iterazione IRL 434 ===
Loss reward (iter 434): 1.4294936656951904
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.12e+03 |
|    critic_loss     | 769      |
|    ent_coef        | 0.337    |
|    ent_coef_loss   | -0.194   |
|    learning_rate   | 0.0003   |
|    n_updates       | 238999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.15e+03 |
|    critic_loss     | 916      |
|    ent_coef        | 0.328    |
|    ent_coef_loss   | -0.023   |
|    learning_rate   | 0.0003   |
|    n_updates       | 239399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.13e+03 |
|    critic_loss     | 825      |
|    ent_coef        | 0.325    |
|    ent_coef_loss   | -0.277   |
|    learning_rate   | 0.0003   |
|    n_updates       | 239799   |
---------------------------------
=== Iterazione IRL 435 ===
Loss reward (iter 435): 4.556660175323486
=== Iterazione IRL 436 ===
Loss reward (iter 436): 4.4681525230407715
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.15e+03 |
|    critic_loss     | 793      |
|    ent_coef        | 0.327    |
|    ent_coef_loss   | -0.134   |
|    learning_rate   | 0.0003   |
|    n_updates       | 240099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.12e+03 |
|    critic_loss     | 785      |
|    ent_coef        | 0.325    |
|    ent_coef_loss   | -0.194   |
|    learning_rate   | 0.0003   |
|    n_updates       | 240499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.15e+03 |
|    critic_loss     | 737      |
|    ent_coef        | 0.327    |
|    ent_coef_loss   | -0.233   |
|    learning_rate   | 0.0003   |
|    n_updates       | 240899   |
---------------------------------
=== Iterazione IRL 437 ===
Loss reward (iter 437): 3.386148691177368
=== Iterazione IRL 438 ===
Loss reward (iter 438): 2.291252851486206
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.17e+03 |
|    critic_loss     | 956      |
|    ent_coef        | 0.328    |
|    ent_coef_loss   | 0.00917  |
|    learning_rate   | 0.0003   |
|    n_updates       | 241199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.15e+03 |
|    critic_loss     | 947      |
|    ent_coef        | 0.333    |
|    ent_coef_loss   | -0.368   |
|    learning_rate   | 0.0003   |
|    n_updates       | 241599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.16e+03 |
|    critic_loss     | 847      |
|    ent_coef        | 0.319    |
|    ent_coef_loss   | -0.262   |
|    learning_rate   | 0.0003   |
|    n_updates       | 241999   |
---------------------------------
=== Iterazione IRL 439 ===
Loss reward (iter 439): 3.3413829803466797
=== Iterazione IRL 440 ===
Loss reward (iter 440): 4.2865495681762695
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.16e+03 |
|    critic_loss     | 847      |
|    ent_coef        | 0.314    |
|    ent_coef_loss   | -0.00409 |
|    learning_rate   | 0.0003   |
|    n_updates       | 242299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.14e+03 |
|    critic_loss     | 680      |
|    ent_coef        | 0.319    |
|    ent_coef_loss   | -0.467   |
|    learning_rate   | 0.0003   |
|    n_updates       | 242699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.15e+03 |
|    critic_loss     | 833      |
|    ent_coef        | 0.316    |
|    ent_coef_loss   | -0.129   |
|    learning_rate   | 0.0003   |
|    n_updates       | 243099   |
---------------------------------
=== Iterazione IRL 441 ===
Loss reward (iter 441): 3.866364002227783
=== Iterazione IRL 442 ===
Loss reward (iter 442): 2.9779930114746094
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.16e+03 |
|    critic_loss     | 762      |
|    ent_coef        | 0.318    |
|    ent_coef_loss   | -0.259   |
|    learning_rate   | 0.0003   |
|    n_updates       | 243399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.17e+03 |
|    critic_loss     | 630      |
|    ent_coef        | 0.318    |
|    ent_coef_loss   | -0.00645 |
|    learning_rate   | 0.0003   |
|    n_updates       | 243799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.17e+03 |
|    critic_loss     | 808      |
|    ent_coef        | 0.319    |
|    ent_coef_loss   | -0.2     |
|    learning_rate   | 0.0003   |
|    n_updates       | 244199   |
---------------------------------
=== Iterazione IRL 443 ===
Loss reward (iter 443): 3.151312828063965
=== Iterazione IRL 444 ===
Loss reward (iter 444): 3.6264610290527344
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.18e+03 |
|    critic_loss     | 946      |
|    ent_coef        | 0.321    |
|    ent_coef_loss   | -0.254   |
|    learning_rate   | 0.0003   |
|    n_updates       | 244499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.18e+03 |
|    critic_loss     | 640      |
|    ent_coef        | 0.311    |
|    ent_coef_loss   | -0.3     |
|    learning_rate   | 0.0003   |
|    n_updates       | 244899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.19e+03 |
|    critic_loss     | 1.07e+03 |
|    ent_coef        | 0.307    |
|    ent_coef_loss   | 0.0329   |
|    learning_rate   | 0.0003   |
|    n_updates       | 245299   |
---------------------------------
=== Iterazione IRL 445 ===
Loss reward (iter 445): 2.2092301845550537
=== Iterazione IRL 446 ===
Loss reward (iter 446): 3.8329429626464844
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.18e+03 |
|    critic_loss     | 868      |
|    ent_coef        | 0.309    |
|    ent_coef_loss   | -0.0403  |
|    learning_rate   | 0.0003   |
|    n_updates       | 245599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.18e+03 |
|    critic_loss     | 726      |
|    ent_coef        | 0.318    |
|    ent_coef_loss   | 0.0928   |
|    learning_rate   | 0.0003   |
|    n_updates       | 245999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.2e+03  |
|    critic_loss     | 771      |
|    ent_coef        | 0.325    |
|    ent_coef_loss   | -0.243   |
|    learning_rate   | 0.0003   |
|    n_updates       | 246399   |
---------------------------------
=== Iterazione IRL 447 ===
Loss reward (iter 447): 3.306004524230957
=== Iterazione IRL 448 ===
Loss reward (iter 448): 3.632842540740967
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.18e+03 |
|    critic_loss     | 862      |
|    ent_coef        | 0.315    |
|    ent_coef_loss   | -0.0771  |
|    learning_rate   | 0.0003   |
|    n_updates       | 246699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.19e+03 |
|    critic_loss     | 739      |
|    ent_coef        | 0.314    |
|    ent_coef_loss   | -0.299   |
|    learning_rate   | 0.0003   |
|    n_updates       | 247099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.2e+03  |
|    critic_loss     | 837      |
|    ent_coef        | 0.317    |
|    ent_coef_loss   | 0.342    |
|    learning_rate   | 0.0003   |
|    n_updates       | 247499   |
---------------------------------
=== Iterazione IRL 449 ===
Loss reward (iter 449): 3.3923866748809814
=== Iterazione IRL 450 ===
Loss reward (iter 450): 3.1315689086914062
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.2e+03  |
|    critic_loss     | 863      |
|    ent_coef        | 0.32     |
|    ent_coef_loss   | -0.0821  |
|    learning_rate   | 0.0003   |
|    n_updates       | 247799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.2e+03  |
|    critic_loss     | 690      |
|    ent_coef        | 0.327    |
|    ent_coef_loss   | -0.188   |
|    learning_rate   | 0.0003   |
|    n_updates       | 248199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.2e+03  |
|    critic_loss     | 705      |
|    ent_coef        | 0.33     |
|    ent_coef_loss   | -0.434   |
|    learning_rate   | 0.0003   |
|    n_updates       | 248599   |
---------------------------------
=== Iterazione IRL 451 ===
Loss reward (iter 451): 4.841172695159912
=== Iterazione IRL 452 ===
Loss reward (iter 452): 3.545048236846924
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.2e+03  |
|    critic_loss     | 873      |
|    ent_coef        | 0.33     |
|    ent_coef_loss   | -0.264   |
|    learning_rate   | 0.0003   |
|    n_updates       | 248899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.21e+03 |
|    critic_loss     | 908      |
|    ent_coef        | 0.333    |
|    ent_coef_loss   | -0.0292  |
|    learning_rate   | 0.0003   |
|    n_updates       | 249299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.2e+03  |
|    critic_loss     | 719      |
|    ent_coef        | 0.336    |
|    ent_coef_loss   | -0.104   |
|    learning_rate   | 0.0003   |
|    n_updates       | 249699   |
---------------------------------
=== Iterazione IRL 453 ===
Loss reward (iter 453): 4.5700225830078125
=== Iterazione IRL 454 ===
Loss reward (iter 454): 3.2089970111846924
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.21e+03 |
|    critic_loss     | 868      |
|    ent_coef        | 0.34     |
|    ent_coef_loss   | 0.284    |
|    learning_rate   | 0.0003   |
|    n_updates       | 249999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.2e+03  |
|    critic_loss     | 819      |
|    ent_coef        | 0.34     |
|    ent_coef_loss   | -0.00197 |
|    learning_rate   | 0.0003   |
|    n_updates       | 250399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.2e+03  |
|    critic_loss     | 786      |
|    ent_coef        | 0.344    |
|    ent_coef_loss   | -0.221   |
|    learning_rate   | 0.0003   |
|    n_updates       | 250799   |
---------------------------------
=== Iterazione IRL 455 ===
Loss reward (iter 455): 2.969564437866211
=== Iterazione IRL 456 ===
Loss reward (iter 456): 2.900385856628418
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.22e+03 |
|    critic_loss     | 677      |
|    ent_coef        | 0.341    |
|    ent_coef_loss   | -0.0573  |
|    learning_rate   | 0.0003   |
|    n_updates       | 251099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.23e+03 |
|    critic_loss     | 903      |
|    ent_coef        | 0.344    |
|    ent_coef_loss   | 0.0183   |
|    learning_rate   | 0.0003   |
|    n_updates       | 251499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.23e+03 |
|    critic_loss     | 1.07e+03 |
|    ent_coef        | 0.344    |
|    ent_coef_loss   | 0.0207   |
|    learning_rate   | 0.0003   |
|    n_updates       | 251899   |
---------------------------------
=== Iterazione IRL 457 ===
Loss reward (iter 457): 2.8042995929718018
=== Iterazione IRL 458 ===
Loss reward (iter 458): 2.9639623165130615
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.21e+03 |
|    critic_loss     | 872      |
|    ent_coef        | 0.345    |
|    ent_coef_loss   | -0.165   |
|    learning_rate   | 0.0003   |
|    n_updates       | 252199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.22e+03 |
|    critic_loss     | 701      |
|    ent_coef        | 0.329    |
|    ent_coef_loss   | 0.018    |
|    learning_rate   | 0.0003   |
|    n_updates       | 252599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.23e+03 |
|    critic_loss     | 878      |
|    ent_coef        | 0.336    |
|    ent_coef_loss   | -0.159   |
|    learning_rate   | 0.0003   |
|    n_updates       | 252999   |
---------------------------------
=== Iterazione IRL 459 ===
Loss reward (iter 459): 3.3646492958068848
=== Iterazione IRL 460 ===
Loss reward (iter 460): 3.3126211166381836
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.23e+03 |
|    critic_loss     | 820      |
|    ent_coef        | 0.332    |
|    ent_coef_loss   | -0.202   |
|    learning_rate   | 0.0003   |
|    n_updates       | 253299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.23e+03 |
|    critic_loss     | 738      |
|    ent_coef        | 0.339    |
|    ent_coef_loss   | -0.152   |
|    learning_rate   | 0.0003   |
|    n_updates       | 253699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.23e+03 |
|    critic_loss     | 688      |
|    ent_coef        | 0.34     |
|    ent_coef_loss   | 0.428    |
|    learning_rate   | 0.0003   |
|    n_updates       | 254099   |
---------------------------------
=== Iterazione IRL 461 ===
Loss reward (iter 461): 4.069890975952148
=== Iterazione IRL 462 ===
Loss reward (iter 462): 4.157192230224609
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.23e+03 |
|    critic_loss     | 855      |
|    ent_coef        | 0.349    |
|    ent_coef_loss   | 0.19     |
|    learning_rate   | 0.0003   |
|    n_updates       | 254399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.22e+03 |
|    critic_loss     | 896      |
|    ent_coef        | 0.349    |
|    ent_coef_loss   | -0.181   |
|    learning_rate   | 0.0003   |
|    n_updates       | 254799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.24e+03 |
|    critic_loss     | 642      |
|    ent_coef        | 0.338    |
|    ent_coef_loss   | 0.276    |
|    learning_rate   | 0.0003   |
|    n_updates       | 255199   |
---------------------------------
=== Iterazione IRL 463 ===
Loss reward (iter 463): 2.9098730087280273
=== Iterazione IRL 464 ===
Loss reward (iter 464): 3.392467498779297
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.24e+03 |
|    critic_loss     | 729      |
|    ent_coef        | 0.34     |
|    ent_coef_loss   | -0.296   |
|    learning_rate   | 0.0003   |
|    n_updates       | 255499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.25e+03 |
|    critic_loss     | 776      |
|    ent_coef        | 0.34     |
|    ent_coef_loss   | -0.274   |
|    learning_rate   | 0.0003   |
|    n_updates       | 255899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.25e+03 |
|    critic_loss     | 1.01e+03 |
|    ent_coef        | 0.347    |
|    ent_coef_loss   | 0.236    |
|    learning_rate   | 0.0003   |
|    n_updates       | 256299   |
---------------------------------
=== Iterazione IRL 465 ===
Loss reward (iter 465): 2.7945730686187744
=== Iterazione IRL 466 ===
Loss reward (iter 466): 3.160041332244873
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.22e+03 |
|    critic_loss     | 792      |
|    ent_coef        | 0.352    |
|    ent_coef_loss   | -0.0431  |
|    learning_rate   | 0.0003   |
|    n_updates       | 256599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.24e+03 |
|    critic_loss     | 696      |
|    ent_coef        | 0.346    |
|    ent_coef_loss   | -0.571   |
|    learning_rate   | 0.0003   |
|    n_updates       | 256999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.23e+03 |
|    critic_loss     | 918      |
|    ent_coef        | 0.341    |
|    ent_coef_loss   | -0.374   |
|    learning_rate   | 0.0003   |
|    n_updates       | 257399   |
---------------------------------
=== Iterazione IRL 467 ===
Loss reward (iter 467): 2.9517221450805664
=== Iterazione IRL 468 ===
Loss reward (iter 468): 2.4621450901031494
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.26e+03 |
|    critic_loss     | 832      |
|    ent_coef        | 0.341    |
|    ent_coef_loss   | -0.339   |
|    learning_rate   | 0.0003   |
|    n_updates       | 257699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.27e+03 |
|    critic_loss     | 843      |
|    ent_coef        | 0.338    |
|    ent_coef_loss   | 0.0807   |
|    learning_rate   | 0.0003   |
|    n_updates       | 258099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.26e+03 |
|    critic_loss     | 852      |
|    ent_coef        | 0.334    |
|    ent_coef_loss   | -0.00795 |
|    learning_rate   | 0.0003   |
|    n_updates       | 258499   |
---------------------------------
=== Iterazione IRL 469 ===
Loss reward (iter 469): 2.987351179122925
=== Iterazione IRL 470 ===
Loss reward (iter 470): 3.4162964820861816
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.25e+03 |
|    critic_loss     | 1e+03    |
|    ent_coef        | 0.334    |
|    ent_coef_loss   | 0.00861  |
|    learning_rate   | 0.0003   |
|    n_updates       | 258799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.27e+03 |
|    critic_loss     | 851      |
|    ent_coef        | 0.339    |
|    ent_coef_loss   | -0.068   |
|    learning_rate   | 0.0003   |
|    n_updates       | 259199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.27e+03 |
|    critic_loss     | 710      |
|    ent_coef        | 0.347    |
|    ent_coef_loss   | -0.0796  |
|    learning_rate   | 0.0003   |
|    n_updates       | 259599   |
---------------------------------
=== Iterazione IRL 471 ===
Loss reward (iter 471): 2.7062301635742188
=== Iterazione IRL 472 ===
Loss reward (iter 472): 3.0582547187805176
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.26e+03 |
|    critic_loss     | 955      |
|    ent_coef        | 0.345    |
|    ent_coef_loss   | 0.0839   |
|    learning_rate   | 0.0003   |
|    n_updates       | 259899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.26e+03 |
|    critic_loss     | 1.06e+03 |
|    ent_coef        | 0.342    |
|    ent_coef_loss   | 0.231    |
|    learning_rate   | 0.0003   |
|    n_updates       | 260299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.26e+03 |
|    critic_loss     | 872      |
|    ent_coef        | 0.345    |
|    ent_coef_loss   | -0.0293  |
|    learning_rate   | 0.0003   |
|    n_updates       | 260699   |
---------------------------------
=== Iterazione IRL 473 ===
Loss reward (iter 473): 2.7807648181915283
=== Iterazione IRL 474 ===
Loss reward (iter 474): 1.9962295293807983
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.26e+03 |
|    critic_loss     | 847      |
|    ent_coef        | 0.341    |
|    ent_coef_loss   | -0.16    |
|    learning_rate   | 0.0003   |
|    n_updates       | 260999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.29e+03 |
|    critic_loss     | 915      |
|    ent_coef        | 0.341    |
|    ent_coef_loss   | 0.268    |
|    learning_rate   | 0.0003   |
|    n_updates       | 261399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.26e+03 |
|    critic_loss     | 854      |
|    ent_coef        | 0.342    |
|    ent_coef_loss   | -0.231   |
|    learning_rate   | 0.0003   |
|    n_updates       | 261799   |
---------------------------------
=== Iterazione IRL 475 ===
Loss reward (iter 475): 3.331890106201172
=== Iterazione IRL 476 ===
Loss reward (iter 476): 3.486956834793091
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.29e+03 |
|    critic_loss     | 806      |
|    ent_coef        | 0.339    |
|    ent_coef_loss   | 0.0125   |
|    learning_rate   | 0.0003   |
|    n_updates       | 262099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.29e+03 |
|    critic_loss     | 864      |
|    ent_coef        | 0.341    |
|    ent_coef_loss   | -0.0434  |
|    learning_rate   | 0.0003   |
|    n_updates       | 262499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.27e+03 |
|    critic_loss     | 837      |
|    ent_coef        | 0.341    |
|    ent_coef_loss   | 0.177    |
|    learning_rate   | 0.0003   |
|    n_updates       | 262899   |
---------------------------------
=== Iterazione IRL 477 ===
Loss reward (iter 477): 2.976691961288452
=== Iterazione IRL 478 ===
Loss reward (iter 478): 4.060353755950928
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.28e+03 |
|    critic_loss     | 764      |
|    ent_coef        | 0.338    |
|    ent_coef_loss   | 0.505    |
|    learning_rate   | 0.0003   |
|    n_updates       | 263199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.29e+03 |
|    critic_loss     | 1.02e+03 |
|    ent_coef        | 0.33     |
|    ent_coef_loss   | 0.189    |
|    learning_rate   | 0.0003   |
|    n_updates       | 263599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.27e+03 |
|    critic_loss     | 860      |
|    ent_coef        | 0.331    |
|    ent_coef_loss   | 0.109    |
|    learning_rate   | 0.0003   |
|    n_updates       | 263999   |
---------------------------------
=== Iterazione IRL 479 ===
Loss reward (iter 479): 2.715222120285034
=== Iterazione IRL 480 ===
Loss reward (iter 480): 3.5320773124694824
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.28e+03 |
|    critic_loss     | 786      |
|    ent_coef        | 0.335    |
|    ent_coef_loss   | 0.057    |
|    learning_rate   | 0.0003   |
|    n_updates       | 264299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.29e+03 |
|    critic_loss     | 827      |
|    ent_coef        | 0.341    |
|    ent_coef_loss   | -0.18    |
|    learning_rate   | 0.0003   |
|    n_updates       | 264699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.31e+03 |
|    critic_loss     | 949      |
|    ent_coef        | 0.33     |
|    ent_coef_loss   | 0.142    |
|    learning_rate   | 0.0003   |
|    n_updates       | 265099   |
---------------------------------
=== Iterazione IRL 481 ===
Loss reward (iter 481): 3.7222797870635986
=== Iterazione IRL 482 ===
Loss reward (iter 482): 3.743093490600586
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.31e+03 |
|    critic_loss     | 813      |
|    ent_coef        | 0.331    |
|    ent_coef_loss   | 0.312    |
|    learning_rate   | 0.0003   |
|    n_updates       | 265399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.29e+03 |
|    critic_loss     | 896      |
|    ent_coef        | 0.323    |
|    ent_coef_loss   | -0.274   |
|    learning_rate   | 0.0003   |
|    n_updates       | 265799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.31e+03 |
|    critic_loss     | 921      |
|    ent_coef        | 0.319    |
|    ent_coef_loss   | 0.231    |
|    learning_rate   | 0.0003   |
|    n_updates       | 266199   |
---------------------------------
=== Iterazione IRL 483 ===
Loss reward (iter 483): 4.598824501037598
=== Iterazione IRL 484 ===
Loss reward (iter 484): 6.133861541748047
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.3e+03  |
|    critic_loss     | 758      |
|    ent_coef        | 0.322    |
|    ent_coef_loss   | 0.573    |
|    learning_rate   | 0.0003   |
|    n_updates       | 266499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.31e+03 |
|    critic_loss     | 713      |
|    ent_coef        | 0.328    |
|    ent_coef_loss   | -0.0669  |
|    learning_rate   | 0.0003   |
|    n_updates       | 266899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.32e+03 |
|    critic_loss     | 790      |
|    ent_coef        | 0.33     |
|    ent_coef_loss   | 0.354    |
|    learning_rate   | 0.0003   |
|    n_updates       | 267299   |
---------------------------------
=== Iterazione IRL 485 ===
Loss reward (iter 485): 6.292032241821289
=== Iterazione IRL 486 ===
Loss reward (iter 486): 4.0685858726501465
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.32e+03 |
|    critic_loss     | 858      |
|    ent_coef        | 0.328    |
|    ent_coef_loss   | -0.0788  |
|    learning_rate   | 0.0003   |
|    n_updates       | 267599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.31e+03 |
|    critic_loss     | 810      |
|    ent_coef        | 0.315    |
|    ent_coef_loss   | 0.333    |
|    learning_rate   | 0.0003   |
|    n_updates       | 267999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.3e+03  |
|    critic_loss     | 1.06e+03 |
|    ent_coef        | 0.325    |
|    ent_coef_loss   | 0.117    |
|    learning_rate   | 0.0003   |
|    n_updates       | 268399   |
---------------------------------
=== Iterazione IRL 487 ===
Loss reward (iter 487): 4.359158039093018
=== Iterazione IRL 488 ===
Loss reward (iter 488): 3.5404443740844727
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.3e+03  |
|    critic_loss     | 801      |
|    ent_coef        | 0.333    |
|    ent_coef_loss   | 0.124    |
|    learning_rate   | 0.0003   |
|    n_updates       | 268699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.31e+03 |
|    critic_loss     | 808      |
|    ent_coef        | 0.329    |
|    ent_coef_loss   | -0.299   |
|    learning_rate   | 0.0003   |
|    n_updates       | 269099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.32e+03 |
|    critic_loss     | 977      |
|    ent_coef        | 0.327    |
|    ent_coef_loss   | 0.537    |
|    learning_rate   | 0.0003   |
|    n_updates       | 269499   |
---------------------------------
=== Iterazione IRL 489 ===
Loss reward (iter 489): 3.3334362506866455
=== Iterazione IRL 490 ===
Loss reward (iter 490): 3.199307441711426
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.3e+03  |
|    critic_loss     | 717      |
|    ent_coef        | 0.322    |
|    ent_coef_loss   | -0.348   |
|    learning_rate   | 0.0003   |
|    n_updates       | 269799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.31e+03 |
|    critic_loss     | 805      |
|    ent_coef        | 0.318    |
|    ent_coef_loss   | -0.303   |
|    learning_rate   | 0.0003   |
|    n_updates       | 270199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.31e+03 |
|    critic_loss     | 1.18e+03 |
|    ent_coef        | 0.324    |
|    ent_coef_loss   | -0.177   |
|    learning_rate   | 0.0003   |
|    n_updates       | 270599   |
---------------------------------
=== Iterazione IRL 491 ===
Loss reward (iter 491): 4.347066879272461
=== Iterazione IRL 492 ===
Loss reward (iter 492): 4.5184736251831055
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.31e+03 |
|    critic_loss     | 628      |
|    ent_coef        | 0.326    |
|    ent_coef_loss   | 0.0899   |
|    learning_rate   | 0.0003   |
|    n_updates       | 270899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.32e+03 |
|    critic_loss     | 779      |
|    ent_coef        | 0.338    |
|    ent_coef_loss   | -0.351   |
|    learning_rate   | 0.0003   |
|    n_updates       | 271299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.34e+03 |
|    critic_loss     | 788      |
|    ent_coef        | 0.343    |
|    ent_coef_loss   | -0.118   |
|    learning_rate   | 0.0003   |
|    n_updates       | 271699   |
---------------------------------
=== Iterazione IRL 493 ===
Loss reward (iter 493): 1.991204023361206
=== Iterazione IRL 494 ===
Loss reward (iter 494): 3.398322820663452
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.34e+03 |
|    critic_loss     | 1.13e+03 |
|    ent_coef        | 0.342    |
|    ent_coef_loss   | 0.263    |
|    learning_rate   | 0.0003   |
|    n_updates       | 271999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.35e+03 |
|    critic_loss     | 1.05e+03 |
|    ent_coef        | 0.343    |
|    ent_coef_loss   | 0.159    |
|    learning_rate   | 0.0003   |
|    n_updates       | 272399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.33e+03 |
|    critic_loss     | 961      |
|    ent_coef        | 0.337    |
|    ent_coef_loss   | 0.402    |
|    learning_rate   | 0.0003   |
|    n_updates       | 272799   |
---------------------------------
=== Iterazione IRL 495 ===
Loss reward (iter 495): 5.148014545440674
=== Iterazione IRL 496 ===
Loss reward (iter 496): 3.4952383041381836
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.33e+03 |
|    critic_loss     | 893      |
|    ent_coef        | 0.336    |
|    ent_coef_loss   | -0.0401  |
|    learning_rate   | 0.0003   |
|    n_updates       | 273099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.35e+03 |
|    critic_loss     | 939      |
|    ent_coef        | 0.33     |
|    ent_coef_loss   | 0.31     |
|    learning_rate   | 0.0003   |
|    n_updates       | 273499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.34e+03 |
|    critic_loss     | 723      |
|    ent_coef        | 0.337    |
|    ent_coef_loss   | -0.233   |
|    learning_rate   | 0.0003   |
|    n_updates       | 273899   |
---------------------------------
=== Iterazione IRL 497 ===
Loss reward (iter 497): 4.700461387634277
=== Iterazione IRL 498 ===
Loss reward (iter 498): 3.8894119262695312
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.35e+03 |
|    critic_loss     | 828      |
|    ent_coef        | 0.34     |
|    ent_coef_loss   | -0.0398  |
|    learning_rate   | 0.0003   |
|    n_updates       | 274199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.34e+03 |
|    critic_loss     | 729      |
|    ent_coef        | 0.335    |
|    ent_coef_loss   | -0.142   |
|    learning_rate   | 0.0003   |
|    n_updates       | 274599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.37e+03 |
|    critic_loss     | 1.24e+03 |
|    ent_coef        | 0.339    |
|    ent_coef_loss   | 0.257    |
|    learning_rate   | 0.0003   |
|    n_updates       | 274999   |
---------------------------------
=== Iterazione IRL 499 ===
Loss reward (iter 499): 3.361833095550537
=== Iterazione IRL 500 ===
Loss reward (iter 500): 4.904541015625
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.34e+03 |
|    critic_loss     | 893      |
|    ent_coef        | 0.347    |
|    ent_coef_loss   | -0.0909  |
|    learning_rate   | 0.0003   |
|    n_updates       | 275299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.35e+03 |
|    critic_loss     | 864      |
|    ent_coef        | 0.356    |
|    ent_coef_loss   | 0.247    |
|    learning_rate   | 0.0003   |
|    n_updates       | 275699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.36e+03 |
|    critic_loss     | 791      |
|    ent_coef        | 0.358    |
|    ent_coef_loss   | 0.119    |
|    learning_rate   | 0.0003   |
|    n_updates       | 276099   |
---------------------------------
=== Iterazione IRL 501 ===
Loss reward (iter 501): 5.067770004272461
=== Iterazione IRL 502 ===
Loss reward (iter 502): 4.4250874519348145
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.37e+03 |
|    critic_loss     | 1.11e+03 |
|    ent_coef        | 0.353    |
|    ent_coef_loss   | 0.194    |
|    learning_rate   | 0.0003   |
|    n_updates       | 276399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.35e+03 |
|    critic_loss     | 831      |
|    ent_coef        | 0.34     |
|    ent_coef_loss   | 0.0457   |
|    learning_rate   | 0.0003   |
|    n_updates       | 276799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.36e+03 |
|    critic_loss     | 1.06e+03 |
|    ent_coef        | 0.345    |
|    ent_coef_loss   | 0.101    |
|    learning_rate   | 0.0003   |
|    n_updates       | 277199   |
---------------------------------
=== Iterazione IRL 503 ===
Loss reward (iter 503): 3.6103391647338867
=== Iterazione IRL 504 ===
Loss reward (iter 504): 4.554871559143066
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.35e+03 |
|    critic_loss     | 1.11e+03 |
|    ent_coef        | 0.348    |
|    ent_coef_loss   | -0.182   |
|    learning_rate   | 0.0003   |
|    n_updates       | 277499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.37e+03 |
|    critic_loss     | 798      |
|    ent_coef        | 0.346    |
|    ent_coef_loss   | -0.0927  |
|    learning_rate   | 0.0003   |
|    n_updates       | 277899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.37e+03 |
|    critic_loss     | 899      |
|    ent_coef        | 0.346    |
|    ent_coef_loss   | 0.00501  |
|    learning_rate   | 0.0003   |
|    n_updates       | 278299   |
---------------------------------
=== Iterazione IRL 505 ===
Loss reward (iter 505): 4.952532768249512
=== Iterazione IRL 506 ===
Loss reward (iter 506): 4.705985069274902
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.35e+03 |
|    critic_loss     | 803      |
|    ent_coef        | 0.349    |
|    ent_coef_loss   | 0.332    |
|    learning_rate   | 0.0003   |
|    n_updates       | 278599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.38e+03 |
|    critic_loss     | 881      |
|    ent_coef        | 0.348    |
|    ent_coef_loss   | -0.25    |
|    learning_rate   | 0.0003   |
|    n_updates       | 278999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.37e+03 |
|    critic_loss     | 1.07e+03 |
|    ent_coef        | 0.349    |
|    ent_coef_loss   | -0.0218  |
|    learning_rate   | 0.0003   |
|    n_updates       | 279399   |
---------------------------------
=== Iterazione IRL 507 ===
Loss reward (iter 507): 5.427323818206787
=== Iterazione IRL 508 ===
Loss reward (iter 508): 4.322016716003418
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.36e+03 |
|    critic_loss     | 946      |
|    ent_coef        | 0.345    |
|    ent_coef_loss   | 0.0508   |
|    learning_rate   | 0.0003   |
|    n_updates       | 279699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.38e+03 |
|    critic_loss     | 1.17e+03 |
|    ent_coef        | 0.345    |
|    ent_coef_loss   | 0.0479   |
|    learning_rate   | 0.0003   |
|    n_updates       | 280099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.37e+03 |
|    critic_loss     | 747      |
|    ent_coef        | 0.339    |
|    ent_coef_loss   | -0.185   |
|    learning_rate   | 0.0003   |
|    n_updates       | 280499   |
---------------------------------
=== Iterazione IRL 509 ===
Loss reward (iter 509): 4.5643181800842285
=== Iterazione IRL 510 ===
Loss reward (iter 510): 3.731048107147217
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.37e+03 |
|    critic_loss     | 957      |
|    ent_coef        | 0.351    |
|    ent_coef_loss   | 0.0952   |
|    learning_rate   | 0.0003   |
|    n_updates       | 280799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.38e+03 |
|    critic_loss     | 801      |
|    ent_coef        | 0.36     |
|    ent_coef_loss   | -0.0983  |
|    learning_rate   | 0.0003   |
|    n_updates       | 281199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.36e+03 |
|    critic_loss     | 796      |
|    ent_coef        | 0.358    |
|    ent_coef_loss   | -0.406   |
|    learning_rate   | 0.0003   |
|    n_updates       | 281599   |
---------------------------------
=== Iterazione IRL 511 ===
Loss reward (iter 511): 4.051962852478027
=== Iterazione IRL 512 ===
Loss reward (iter 512): 3.9393205642700195
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.38e+03 |
|    critic_loss     | 1.03e+03 |
|    ent_coef        | 0.356    |
|    ent_coef_loss   | -0.205   |
|    learning_rate   | 0.0003   |
|    n_updates       | 281899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.38e+03 |
|    critic_loss     | 1.11e+03 |
|    ent_coef        | 0.359    |
|    ent_coef_loss   | 0.118    |
|    learning_rate   | 0.0003   |
|    n_updates       | 282299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.37e+03 |
|    critic_loss     | 920      |
|    ent_coef        | 0.358    |
|    ent_coef_loss   | 0.038    |
|    learning_rate   | 0.0003   |
|    n_updates       | 282699   |
---------------------------------
=== Iterazione IRL 513 ===
Loss reward (iter 513): 4.385490417480469
=== Iterazione IRL 514 ===
Loss reward (iter 514): 3.2584943771362305
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.38e+03 |
|    critic_loss     | 1.03e+03 |
|    ent_coef        | 0.36     |
|    ent_coef_loss   | 0.000906 |
|    learning_rate   | 0.0003   |
|    n_updates       | 282999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.39e+03 |
|    critic_loss     | 1.33e+03 |
|    ent_coef        | 0.369    |
|    ent_coef_loss   | 0.141    |
|    learning_rate   | 0.0003   |
|    n_updates       | 283399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 131      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.39e+03 |
|    critic_loss     | 844      |
|    ent_coef        | 0.368    |
|    ent_coef_loss   | 0.488    |
|    learning_rate   | 0.0003   |
|    n_updates       | 283799   |
---------------------------------
=== Iterazione IRL 515 ===
Loss reward (iter 515): 2.358781576156616
=== Iterazione IRL 516 ===
Loss reward (iter 516): 3.345625162124634
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.39e+03 |
|    critic_loss     | 1.01e+03 |
|    ent_coef        | 0.369    |
|    ent_coef_loss   | -0.17    |
|    learning_rate   | 0.0003   |
|    n_updates       | 284099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.41e+03 |
|    critic_loss     | 1.06e+03 |
|    ent_coef        | 0.365    |
|    ent_coef_loss   | 0.285    |
|    learning_rate   | 0.0003   |
|    n_updates       | 284499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 131      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.38e+03 |
|    critic_loss     | 758      |
|    ent_coef        | 0.352    |
|    ent_coef_loss   | -0.692   |
|    learning_rate   | 0.0003   |
|    n_updates       | 284899   |
---------------------------------
=== Iterazione IRL 517 ===
Loss reward (iter 517): 4.596742153167725
=== Iterazione IRL 518 ===
Loss reward (iter 518): 4.663166046142578
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.37e+03 |
|    critic_loss     | 1.15e+03 |
|    ent_coef        | 0.349    |
|    ent_coef_loss   | -0.365   |
|    learning_rate   | 0.0003   |
|    n_updates       | 285199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.38e+03 |
|    critic_loss     | 843      |
|    ent_coef        | 0.35     |
|    ent_coef_loss   | -0.42    |
|    learning_rate   | 0.0003   |
|    n_updates       | 285599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.39e+03 |
|    critic_loss     | 841      |
|    ent_coef        | 0.353    |
|    ent_coef_loss   | -0.507   |
|    learning_rate   | 0.0003   |
|    n_updates       | 285999   |
---------------------------------
=== Iterazione IRL 519 ===
Loss reward (iter 519): 3.480020761489868
=== Iterazione IRL 520 ===
Loss reward (iter 520): 3.094407558441162
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.39e+03 |
|    critic_loss     | 1e+03    |
|    ent_coef        | 0.351    |
|    ent_coef_loss   | 0.107    |
|    learning_rate   | 0.0003   |
|    n_updates       | 286299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.41e+03 |
|    critic_loss     | 765      |
|    ent_coef        | 0.355    |
|    ent_coef_loss   | -0.356   |
|    learning_rate   | 0.0003   |
|    n_updates       | 286699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.42e+03 |
|    critic_loss     | 1.01e+03 |
|    ent_coef        | 0.35     |
|    ent_coef_loss   | -0.129   |
|    learning_rate   | 0.0003   |
|    n_updates       | 287099   |
---------------------------------
=== Iterazione IRL 521 ===
Loss reward (iter 521): 3.6404666900634766
=== Iterazione IRL 522 ===
Loss reward (iter 522): 5.037509441375732
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.41e+03 |
|    critic_loss     | 939      |
|    ent_coef        | 0.347    |
|    ent_coef_loss   | 0.254    |
|    learning_rate   | 0.0003   |
|    n_updates       | 287399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.41e+03 |
|    critic_loss     | 981      |
|    ent_coef        | 0.347    |
|    ent_coef_loss   | -0.293   |
|    learning_rate   | 0.0003   |
|    n_updates       | 287799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.41e+03 |
|    critic_loss     | 833      |
|    ent_coef        | 0.337    |
|    ent_coef_loss   | 0.0657   |
|    learning_rate   | 0.0003   |
|    n_updates       | 288199   |
---------------------------------
=== Iterazione IRL 523 ===
Loss reward (iter 523): 4.104445457458496
=== Iterazione IRL 524 ===
Loss reward (iter 524): 2.9162778854370117
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.41e+03 |
|    critic_loss     | 1.01e+03 |
|    ent_coef        | 0.336    |
|    ent_coef_loss   | 0.305    |
|    learning_rate   | 0.0003   |
|    n_updates       | 288499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.42e+03 |
|    critic_loss     | 993      |
|    ent_coef        | 0.342    |
|    ent_coef_loss   | -0.213   |
|    learning_rate   | 0.0003   |
|    n_updates       | 288899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.41e+03 |
|    critic_loss     | 1.02e+03 |
|    ent_coef        | 0.336    |
|    ent_coef_loss   | 0.207    |
|    learning_rate   | 0.0003   |
|    n_updates       | 289299   |
---------------------------------
=== Iterazione IRL 525 ===
Loss reward (iter 525): 2.9249331951141357
=== Iterazione IRL 526 ===
Loss reward (iter 526): 3.449524402618408
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.43e+03 |
|    critic_loss     | 1.02e+03 |
|    ent_coef        | 0.34     |
|    ent_coef_loss   | -0.717   |
|    learning_rate   | 0.0003   |
|    n_updates       | 289599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.4e+03  |
|    critic_loss     | 803      |
|    ent_coef        | 0.343    |
|    ent_coef_loss   | -0.229   |
|    learning_rate   | 0.0003   |
|    n_updates       | 289999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 131      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.41e+03 |
|    critic_loss     | 1.01e+03 |
|    ent_coef        | 0.339    |
|    ent_coef_loss   | -0.259   |
|    learning_rate   | 0.0003   |
|    n_updates       | 290399   |
---------------------------------
=== Iterazione IRL 527 ===
Loss reward (iter 527): 2.9014055728912354
=== Iterazione IRL 528 ===
Loss reward (iter 528): 3.0306129455566406
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.41e+03 |
|    critic_loss     | 1.05e+03 |
|    ent_coef        | 0.337    |
|    ent_coef_loss   | -0.129   |
|    learning_rate   | 0.0003   |
|    n_updates       | 290699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.43e+03 |
|    critic_loss     | 887      |
|    ent_coef        | 0.334    |
|    ent_coef_loss   | -0.0406  |
|    learning_rate   | 0.0003   |
|    n_updates       | 291099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.44e+03 |
|    critic_loss     | 1.2e+03  |
|    ent_coef        | 0.332    |
|    ent_coef_loss   | -0.0471  |
|    learning_rate   | 0.0003   |
|    n_updates       | 291499   |
---------------------------------
=== Iterazione IRL 529 ===
Loss reward (iter 529): 4.133556365966797
=== Iterazione IRL 530 ===
Loss reward (iter 530): 3.56451678276062
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.43e+03 |
|    critic_loss     | 909      |
|    ent_coef        | 0.341    |
|    ent_coef_loss   | 0.237    |
|    learning_rate   | 0.0003   |
|    n_updates       | 291799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.44e+03 |
|    critic_loss     | 1.05e+03 |
|    ent_coef        | 0.352    |
|    ent_coef_loss   | 0.101    |
|    learning_rate   | 0.0003   |
|    n_updates       | 292199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 131      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.45e+03 |
|    critic_loss     | 991      |
|    ent_coef        | 0.356    |
|    ent_coef_loss   | 0.127    |
|    learning_rate   | 0.0003   |
|    n_updates       | 292599   |
---------------------------------
=== Iterazione IRL 531 ===
Loss reward (iter 531): 3.4579315185546875
=== Iterazione IRL 532 ===
Loss reward (iter 532): 3.484335422515869
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.43e+03 |
|    critic_loss     | 785      |
|    ent_coef        | 0.359    |
|    ent_coef_loss   | -0.18    |
|    learning_rate   | 0.0003   |
|    n_updates       | 292899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.46e+03 |
|    critic_loss     | 1.01e+03 |
|    ent_coef        | 0.35     |
|    ent_coef_loss   | 0.201    |
|    learning_rate   | 0.0003   |
|    n_updates       | 293299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.45e+03 |
|    critic_loss     | 1.13e+03 |
|    ent_coef        | 0.361    |
|    ent_coef_loss   | 0.121    |
|    learning_rate   | 0.0003   |
|    n_updates       | 293699   |
---------------------------------
=== Iterazione IRL 533 ===
Loss reward (iter 533): 3.1715176105499268
=== Iterazione IRL 534 ===
Loss reward (iter 534): 2.9214529991149902
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.45e+03 |
|    critic_loss     | 1.09e+03 |
|    ent_coef        | 0.362    |
|    ent_coef_loss   | -0.0743  |
|    learning_rate   | 0.0003   |
|    n_updates       | 293999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.44e+03 |
|    critic_loss     | 1e+03    |
|    ent_coef        | 0.361    |
|    ent_coef_loss   | -0.0956  |
|    learning_rate   | 0.0003   |
|    n_updates       | 294399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.44e+03 |
|    critic_loss     | 1.07e+03 |
|    ent_coef        | 0.355    |
|    ent_coef_loss   | 0.0711   |
|    learning_rate   | 0.0003   |
|    n_updates       | 294799   |
---------------------------------
=== Iterazione IRL 535 ===
Loss reward (iter 535): 3.0978293418884277
=== Iterazione IRL 536 ===
Loss reward (iter 536): 3.086454153060913
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.47e+03 |
|    critic_loss     | 1.01e+03 |
|    ent_coef        | 0.359    |
|    ent_coef_loss   | 0.202    |
|    learning_rate   | 0.0003   |
|    n_updates       | 295099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.46e+03 |
|    critic_loss     | 990      |
|    ent_coef        | 0.355    |
|    ent_coef_loss   | -0.215   |
|    learning_rate   | 0.0003   |
|    n_updates       | 295499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.45e+03 |
|    critic_loss     | 1.03e+03 |
|    ent_coef        | 0.36     |
|    ent_coef_loss   | 0.104    |
|    learning_rate   | 0.0003   |
|    n_updates       | 295899   |
---------------------------------
=== Iterazione IRL 537 ===
Loss reward (iter 537): 4.8765716552734375
=== Iterazione IRL 538 ===
Loss reward (iter 538): 3.285693407058716
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.45e+03 |
|    critic_loss     | 811      |
|    ent_coef        | 0.364    |
|    ent_coef_loss   | 0.00962  |
|    learning_rate   | 0.0003   |
|    n_updates       | 296199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.45e+03 |
|    critic_loss     | 1.14e+03 |
|    ent_coef        | 0.355    |
|    ent_coef_loss   | 0.202    |
|    learning_rate   | 0.0003   |
|    n_updates       | 296599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.47e+03 |
|    critic_loss     | 901      |
|    ent_coef        | 0.358    |
|    ent_coef_loss   | -0.13    |
|    learning_rate   | 0.0003   |
|    n_updates       | 296999   |
---------------------------------
=== Iterazione IRL 539 ===
Loss reward (iter 539): 3.7608468532562256
=== Iterazione IRL 540 ===
Loss reward (iter 540): 3.811549663543701
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.46e+03 |
|    critic_loss     | 1.17e+03 |
|    ent_coef        | 0.353    |
|    ent_coef_loss   | 0.138    |
|    learning_rate   | 0.0003   |
|    n_updates       | 297299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.46e+03 |
|    critic_loss     | 1.17e+03 |
|    ent_coef        | 0.359    |
|    ent_coef_loss   | -0.135   |
|    learning_rate   | 0.0003   |
|    n_updates       | 297699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.47e+03 |
|    critic_loss     | 1.16e+03 |
|    ent_coef        | 0.357    |
|    ent_coef_loss   | -0.0461  |
|    learning_rate   | 0.0003   |
|    n_updates       | 298099   |
---------------------------------
=== Iterazione IRL 541 ===
Loss reward (iter 541): 4.870035648345947
=== Iterazione IRL 542 ===
Loss reward (iter 542): 4.148161888122559
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.47e+03 |
|    critic_loss     | 986      |
|    ent_coef        | 0.361    |
|    ent_coef_loss   | 0.0967   |
|    learning_rate   | 0.0003   |
|    n_updates       | 298399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.47e+03 |
|    critic_loss     | 1.15e+03 |
|    ent_coef        | 0.36     |
|    ent_coef_loss   | -0.0791  |
|    learning_rate   | 0.0003   |
|    n_updates       | 298799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.49e+03 |
|    critic_loss     | 1.5e+03  |
|    ent_coef        | 0.366    |
|    ent_coef_loss   | -0.41    |
|    learning_rate   | 0.0003   |
|    n_updates       | 299199   |
---------------------------------
=== Iterazione IRL 543 ===
Loss reward (iter 543): 4.241100788116455
=== Iterazione IRL 544 ===
Loss reward (iter 544): 3.599216938018799
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.49e+03 |
|    critic_loss     | 1.1e+03  |
|    ent_coef        | 0.372    |
|    ent_coef_loss   | 0.225    |
|    learning_rate   | 0.0003   |
|    n_updates       | 299499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.49e+03 |
|    critic_loss     | 1.05e+03 |
|    ent_coef        | 0.375    |
|    ent_coef_loss   | 0.453    |
|    learning_rate   | 0.0003   |
|    n_updates       | 299899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.47e+03 |
|    critic_loss     | 1.1e+03  |
|    ent_coef        | 0.385    |
|    ent_coef_loss   | -0.238   |
|    learning_rate   | 0.0003   |
|    n_updates       | 300299   |
---------------------------------
=== Iterazione IRL 545 ===
Loss reward (iter 545): 4.520973205566406
=== Iterazione IRL 546 ===
Loss reward (iter 546): 4.107065200805664
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.47e+03 |
|    critic_loss     | 657      |
|    ent_coef        | 0.371    |
|    ent_coef_loss   | -0.111   |
|    learning_rate   | 0.0003   |
|    n_updates       | 300599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.48e+03 |
|    critic_loss     | 985      |
|    ent_coef        | 0.365    |
|    ent_coef_loss   | -0.0206  |
|    learning_rate   | 0.0003   |
|    n_updates       | 300999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.49e+03 |
|    critic_loss     | 1.19e+03 |
|    ent_coef        | 0.368    |
|    ent_coef_loss   | 0.0874   |
|    learning_rate   | 0.0003   |
|    n_updates       | 301399   |
---------------------------------
=== Iterazione IRL 547 ===
Loss reward (iter 547): 4.715517520904541
=== Iterazione IRL 548 ===
Loss reward (iter 548): 4.655189514160156
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.48e+03 |
|    critic_loss     | 999      |
|    ent_coef        | 0.373    |
|    ent_coef_loss   | -0.0774  |
|    learning_rate   | 0.0003   |
|    n_updates       | 301699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.48e+03 |
|    critic_loss     | 1.19e+03 |
|    ent_coef        | 0.378    |
|    ent_coef_loss   | 0.159    |
|    learning_rate   | 0.0003   |
|    n_updates       | 302099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.51e+03 |
|    critic_loss     | 1.22e+03 |
|    ent_coef        | 0.375    |
|    ent_coef_loss   | 0.221    |
|    learning_rate   | 0.0003   |
|    n_updates       | 302499   |
---------------------------------
=== Iterazione IRL 549 ===
Loss reward (iter 549): 4.499156951904297
=== Iterazione IRL 550 ===
Loss reward (iter 550): 3.9264302253723145
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.47e+03 |
|    critic_loss     | 1.24e+03 |
|    ent_coef        | 0.381    |
|    ent_coef_loss   | -0.385   |
|    learning_rate   | 0.0003   |
|    n_updates       | 302799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.51e+03 |
|    critic_loss     | 1.29e+03 |
|    ent_coef        | 0.391    |
|    ent_coef_loss   | -0.204   |
|    learning_rate   | 0.0003   |
|    n_updates       | 303199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.48e+03 |
|    critic_loss     | 1.18e+03 |
|    ent_coef        | 0.385    |
|    ent_coef_loss   | 0.0626   |
|    learning_rate   | 0.0003   |
|    n_updates       | 303599   |
---------------------------------
=== Iterazione IRL 551 ===
Loss reward (iter 551): 4.554304122924805
=== Iterazione IRL 552 ===
Loss reward (iter 552): 4.3998260498046875
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.49e+03 |
|    critic_loss     | 927      |
|    ent_coef        | 0.382    |
|    ent_coef_loss   | -0.0933  |
|    learning_rate   | 0.0003   |
|    n_updates       | 303899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.51e+03 |
|    critic_loss     | 1.16e+03 |
|    ent_coef        | 0.377    |
|    ent_coef_loss   | 0.119    |
|    learning_rate   | 0.0003   |
|    n_updates       | 304299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.49e+03 |
|    critic_loss     | 1.1e+03  |
|    ent_coef        | 0.386    |
|    ent_coef_loss   | 0.0331   |
|    learning_rate   | 0.0003   |
|    n_updates       | 304699   |
---------------------------------
=== Iterazione IRL 553 ===
Loss reward (iter 553): 4.089940547943115
=== Iterazione IRL 554 ===
Loss reward (iter 554): 3.578240394592285
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.51e+03 |
|    critic_loss     | 1.16e+03 |
|    ent_coef        | 0.376    |
|    ent_coef_loss   | 0.17     |
|    learning_rate   | 0.0003   |
|    n_updates       | 304999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.51e+03 |
|    critic_loss     | 1.43e+03 |
|    ent_coef        | 0.381    |
|    ent_coef_loss   | 0.0185   |
|    learning_rate   | 0.0003   |
|    n_updates       | 305399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.51e+03 |
|    critic_loss     | 1.26e+03 |
|    ent_coef        | 0.384    |
|    ent_coef_loss   | -0.202   |
|    learning_rate   | 0.0003   |
|    n_updates       | 305799   |
---------------------------------
=== Iterazione IRL 555 ===
Loss reward (iter 555): 4.341396808624268
=== Iterazione IRL 556 ===
Loss reward (iter 556): 4.668201446533203
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.52e+03 |
|    critic_loss     | 1.38e+03 |
|    ent_coef        | 0.373    |
|    ent_coef_loss   | 0.259    |
|    learning_rate   | 0.0003   |
|    n_updates       | 306099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.51e+03 |
|    critic_loss     | 1.29e+03 |
|    ent_coef        | 0.374    |
|    ent_coef_loss   | 0.0811   |
|    learning_rate   | 0.0003   |
|    n_updates       | 306499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.52e+03 |
|    critic_loss     | 1.04e+03 |
|    ent_coef        | 0.373    |
|    ent_coef_loss   | -0.195   |
|    learning_rate   | 0.0003   |
|    n_updates       | 306899   |
---------------------------------
=== Iterazione IRL 557 ===
Loss reward (iter 557): 3.896559238433838
=== Iterazione IRL 558 ===
Loss reward (iter 558): 3.813541889190674
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.51e+03 |
|    critic_loss     | 1.29e+03 |
|    ent_coef        | 0.372    |
|    ent_coef_loss   | -0.0483  |
|    learning_rate   | 0.0003   |
|    n_updates       | 307199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.53e+03 |
|    critic_loss     | 1.11e+03 |
|    ent_coef        | 0.367    |
|    ent_coef_loss   | -0.165   |
|    learning_rate   | 0.0003   |
|    n_updates       | 307599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.53e+03 |
|    critic_loss     | 1.18e+03 |
|    ent_coef        | 0.364    |
|    ent_coef_loss   | -0.133   |
|    learning_rate   | 0.0003   |
|    n_updates       | 307999   |
---------------------------------
=== Iterazione IRL 559 ===
Loss reward (iter 559): 4.430942535400391
=== Iterazione IRL 560 ===
Loss reward (iter 560): 3.8093245029449463
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.52e+03 |
|    critic_loss     | 1.33e+03 |
|    ent_coef        | 0.358    |
|    ent_coef_loss   | 0.00929  |
|    learning_rate   | 0.0003   |
|    n_updates       | 308299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.51e+03 |
|    critic_loss     | 1.13e+03 |
|    ent_coef        | 0.365    |
|    ent_coef_loss   | -0.344   |
|    learning_rate   | 0.0003   |
|    n_updates       | 308699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.5e+03  |
|    critic_loss     | 927      |
|    ent_coef        | 0.361    |
|    ent_coef_loss   | 0.0529   |
|    learning_rate   | 0.0003   |
|    n_updates       | 309099   |
---------------------------------
=== Iterazione IRL 561 ===
Loss reward (iter 561): 3.7019596099853516
=== Iterazione IRL 562 ===
Loss reward (iter 562): 4.038763523101807
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.51e+03 |
|    critic_loss     | 1.14e+03 |
|    ent_coef        | 0.362    |
|    ent_coef_loss   | -0.0806  |
|    learning_rate   | 0.0003   |
|    n_updates       | 309399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.52e+03 |
|    critic_loss     | 1.02e+03 |
|    ent_coef        | 0.364    |
|    ent_coef_loss   | -0.237   |
|    learning_rate   | 0.0003   |
|    n_updates       | 309799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.53e+03 |
|    critic_loss     | 1.2e+03  |
|    ent_coef        | 0.35     |
|    ent_coef_loss   | -0.184   |
|    learning_rate   | 0.0003   |
|    n_updates       | 310199   |
---------------------------------
=== Iterazione IRL 563 ===
Loss reward (iter 563): 3.2965214252471924
=== Iterazione IRL 564 ===
Loss reward (iter 564): 2.9898369312286377
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.54e+03 |
|    critic_loss     | 1.66e+03 |
|    ent_coef        | 0.341    |
|    ent_coef_loss   | 0.0586   |
|    learning_rate   | 0.0003   |
|    n_updates       | 310499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.53e+03 |
|    critic_loss     | 1.25e+03 |
|    ent_coef        | 0.35     |
|    ent_coef_loss   | 0.0728   |
|    learning_rate   | 0.0003   |
|    n_updates       | 310899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.53e+03 |
|    critic_loss     | 931      |
|    ent_coef        | 0.363    |
|    ent_coef_loss   | -0.123   |
|    learning_rate   | 0.0003   |
|    n_updates       | 311299   |
---------------------------------
=== Iterazione IRL 565 ===
Loss reward (iter 565): 2.913822650909424
=== Iterazione IRL 566 ===
Loss reward (iter 566): 3.20743465423584
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.53e+03 |
|    critic_loss     | 1.1e+03  |
|    ent_coef        | 0.372    |
|    ent_coef_loss   | 0.0156   |
|    learning_rate   | 0.0003   |
|    n_updates       | 311599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.53e+03 |
|    critic_loss     | 887      |
|    ent_coef        | 0.367    |
|    ent_coef_loss   | -0.235   |
|    learning_rate   | 0.0003   |
|    n_updates       | 311999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.55e+03 |
|    critic_loss     | 1.22e+03 |
|    ent_coef        | 0.356    |
|    ent_coef_loss   | 0.0603   |
|    learning_rate   | 0.0003   |
|    n_updates       | 312399   |
---------------------------------
=== Iterazione IRL 567 ===
Loss reward (iter 567): 3.291487216949463
=== Iterazione IRL 568 ===
Loss reward (iter 568): 3.336555004119873
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.53e+03 |
|    critic_loss     | 1.06e+03 |
|    ent_coef        | 0.359    |
|    ent_coef_loss   | -0.442   |
|    learning_rate   | 0.0003   |
|    n_updates       | 312699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.57e+03 |
|    critic_loss     | 1.35e+03 |
|    ent_coef        | 0.371    |
|    ent_coef_loss   | 0.125    |
|    learning_rate   | 0.0003   |
|    n_updates       | 313099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.53e+03 |
|    critic_loss     | 1.31e+03 |
|    ent_coef        | 0.371    |
|    ent_coef_loss   | 0.303    |
|    learning_rate   | 0.0003   |
|    n_updates       | 313499   |
---------------------------------
=== Iterazione IRL 569 ===
Loss reward (iter 569): 3.8249826431274414
=== Iterazione IRL 570 ===
Loss reward (iter 570): 3.661304235458374
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.57e+03 |
|    critic_loss     | 1.23e+03 |
|    ent_coef        | 0.368    |
|    ent_coef_loss   | 0.146    |
|    learning_rate   | 0.0003   |
|    n_updates       | 313799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.53e+03 |
|    critic_loss     | 1.04e+03 |
|    ent_coef        | 0.372    |
|    ent_coef_loss   | 0.0271   |
|    learning_rate   | 0.0003   |
|    n_updates       | 314199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.55e+03 |
|    critic_loss     | 1.22e+03 |
|    ent_coef        | 0.369    |
|    ent_coef_loss   | -0.247   |
|    learning_rate   | 0.0003   |
|    n_updates       | 314599   |
---------------------------------
=== Iterazione IRL 571 ===
Loss reward (iter 571): 3.137141227722168
=== Iterazione IRL 572 ===
Loss reward (iter 572): 2.74379301071167
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.55e+03 |
|    critic_loss     | 1.04e+03 |
|    ent_coef        | 0.366    |
|    ent_coef_loss   | -0.028   |
|    learning_rate   | 0.0003   |
|    n_updates       | 314899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.55e+03 |
|    critic_loss     | 1.36e+03 |
|    ent_coef        | 0.373    |
|    ent_coef_loss   | -0.241   |
|    learning_rate   | 0.0003   |
|    n_updates       | 315299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.55e+03 |
|    critic_loss     | 1.16e+03 |
|    ent_coef        | 0.381    |
|    ent_coef_loss   | 0.0603   |
|    learning_rate   | 0.0003   |
|    n_updates       | 315699   |
---------------------------------
=== Iterazione IRL 573 ===
Loss reward (iter 573): 4.343854904174805
=== Iterazione IRL 574 ===
Loss reward (iter 574): 4.109099388122559
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.56e+03 |
|    critic_loss     | 1.35e+03 |
|    ent_coef        | 0.37     |
|    ent_coef_loss   | -0.0912  |
|    learning_rate   | 0.0003   |
|    n_updates       | 315999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.54e+03 |
|    critic_loss     | 1.05e+03 |
|    ent_coef        | 0.368    |
|    ent_coef_loss   | -0.358   |
|    learning_rate   | 0.0003   |
|    n_updates       | 316399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.54e+03 |
|    critic_loss     | 1.18e+03 |
|    ent_coef        | 0.363    |
|    ent_coef_loss   | -0.0648  |
|    learning_rate   | 0.0003   |
|    n_updates       | 316799   |
---------------------------------
=== Iterazione IRL 575 ===
Loss reward (iter 575): 2.927579164505005
=== Iterazione IRL 576 ===
Loss reward (iter 576): 3.5354392528533936
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.54e+03 |
|    critic_loss     | 1.17e+03 |
|    ent_coef        | 0.365    |
|    ent_coef_loss   | 0.216    |
|    learning_rate   | 0.0003   |
|    n_updates       | 317099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.57e+03 |
|    critic_loss     | 1.2e+03  |
|    ent_coef        | 0.373    |
|    ent_coef_loss   | -0.109   |
|    learning_rate   | 0.0003   |
|    n_updates       | 317499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.54e+03 |
|    critic_loss     | 955      |
|    ent_coef        | 0.368    |
|    ent_coef_loss   | 0.0181   |
|    learning_rate   | 0.0003   |
|    n_updates       | 317899   |
---------------------------------
=== Iterazione IRL 577 ===
Loss reward (iter 577): 3.662245512008667
=== Iterazione IRL 578 ===
Loss reward (iter 578): 2.7734451293945312
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.58e+03 |
|    critic_loss     | 1.18e+03 |
|    ent_coef        | 0.37     |
|    ent_coef_loss   | 0.173    |
|    learning_rate   | 0.0003   |
|    n_updates       | 318199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.55e+03 |
|    critic_loss     | 1.24e+03 |
|    ent_coef        | 0.369    |
|    ent_coef_loss   | -0.0154  |
|    learning_rate   | 0.0003   |
|    n_updates       | 318599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.56e+03 |
|    critic_loss     | 1.25e+03 |
|    ent_coef        | 0.376    |
|    ent_coef_loss   | -0.136   |
|    learning_rate   | 0.0003   |
|    n_updates       | 318999   |
---------------------------------
=== Iterazione IRL 579 ===
Loss reward (iter 579): 2.4296393394470215
=== Iterazione IRL 580 ===
Loss reward (iter 580): 3.210648775100708
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.57e+03 |
|    critic_loss     | 1.03e+03 |
|    ent_coef        | 0.371    |
|    ent_coef_loss   | 0.422    |
|    learning_rate   | 0.0003   |
|    n_updates       | 319299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.58e+03 |
|    critic_loss     | 1.51e+03 |
|    ent_coef        | 0.366    |
|    ent_coef_loss   | -0.203   |
|    learning_rate   | 0.0003   |
|    n_updates       | 319699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.56e+03 |
|    critic_loss     | 1.15e+03 |
|    ent_coef        | 0.359    |
|    ent_coef_loss   | 0.0601   |
|    learning_rate   | 0.0003   |
|    n_updates       | 320099   |
---------------------------------
=== Iterazione IRL 581 ===
Loss reward (iter 581): 3.4091243743896484
=== Iterazione IRL 582 ===
Loss reward (iter 582): 3.0683486461639404
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.57e+03 |
|    critic_loss     | 1.25e+03 |
|    ent_coef        | 0.364    |
|    ent_coef_loss   | 0.00205  |
|    learning_rate   | 0.0003   |
|    n_updates       | 320399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.58e+03 |
|    critic_loss     | 1.39e+03 |
|    ent_coef        | 0.373    |
|    ent_coef_loss   | -0.022   |
|    learning_rate   | 0.0003   |
|    n_updates       | 320799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.56e+03 |
|    critic_loss     | 1.28e+03 |
|    ent_coef        | 0.381    |
|    ent_coef_loss   | 0.206    |
|    learning_rate   | 0.0003   |
|    n_updates       | 321199   |
---------------------------------
=== Iterazione IRL 583 ===
Loss reward (iter 583): 3.9199378490448
=== Iterazione IRL 584 ===
Loss reward (iter 584): 3.582199811935425
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.56e+03 |
|    critic_loss     | 1.37e+03 |
|    ent_coef        | 0.377    |
|    ent_coef_loss   | 0.253    |
|    learning_rate   | 0.0003   |
|    n_updates       | 321499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.57e+03 |
|    critic_loss     | 1.35e+03 |
|    ent_coef        | 0.366    |
|    ent_coef_loss   | -0.163   |
|    learning_rate   | 0.0003   |
|    n_updates       | 321899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.58e+03 |
|    critic_loss     | 1.44e+03 |
|    ent_coef        | 0.365    |
|    ent_coef_loss   | -0.292   |
|    learning_rate   | 0.0003   |
|    n_updates       | 322299   |
---------------------------------
=== Iterazione IRL 585 ===
Loss reward (iter 585): 2.8215982913970947
=== Iterazione IRL 586 ===
Loss reward (iter 586): 3.467231273651123
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.59e+03 |
|    critic_loss     | 1.29e+03 |
|    ent_coef        | 0.359    |
|    ent_coef_loss   | 0.133    |
|    learning_rate   | 0.0003   |
|    n_updates       | 322599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.59e+03 |
|    critic_loss     | 1.36e+03 |
|    ent_coef        | 0.361    |
|    ent_coef_loss   | 0.269    |
|    learning_rate   | 0.0003   |
|    n_updates       | 322999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.6e+03  |
|    critic_loss     | 1.18e+03 |
|    ent_coef        | 0.372    |
|    ent_coef_loss   | 0.0828   |
|    learning_rate   | 0.0003   |
|    n_updates       | 323399   |
---------------------------------
=== Iterazione IRL 587 ===
Loss reward (iter 587): 3.8002662658691406
=== Iterazione IRL 588 ===
Loss reward (iter 588): 4.1398468017578125
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.61e+03 |
|    critic_loss     | 1.1e+03  |
|    ent_coef        | 0.384    |
|    ent_coef_loss   | 0.0783   |
|    learning_rate   | 0.0003   |
|    n_updates       | 323699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.61e+03 |
|    critic_loss     | 1.65e+03 |
|    ent_coef        | 0.381    |
|    ent_coef_loss   | 0.217    |
|    learning_rate   | 0.0003   |
|    n_updates       | 324099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.61e+03 |
|    critic_loss     | 1.24e+03 |
|    ent_coef        | 0.371    |
|    ent_coef_loss   | 0.146    |
|    learning_rate   | 0.0003   |
|    n_updates       | 324499   |
---------------------------------
=== Iterazione IRL 589 ===
Loss reward (iter 589): 3.1501429080963135
=== Iterazione IRL 590 ===
Loss reward (iter 590): 3.1124250888824463
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.59e+03 |
|    critic_loss     | 1.23e+03 |
|    ent_coef        | 0.375    |
|    ent_coef_loss   | 0.101    |
|    learning_rate   | 0.0003   |
|    n_updates       | 324799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.59e+03 |
|    critic_loss     | 1.23e+03 |
|    ent_coef        | 0.385    |
|    ent_coef_loss   | -0.276   |
|    learning_rate   | 0.0003   |
|    n_updates       | 325199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.59e+03 |
|    critic_loss     | 1.47e+03 |
|    ent_coef        | 0.383    |
|    ent_coef_loss   | 0.113    |
|    learning_rate   | 0.0003   |
|    n_updates       | 325599   |
---------------------------------
=== Iterazione IRL 591 ===
Loss reward (iter 591): 2.893829107284546
=== Iterazione IRL 592 ===
Loss reward (iter 592): 3.0296480655670166
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.6e+03  |
|    critic_loss     | 1.26e+03 |
|    ent_coef        | 0.374    |
|    ent_coef_loss   | 0.0733   |
|    learning_rate   | 0.0003   |
|    n_updates       | 325899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.61e+03 |
|    critic_loss     | 1.54e+03 |
|    ent_coef        | 0.369    |
|    ent_coef_loss   | -0.00676 |
|    learning_rate   | 0.0003   |
|    n_updates       | 326299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.61e+03 |
|    critic_loss     | 1.73e+03 |
|    ent_coef        | 0.369    |
|    ent_coef_loss   | 0.106    |
|    learning_rate   | 0.0003   |
|    n_updates       | 326699   |
---------------------------------
=== Iterazione IRL 593 ===
Loss reward (iter 593): 3.7683629989624023
=== Iterazione IRL 594 ===
Loss reward (iter 594): 3.5173110961914062
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.61e+03 |
|    critic_loss     | 1.47e+03 |
|    ent_coef        | 0.362    |
|    ent_coef_loss   | -0.0635  |
|    learning_rate   | 0.0003   |
|    n_updates       | 326999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.62e+03 |
|    critic_loss     | 1.33e+03 |
|    ent_coef        | 0.358    |
|    ent_coef_loss   | 0.407    |
|    learning_rate   | 0.0003   |
|    n_updates       | 327399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.64e+03 |
|    critic_loss     | 2.12e+03 |
|    ent_coef        | 0.356    |
|    ent_coef_loss   | 0.0571   |
|    learning_rate   | 0.0003   |
|    n_updates       | 327799   |
---------------------------------
=== Iterazione IRL 595 ===
Loss reward (iter 595): 3.649749994277954
=== Iterazione IRL 596 ===
Loss reward (iter 596): 3.419902801513672
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.62e+03 |
|    critic_loss     | 1.64e+03 |
|    ent_coef        | 0.346    |
|    ent_coef_loss   | -0.0435  |
|    learning_rate   | 0.0003   |
|    n_updates       | 328099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.63e+03 |
|    critic_loss     | 1.37e+03 |
|    ent_coef        | 0.371    |
|    ent_coef_loss   | 0.281    |
|    learning_rate   | 0.0003   |
|    n_updates       | 328499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.61e+03 |
|    critic_loss     | 1.53e+03 |
|    ent_coef        | 0.369    |
|    ent_coef_loss   | -0.0992  |
|    learning_rate   | 0.0003   |
|    n_updates       | 328899   |
---------------------------------
=== Iterazione IRL 597 ===
Loss reward (iter 597): 2.9198617935180664
=== Iterazione IRL 598 ===
Loss reward (iter 598): 2.8326616287231445
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.62e+03 |
|    critic_loss     | 1.56e+03 |
|    ent_coef        | 0.374    |
|    ent_coef_loss   | -0.28    |
|    learning_rate   | 0.0003   |
|    n_updates       | 329199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.63e+03 |
|    critic_loss     | 1.45e+03 |
|    ent_coef        | 0.368    |
|    ent_coef_loss   | -0.183   |
|    learning_rate   | 0.0003   |
|    n_updates       | 329599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.61e+03 |
|    critic_loss     | 1.25e+03 |
|    ent_coef        | 0.374    |
|    ent_coef_loss   | 0.467    |
|    learning_rate   | 0.0003   |
|    n_updates       | 329999   |
---------------------------------
=== Iterazione IRL 599 ===
Loss reward (iter 599): 1.2693679332733154
=== Iterazione IRL 600 ===
Loss reward (iter 600): 1.1574702262878418
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.61e+03 |
|    critic_loss     | 1.64e+03 |
|    ent_coef        | 0.367    |
|    ent_coef_loss   | -0.259   |
|    learning_rate   | 0.0003   |
|    n_updates       | 330299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.62e+03 |
|    critic_loss     | 1.46e+03 |
|    ent_coef        | 0.36     |
|    ent_coef_loss   | -0.433   |
|    learning_rate   | 0.0003   |
|    n_updates       | 330699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.62e+03 |
|    critic_loss     | 1.51e+03 |
|    ent_coef        | 0.369    |
|    ent_coef_loss   | 0.125    |
|    learning_rate   | 0.0003   |
|    n_updates       | 331099   |
---------------------------------
=== Iterazione IRL 601 ===
Loss reward (iter 601): 3.3177549839019775
=== Iterazione IRL 602 ===
Loss reward (iter 602): 2.837911367416382
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.63e+03 |
|    critic_loss     | 1.34e+03 |
|    ent_coef        | 0.372    |
|    ent_coef_loss   | 0.343    |
|    learning_rate   | 0.0003   |
|    n_updates       | 331399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.63e+03 |
|    critic_loss     | 1.53e+03 |
|    ent_coef        | 0.371    |
|    ent_coef_loss   | -0.0564  |
|    learning_rate   | 0.0003   |
|    n_updates       | 331799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.63e+03 |
|    critic_loss     | 1.09e+03 |
|    ent_coef        | 0.369    |
|    ent_coef_loss   | -0.297   |
|    learning_rate   | 0.0003   |
|    n_updates       | 332199   |
---------------------------------
=== Iterazione IRL 603 ===
Loss reward (iter 603): 3.1023685932159424
=== Iterazione IRL 604 ===
Loss reward (iter 604): 2.3575472831726074
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.63e+03 |
|    critic_loss     | 1.38e+03 |
|    ent_coef        | 0.37     |
|    ent_coef_loss   | 0.109    |
|    learning_rate   | 0.0003   |
|    n_updates       | 332499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.63e+03 |
|    critic_loss     | 1.35e+03 |
|    ent_coef        | 0.353    |
|    ent_coef_loss   | -0.158   |
|    learning_rate   | 0.0003   |
|    n_updates       | 332899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.64e+03 |
|    critic_loss     | 1.38e+03 |
|    ent_coef        | 0.364    |
|    ent_coef_loss   | 0.44     |
|    learning_rate   | 0.0003   |
|    n_updates       | 333299   |
---------------------------------
=== Iterazione IRL 605 ===
Loss reward (iter 605): 2.5953521728515625
=== Iterazione IRL 606 ===
Loss reward (iter 606): 0.7342389822006226
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.65e+03 |
|    critic_loss     | 1.64e+03 |
|    ent_coef        | 0.369    |
|    ent_coef_loss   | 0.336    |
|    learning_rate   | 0.0003   |
|    n_updates       | 333599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.64e+03 |
|    critic_loss     | 959      |
|    ent_coef        | 0.365    |
|    ent_coef_loss   | -0.258   |
|    learning_rate   | 0.0003   |
|    n_updates       | 333999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.65e+03 |
|    critic_loss     | 1.72e+03 |
|    ent_coef        | 0.354    |
|    ent_coef_loss   | -0.285   |
|    learning_rate   | 0.0003   |
|    n_updates       | 334399   |
---------------------------------
=== Iterazione IRL 607 ===
Loss reward (iter 607): 4.092740058898926
=== Iterazione IRL 608 ===
Loss reward (iter 608): 4.054023742675781
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.64e+03 |
|    critic_loss     | 1.42e+03 |
|    ent_coef        | 0.363    |
|    ent_coef_loss   | 0.245    |
|    learning_rate   | 0.0003   |
|    n_updates       | 334699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.68e+03 |
|    critic_loss     | 1.77e+03 |
|    ent_coef        | 0.363    |
|    ent_coef_loss   | 0.0685   |
|    learning_rate   | 0.0003   |
|    n_updates       | 335099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.65e+03 |
|    critic_loss     | 1.4e+03  |
|    ent_coef        | 0.368    |
|    ent_coef_loss   | 0.365    |
|    learning_rate   | 0.0003   |
|    n_updates       | 335499   |
---------------------------------
=== Iterazione IRL 609 ===
Loss reward (iter 609): 3.2397141456604004
=== Iterazione IRL 610 ===
Loss reward (iter 610): 2.823819637298584
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.67e+03 |
|    critic_loss     | 1.31e+03 |
|    ent_coef        | 0.369    |
|    ent_coef_loss   | -0.198   |
|    learning_rate   | 0.0003   |
|    n_updates       | 335799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.66e+03 |
|    critic_loss     | 1.64e+03 |
|    ent_coef        | 0.36     |
|    ent_coef_loss   | -0.123   |
|    learning_rate   | 0.0003   |
|    n_updates       | 336199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.66e+03 |
|    critic_loss     | 1.82e+03 |
|    ent_coef        | 0.362    |
|    ent_coef_loss   | -0.165   |
|    learning_rate   | 0.0003   |
|    n_updates       | 336599   |
---------------------------------
=== Iterazione IRL 611 ===
Loss reward (iter 611): 4.454763889312744
=== Iterazione IRL 612 ===
Loss reward (iter 612): 4.82021427154541
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.66e+03 |
|    critic_loss     | 1.2e+03  |
|    ent_coef        | 0.358    |
|    ent_coef_loss   | -0.357   |
|    learning_rate   | 0.0003   |
|    n_updates       | 336899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.66e+03 |
|    critic_loss     | 1.57e+03 |
|    ent_coef        | 0.351    |
|    ent_coef_loss   | 0.0839   |
|    learning_rate   | 0.0003   |
|    n_updates       | 337299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.66e+03 |
|    critic_loss     | 1.48e+03 |
|    ent_coef        | 0.335    |
|    ent_coef_loss   | 0.198    |
|    learning_rate   | 0.0003   |
|    n_updates       | 337699   |
---------------------------------
=== Iterazione IRL 613 ===
Loss reward (iter 613): 4.4376983642578125
=== Iterazione IRL 614 ===
Loss reward (iter 614): 3.685828447341919
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.67e+03 |
|    critic_loss     | 1.78e+03 |
|    ent_coef        | 0.339    |
|    ent_coef_loss   | 0.423    |
|    learning_rate   | 0.0003   |
|    n_updates       | 337999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.69e+03 |
|    critic_loss     | 2.23e+03 |
|    ent_coef        | 0.346    |
|    ent_coef_loss   | 0.282    |
|    learning_rate   | 0.0003   |
|    n_updates       | 338399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.65e+03 |
|    critic_loss     | 1.04e+03 |
|    ent_coef        | 0.358    |
|    ent_coef_loss   | -0.0724  |
|    learning_rate   | 0.0003   |
|    n_updates       | 338799   |
---------------------------------
=== Iterazione IRL 615 ===
Loss reward (iter 615): 3.7944176197052
=== Iterazione IRL 616 ===
Loss reward (iter 616): 4.299707412719727
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.67e+03 |
|    critic_loss     | 1.41e+03 |
|    ent_coef        | 0.351    |
|    ent_coef_loss   | -0.234   |
|    learning_rate   | 0.0003   |
|    n_updates       | 339099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.69e+03 |
|    critic_loss     | 1.55e+03 |
|    ent_coef        | 0.344    |
|    ent_coef_loss   | 0.0248   |
|    learning_rate   | 0.0003   |
|    n_updates       | 339499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.69e+03 |
|    critic_loss     | 1.94e+03 |
|    ent_coef        | 0.351    |
|    ent_coef_loss   | 0.161    |
|    learning_rate   | 0.0003   |
|    n_updates       | 339899   |
---------------------------------
=== Iterazione IRL 617 ===
Loss reward (iter 617): 2.437819719314575
=== Iterazione IRL 618 ===
Loss reward (iter 618): 3.201975107192993
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.68e+03 |
|    critic_loss     | 1.86e+03 |
|    ent_coef        | 0.35     |
|    ent_coef_loss   | -0.132   |
|    learning_rate   | 0.0003   |
|    n_updates       | 340199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.68e+03 |
|    critic_loss     | 1.99e+03 |
|    ent_coef        | 0.354    |
|    ent_coef_loss   | 0.173    |
|    learning_rate   | 0.0003   |
|    n_updates       | 340599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.68e+03 |
|    critic_loss     | 1.34e+03 |
|    ent_coef        | 0.356    |
|    ent_coef_loss   | -0.476   |
|    learning_rate   | 0.0003   |
|    n_updates       | 340999   |
---------------------------------
=== Iterazione IRL 619 ===
Loss reward (iter 619): 3.1998291015625
=== Iterazione IRL 620 ===
Loss reward (iter 620): 3.8159382343292236
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.71e+03 |
|    critic_loss     | 1.82e+03 |
|    ent_coef        | 0.351    |
|    ent_coef_loss   | -0.132   |
|    learning_rate   | 0.0003   |
|    n_updates       | 341299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.68e+03 |
|    critic_loss     | 1.42e+03 |
|    ent_coef        | 0.366    |
|    ent_coef_loss   | 0.719    |
|    learning_rate   | 0.0003   |
|    n_updates       | 341699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.68e+03 |
|    critic_loss     | 1.63e+03 |
|    ent_coef        | 0.371    |
|    ent_coef_loss   | 0.108    |
|    learning_rate   | 0.0003   |
|    n_updates       | 342099   |
---------------------------------
=== Iterazione IRL 621 ===
Loss reward (iter 621): 2.6966748237609863
=== Iterazione IRL 622 ===
Loss reward (iter 622): 2.147782325744629
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.74e+03 |
|    critic_loss     | 2.44e+03 |
|    ent_coef        | 0.371    |
|    ent_coef_loss   | 0.451    |
|    learning_rate   | 0.0003   |
|    n_updates       | 342399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.69e+03 |
|    critic_loss     | 1.94e+03 |
|    ent_coef        | 0.365    |
|    ent_coef_loss   | -0.235   |
|    learning_rate   | 0.0003   |
|    n_updates       | 342799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.69e+03 |
|    critic_loss     | 1.6e+03  |
|    ent_coef        | 0.352    |
|    ent_coef_loss   | -0.0989  |
|    learning_rate   | 0.0003   |
|    n_updates       | 343199   |
---------------------------------
=== Iterazione IRL 623 ===
Loss reward (iter 623): 3.267376184463501
=== Iterazione IRL 624 ===
Loss reward (iter 624): 2.91701078414917
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.71e+03 |
|    critic_loss     | 2.09e+03 |
|    ent_coef        | 0.356    |
|    ent_coef_loss   | -0.297   |
|    learning_rate   | 0.0003   |
|    n_updates       | 343499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.7e+03  |
|    critic_loss     | 1.99e+03 |
|    ent_coef        | 0.361    |
|    ent_coef_loss   | 0.205    |
|    learning_rate   | 0.0003   |
|    n_updates       | 343899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.7e+03  |
|    critic_loss     | 1.73e+03 |
|    ent_coef        | 0.361    |
|    ent_coef_loss   | -0.226   |
|    learning_rate   | 0.0003   |
|    n_updates       | 344299   |
---------------------------------
=== Iterazione IRL 625 ===
Loss reward (iter 625): 4.184697151184082
=== Iterazione IRL 626 ===
Loss reward (iter 626): 3.2817234992980957
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.72e+03 |
|    critic_loss     | 1.74e+03 |
|    ent_coef        | 0.363    |
|    ent_coef_loss   | 0.0298   |
|    learning_rate   | 0.0003   |
|    n_updates       | 344599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.72e+03 |
|    critic_loss     | 1.91e+03 |
|    ent_coef        | 0.364    |
|    ent_coef_loss   | 0.311    |
|    learning_rate   | 0.0003   |
|    n_updates       | 344999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.73e+03 |
|    critic_loss     | 1.71e+03 |
|    ent_coef        | 0.368    |
|    ent_coef_loss   | 0.0418   |
|    learning_rate   | 0.0003   |
|    n_updates       | 345399   |
---------------------------------
=== Iterazione IRL 627 ===
Loss reward (iter 627): 2.6462364196777344
=== Iterazione IRL 628 ===
Loss reward (iter 628): 3.22796368598938
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.71e+03 |
|    critic_loss     | 1.34e+03 |
|    ent_coef        | 0.372    |
|    ent_coef_loss   | 0.0526   |
|    learning_rate   | 0.0003   |
|    n_updates       | 345699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.73e+03 |
|    critic_loss     | 1.5e+03  |
|    ent_coef        | 0.368    |
|    ent_coef_loss   | 0.132    |
|    learning_rate   | 0.0003   |
|    n_updates       | 346099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.73e+03 |
|    critic_loss     | 1.56e+03 |
|    ent_coef        | 0.366    |
|    ent_coef_loss   | -0.0224  |
|    learning_rate   | 0.0003   |
|    n_updates       | 346499   |
---------------------------------
=== Iterazione IRL 629 ===
Loss reward (iter 629): 3.4820966720581055
=== Iterazione IRL 630 ===
Loss reward (iter 630): 2.3255679607391357
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.72e+03 |
|    critic_loss     | 2.26e+03 |
|    ent_coef        | 0.356    |
|    ent_coef_loss   | -0.152   |
|    learning_rate   | 0.0003   |
|    n_updates       | 346799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.71e+03 |
|    critic_loss     | 1.95e+03 |
|    ent_coef        | 0.353    |
|    ent_coef_loss   | -0.124   |
|    learning_rate   | 0.0003   |
|    n_updates       | 347199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.73e+03 |
|    critic_loss     | 1.61e+03 |
|    ent_coef        | 0.358    |
|    ent_coef_loss   | 0.0554   |
|    learning_rate   | 0.0003   |
|    n_updates       | 347599   |
---------------------------------
=== Iterazione IRL 631 ===
Loss reward (iter 631): 2.841465473175049
=== Iterazione IRL 632 ===
Loss reward (iter 632): 2.678894519805908
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.75e+03 |
|    critic_loss     | 1.62e+03 |
|    ent_coef        | 0.364    |
|    ent_coef_loss   | 0.12     |
|    learning_rate   | 0.0003   |
|    n_updates       | 347899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.73e+03 |
|    critic_loss     | 1.5e+03  |
|    ent_coef        | 0.365    |
|    ent_coef_loss   | -0.168   |
|    learning_rate   | 0.0003   |
|    n_updates       | 348299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.72e+03 |
|    critic_loss     | 1.72e+03 |
|    ent_coef        | 0.368    |
|    ent_coef_loss   | 0.138    |
|    learning_rate   | 0.0003   |
|    n_updates       | 348699   |
---------------------------------
=== Iterazione IRL 633 ===
Loss reward (iter 633): 2.861953020095825
=== Iterazione IRL 634 ===
Loss reward (iter 634): 2.978044271469116
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.74e+03 |
|    critic_loss     | 1.42e+03 |
|    ent_coef        | 0.368    |
|    ent_coef_loss   | -0.14    |
|    learning_rate   | 0.0003   |
|    n_updates       | 348999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.74e+03 |
|    critic_loss     | 1.58e+03 |
|    ent_coef        | 0.369    |
|    ent_coef_loss   | -0.119   |
|    learning_rate   | 0.0003   |
|    n_updates       | 349399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.73e+03 |
|    critic_loss     | 1.76e+03 |
|    ent_coef        | 0.37     |
|    ent_coef_loss   | -0.519   |
|    learning_rate   | 0.0003   |
|    n_updates       | 349799   |
---------------------------------
=== Iterazione IRL 635 ===
Loss reward (iter 635): 3.146899700164795
=== Iterazione IRL 636 ===
Loss reward (iter 636): 3.292466878890991
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.76e+03 |
|    critic_loss     | 2.07e+03 |
|    ent_coef        | 0.377    |
|    ent_coef_loss   | -0.257   |
|    learning_rate   | 0.0003   |
|    n_updates       | 350099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.75e+03 |
|    critic_loss     | 1.62e+03 |
|    ent_coef        | 0.374    |
|    ent_coef_loss   | 0.0285   |
|    learning_rate   | 0.0003   |
|    n_updates       | 350499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.76e+03 |
|    critic_loss     | 1.89e+03 |
|    ent_coef        | 0.373    |
|    ent_coef_loss   | -0.445   |
|    learning_rate   | 0.0003   |
|    n_updates       | 350899   |
---------------------------------
=== Iterazione IRL 637 ===
Loss reward (iter 637): 3.195943832397461
=== Iterazione IRL 638 ===
Loss reward (iter 638): 3.1512341499328613
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.74e+03 |
|    critic_loss     | 1.33e+03 |
|    ent_coef        | 0.365    |
|    ent_coef_loss   | -0.0738  |
|    learning_rate   | 0.0003   |
|    n_updates       | 351199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.74e+03 |
|    critic_loss     | 1.57e+03 |
|    ent_coef        | 0.366    |
|    ent_coef_loss   | -0.116   |
|    learning_rate   | 0.0003   |
|    n_updates       | 351599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.75e+03 |
|    critic_loss     | 1.55e+03 |
|    ent_coef        | 0.359    |
|    ent_coef_loss   | -0.341   |
|    learning_rate   | 0.0003   |
|    n_updates       | 351999   |
---------------------------------
=== Iterazione IRL 639 ===
Loss reward (iter 639): 3.5464158058166504
=== Iterazione IRL 640 ===
Loss reward (iter 640): 3.0502963066101074
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.75e+03 |
|    critic_loss     | 1.65e+03 |
|    ent_coef        | 0.352    |
|    ent_coef_loss   | 0.0849   |
|    learning_rate   | 0.0003   |
|    n_updates       | 352299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.73e+03 |
|    critic_loss     | 1.61e+03 |
|    ent_coef        | 0.348    |
|    ent_coef_loss   | -0.183   |
|    learning_rate   | 0.0003   |
|    n_updates       | 352699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.77e+03 |
|    critic_loss     | 2.02e+03 |
|    ent_coef        | 0.367    |
|    ent_coef_loss   | 0.157    |
|    learning_rate   | 0.0003   |
|    n_updates       | 353099   |
---------------------------------
=== Iterazione IRL 641 ===
Loss reward (iter 641): 3.2755751609802246
=== Iterazione IRL 642 ===
Loss reward (iter 642): 2.8596181869506836
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.74e+03 |
|    critic_loss     | 1.47e+03 |
|    ent_coef        | 0.359    |
|    ent_coef_loss   | -0.0397  |
|    learning_rate   | 0.0003   |
|    n_updates       | 353399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.75e+03 |
|    critic_loss     | 1.52e+03 |
|    ent_coef        | 0.35     |
|    ent_coef_loss   | -0.126   |
|    learning_rate   | 0.0003   |
|    n_updates       | 353799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.78e+03 |
|    critic_loss     | 2.34e+03 |
|    ent_coef        | 0.354    |
|    ent_coef_loss   | 0.00318  |
|    learning_rate   | 0.0003   |
|    n_updates       | 354199   |
---------------------------------
=== Iterazione IRL 643 ===
Loss reward (iter 643): 2.7596123218536377
=== Iterazione IRL 644 ===
Loss reward (iter 644): 3.3082921504974365
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.78e+03 |
|    critic_loss     | 1.68e+03 |
|    ent_coef        | 0.363    |
|    ent_coef_loss   | 0.345    |
|    learning_rate   | 0.0003   |
|    n_updates       | 354499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.75e+03 |
|    critic_loss     | 1.26e+03 |
|    ent_coef        | 0.37     |
|    ent_coef_loss   | 0.0765   |
|    learning_rate   | 0.0003   |
|    n_updates       | 354899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.78e+03 |
|    critic_loss     | 2.01e+03 |
|    ent_coef        | 0.366    |
|    ent_coef_loss   | 0.422    |
|    learning_rate   | 0.0003   |
|    n_updates       | 355299   |
---------------------------------
=== Iterazione IRL 645 ===
Loss reward (iter 645): 3.4330825805664062
=== Iterazione IRL 646 ===
Loss reward (iter 646): 2.7505900859832764
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.75e+03 |
|    critic_loss     | 2.19e+03 |
|    ent_coef        | 0.361    |
|    ent_coef_loss   | -0.302   |
|    learning_rate   | 0.0003   |
|    n_updates       | 355599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.75e+03 |
|    critic_loss     | 1.59e+03 |
|    ent_coef        | 0.356    |
|    ent_coef_loss   | 0.199    |
|    learning_rate   | 0.0003   |
|    n_updates       | 355999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.78e+03 |
|    critic_loss     | 2.11e+03 |
|    ent_coef        | 0.36     |
|    ent_coef_loss   | 0.272    |
|    learning_rate   | 0.0003   |
|    n_updates       | 356399   |
---------------------------------
=== Iterazione IRL 647 ===
Loss reward (iter 647): 4.055566787719727
=== Iterazione IRL 648 ===
Loss reward (iter 648): 3.217960834503174
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.79e+03 |
|    critic_loss     | 1.56e+03 |
|    ent_coef        | 0.365    |
|    ent_coef_loss   | -0.254   |
|    learning_rate   | 0.0003   |
|    n_updates       | 356699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.76e+03 |
|    critic_loss     | 1.41e+03 |
|    ent_coef        | 0.356    |
|    ent_coef_loss   | 0.514    |
|    learning_rate   | 0.0003   |
|    n_updates       | 357099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.78e+03 |
|    critic_loss     | 1.88e+03 |
|    ent_coef        | 0.367    |
|    ent_coef_loss   | 0.0482   |
|    learning_rate   | 0.0003   |
|    n_updates       | 357499   |
---------------------------------
=== Iterazione IRL 649 ===
Loss reward (iter 649): 3.044217109680176
=== Iterazione IRL 650 ===
Loss reward (iter 650): 3.3268465995788574
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.79e+03 |
|    critic_loss     | 1.74e+03 |
|    ent_coef        | 0.37     |
|    ent_coef_loss   | 0.202    |
|    learning_rate   | 0.0003   |
|    n_updates       | 357799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.81e+03 |
|    critic_loss     | 1.75e+03 |
|    ent_coef        | 0.371    |
|    ent_coef_loss   | -0.241   |
|    learning_rate   | 0.0003   |
|    n_updates       | 358199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.8e+03  |
|    critic_loss     | 1.75e+03 |
|    ent_coef        | 0.376    |
|    ent_coef_loss   | 0.033    |
|    learning_rate   | 0.0003   |
|    n_updates       | 358599   |
---------------------------------
=== Iterazione IRL 651 ===
Loss reward (iter 651): 2.2973289489746094
=== Iterazione IRL 652 ===
Loss reward (iter 652): 3.0300092697143555
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.8e+03  |
|    critic_loss     | 1.77e+03 |
|    ent_coef        | 0.37     |
|    ent_coef_loss   | 0.171    |
|    learning_rate   | 0.0003   |
|    n_updates       | 358899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.77e+03 |
|    critic_loss     | 1.32e+03 |
|    ent_coef        | 0.372    |
|    ent_coef_loss   | 0.233    |
|    learning_rate   | 0.0003   |
|    n_updates       | 359299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.79e+03 |
|    critic_loss     | 1.82e+03 |
|    ent_coef        | 0.37     |
|    ent_coef_loss   | 0.347    |
|    learning_rate   | 0.0003   |
|    n_updates       | 359699   |
---------------------------------
=== Iterazione IRL 653 ===
Loss reward (iter 653): 4.076818943023682
=== Iterazione IRL 654 ===
Loss reward (iter 654): 3.900643825531006
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.79e+03 |
|    critic_loss     | 1.72e+03 |
|    ent_coef        | 0.379    |
|    ent_coef_loss   | 0.179    |
|    learning_rate   | 0.0003   |
|    n_updates       | 359999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.81e+03 |
|    critic_loss     | 1.83e+03 |
|    ent_coef        | 0.381    |
|    ent_coef_loss   | -0.226   |
|    learning_rate   | 0.0003   |
|    n_updates       | 360399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.78e+03 |
|    critic_loss     | 1.24e+03 |
|    ent_coef        | 0.382    |
|    ent_coef_loss   | -0.0273  |
|    learning_rate   | 0.0003   |
|    n_updates       | 360799   |
---------------------------------
=== Iterazione IRL 655 ===
Loss reward (iter 655): 3.0439939498901367
=== Iterazione IRL 656 ===
Loss reward (iter 656): 3.2818655967712402
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.82e+03 |
|    critic_loss     | 1.63e+03 |
|    ent_coef        | 0.378    |
|    ent_coef_loss   | -0.048   |
|    learning_rate   | 0.0003   |
|    n_updates       | 361099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.8e+03  |
|    critic_loss     | 2.14e+03 |
|    ent_coef        | 0.379    |
|    ent_coef_loss   | 0.00961  |
|    learning_rate   | 0.0003   |
|    n_updates       | 361499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.79e+03 |
|    critic_loss     | 1.99e+03 |
|    ent_coef        | 0.372    |
|    ent_coef_loss   | -0.133   |
|    learning_rate   | 0.0003   |
|    n_updates       | 361899   |
---------------------------------
=== Iterazione IRL 657 ===
Loss reward (iter 657): 3.235991954803467
=== Iterazione IRL 658 ===
Loss reward (iter 658): 2.7365174293518066
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.8e+03  |
|    critic_loss     | 1.72e+03 |
|    ent_coef        | 0.362    |
|    ent_coef_loss   | -0.0708  |
|    learning_rate   | 0.0003   |
|    n_updates       | 362199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.81e+03 |
|    critic_loss     | 1.79e+03 |
|    ent_coef        | 0.357    |
|    ent_coef_loss   | 0.0955   |
|    learning_rate   | 0.0003   |
|    n_updates       | 362599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.79e+03 |
|    critic_loss     | 2.49e+03 |
|    ent_coef        | 0.358    |
|    ent_coef_loss   | -0.259   |
|    learning_rate   | 0.0003   |
|    n_updates       | 362999   |
---------------------------------
=== Iterazione IRL 659 ===
Loss reward (iter 659): 1.4681748151779175
=== Iterazione IRL 660 ===
Loss reward (iter 660): 2.223076820373535
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.8e+03  |
|    critic_loss     | 2e+03    |
|    ent_coef        | 0.348    |
|    ent_coef_loss   | 0.257    |
|    learning_rate   | 0.0003   |
|    n_updates       | 363299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.78e+03 |
|    critic_loss     | 1.24e+03 |
|    ent_coef        | 0.365    |
|    ent_coef_loss   | -0.0427  |
|    learning_rate   | 0.0003   |
|    n_updates       | 363699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.79e+03 |
|    critic_loss     | 1.49e+03 |
|    ent_coef        | 0.372    |
|    ent_coef_loss   | 0.00208  |
|    learning_rate   | 0.0003   |
|    n_updates       | 364099   |
---------------------------------
=== Iterazione IRL 661 ===
Loss reward (iter 661): 1.8974676132202148
=== Iterazione IRL 662 ===
Loss reward (iter 662): 3.6473593711853027
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.82e+03 |
|    critic_loss     | 2.32e+03 |
|    ent_coef        | 0.372    |
|    ent_coef_loss   | 0.22     |
|    learning_rate   | 0.0003   |
|    n_updates       | 364399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.83e+03 |
|    critic_loss     | 1.93e+03 |
|    ent_coef        | 0.373    |
|    ent_coef_loss   | -0.191   |
|    learning_rate   | 0.0003   |
|    n_updates       | 364799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.82e+03 |
|    critic_loss     | 1.62e+03 |
|    ent_coef        | 0.369    |
|    ent_coef_loss   | -0.174   |
|    learning_rate   | 0.0003   |
|    n_updates       | 365199   |
---------------------------------
=== Iterazione IRL 663 ===
Loss reward (iter 663): 3.6807146072387695
=== Iterazione IRL 664 ===
Loss reward (iter 664): 3.049678325653076
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.79e+03 |
|    critic_loss     | 1.63e+03 |
|    ent_coef        | 0.376    |
|    ent_coef_loss   | 0.281    |
|    learning_rate   | 0.0003   |
|    n_updates       | 365499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.82e+03 |
|    critic_loss     | 1.64e+03 |
|    ent_coef        | 0.354    |
|    ent_coef_loss   | 0.2      |
|    learning_rate   | 0.0003   |
|    n_updates       | 365899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.81e+03 |
|    critic_loss     | 2.05e+03 |
|    ent_coef        | 0.354    |
|    ent_coef_loss   | 0.0391   |
|    learning_rate   | 0.0003   |
|    n_updates       | 366299   |
---------------------------------
=== Iterazione IRL 665 ===
Loss reward (iter 665): 2.2376956939697266
=== Iterazione IRL 666 ===
Loss reward (iter 666): 2.898045063018799
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.83e+03 |
|    critic_loss     | 1.86e+03 |
|    ent_coef        | 0.364    |
|    ent_coef_loss   | 0.47     |
|    learning_rate   | 0.0003   |
|    n_updates       | 366599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.8e+03  |
|    critic_loss     | 1.98e+03 |
|    ent_coef        | 0.373    |
|    ent_coef_loss   | -0.282   |
|    learning_rate   | 0.0003   |
|    n_updates       | 366999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.82e+03 |
|    critic_loss     | 2e+03    |
|    ent_coef        | 0.368    |
|    ent_coef_loss   | 0.106    |
|    learning_rate   | 0.0003   |
|    n_updates       | 367399   |
---------------------------------
=== Iterazione IRL 667 ===
Loss reward (iter 667): 3.4340240955352783
=== Iterazione IRL 668 ===
Loss reward (iter 668): 3.4176506996154785
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.84e+03 |
|    critic_loss     | 1.65e+03 |
|    ent_coef        | 0.354    |
|    ent_coef_loss   | -0.101   |
|    learning_rate   | 0.0003   |
|    n_updates       | 367699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.82e+03 |
|    critic_loss     | 2.31e+03 |
|    ent_coef        | 0.343    |
|    ent_coef_loss   | 0.202    |
|    learning_rate   | 0.0003   |
|    n_updates       | 368099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.83e+03 |
|    critic_loss     | 1.58e+03 |
|    ent_coef        | 0.349    |
|    ent_coef_loss   | 0.134    |
|    learning_rate   | 0.0003   |
|    n_updates       | 368499   |
---------------------------------
=== Iterazione IRL 669 ===
Loss reward (iter 669): 2.0852458477020264
=== Iterazione IRL 670 ===
Loss reward (iter 670): 1.978670358657837
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.82e+03 |
|    critic_loss     | 1.84e+03 |
|    ent_coef        | 0.351    |
|    ent_coef_loss   | -0.0902  |
|    learning_rate   | 0.0003   |
|    n_updates       | 368799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.83e+03 |
|    critic_loss     | 1.56e+03 |
|    ent_coef        | 0.358    |
|    ent_coef_loss   | 0.0662   |
|    learning_rate   | 0.0003   |
|    n_updates       | 369199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.86e+03 |
|    critic_loss     | 1.64e+03 |
|    ent_coef        | 0.353    |
|    ent_coef_loss   | -0.036   |
|    learning_rate   | 0.0003   |
|    n_updates       | 369599   |
---------------------------------
=== Iterazione IRL 671 ===
Loss reward (iter 671): 3.712730884552002
=== Iterazione IRL 672 ===
Loss reward (iter 672): 3.375235080718994
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.82e+03 |
|    critic_loss     | 1.62e+03 |
|    ent_coef        | 0.358    |
|    ent_coef_loss   | -0.133   |
|    learning_rate   | 0.0003   |
|    n_updates       | 369899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.81e+03 |
|    critic_loss     | 2.01e+03 |
|    ent_coef        | 0.354    |
|    ent_coef_loss   | -0.126   |
|    learning_rate   | 0.0003   |
|    n_updates       | 370299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.82e+03 |
|    critic_loss     | 1.58e+03 |
|    ent_coef        | 0.349    |
|    ent_coef_loss   | 0.115    |
|    learning_rate   | 0.0003   |
|    n_updates       | 370699   |
---------------------------------
=== Iterazione IRL 673 ===
Loss reward (iter 673): 3.735355854034424
=== Iterazione IRL 674 ===
Loss reward (iter 674): 3.232846975326538
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.81e+03 |
|    critic_loss     | 2.37e+03 |
|    ent_coef        | 0.353    |
|    ent_coef_loss   | 0.0614   |
|    learning_rate   | 0.0003   |
|    n_updates       | 370999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.84e+03 |
|    critic_loss     | 1.67e+03 |
|    ent_coef        | 0.357    |
|    ent_coef_loss   | -0.082   |
|    learning_rate   | 0.0003   |
|    n_updates       | 371399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.82e+03 |
|    critic_loss     | 1.54e+03 |
|    ent_coef        | 0.362    |
|    ent_coef_loss   | 0.116    |
|    learning_rate   | 0.0003   |
|    n_updates       | 371799   |
---------------------------------
=== Iterazione IRL 675 ===
Loss reward (iter 675): 2.6777219772338867
=== Iterazione IRL 676 ===
Loss reward (iter 676): 3.50344181060791
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.83e+03 |
|    critic_loss     | 1.61e+03 |
|    ent_coef        | 0.352    |
|    ent_coef_loss   | -0.326   |
|    learning_rate   | 0.0003   |
|    n_updates       | 372099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.84e+03 |
|    critic_loss     | 1.97e+03 |
|    ent_coef        | 0.353    |
|    ent_coef_loss   | 0.0223   |
|    learning_rate   | 0.0003   |
|    n_updates       | 372499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.83e+03 |
|    critic_loss     | 1.69e+03 |
|    ent_coef        | 0.371    |
|    ent_coef_loss   | 0.127    |
|    learning_rate   | 0.0003   |
|    n_updates       | 372899   |
---------------------------------
=== Iterazione IRL 677 ===
Loss reward (iter 677): 3.862703323364258
=== Iterazione IRL 678 ===
Loss reward (iter 678): 2.633882999420166
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.85e+03 |
|    critic_loss     | 2.37e+03 |
|    ent_coef        | 0.383    |
|    ent_coef_loss   | 0.129    |
|    learning_rate   | 0.0003   |
|    n_updates       | 373199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.87e+03 |
|    critic_loss     | 1.55e+03 |
|    ent_coef        | 0.375    |
|    ent_coef_loss   | -0.235   |
|    learning_rate   | 0.0003   |
|    n_updates       | 373599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.84e+03 |
|    critic_loss     | 1.57e+03 |
|    ent_coef        | 0.393    |
|    ent_coef_loss   | -0.243   |
|    learning_rate   | 0.0003   |
|    n_updates       | 373999   |
---------------------------------
=== Iterazione IRL 679 ===
Loss reward (iter 679): 3.8768701553344727
=== Iterazione IRL 680 ===
Loss reward (iter 680): 2.7547390460968018
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.82e+03 |
|    critic_loss     | 1.65e+03 |
|    ent_coef        | 0.378    |
|    ent_coef_loss   | -0.448   |
|    learning_rate   | 0.0003   |
|    n_updates       | 374299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.85e+03 |
|    critic_loss     | 1.78e+03 |
|    ent_coef        | 0.373    |
|    ent_coef_loss   | -0.0716  |
|    learning_rate   | 0.0003   |
|    n_updates       | 374699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.84e+03 |
|    critic_loss     | 1.65e+03 |
|    ent_coef        | 0.382    |
|    ent_coef_loss   | -0.0629  |
|    learning_rate   | 0.0003   |
|    n_updates       | 375099   |
---------------------------------
=== Iterazione IRL 681 ===
Loss reward (iter 681): 2.5542075634002686
=== Iterazione IRL 682 ===
Loss reward (iter 682): 1.7686280012130737
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.86e+03 |
|    critic_loss     | 2.03e+03 |
|    ent_coef        | 0.381    |
|    ent_coef_loss   | -0.293   |
|    learning_rate   | 0.0003   |
|    n_updates       | 375399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.86e+03 |
|    critic_loss     | 2.08e+03 |
|    ent_coef        | 0.369    |
|    ent_coef_loss   | -0.585   |
|    learning_rate   | 0.0003   |
|    n_updates       | 375799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.86e+03 |
|    critic_loss     | 1.84e+03 |
|    ent_coef        | 0.381    |
|    ent_coef_loss   | 0.0952   |
|    learning_rate   | 0.0003   |
|    n_updates       | 376199   |
---------------------------------
=== Iterazione IRL 683 ===
Loss reward (iter 683): 3.7521705627441406
=== Iterazione IRL 684 ===
Loss reward (iter 684): 3.4567227363586426
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.84e+03 |
|    critic_loss     | 1.73e+03 |
|    ent_coef        | 0.391    |
|    ent_coef_loss   | -0.0791  |
|    learning_rate   | 0.0003   |
|    n_updates       | 376499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.85e+03 |
|    critic_loss     | 1.52e+03 |
|    ent_coef        | 0.397    |
|    ent_coef_loss   | 0.113    |
|    learning_rate   | 0.0003   |
|    n_updates       | 376899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.84e+03 |
|    critic_loss     | 1.98e+03 |
|    ent_coef        | 0.388    |
|    ent_coef_loss   | 0.161    |
|    learning_rate   | 0.0003   |
|    n_updates       | 377299   |
---------------------------------
=== Iterazione IRL 685 ===
Loss reward (iter 685): 2.9657607078552246
=== Iterazione IRL 686 ===
Loss reward (iter 686): 2.748481273651123
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.87e+03 |
|    critic_loss     | 2.23e+03 |
|    ent_coef        | 0.386    |
|    ent_coef_loss   | 0.156    |
|    learning_rate   | 0.0003   |
|    n_updates       | 377599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.85e+03 |
|    critic_loss     | 1.45e+03 |
|    ent_coef        | 0.395    |
|    ent_coef_loss   | -0.235   |
|    learning_rate   | 0.0003   |
|    n_updates       | 377999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.87e+03 |
|    critic_loss     | 1.84e+03 |
|    ent_coef        | 0.399    |
|    ent_coef_loss   | 0.161    |
|    learning_rate   | 0.0003   |
|    n_updates       | 378399   |
---------------------------------
=== Iterazione IRL 687 ===
Loss reward (iter 687): 4.008514881134033
=== Iterazione IRL 688 ===
Loss reward (iter 688): 4.482025623321533
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.89e+03 |
|    critic_loss     | 2.34e+03 |
|    ent_coef        | 0.383    |
|    ent_coef_loss   | 0.133    |
|    learning_rate   | 0.0003   |
|    n_updates       | 378699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.87e+03 |
|    critic_loss     | 1.84e+03 |
|    ent_coef        | 0.38     |
|    ent_coef_loss   | 0.0775   |
|    learning_rate   | 0.0003   |
|    n_updates       | 379099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.84e+03 |
|    critic_loss     | 1.59e+03 |
|    ent_coef        | 0.37     |
|    ent_coef_loss   | -0.0182  |
|    learning_rate   | 0.0003   |
|    n_updates       | 379499   |
---------------------------------
=== Iterazione IRL 689 ===
Loss reward (iter 689): 4.374844551086426
=== Iterazione IRL 690 ===
Loss reward (iter 690): 4.254133701324463
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.85e+03 |
|    critic_loss     | 2.44e+03 |
|    ent_coef        | 0.37     |
|    ent_coef_loss   | -0.026   |
|    learning_rate   | 0.0003   |
|    n_updates       | 379799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.87e+03 |
|    critic_loss     | 1.78e+03 |
|    ent_coef        | 0.379    |
|    ent_coef_loss   | 0.181    |
|    learning_rate   | 0.0003   |
|    n_updates       | 380199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.91e+03 |
|    critic_loss     | 2.86e+03 |
|    ent_coef        | 0.37     |
|    ent_coef_loss   | 0.254    |
|    learning_rate   | 0.0003   |
|    n_updates       | 380599   |
---------------------------------
=== Iterazione IRL 691 ===
Loss reward (iter 691): 4.414298057556152
=== Iterazione IRL 692 ===
Loss reward (iter 692): 4.630877494812012
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.87e+03 |
|    critic_loss     | 1.76e+03 |
|    ent_coef        | 0.364    |
|    ent_coef_loss   | -0.159   |
|    learning_rate   | 0.0003   |
|    n_updates       | 380899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.87e+03 |
|    critic_loss     | 1.77e+03 |
|    ent_coef        | 0.382    |
|    ent_coef_loss   | 0.0581   |
|    learning_rate   | 0.0003   |
|    n_updates       | 381299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.88e+03 |
|    critic_loss     | 1.94e+03 |
|    ent_coef        | 0.378    |
|    ent_coef_loss   | -0.0479  |
|    learning_rate   | 0.0003   |
|    n_updates       | 381699   |
---------------------------------
=== Iterazione IRL 693 ===
Loss reward (iter 693): 3.722137212753296
=== Iterazione IRL 694 ===
Loss reward (iter 694): 3.4308900833129883
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.86e+03 |
|    critic_loss     | 1.69e+03 |
|    ent_coef        | 0.382    |
|    ent_coef_loss   | -0.26    |
|    learning_rate   | 0.0003   |
|    n_updates       | 381999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.88e+03 |
|    critic_loss     | 1.76e+03 |
|    ent_coef        | 0.382    |
|    ent_coef_loss   | 0.0972   |
|    learning_rate   | 0.0003   |
|    n_updates       | 382399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.87e+03 |
|    critic_loss     | 2.09e+03 |
|    ent_coef        | 0.389    |
|    ent_coef_loss   | 0.142    |
|    learning_rate   | 0.0003   |
|    n_updates       | 382799   |
---------------------------------
=== Iterazione IRL 695 ===
Loss reward (iter 695): 3.834094524383545
=== Iterazione IRL 696 ===
Loss reward (iter 696): 3.4159741401672363
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.86e+03 |
|    critic_loss     | 1.83e+03 |
|    ent_coef        | 0.393    |
|    ent_coef_loss   | -0.148   |
|    learning_rate   | 0.0003   |
|    n_updates       | 383099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.85e+03 |
|    critic_loss     | 1.58e+03 |
|    ent_coef        | 0.396    |
|    ent_coef_loss   | -0.0187  |
|    learning_rate   | 0.0003   |
|    n_updates       | 383499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.91e+03 |
|    critic_loss     | 2.09e+03 |
|    ent_coef        | 0.401    |
|    ent_coef_loss   | -0.0863  |
|    learning_rate   | 0.0003   |
|    n_updates       | 383899   |
---------------------------------
=== Iterazione IRL 697 ===
Loss reward (iter 697): 3.4062836170196533
=== Iterazione IRL 698 ===
Loss reward (iter 698): 3.0976240634918213
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.87e+03 |
|    critic_loss     | 1.67e+03 |
|    ent_coef        | 0.399    |
|    ent_coef_loss   | -0.143   |
|    learning_rate   | 0.0003   |
|    n_updates       | 384199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.87e+03 |
|    critic_loss     | 1.77e+03 |
|    ent_coef        | 0.392    |
|    ent_coef_loss   | -0.173   |
|    learning_rate   | 0.0003   |
|    n_updates       | 384599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.87e+03 |
|    critic_loss     | 2.54e+03 |
|    ent_coef        | 0.388    |
|    ent_coef_loss   | -0.226   |
|    learning_rate   | 0.0003   |
|    n_updates       | 384999   |
---------------------------------
=== Iterazione IRL 699 ===
Loss reward (iter 699): 2.2343101501464844
=== Iterazione IRL 700 ===
Loss reward (iter 700): 4.017934799194336
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.91e+03 |
|    critic_loss     | 2.61e+03 |
|    ent_coef        | 0.392    |
|    ent_coef_loss   | 0.112    |
|    learning_rate   | 0.0003   |
|    n_updates       | 385299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.89e+03 |
|    critic_loss     | 1.58e+03 |
|    ent_coef        | 0.386    |
|    ent_coef_loss   | -0.231   |
|    learning_rate   | 0.0003   |
|    n_updates       | 385699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.87e+03 |
|    critic_loss     | 1.56e+03 |
|    ent_coef        | 0.381    |
|    ent_coef_loss   | -0.0883  |
|    learning_rate   | 0.0003   |
|    n_updates       | 386099   |
---------------------------------
=== Iterazione IRL 701 ===
Loss reward (iter 701): 3.9751243591308594
=== Iterazione IRL 702 ===
Loss reward (iter 702): 3.359400510787964
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.85e+03 |
|    critic_loss     | 1.63e+03 |
|    ent_coef        | 0.378    |
|    ent_coef_loss   | -0.261   |
|    learning_rate   | 0.0003   |
|    n_updates       | 386399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.93e+03 |
|    critic_loss     | 2.22e+03 |
|    ent_coef        | 0.379    |
|    ent_coef_loss   | -0.0299  |
|    learning_rate   | 0.0003   |
|    n_updates       | 386799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.87e+03 |
|    critic_loss     | 1.6e+03  |
|    ent_coef        | 0.373    |
|    ent_coef_loss   | -0.277   |
|    learning_rate   | 0.0003   |
|    n_updates       | 387199   |
---------------------------------
=== Iterazione IRL 703 ===
Loss reward (iter 703): 2.8059890270233154
=== Iterazione IRL 704 ===
Loss reward (iter 704): 2.7589457035064697
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.92e+03 |
|    critic_loss     | 1.98e+03 |
|    ent_coef        | 0.361    |
|    ent_coef_loss   | 0.0732   |
|    learning_rate   | 0.0003   |
|    n_updates       | 387499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.9e+03  |
|    critic_loss     | 2.29e+03 |
|    ent_coef        | 0.359    |
|    ent_coef_loss   | -0.0155  |
|    learning_rate   | 0.0003   |
|    n_updates       | 387899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.89e+03 |
|    critic_loss     | 1.97e+03 |
|    ent_coef        | 0.365    |
|    ent_coef_loss   | 0.115    |
|    learning_rate   | 0.0003   |
|    n_updates       | 388299   |
---------------------------------
=== Iterazione IRL 705 ===
Loss reward (iter 705): 2.7198586463928223
=== Iterazione IRL 706 ===
Loss reward (iter 706): 2.8223865032196045
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.89e+03 |
|    critic_loss     | 2.02e+03 |
|    ent_coef        | 0.365    |
|    ent_coef_loss   | 0.103    |
|    learning_rate   | 0.0003   |
|    n_updates       | 388599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.93e+03 |
|    critic_loss     | 2.1e+03  |
|    ent_coef        | 0.375    |
|    ent_coef_loss   | 0.61     |
|    learning_rate   | 0.0003   |
|    n_updates       | 388999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.88e+03 |
|    critic_loss     | 2.03e+03 |
|    ent_coef        | 0.361    |
|    ent_coef_loss   | 0.108    |
|    learning_rate   | 0.0003   |
|    n_updates       | 389399   |
---------------------------------
=== Iterazione IRL 707 ===
Loss reward (iter 707): 3.3548102378845215
=== Iterazione IRL 708 ===
Loss reward (iter 708): 2.8940563201904297
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.91e+03 |
|    critic_loss     | 2.1e+03  |
|    ent_coef        | 0.378    |
|    ent_coef_loss   | -0.115   |
|    learning_rate   | 0.0003   |
|    n_updates       | 389699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.89e+03 |
|    critic_loss     | 1.8e+03  |
|    ent_coef        | 0.39     |
|    ent_coef_loss   | -0.436   |
|    learning_rate   | 0.0003   |
|    n_updates       | 390099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.92e+03 |
|    critic_loss     | 2.32e+03 |
|    ent_coef        | 0.404    |
|    ent_coef_loss   | 0.148    |
|    learning_rate   | 0.0003   |
|    n_updates       | 390499   |
---------------------------------
=== Iterazione IRL 709 ===
Loss reward (iter 709): 2.5068306922912598
=== Iterazione IRL 710 ===
Loss reward (iter 710): 2.678569793701172
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.9e+03  |
|    critic_loss     | 1.94e+03 |
|    ent_coef        | 0.394    |
|    ent_coef_loss   | -0.00393 |
|    learning_rate   | 0.0003   |
|    n_updates       | 390799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.91e+03 |
|    critic_loss     | 2.26e+03 |
|    ent_coef        | 0.394    |
|    ent_coef_loss   | 0.333    |
|    learning_rate   | 0.0003   |
|    n_updates       | 391199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.93e+03 |
|    critic_loss     | 2.5e+03  |
|    ent_coef        | 0.408    |
|    ent_coef_loss   | -0.00432 |
|    learning_rate   | 0.0003   |
|    n_updates       | 391599   |
---------------------------------
=== Iterazione IRL 711 ===
Loss reward (iter 711): 4.356325149536133
=== Iterazione IRL 712 ===
Loss reward (iter 712): 3.8837287425994873
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.9e+03  |
|    critic_loss     | 1.24e+03 |
|    ent_coef        | 0.399    |
|    ent_coef_loss   | 0.0381   |
|    learning_rate   | 0.0003   |
|    n_updates       | 391899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.88e+03 |
|    critic_loss     | 1.75e+03 |
|    ent_coef        | 0.38     |
|    ent_coef_loss   | -0.245   |
|    learning_rate   | 0.0003   |
|    n_updates       | 392299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.91e+03 |
|    critic_loss     | 2.34e+03 |
|    ent_coef        | 0.386    |
|    ent_coef_loss   | 0.187    |
|    learning_rate   | 0.0003   |
|    n_updates       | 392699   |
---------------------------------
=== Iterazione IRL 713 ===
Loss reward (iter 713): 2.2132368087768555
=== Iterazione IRL 714 ===
Loss reward (iter 714): 3.282261610031128
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.92e+03 |
|    critic_loss     | 2.32e+03 |
|    ent_coef        | 0.398    |
|    ent_coef_loss   | -0.359   |
|    learning_rate   | 0.0003   |
|    n_updates       | 392999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.95e+03 |
|    critic_loss     | 2.28e+03 |
|    ent_coef        | 0.411    |
|    ent_coef_loss   | 0.468    |
|    learning_rate   | 0.0003   |
|    n_updates       | 393399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.94e+03 |
|    critic_loss     | 1.97e+03 |
|    ent_coef        | 0.405    |
|    ent_coef_loss   | -0.0563  |
|    learning_rate   | 0.0003   |
|    n_updates       | 393799   |
---------------------------------
=== Iterazione IRL 715 ===
Loss reward (iter 715): 4.071117877960205
=== Iterazione IRL 716 ===
Loss reward (iter 716): 4.066908836364746
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.9e+03  |
|    critic_loss     | 2.35e+03 |
|    ent_coef        | 0.393    |
|    ent_coef_loss   | -0.307   |
|    learning_rate   | 0.0003   |
|    n_updates       | 394099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.93e+03 |
|    critic_loss     | 1.83e+03 |
|    ent_coef        | 0.383    |
|    ent_coef_loss   | 0.0423   |
|    learning_rate   | 0.0003   |
|    n_updates       | 394499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.91e+03 |
|    critic_loss     | 1.84e+03 |
|    ent_coef        | 0.384    |
|    ent_coef_loss   | 0.0371   |
|    learning_rate   | 0.0003   |
|    n_updates       | 394899   |
---------------------------------
=== Iterazione IRL 717 ===
Loss reward (iter 717): 4.133579254150391
=== Iterazione IRL 718 ===
Loss reward (iter 718): 3.1251955032348633
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.92e+03 |
|    critic_loss     | 2.15e+03 |
|    ent_coef        | 0.393    |
|    ent_coef_loss   | 0.384    |
|    learning_rate   | 0.0003   |
|    n_updates       | 395199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.94e+03 |
|    critic_loss     | 2.28e+03 |
|    ent_coef        | 0.423    |
|    ent_coef_loss   | -0.319   |
|    learning_rate   | 0.0003   |
|    n_updates       | 395599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.91e+03 |
|    critic_loss     | 1.99e+03 |
|    ent_coef        | 0.411    |
|    ent_coef_loss   | 0.0516   |
|    learning_rate   | 0.0003   |
|    n_updates       | 395999   |
---------------------------------
=== Iterazione IRL 719 ===
Loss reward (iter 719): 3.0569581985473633
=== Iterazione IRL 720 ===
Loss reward (iter 720): 3.91430401802063
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.9e+03  |
|    critic_loss     | 2.03e+03 |
|    ent_coef        | 0.415    |
|    ent_coef_loss   | -0.345   |
|    learning_rate   | 0.0003   |
|    n_updates       | 396299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.92e+03 |
|    critic_loss     | 1.94e+03 |
|    ent_coef        | 0.414    |
|    ent_coef_loss   | 0.00364  |
|    learning_rate   | 0.0003   |
|    n_updates       | 396699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.93e+03 |
|    critic_loss     | 1.47e+03 |
|    ent_coef        | 0.383    |
|    ent_coef_loss   | -0.402   |
|    learning_rate   | 0.0003   |
|    n_updates       | 397099   |
---------------------------------
=== Iterazione IRL 721 ===
Loss reward (iter 721): 2.8738131523132324
=== Iterazione IRL 722 ===
Loss reward (iter 722): 2.5826938152313232
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.94e+03 |
|    critic_loss     | 2.24e+03 |
|    ent_coef        | 0.386    |
|    ent_coef_loss   | -0.125   |
|    learning_rate   | 0.0003   |
|    n_updates       | 397399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.93e+03 |
|    critic_loss     | 1.86e+03 |
|    ent_coef        | 0.375    |
|    ent_coef_loss   | -0.0997  |
|    learning_rate   | 0.0003   |
|    n_updates       | 397799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.94e+03 |
|    critic_loss     | 1.99e+03 |
|    ent_coef        | 0.375    |
|    ent_coef_loss   | 0.353    |
|    learning_rate   | 0.0003   |
|    n_updates       | 398199   |
---------------------------------
=== Iterazione IRL 723 ===
Loss reward (iter 723): 5.795141220092773
=== Iterazione IRL 724 ===
Loss reward (iter 724): 3.967970371246338
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.93e+03 |
|    critic_loss     | 1.78e+03 |
|    ent_coef        | 0.386    |
|    ent_coef_loss   | -0.28    |
|    learning_rate   | 0.0003   |
|    n_updates       | 398499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.95e+03 |
|    critic_loss     | 1.55e+03 |
|    ent_coef        | 0.375    |
|    ent_coef_loss   | -0.117   |
|    learning_rate   | 0.0003   |
|    n_updates       | 398899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.94e+03 |
|    critic_loss     | 1.88e+03 |
|    ent_coef        | 0.366    |
|    ent_coef_loss   | -0.179   |
|    learning_rate   | 0.0003   |
|    n_updates       | 399299   |
---------------------------------
=== Iterazione IRL 725 ===
Loss reward (iter 725): 3.6334996223449707
=== Iterazione IRL 726 ===
Loss reward (iter 726): 3.3646185398101807
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.95e+03 |
|    critic_loss     | 2.55e+03 |
|    ent_coef        | 0.369    |
|    ent_coef_loss   | 0.154    |
|    learning_rate   | 0.0003   |
|    n_updates       | 399599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.93e+03 |
|    critic_loss     | 2e+03    |
|    ent_coef        | 0.369    |
|    ent_coef_loss   | -0.0831  |
|    learning_rate   | 0.0003   |
|    n_updates       | 399999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.94e+03 |
|    critic_loss     | 2.31e+03 |
|    ent_coef        | 0.381    |
|    ent_coef_loss   | 0.0297   |
|    learning_rate   | 0.0003   |
|    n_updates       | 400399   |
---------------------------------
=== Iterazione IRL 727 ===
Loss reward (iter 727): 2.8859055042266846
=== Iterazione IRL 728 ===
Loss reward (iter 728): 5.282791614532471
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.97e+03 |
|    critic_loss     | 2.19e+03 |
|    ent_coef        | 0.388    |
|    ent_coef_loss   | 0.0478   |
|    learning_rate   | 0.0003   |
|    n_updates       | 400699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.95e+03 |
|    critic_loss     | 2.18e+03 |
|    ent_coef        | 0.393    |
|    ent_coef_loss   | 0.0809   |
|    learning_rate   | 0.0003   |
|    n_updates       | 401099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.97e+03 |
|    critic_loss     | 1.77e+03 |
|    ent_coef        | 0.396    |
|    ent_coef_loss   | -0.0683  |
|    learning_rate   | 0.0003   |
|    n_updates       | 401499   |
---------------------------------
=== Iterazione IRL 729 ===
Loss reward (iter 729): 4.928826808929443
=== Iterazione IRL 730 ===
Loss reward (iter 730): 3.0549468994140625
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.96e+03 |
|    critic_loss     | 1.55e+03 |
|    ent_coef        | 0.403    |
|    ent_coef_loss   | -0.00949 |
|    learning_rate   | 0.0003   |
|    n_updates       | 401799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.94e+03 |
|    critic_loss     | 1.44e+03 |
|    ent_coef        | 0.403    |
|    ent_coef_loss   | -0.0465  |
|    learning_rate   | 0.0003   |
|    n_updates       | 402199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.01e+03 |
|    critic_loss     | 2.71e+03 |
|    ent_coef        | 0.403    |
|    ent_coef_loss   | 0.259    |
|    learning_rate   | 0.0003   |
|    n_updates       | 402599   |
---------------------------------
=== Iterazione IRL 731 ===
Loss reward (iter 731): 3.6934902667999268
=== Iterazione IRL 732 ===
Loss reward (iter 732): 2.7868194580078125
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.97e+03 |
|    critic_loss     | 2.29e+03 |
|    ent_coef        | 0.396    |
|    ent_coef_loss   | -0.315   |
|    learning_rate   | 0.0003   |
|    n_updates       | 402899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.97e+03 |
|    critic_loss     | 2.3e+03  |
|    ent_coef        | 0.399    |
|    ent_coef_loss   | -0.273   |
|    learning_rate   | 0.0003   |
|    n_updates       | 403299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.94e+03 |
|    critic_loss     | 2.02e+03 |
|    ent_coef        | 0.395    |
|    ent_coef_loss   | 0.038    |
|    learning_rate   | 0.0003   |
|    n_updates       | 403699   |
---------------------------------
=== Iterazione IRL 733 ===
Loss reward (iter 733): 4.285841941833496
=== Iterazione IRL 734 ===
Loss reward (iter 734): 3.032219648361206
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.95e+03 |
|    critic_loss     | 1.75e+03 |
|    ent_coef        | 0.386    |
|    ent_coef_loss   | -0.012   |
|    learning_rate   | 0.0003   |
|    n_updates       | 403999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.96e+03 |
|    critic_loss     | 1.74e+03 |
|    ent_coef        | 0.39     |
|    ent_coef_loss   | -0.0489  |
|    learning_rate   | 0.0003   |
|    n_updates       | 404399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.97e+03 |
|    critic_loss     | 2.28e+03 |
|    ent_coef        | 0.381    |
|    ent_coef_loss   | -0.0638  |
|    learning_rate   | 0.0003   |
|    n_updates       | 404799   |
---------------------------------
=== Iterazione IRL 735 ===
Loss reward (iter 735): 2.984189033508301
=== Iterazione IRL 736 ===
Loss reward (iter 736): 2.887091875076294
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.95e+03 |
|    critic_loss     | 1.9e+03  |
|    ent_coef        | 0.403    |
|    ent_coef_loss   | -0.292   |
|    learning_rate   | 0.0003   |
|    n_updates       | 405099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.97e+03 |
|    critic_loss     | 2.26e+03 |
|    ent_coef        | 0.381    |
|    ent_coef_loss   | -0.0121  |
|    learning_rate   | 0.0003   |
|    n_updates       | 405499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.95e+03 |
|    critic_loss     | 1.94e+03 |
|    ent_coef        | 0.373    |
|    ent_coef_loss   | 0.0174   |
|    learning_rate   | 0.0003   |
|    n_updates       | 405899   |
---------------------------------
=== Iterazione IRL 737 ===
Loss reward (iter 737): 4.107281684875488
=== Iterazione IRL 738 ===
Loss reward (iter 738): 3.390065908432007
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.95e+03 |
|    critic_loss     | 1.74e+03 |
|    ent_coef        | 0.376    |
|    ent_coef_loss   | 0.095    |
|    learning_rate   | 0.0003   |
|    n_updates       | 406199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.95e+03 |
|    critic_loss     | 2.17e+03 |
|    ent_coef        | 0.375    |
|    ent_coef_loss   | 0.183    |
|    learning_rate   | 0.0003   |
|    n_updates       | 406599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2e+03    |
|    critic_loss     | 2.5e+03  |
|    ent_coef        | 0.397    |
|    ent_coef_loss   | 0.284    |
|    learning_rate   | 0.0003   |
|    n_updates       | 406999   |
---------------------------------
=== Iterazione IRL 739 ===
Loss reward (iter 739): 4.805521011352539
=== Iterazione IRL 740 ===
Loss reward (iter 740): 2.8667783737182617
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.98e+03 |
|    critic_loss     | 2.19e+03 |
|    ent_coef        | 0.39     |
|    ent_coef_loss   | -0.254   |
|    learning_rate   | 0.0003   |
|    n_updates       | 407299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.98e+03 |
|    critic_loss     | 2.12e+03 |
|    ent_coef        | 0.389    |
|    ent_coef_loss   | -0.202   |
|    learning_rate   | 0.0003   |
|    n_updates       | 407699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.01e+03 |
|    critic_loss     | 2.21e+03 |
|    ent_coef        | 0.411    |
|    ent_coef_loss   | 0.281    |
|    learning_rate   | 0.0003   |
|    n_updates       | 408099   |
---------------------------------
=== Iterazione IRL 741 ===
Loss reward (iter 741): 1.9840741157531738
=== Iterazione IRL 742 ===
Loss reward (iter 742): 3.2418370246887207
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.97e+03 |
|    critic_loss     | 1.82e+03 |
|    ent_coef        | 0.415    |
|    ent_coef_loss   | 0.273    |
|    learning_rate   | 0.0003   |
|    n_updates       | 408399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.97e+03 |
|    critic_loss     | 1.99e+03 |
|    ent_coef        | 0.44     |
|    ent_coef_loss   | 0.125    |
|    learning_rate   | 0.0003   |
|    n_updates       | 408799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.98e+03 |
|    critic_loss     | 1.94e+03 |
|    ent_coef        | 0.44     |
|    ent_coef_loss   | -0.192   |
|    learning_rate   | 0.0003   |
|    n_updates       | 409199   |
---------------------------------
=== Iterazione IRL 743 ===
Loss reward (iter 743): 3.9507107734680176
=== Iterazione IRL 744 ===
Loss reward (iter 744): 3.8545913696289062
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.95e+03 |
|    critic_loss     | 1.81e+03 |
|    ent_coef        | 0.424    |
|    ent_coef_loss   | -0.347   |
|    learning_rate   | 0.0003   |
|    n_updates       | 409499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.99e+03 |
|    critic_loss     | 2.66e+03 |
|    ent_coef        | 0.414    |
|    ent_coef_loss   | 0.265    |
|    learning_rate   | 0.0003   |
|    n_updates       | 409899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.96e+03 |
|    critic_loss     | 1.88e+03 |
|    ent_coef        | 0.408    |
|    ent_coef_loss   | -0.295   |
|    learning_rate   | 0.0003   |
|    n_updates       | 410299   |
---------------------------------
=== Iterazione IRL 745 ===
Loss reward (iter 745): 2.9396743774414062
=== Iterazione IRL 746 ===
Loss reward (iter 746): 3.0592291355133057
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.99e+03 |
|    critic_loss     | 2.23e+03 |
|    ent_coef        | 0.405    |
|    ent_coef_loss   | -0.0313  |
|    learning_rate   | 0.0003   |
|    n_updates       | 410599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.98e+03 |
|    critic_loss     | 2.08e+03 |
|    ent_coef        | 0.402    |
|    ent_coef_loss   | -0.0685  |
|    learning_rate   | 0.0003   |
|    n_updates       | 410999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.99e+03 |
|    critic_loss     | 1.97e+03 |
|    ent_coef        | 0.394    |
|    ent_coef_loss   | -0.0242  |
|    learning_rate   | 0.0003   |
|    n_updates       | 411399   |
---------------------------------
=== Iterazione IRL 747 ===
Loss reward (iter 747): 3.22525691986084
=== Iterazione IRL 748 ===
Loss reward (iter 748): 4.968914985656738
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.99e+03 |
|    critic_loss     | 2.13e+03 |
|    ent_coef        | 0.385    |
|    ent_coef_loss   | -0.0456  |
|    learning_rate   | 0.0003   |
|    n_updates       | 411699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.99e+03 |
|    critic_loss     | 2.44e+03 |
|    ent_coef        | 0.389    |
|    ent_coef_loss   | 0.0258   |
|    learning_rate   | 0.0003   |
|    n_updates       | 412099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.99e+03 |
|    critic_loss     | 2.38e+03 |
|    ent_coef        | 0.369    |
|    ent_coef_loss   | 0.086    |
|    learning_rate   | 0.0003   |
|    n_updates       | 412499   |
---------------------------------
=== Iterazione IRL 749 ===
Loss reward (iter 749): 4.064878463745117
=== Iterazione IRL 750 ===
Loss reward (iter 750): 3.4613239765167236
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.99e+03 |
|    critic_loss     | 1.8e+03  |
|    ent_coef        | 0.37     |
|    ent_coef_loss   | 0.207    |
|    learning_rate   | 0.0003   |
|    n_updates       | 412799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2e+03    |
|    critic_loss     | 2.1e+03  |
|    ent_coef        | 0.387    |
|    ent_coef_loss   | 0.3      |
|    learning_rate   | 0.0003   |
|    n_updates       | 413199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2e+03    |
|    critic_loss     | 2.37e+03 |
|    ent_coef        | 0.384    |
|    ent_coef_loss   | 0.182    |
|    learning_rate   | 0.0003   |
|    n_updates       | 413599   |
---------------------------------
=== Iterazione IRL 751 ===
Loss reward (iter 751): 3.727853298187256
=== Iterazione IRL 752 ===
Loss reward (iter 752): 2.1340489387512207
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.97e+03 |
|    critic_loss     | 2.04e+03 |
|    ent_coef        | 0.383    |
|    ent_coef_loss   | 0.0957   |
|    learning_rate   | 0.0003   |
|    n_updates       | 413899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.99e+03 |
|    critic_loss     | 2.2e+03  |
|    ent_coef        | 0.376    |
|    ent_coef_loss   | 0.0716   |
|    learning_rate   | 0.0003   |
|    n_updates       | 414299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.02e+03 |
|    critic_loss     | 2.89e+03 |
|    ent_coef        | 0.377    |
|    ent_coef_loss   | 0.165    |
|    learning_rate   | 0.0003   |
|    n_updates       | 414699   |
---------------------------------
=== Iterazione IRL 753 ===
Loss reward (iter 753): 3.471742630004883
=== Iterazione IRL 754 ===
Loss reward (iter 754): 3.4597201347351074
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2e+03    |
|    critic_loss     | 2.36e+03 |
|    ent_coef        | 0.378    |
|    ent_coef_loss   | -0.35    |
|    learning_rate   | 0.0003   |
|    n_updates       | 414999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.99e+03 |
|    critic_loss     | 1.89e+03 |
|    ent_coef        | 0.361    |
|    ent_coef_loss   | -0.371   |
|    learning_rate   | 0.0003   |
|    n_updates       | 415399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.01e+03 |
|    critic_loss     | 2.62e+03 |
|    ent_coef        | 0.366    |
|    ent_coef_loss   | -0.221   |
|    learning_rate   | 0.0003   |
|    n_updates       | 415799   |
---------------------------------
=== Iterazione IRL 755 ===
Loss reward (iter 755): 3.8584344387054443
=== Iterazione IRL 756 ===
Loss reward (iter 756): 4.595198631286621
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.03e+03 |
|    critic_loss     | 1.8e+03  |
|    ent_coef        | 0.353    |
|    ent_coef_loss   | 0.227    |
|    learning_rate   | 0.0003   |
|    n_updates       | 416099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.01e+03 |
|    critic_loss     | 2.19e+03 |
|    ent_coef        | 0.36     |
|    ent_coef_loss   | -0.428   |
|    learning_rate   | 0.0003   |
|    n_updates       | 416499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.01e+03 |
|    critic_loss     | 2.42e+03 |
|    ent_coef        | 0.351    |
|    ent_coef_loss   | 0.118    |
|    learning_rate   | 0.0003   |
|    n_updates       | 416899   |
---------------------------------
=== Iterazione IRL 757 ===
Loss reward (iter 757): 2.69064998626709
=== Iterazione IRL 758 ===
Loss reward (iter 758): 3.4220523834228516
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.99e+03 |
|    critic_loss     | 2e+03    |
|    ent_coef        | 0.349    |
|    ent_coef_loss   | 0.0582   |
|    learning_rate   | 0.0003   |
|    n_updates       | 417199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.06e+03 |
|    critic_loss     | 2.43e+03 |
|    ent_coef        | 0.363    |
|    ent_coef_loss   | 0.423    |
|    learning_rate   | 0.0003   |
|    n_updates       | 417599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.98e+03 |
|    critic_loss     | 2.39e+03 |
|    ent_coef        | 0.374    |
|    ent_coef_loss   | -0.266   |
|    learning_rate   | 0.0003   |
|    n_updates       | 417999   |
---------------------------------
=== Iterazione IRL 759 ===
Loss reward (iter 759): 3.3507399559020996
=== Iterazione IRL 760 ===
Loss reward (iter 760): 3.2951149940490723
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.01e+03 |
|    critic_loss     | 2.43e+03 |
|    ent_coef        | 0.378    |
|    ent_coef_loss   | 0.165    |
|    learning_rate   | 0.0003   |
|    n_updates       | 418299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.01e+03 |
|    critic_loss     | 2e+03    |
|    ent_coef        | 0.359    |
|    ent_coef_loss   | -0.146   |
|    learning_rate   | 0.0003   |
|    n_updates       | 418699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.99e+03 |
|    critic_loss     | 1.52e+03 |
|    ent_coef        | 0.364    |
|    ent_coef_loss   | 0.0912   |
|    learning_rate   | 0.0003   |
|    n_updates       | 419099   |
---------------------------------
=== Iterazione IRL 761 ===
Loss reward (iter 761): 3.2739639282226562
=== Iterazione IRL 762 ===
Loss reward (iter 762): 3.0592408180236816
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.03e+03 |
|    critic_loss     | 2.43e+03 |
|    ent_coef        | 0.371    |
|    ent_coef_loss   | -0.0472  |
|    learning_rate   | 0.0003   |
|    n_updates       | 419399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.05e+03 |
|    critic_loss     | 2.08e+03 |
|    ent_coef        | 0.375    |
|    ent_coef_loss   | 0.322    |
|    learning_rate   | 0.0003   |
|    n_updates       | 419799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.02e+03 |
|    critic_loss     | 2.8e+03  |
|    ent_coef        | 0.37     |
|    ent_coef_loss   | 0.00424  |
|    learning_rate   | 0.0003   |
|    n_updates       | 420199   |
---------------------------------
=== Iterazione IRL 763 ===
Loss reward (iter 763): 2.6570069789886475
=== Iterazione IRL 764 ===
Loss reward (iter 764): 2.951629638671875
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.01e+03 |
|    critic_loss     | 2.48e+03 |
|    ent_coef        | 0.372    |
|    ent_coef_loss   | 0.302    |
|    learning_rate   | 0.0003   |
|    n_updates       | 420499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.02e+03 |
|    critic_loss     | 2.39e+03 |
|    ent_coef        | 0.383    |
|    ent_coef_loss   | 0.11     |
|    learning_rate   | 0.0003   |
|    n_updates       | 420899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2e+03    |
|    critic_loss     | 2.26e+03 |
|    ent_coef        | 0.376    |
|    ent_coef_loss   | 0.011    |
|    learning_rate   | 0.0003   |
|    n_updates       | 421299   |
---------------------------------
=== Iterazione IRL 765 ===
Loss reward (iter 765): 3.9687023162841797
=== Iterazione IRL 766 ===
Loss reward (iter 766): 3.685854434967041
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.01e+03 |
|    critic_loss     | 2.43e+03 |
|    ent_coef        | 0.38     |
|    ent_coef_loss   | -0.0431  |
|    learning_rate   | 0.0003   |
|    n_updates       | 421599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.04e+03 |
|    critic_loss     | 2.37e+03 |
|    ent_coef        | 0.377    |
|    ent_coef_loss   | 0.0345   |
|    learning_rate   | 0.0003   |
|    n_updates       | 421999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.03e+03 |
|    critic_loss     | 2e+03    |
|    ent_coef        | 0.382    |
|    ent_coef_loss   | 0.039    |
|    learning_rate   | 0.0003   |
|    n_updates       | 422399   |
---------------------------------
=== Iterazione IRL 767 ===
Loss reward (iter 767): 5.00996732711792
=== Iterazione IRL 768 ===
Loss reward (iter 768): 3.3946971893310547
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.02e+03 |
|    critic_loss     | 1.98e+03 |
|    ent_coef        | 0.389    |
|    ent_coef_loss   | -0.0659  |
|    learning_rate   | 0.0003   |
|    n_updates       | 422699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.01e+03 |
|    critic_loss     | 1.82e+03 |
|    ent_coef        | 0.394    |
|    ent_coef_loss   | -0.113   |
|    learning_rate   | 0.0003   |
|    n_updates       | 423099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.03e+03 |
|    critic_loss     | 2.45e+03 |
|    ent_coef        | 0.385    |
|    ent_coef_loss   | 0.113    |
|    learning_rate   | 0.0003   |
|    n_updates       | 423499   |
---------------------------------
=== Iterazione IRL 769 ===
Loss reward (iter 769): 3.8254261016845703
=== Iterazione IRL 770 ===
Loss reward (iter 770): 3.6254210472106934
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.01e+03 |
|    critic_loss     | 2.24e+03 |
|    ent_coef        | 0.383    |
|    ent_coef_loss   | 0.289    |
|    learning_rate   | 0.0003   |
|    n_updates       | 423799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.02e+03 |
|    critic_loss     | 2.57e+03 |
|    ent_coef        | 0.376    |
|    ent_coef_loss   | 0.0929   |
|    learning_rate   | 0.0003   |
|    n_updates       | 424199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.03e+03 |
|    critic_loss     | 1.78e+03 |
|    ent_coef        | 0.405    |
|    ent_coef_loss   | 0.649    |
|    learning_rate   | 0.0003   |
|    n_updates       | 424599   |
---------------------------------
=== Iterazione IRL 771 ===
Loss reward (iter 771): 3.207793712615967
=== Iterazione IRL 772 ===
Loss reward (iter 772): 3.212475538253784
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.06e+03 |
|    critic_loss     | 3.01e+03 |
|    ent_coef        | 0.401    |
|    ent_coef_loss   | -0.271   |
|    learning_rate   | 0.0003   |
|    n_updates       | 424899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.04e+03 |
|    critic_loss     | 2.44e+03 |
|    ent_coef        | 0.395    |
|    ent_coef_loss   | 0.342    |
|    learning_rate   | 0.0003   |
|    n_updates       | 425299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.03e+03 |
|    critic_loss     | 1.98e+03 |
|    ent_coef        | 0.407    |
|    ent_coef_loss   | -0.0138  |
|    learning_rate   | 0.0003   |
|    n_updates       | 425699   |
---------------------------------
=== Iterazione IRL 773 ===
Loss reward (iter 773): 2.799224376678467
=== Iterazione IRL 774 ===
Loss reward (iter 774): 2.378878355026245
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.02e+03 |
|    critic_loss     | 1.94e+03 |
|    ent_coef        | 0.393    |
|    ent_coef_loss   | -0.0103  |
|    learning_rate   | 0.0003   |
|    n_updates       | 425999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.03e+03 |
|    critic_loss     | 2.44e+03 |
|    ent_coef        | 0.406    |
|    ent_coef_loss   | 0.0214   |
|    learning_rate   | 0.0003   |
|    n_updates       | 426399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.07e+03 |
|    critic_loss     | 1.92e+03 |
|    ent_coef        | 0.409    |
|    ent_coef_loss   | -0.285   |
|    learning_rate   | 0.0003   |
|    n_updates       | 426799   |
---------------------------------
=== Iterazione IRL 775 ===
Loss reward (iter 775): 3.775763750076294
=== Iterazione IRL 776 ===
Loss reward (iter 776): 3.628422737121582
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.05e+03 |
|    critic_loss     | 2.12e+03 |
|    ent_coef        | 0.405    |
|    ent_coef_loss   | 0.0524   |
|    learning_rate   | 0.0003   |
|    n_updates       | 427099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.02e+03 |
|    critic_loss     | 1.59e+03 |
|    ent_coef        | 0.397    |
|    ent_coef_loss   | 0.34     |
|    learning_rate   | 0.0003   |
|    n_updates       | 427499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.04e+03 |
|    critic_loss     | 2.28e+03 |
|    ent_coef        | 0.394    |
|    ent_coef_loss   | -0.0214  |
|    learning_rate   | 0.0003   |
|    n_updates       | 427899   |
---------------------------------
=== Iterazione IRL 777 ===
Loss reward (iter 777): 3.430877685546875
=== Iterazione IRL 778 ===
Loss reward (iter 778): 3.3929836750030518
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.04e+03 |
|    critic_loss     | 2.36e+03 |
|    ent_coef        | 0.391    |
|    ent_coef_loss   | -0.428   |
|    learning_rate   | 0.0003   |
|    n_updates       | 428199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.08e+03 |
|    critic_loss     | 3.3e+03  |
|    ent_coef        | 0.392    |
|    ent_coef_loss   | -0.069   |
|    learning_rate   | 0.0003   |
|    n_updates       | 428599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.04e+03 |
|    critic_loss     | 1.98e+03 |
|    ent_coef        | 0.399    |
|    ent_coef_loss   | -0.361   |
|    learning_rate   | 0.0003   |
|    n_updates       | 428999   |
---------------------------------
=== Iterazione IRL 779 ===
Loss reward (iter 779): 3.0403225421905518
=== Iterazione IRL 780 ===
Loss reward (iter 780): 3.3854458332061768
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.03e+03 |
|    critic_loss     | 2.28e+03 |
|    ent_coef        | 0.388    |
|    ent_coef_loss   | 0.0464   |
|    learning_rate   | 0.0003   |
|    n_updates       | 429299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.04e+03 |
|    critic_loss     | 2.1e+03  |
|    ent_coef        | 0.387    |
|    ent_coef_loss   | 0.162    |
|    learning_rate   | 0.0003   |
|    n_updates       | 429699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.06e+03 |
|    critic_loss     | 1.97e+03 |
|    ent_coef        | 0.385    |
|    ent_coef_loss   | 0.25     |
|    learning_rate   | 0.0003   |
|    n_updates       | 430099   |
---------------------------------
=== Iterazione IRL 781 ===
Loss reward (iter 781): 2.9894702434539795
=== Iterazione IRL 782 ===
Loss reward (iter 782): 3.308143377304077
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.03e+03 |
|    critic_loss     | 1.85e+03 |
|    ent_coef        | 0.407    |
|    ent_coef_loss   | 0.212    |
|    learning_rate   | 0.0003   |
|    n_updates       | 430399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.01e+03 |
|    critic_loss     | 1.91e+03 |
|    ent_coef        | 0.417    |
|    ent_coef_loss   | 0.146    |
|    learning_rate   | 0.0003   |
|    n_updates       | 430799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.04e+03 |
|    critic_loss     | 1.81e+03 |
|    ent_coef        | 0.406    |
|    ent_coef_loss   | -0.0349  |
|    learning_rate   | 0.0003   |
|    n_updates       | 431199   |
---------------------------------
=== Iterazione IRL 783 ===
Loss reward (iter 783): 2.362586259841919
=== Iterazione IRL 784 ===
Loss reward (iter 784): 2.5324742794036865
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.04e+03 |
|    critic_loss     | 1.93e+03 |
|    ent_coef        | 0.397    |
|    ent_coef_loss   | -0.0193  |
|    learning_rate   | 0.0003   |
|    n_updates       | 431499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.06e+03 |
|    critic_loss     | 2e+03    |
|    ent_coef        | 0.395    |
|    ent_coef_loss   | 0.0718   |
|    learning_rate   | 0.0003   |
|    n_updates       | 431899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.06e+03 |
|    critic_loss     | 2.2e+03  |
|    ent_coef        | 0.398    |
|    ent_coef_loss   | 0.211    |
|    learning_rate   | 0.0003   |
|    n_updates       | 432299   |
---------------------------------
=== Iterazione IRL 785 ===
Loss reward (iter 785): 4.094038486480713
=== Iterazione IRL 786 ===
Loss reward (iter 786): 3.8507938385009766
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.04e+03 |
|    critic_loss     | 2.36e+03 |
|    ent_coef        | 0.401    |
|    ent_coef_loss   | 0.169    |
|    learning_rate   | 0.0003   |
|    n_updates       | 432599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.03e+03 |
|    critic_loss     | 1.94e+03 |
|    ent_coef        | 0.393    |
|    ent_coef_loss   | 0.0267   |
|    learning_rate   | 0.0003   |
|    n_updates       | 432999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.03e+03 |
|    critic_loss     | 1.71e+03 |
|    ent_coef        | 0.418    |
|    ent_coef_loss   | -0.0218  |
|    learning_rate   | 0.0003   |
|    n_updates       | 433399   |
---------------------------------
=== Iterazione IRL 787 ===
Loss reward (iter 787): 3.331827163696289
=== Iterazione IRL 788 ===
Loss reward (iter 788): 2.0958306789398193
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.04e+03 |
|    critic_loss     | 2.04e+03 |
|    ent_coef        | 0.423    |
|    ent_coef_loss   | 0.413    |
|    learning_rate   | 0.0003   |
|    n_updates       | 433699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.03e+03 |
|    critic_loss     | 1.75e+03 |
|    ent_coef        | 0.423    |
|    ent_coef_loss   | -0.283   |
|    learning_rate   | 0.0003   |
|    n_updates       | 434099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.06e+03 |
|    critic_loss     | 1.76e+03 |
|    ent_coef        | 0.413    |
|    ent_coef_loss   | 0.103    |
|    learning_rate   | 0.0003   |
|    n_updates       | 434499   |
---------------------------------
=== Iterazione IRL 789 ===
Loss reward (iter 789): 2.5901198387145996
=== Iterazione IRL 790 ===
Loss reward (iter 790): 2.555314540863037
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.07e+03 |
|    critic_loss     | 1.99e+03 |
|    ent_coef        | 0.411    |
|    ent_coef_loss   | -0.15    |
|    learning_rate   | 0.0003   |
|    n_updates       | 434799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.05e+03 |
|    critic_loss     | 2.12e+03 |
|    ent_coef        | 0.411    |
|    ent_coef_loss   | 0.0425   |
|    learning_rate   | 0.0003   |
|    n_updates       | 435199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.08e+03 |
|    critic_loss     | 2.53e+03 |
|    ent_coef        | 0.415    |
|    ent_coef_loss   | -0.0962  |
|    learning_rate   | 0.0003   |
|    n_updates       | 435599   |
---------------------------------
=== Iterazione IRL 791 ===
Loss reward (iter 791): 4.777444362640381
=== Iterazione IRL 792 ===
Loss reward (iter 792): 3.614100933074951
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.04e+03 |
|    critic_loss     | 2.32e+03 |
|    ent_coef        | 0.411    |
|    ent_coef_loss   | 0.0354   |
|    learning_rate   | 0.0003   |
|    n_updates       | 435899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.06e+03 |
|    critic_loss     | 2.01e+03 |
|    ent_coef        | 0.402    |
|    ent_coef_loss   | 0.141    |
|    learning_rate   | 0.0003   |
|    n_updates       | 436299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.08e+03 |
|    critic_loss     | 2.57e+03 |
|    ent_coef        | 0.412    |
|    ent_coef_loss   | 0.403    |
|    learning_rate   | 0.0003   |
|    n_updates       | 436699   |
---------------------------------
=== Iterazione IRL 793 ===
Loss reward (iter 793): 4.492166519165039
=== Iterazione IRL 794 ===
Loss reward (iter 794): 3.9205918312072754
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.06e+03 |
|    critic_loss     | 2.11e+03 |
|    ent_coef        | 0.415    |
|    ent_coef_loss   | -0.0331  |
|    learning_rate   | 0.0003   |
|    n_updates       | 436999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.05e+03 |
|    critic_loss     | 1.81e+03 |
|    ent_coef        | 0.405    |
|    ent_coef_loss   | 0.211    |
|    learning_rate   | 0.0003   |
|    n_updates       | 437399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.05e+03 |
|    critic_loss     | 2.35e+03 |
|    ent_coef        | 0.415    |
|    ent_coef_loss   | -0.182   |
|    learning_rate   | 0.0003   |
|    n_updates       | 437799   |
---------------------------------
=== Iterazione IRL 795 ===
Loss reward (iter 795): 3.296602725982666
=== Iterazione IRL 796 ===
Loss reward (iter 796): 4.058805465698242
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.08e+03 |
|    critic_loss     | 1.46e+03 |
|    ent_coef        | 0.417    |
|    ent_coef_loss   | -0.114   |
|    learning_rate   | 0.0003   |
|    n_updates       | 438099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.07e+03 |
|    critic_loss     | 2.28e+03 |
|    ent_coef        | 0.418    |
|    ent_coef_loss   | -0.169   |
|    learning_rate   | 0.0003   |
|    n_updates       | 438499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.07e+03 |
|    critic_loss     | 2.44e+03 |
|    ent_coef        | 0.411    |
|    ent_coef_loss   | -0.0231  |
|    learning_rate   | 0.0003   |
|    n_updates       | 438899   |
---------------------------------
=== Iterazione IRL 797 ===
Loss reward (iter 797): 3.4053940773010254
=== Iterazione IRL 798 ===
Loss reward (iter 798): 3.4332141876220703
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.08e+03 |
|    critic_loss     | 2.54e+03 |
|    ent_coef        | 0.424    |
|    ent_coef_loss   | 0.219    |
|    learning_rate   | 0.0003   |
|    n_updates       | 439199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.07e+03 |
|    critic_loss     | 2.38e+03 |
|    ent_coef        | 0.421    |
|    ent_coef_loss   | 0.0441   |
|    learning_rate   | 0.0003   |
|    n_updates       | 439599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.08e+03 |
|    critic_loss     | 2.37e+03 |
|    ent_coef        | 0.422    |
|    ent_coef_loss   | -0.0846  |
|    learning_rate   | 0.0003   |
|    n_updates       | 439999   |
---------------------------------
=== Iterazione IRL 799 ===
Loss reward (iter 799): 3.593904495239258
=== Iterazione IRL 800 ===
Loss reward (iter 800): 2.9888267517089844
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.06e+03 |
|    critic_loss     | 1.85e+03 |
|    ent_coef        | 0.42     |
|    ent_coef_loss   | -0.117   |
|    learning_rate   | 0.0003   |
|    n_updates       | 440299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.06e+03 |
|    critic_loss     | 1.65e+03 |
|    ent_coef        | 0.415    |
|    ent_coef_loss   | -0.205   |
|    learning_rate   | 0.0003   |
|    n_updates       | 440699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.12e+03 |
|    critic_loss     | 3.08e+03 |
|    ent_coef        | 0.41     |
|    ent_coef_loss   | 0.159    |
|    learning_rate   | 0.0003   |
|    n_updates       | 441099   |
---------------------------------
=== Iterazione IRL 801 ===
Loss reward (iter 801): 3.562729835510254
=== Iterazione IRL 802 ===
Loss reward (iter 802): 4.240467548370361
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.08e+03 |
|    critic_loss     | 2.48e+03 |
|    ent_coef        | 0.415    |
|    ent_coef_loss   | 0.0214   |
|    learning_rate   | 0.0003   |
|    n_updates       | 441399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.07e+03 |
|    critic_loss     | 2.19e+03 |
|    ent_coef        | 0.394    |
|    ent_coef_loss   | -0.0646  |
|    learning_rate   | 0.0003   |
|    n_updates       | 441799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.07e+03 |
|    critic_loss     | 2.55e+03 |
|    ent_coef        | 0.402    |
|    ent_coef_loss   | -0.0929  |
|    learning_rate   | 0.0003   |
|    n_updates       | 442199   |
---------------------------------
=== Iterazione IRL 803 ===
Loss reward (iter 803): 3.7194631099700928
=== Iterazione IRL 804 ===
Loss reward (iter 804): 3.0971145629882812
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.07e+03 |
|    critic_loss     | 2.15e+03 |
|    ent_coef        | 0.417    |
|    ent_coef_loss   | -0.0061  |
|    learning_rate   | 0.0003   |
|    n_updates       | 442499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.08e+03 |
|    critic_loss     | 1.95e+03 |
|    ent_coef        | 0.417    |
|    ent_coef_loss   | -0.118   |
|    learning_rate   | 0.0003   |
|    n_updates       | 442899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.09e+03 |
|    critic_loss     | 2.29e+03 |
|    ent_coef        | 0.431    |
|    ent_coef_loss   | 0.058    |
|    learning_rate   | 0.0003   |
|    n_updates       | 443299   |
---------------------------------
=== Iterazione IRL 805 ===
Loss reward (iter 805): 3.9338748455047607
=== Iterazione IRL 806 ===
Loss reward (iter 806): 3.877946615219116
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.09e+03 |
|    critic_loss     | 2.17e+03 |
|    ent_coef        | 0.443    |
|    ent_coef_loss   | 0.00121  |
|    learning_rate   | 0.0003   |
|    n_updates       | 443599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.06e+03 |
|    critic_loss     | 1.54e+03 |
|    ent_coef        | 0.438    |
|    ent_coef_loss   | 0.0305   |
|    learning_rate   | 0.0003   |
|    n_updates       | 443999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.09e+03 |
|    critic_loss     | 1.91e+03 |
|    ent_coef        | 0.43     |
|    ent_coef_loss   | 0.0126   |
|    learning_rate   | 0.0003   |
|    n_updates       | 444399   |
---------------------------------
=== Iterazione IRL 807 ===
Loss reward (iter 807): 3.621270179748535
=== Iterazione IRL 808 ===
Loss reward (iter 808): 3.5866427421569824
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.08e+03 |
|    critic_loss     | 2.77e+03 |
|    ent_coef        | 0.424    |
|    ent_coef_loss   | 0.187    |
|    learning_rate   | 0.0003   |
|    n_updates       | 444699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.09e+03 |
|    critic_loss     | 2.74e+03 |
|    ent_coef        | 0.421    |
|    ent_coef_loss   | -0.163   |
|    learning_rate   | 0.0003   |
|    n_updates       | 445099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.09e+03 |
|    critic_loss     | 2.52e+03 |
|    ent_coef        | 0.426    |
|    ent_coef_loss   | -0.143   |
|    learning_rate   | 0.0003   |
|    n_updates       | 445499   |
---------------------------------
=== Iterazione IRL 809 ===
Loss reward (iter 809): 3.6988329887390137
=== Iterazione IRL 810 ===
Loss reward (iter 810): 3.126620054244995
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.07e+03 |
|    critic_loss     | 1.86e+03 |
|    ent_coef        | 0.444    |
|    ent_coef_loss   | -0.0596  |
|    learning_rate   | 0.0003   |
|    n_updates       | 445799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.09e+03 |
|    critic_loss     | 2.85e+03 |
|    ent_coef        | 0.46     |
|    ent_coef_loss   | -0.101   |
|    learning_rate   | 0.0003   |
|    n_updates       | 446199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.07e+03 |
|    critic_loss     | 2.02e+03 |
|    ent_coef        | 0.46     |
|    ent_coef_loss   | -0.0457  |
|    learning_rate   | 0.0003   |
|    n_updates       | 446599   |
---------------------------------
=== Iterazione IRL 811 ===
Loss reward (iter 811): 2.9832563400268555
=== Iterazione IRL 812 ===
Loss reward (iter 812): 2.782146692276001
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.09e+03 |
|    critic_loss     | 1.99e+03 |
|    ent_coef        | 0.447    |
|    ent_coef_loss   | -0.101   |
|    learning_rate   | 0.0003   |
|    n_updates       | 446899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.11e+03 |
|    critic_loss     | 1.96e+03 |
|    ent_coef        | 0.431    |
|    ent_coef_loss   | -0.147   |
|    learning_rate   | 0.0003   |
|    n_updates       | 447299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.08e+03 |
|    critic_loss     | 2.32e+03 |
|    ent_coef        | 0.415    |
|    ent_coef_loss   | 0.0498   |
|    learning_rate   | 0.0003   |
|    n_updates       | 447699   |
---------------------------------
=== Iterazione IRL 813 ===
Loss reward (iter 813): 2.9969189167022705
=== Iterazione IRL 814 ===
Loss reward (iter 814): 3.5624263286590576
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.1e+03  |
|    critic_loss     | 2.4e+03  |
|    ent_coef        | 0.434    |
|    ent_coef_loss   | 0.125    |
|    learning_rate   | 0.0003   |
|    n_updates       | 447999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.11e+03 |
|    critic_loss     | 1.92e+03 |
|    ent_coef        | 0.434    |
|    ent_coef_loss   | 0.0392   |
|    learning_rate   | 0.0003   |
|    n_updates       | 448399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.07e+03 |
|    critic_loss     | 1.69e+03 |
|    ent_coef        | 0.444    |
|    ent_coef_loss   | -0.00875 |
|    learning_rate   | 0.0003   |
|    n_updates       | 448799   |
---------------------------------
=== Iterazione IRL 815 ===
Loss reward (iter 815): 3.4383788108825684
=== Iterazione IRL 816 ===
Loss reward (iter 816): 3.199805736541748
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.1e+03  |
|    critic_loss     | 1.9e+03  |
|    ent_coef        | 0.454    |
|    ent_coef_loss   | 0.114    |
|    learning_rate   | 0.0003   |
|    n_updates       | 449099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.1e+03  |
|    critic_loss     | 3.78e+03 |
|    ent_coef        | 0.451    |
|    ent_coef_loss   | -0.0253  |
|    learning_rate   | 0.0003   |
|    n_updates       | 449499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.1e+03  |
|    critic_loss     | 1.78e+03 |
|    ent_coef        | 0.443    |
|    ent_coef_loss   | -0.112   |
|    learning_rate   | 0.0003   |
|    n_updates       | 449899   |
---------------------------------
=== Iterazione IRL 817 ===
Loss reward (iter 817): 3.141862392425537
=== Iterazione IRL 818 ===
Loss reward (iter 818): 2.4392731189727783
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.1e+03  |
|    critic_loss     | 3.65e+03 |
|    ent_coef        | 0.433    |
|    ent_coef_loss   | -0.0429  |
|    learning_rate   | 0.0003   |
|    n_updates       | 450199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.09e+03 |
|    critic_loss     | 2.27e+03 |
|    ent_coef        | 0.428    |
|    ent_coef_loss   | -0.0734  |
|    learning_rate   | 0.0003   |
|    n_updates       | 450599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.1e+03  |
|    critic_loss     | 2.23e+03 |
|    ent_coef        | 0.431    |
|    ent_coef_loss   | -0.0748  |
|    learning_rate   | 0.0003   |
|    n_updates       | 450999   |
---------------------------------
=== Iterazione IRL 819 ===
Loss reward (iter 819): 4.159705638885498
=== Iterazione IRL 820 ===
Loss reward (iter 820): 3.4359018802642822
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.09e+03 |
|    critic_loss     | 2.04e+03 |
|    ent_coef        | 0.431    |
|    ent_coef_loss   | -0.204   |
|    learning_rate   | 0.0003   |
|    n_updates       | 451299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.13e+03 |
|    critic_loss     | 2.7e+03  |
|    ent_coef        | 0.44     |
|    ent_coef_loss   | 0.0441   |
|    learning_rate   | 0.0003   |
|    n_updates       | 451699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.1e+03  |
|    critic_loss     | 2.2e+03  |
|    ent_coef        | 0.425    |
|    ent_coef_loss   | 0.0931   |
|    learning_rate   | 0.0003   |
|    n_updates       | 452099   |
---------------------------------
=== Iterazione IRL 821 ===
Loss reward (iter 821): 3.7066690921783447
=== Iterazione IRL 822 ===
Loss reward (iter 822): 2.573251962661743
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.1e+03  |
|    critic_loss     | 2.21e+03 |
|    ent_coef        | 0.426    |
|    ent_coef_loss   | 0.19     |
|    learning_rate   | 0.0003   |
|    n_updates       | 452399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.1e+03  |
|    critic_loss     | 2.35e+03 |
|    ent_coef        | 0.432    |
|    ent_coef_loss   | 0.129    |
|    learning_rate   | 0.0003   |
|    n_updates       | 452799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.11e+03 |
|    critic_loss     | 1.98e+03 |
|    ent_coef        | 0.447    |
|    ent_coef_loss   | -0.085   |
|    learning_rate   | 0.0003   |
|    n_updates       | 453199   |
---------------------------------
=== Iterazione IRL 823 ===
Loss reward (iter 823): 3.391711711883545
=== Iterazione IRL 824 ===
Loss reward (iter 824): 4.107590675354004
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.07e+03 |
|    critic_loss     | 2.1e+03  |
|    ent_coef        | 0.431    |
|    ent_coef_loss   | 0.0741   |
|    learning_rate   | 0.0003   |
|    n_updates       | 453499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.09e+03 |
|    critic_loss     | 1.87e+03 |
|    ent_coef        | 0.445    |
|    ent_coef_loss   | 0.00684  |
|    learning_rate   | 0.0003   |
|    n_updates       | 453899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.08e+03 |
|    critic_loss     | 2.36e+03 |
|    ent_coef        | 0.45     |
|    ent_coef_loss   | 0.164    |
|    learning_rate   | 0.0003   |
|    n_updates       | 454299   |
---------------------------------
=== Iterazione IRL 825 ===
Loss reward (iter 825): 2.5493288040161133
=== Iterazione IRL 826 ===
Loss reward (iter 826): 4.890279293060303
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.12e+03 |
|    critic_loss     | 2.28e+03 |
|    ent_coef        | 0.43     |
|    ent_coef_loss   | -0.476   |
|    learning_rate   | 0.0003   |
|    n_updates       | 454599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.06e+03 |
|    critic_loss     | 1.83e+03 |
|    ent_coef        | 0.439    |
|    ent_coef_loss   | -0.171   |
|    learning_rate   | 0.0003   |
|    n_updates       | 454999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.11e+03 |
|    critic_loss     | 2.47e+03 |
|    ent_coef        | 0.441    |
|    ent_coef_loss   | -0.211   |
|    learning_rate   | 0.0003   |
|    n_updates       | 455399   |
---------------------------------
=== Iterazione IRL 827 ===
Loss reward (iter 827): 3.6592495441436768
=== Iterazione IRL 828 ===
Loss reward (iter 828): 3.643101453781128
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.09e+03 |
|    critic_loss     | 2.3e+03  |
|    ent_coef        | 0.429    |
|    ent_coef_loss   | 0.0514   |
|    learning_rate   | 0.0003   |
|    n_updates       | 455699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.07e+03 |
|    critic_loss     | 2.16e+03 |
|    ent_coef        | 0.427    |
|    ent_coef_loss   | -0.3     |
|    learning_rate   | 0.0003   |
|    n_updates       | 456099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.09e+03 |
|    critic_loss     | 2.1e+03  |
|    ent_coef        | 0.422    |
|    ent_coef_loss   | -0.342   |
|    learning_rate   | 0.0003   |
|    n_updates       | 456499   |
---------------------------------
=== Iterazione IRL 829 ===
Loss reward (iter 829): 4.3762688636779785
=== Iterazione IRL 830 ===
Loss reward (iter 830): 3.415071487426758
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.08e+03 |
|    critic_loss     | 2.08e+03 |
|    ent_coef        | 0.421    |
|    ent_coef_loss   | -0.286   |
|    learning_rate   | 0.0003   |
|    n_updates       | 456799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.11e+03 |
|    critic_loss     | 2.4e+03  |
|    ent_coef        | 0.42     |
|    ent_coef_loss   | 0.27     |
|    learning_rate   | 0.0003   |
|    n_updates       | 457199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.07e+03 |
|    critic_loss     | 2.16e+03 |
|    ent_coef        | 0.424    |
|    ent_coef_loss   | 0.155    |
|    learning_rate   | 0.0003   |
|    n_updates       | 457599   |
---------------------------------
=== Iterazione IRL 831 ===
Loss reward (iter 831): 3.467954158782959
=== Iterazione IRL 832 ===
Loss reward (iter 832): 3.503560781478882
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.09e+03 |
|    critic_loss     | 2.12e+03 |
|    ent_coef        | 0.426    |
|    ent_coef_loss   | -0.108   |
|    learning_rate   | 0.0003   |
|    n_updates       | 457899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.1e+03  |
|    critic_loss     | 2.12e+03 |
|    ent_coef        | 0.424    |
|    ent_coef_loss   | 0.0323   |
|    learning_rate   | 0.0003   |
|    n_updates       | 458299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.14e+03 |
|    critic_loss     | 2.4e+03  |
|    ent_coef        | 0.414    |
|    ent_coef_loss   | 0.122    |
|    learning_rate   | 0.0003   |
|    n_updates       | 458699   |
---------------------------------
=== Iterazione IRL 833 ===
Loss reward (iter 833): 3.51060152053833
=== Iterazione IRL 834 ===
Loss reward (iter 834): 3.229043960571289
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.12e+03 |
|    critic_loss     | 2.1e+03  |
|    ent_coef        | 0.409    |
|    ent_coef_loss   | -0.0069  |
|    learning_rate   | 0.0003   |
|    n_updates       | 458999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.15e+03 |
|    critic_loss     | 2.73e+03 |
|    ent_coef        | 0.396    |
|    ent_coef_loss   | 0.0659   |
|    learning_rate   | 0.0003   |
|    n_updates       | 459399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.09e+03 |
|    critic_loss     | 2.46e+03 |
|    ent_coef        | 0.41     |
|    ent_coef_loss   | 0.29     |
|    learning_rate   | 0.0003   |
|    n_updates       | 459799   |
---------------------------------
=== Iterazione IRL 835 ===
Loss reward (iter 835): 3.874793529510498
=== Iterazione IRL 836 ===
Loss reward (iter 836): 3.9620532989501953
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.09e+03 |
|    critic_loss     | 2.12e+03 |
|    ent_coef        | 0.406    |
|    ent_coef_loss   | -0.0709  |
|    learning_rate   | 0.0003   |
|    n_updates       | 460099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.12e+03 |
|    critic_loss     | 2.66e+03 |
|    ent_coef        | 0.414    |
|    ent_coef_loss   | -0.0345  |
|    learning_rate   | 0.0003   |
|    n_updates       | 460499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.09e+03 |
|    critic_loss     | 1.97e+03 |
|    ent_coef        | 0.411    |
|    ent_coef_loss   | -0.0475  |
|    learning_rate   | 0.0003   |
|    n_updates       | 460899   |
---------------------------------
=== Iterazione IRL 837 ===
Loss reward (iter 837): 3.5202603340148926
=== Iterazione IRL 838 ===
Loss reward (iter 838): 2.5702357292175293
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.14e+03 |
|    critic_loss     | 2.65e+03 |
|    ent_coef        | 0.427    |
|    ent_coef_loss   | -0.0856  |
|    learning_rate   | 0.0003   |
|    n_updates       | 461199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.1e+03  |
|    critic_loss     | 2.19e+03 |
|    ent_coef        | 0.425    |
|    ent_coef_loss   | 0.0038   |
|    learning_rate   | 0.0003   |
|    n_updates       | 461599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.1e+03  |
|    critic_loss     | 2.2e+03  |
|    ent_coef        | 0.429    |
|    ent_coef_loss   | 0.191    |
|    learning_rate   | 0.0003   |
|    n_updates       | 461999   |
---------------------------------
=== Iterazione IRL 839 ===
Loss reward (iter 839): 3.4362032413482666
=== Iterazione IRL 840 ===
Loss reward (iter 840): 3.925966739654541
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.11e+03 |
|    critic_loss     | 2.42e+03 |
|    ent_coef        | 0.433    |
|    ent_coef_loss   | -0.217   |
|    learning_rate   | 0.0003   |
|    n_updates       | 462299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.12e+03 |
|    critic_loss     | 2.78e+03 |
|    ent_coef        | 0.434    |
|    ent_coef_loss   | -0.184   |
|    learning_rate   | 0.0003   |
|    n_updates       | 462699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.1e+03  |
|    critic_loss     | 1.84e+03 |
|    ent_coef        | 0.433    |
|    ent_coef_loss   | 0.14     |
|    learning_rate   | 0.0003   |
|    n_updates       | 463099   |
---------------------------------
=== Iterazione IRL 841 ===
Loss reward (iter 841): 3.439439058303833
=== Iterazione IRL 842 ===
Loss reward (iter 842): 4.003223419189453
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.08e+03 |
|    critic_loss     | 1.51e+03 |
|    ent_coef        | 0.433    |
|    ent_coef_loss   | -0.164   |
|    learning_rate   | 0.0003   |
|    n_updates       | 463399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.14e+03 |
|    critic_loss     | 2.63e+03 |
|    ent_coef        | 0.447    |
|    ent_coef_loss   | -0.0688  |
|    learning_rate   | 0.0003   |
|    n_updates       | 463799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.13e+03 |
|    critic_loss     | 2.21e+03 |
|    ent_coef        | 0.439    |
|    ent_coef_loss   | 0.0764   |
|    learning_rate   | 0.0003   |
|    n_updates       | 464199   |
---------------------------------
=== Iterazione IRL 843 ===
Loss reward (iter 843): 1.7801008224487305
=== Iterazione IRL 844 ===
Loss reward (iter 844): 4.041558265686035
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.16e+03 |
|    critic_loss     | 2.21e+03 |
|    ent_coef        | 0.429    |
|    ent_coef_loss   | -0.0605  |
|    learning_rate   | 0.0003   |
|    n_updates       | 464499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.12e+03 |
|    critic_loss     | 2.23e+03 |
|    ent_coef        | 0.425    |
|    ent_coef_loss   | 0.135    |
|    learning_rate   | 0.0003   |
|    n_updates       | 464899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.13e+03 |
|    critic_loss     | 1.94e+03 |
|    ent_coef        | 0.42     |
|    ent_coef_loss   | -0.111   |
|    learning_rate   | 0.0003   |
|    n_updates       | 465299   |
---------------------------------
=== Iterazione IRL 845 ===
Loss reward (iter 845): 3.9914207458496094
=== Iterazione IRL 846 ===
Loss reward (iter 846): 3.066330909729004
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.1e+03  |
|    critic_loss     | 2.12e+03 |
|    ent_coef        | 0.433    |
|    ent_coef_loss   | -0.129   |
|    learning_rate   | 0.0003   |
|    n_updates       | 465599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.11e+03 |
|    critic_loss     | 2.05e+03 |
|    ent_coef        | 0.433    |
|    ent_coef_loss   | 0.0561   |
|    learning_rate   | 0.0003   |
|    n_updates       | 465999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.11e+03 |
|    critic_loss     | 2.43e+03 |
|    ent_coef        | 0.439    |
|    ent_coef_loss   | 0.385    |
|    learning_rate   | 0.0003   |
|    n_updates       | 466399   |
---------------------------------
=== Iterazione IRL 847 ===
Loss reward (iter 847): 3.7789177894592285
=== Iterazione IRL 848 ===
Loss reward (iter 848): 4.040616989135742
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.12e+03 |
|    critic_loss     | 2.08e+03 |
|    ent_coef        | 0.451    |
|    ent_coef_loss   | 0.0327   |
|    learning_rate   | 0.0003   |
|    n_updates       | 466699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.13e+03 |
|    critic_loss     | 2.74e+03 |
|    ent_coef        | 0.447    |
|    ent_coef_loss   | 0.465    |
|    learning_rate   | 0.0003   |
|    n_updates       | 467099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.08e+03 |
|    critic_loss     | 2.41e+03 |
|    ent_coef        | 0.47     |
|    ent_coef_loss   | 0.0657   |
|    learning_rate   | 0.0003   |
|    n_updates       | 467499   |
---------------------------------
=== Iterazione IRL 849 ===
Loss reward (iter 849): 5.2671356201171875
=== Iterazione IRL 850 ===
Loss reward (iter 850): 4.374806880950928
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.1e+03  |
|    critic_loss     | 2.2e+03  |
|    ent_coef        | 0.479    |
|    ent_coef_loss   | -0.163   |
|    learning_rate   | 0.0003   |
|    n_updates       | 467799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.15e+03 |
|    critic_loss     | 2.84e+03 |
|    ent_coef        | 0.467    |
|    ent_coef_loss   | 0.11     |
|    learning_rate   | 0.0003   |
|    n_updates       | 468199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.11e+03 |
|    critic_loss     | 2.07e+03 |
|    ent_coef        | 0.469    |
|    ent_coef_loss   | -0.0219  |
|    learning_rate   | 0.0003   |
|    n_updates       | 468599   |
---------------------------------
=== Iterazione IRL 851 ===
Loss reward (iter 851): 3.1663365364074707
=== Iterazione IRL 852 ===
Loss reward (iter 852): 4.721077919006348
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.13e+03 |
|    critic_loss     | 1.54e+03 |
|    ent_coef        | 0.477    |
|    ent_coef_loss   | -0.111   |
|    learning_rate   | 0.0003   |
|    n_updates       | 468899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.12e+03 |
|    critic_loss     | 1.67e+03 |
|    ent_coef        | 0.495    |
|    ent_coef_loss   | 0.0607   |
|    learning_rate   | 0.0003   |
|    n_updates       | 469299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.11e+03 |
|    critic_loss     | 2.13e+03 |
|    ent_coef        | 0.497    |
|    ent_coef_loss   | -0.0402  |
|    learning_rate   | 0.0003   |
|    n_updates       | 469699   |
---------------------------------
=== Iterazione IRL 853 ===
Loss reward (iter 853): 3.4817914962768555
=== Iterazione IRL 854 ===
Loss reward (iter 854): 2.984318733215332
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.13e+03 |
|    critic_loss     | 2.65e+03 |
|    ent_coef        | 0.494    |
|    ent_coef_loss   | -0.0553  |
|    learning_rate   | 0.0003   |
|    n_updates       | 469999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.13e+03 |
|    critic_loss     | 2.57e+03 |
|    ent_coef        | 0.511    |
|    ent_coef_loss   | 0.123    |
|    learning_rate   | 0.0003   |
|    n_updates       | 470399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.12e+03 |
|    critic_loss     | 2.3e+03  |
|    ent_coef        | 0.496    |
|    ent_coef_loss   | -0.133   |
|    learning_rate   | 0.0003   |
|    n_updates       | 470799   |
---------------------------------
=== Iterazione IRL 855 ===
Loss reward (iter 855): 3.827023983001709
=== Iterazione IRL 856 ===
Loss reward (iter 856): 3.8186793327331543
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.14e+03 |
|    critic_loss     | 2.08e+03 |
|    ent_coef        | 0.508    |
|    ent_coef_loss   | 0.187    |
|    learning_rate   | 0.0003   |
|    n_updates       | 471099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.11e+03 |
|    critic_loss     | 2.33e+03 |
|    ent_coef        | 0.517    |
|    ent_coef_loss   | -0.146   |
|    learning_rate   | 0.0003   |
|    n_updates       | 471499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.12e+03 |
|    critic_loss     | 2.01e+03 |
|    ent_coef        | 0.509    |
|    ent_coef_loss   | -0.069   |
|    learning_rate   | 0.0003   |
|    n_updates       | 471899   |
---------------------------------
=== Iterazione IRL 857 ===
Loss reward (iter 857): 3.9105961322784424
=== Iterazione IRL 858 ===
Loss reward (iter 858): 3.9624223709106445
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.13e+03 |
|    critic_loss     | 2.98e+03 |
|    ent_coef        | 0.518    |
|    ent_coef_loss   | 0.185    |
|    learning_rate   | 0.0003   |
|    n_updates       | 472199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.13e+03 |
|    critic_loss     | 2.67e+03 |
|    ent_coef        | 0.502    |
|    ent_coef_loss   | -0.0328  |
|    learning_rate   | 0.0003   |
|    n_updates       | 472599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.15e+03 |
|    critic_loss     | 2.34e+03 |
|    ent_coef        | 0.503    |
|    ent_coef_loss   | -0.173   |
|    learning_rate   | 0.0003   |
|    n_updates       | 472999   |
---------------------------------
=== Iterazione IRL 859 ===
Loss reward (iter 859): 3.484821081161499
=== Iterazione IRL 860 ===
Loss reward (iter 860): 2.7996506690979004
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.12e+03 |
|    critic_loss     | 2.13e+03 |
|    ent_coef        | 0.494    |
|    ent_coef_loss   | -0.349   |
|    learning_rate   | 0.0003   |
|    n_updates       | 473299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.12e+03 |
|    critic_loss     | 2.25e+03 |
|    ent_coef        | 0.481    |
|    ent_coef_loss   | 0.082    |
|    learning_rate   | 0.0003   |
|    n_updates       | 473699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.11e+03 |
|    critic_loss     | 2.2e+03  |
|    ent_coef        | 0.493    |
|    ent_coef_loss   | -0.0114  |
|    learning_rate   | 0.0003   |
|    n_updates       | 474099   |
---------------------------------
=== Iterazione IRL 861 ===
Loss reward (iter 861): 3.0674726963043213
=== Iterazione IRL 862 ===
Loss reward (iter 862): 3.8887617588043213
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.13e+03 |
|    critic_loss     | 2.94e+03 |
|    ent_coef        | 0.481    |
|    ent_coef_loss   | -0.282   |
|    learning_rate   | 0.0003   |
|    n_updates       | 474399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.13e+03 |
|    critic_loss     | 2.97e+03 |
|    ent_coef        | 0.477    |
|    ent_coef_loss   | -0.0736  |
|    learning_rate   | 0.0003   |
|    n_updates       | 474799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.19e+03 |
|    critic_loss     | 2.99e+03 |
|    ent_coef        | 0.477    |
|    ent_coef_loss   | 0.276    |
|    learning_rate   | 0.0003   |
|    n_updates       | 475199   |
---------------------------------
=== Iterazione IRL 863 ===
Loss reward (iter 863): 2.417503595352173
=== Iterazione IRL 864 ===
Loss reward (iter 864): 2.6013238430023193
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.13e+03 |
|    critic_loss     | 2.63e+03 |
|    ent_coef        | 0.482    |
|    ent_coef_loss   | -0.165   |
|    learning_rate   | 0.0003   |
|    n_updates       | 475499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.12e+03 |
|    critic_loss     | 2.53e+03 |
|    ent_coef        | 0.478    |
|    ent_coef_loss   | -0.107   |
|    learning_rate   | 0.0003   |
|    n_updates       | 475899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.15e+03 |
|    critic_loss     | 2.49e+03 |
|    ent_coef        | 0.471    |
|    ent_coef_loss   | 0.0845   |
|    learning_rate   | 0.0003   |
|    n_updates       | 476299   |
---------------------------------
=== Iterazione IRL 865 ===
Loss reward (iter 865): 3.087402820587158
=== Iterazione IRL 866 ===
Loss reward (iter 866): 3.686687707901001
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.13e+03 |
|    critic_loss     | 2.67e+03 |
|    ent_coef        | 0.477    |
|    ent_coef_loss   | 0.015    |
|    learning_rate   | 0.0003   |
|    n_updates       | 476599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.14e+03 |
|    critic_loss     | 2.27e+03 |
|    ent_coef        | 0.461    |
|    ent_coef_loss   | 0.0749   |
|    learning_rate   | 0.0003   |
|    n_updates       | 476999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.17e+03 |
|    critic_loss     | 2.47e+03 |
|    ent_coef        | 0.448    |
|    ent_coef_loss   | -0.203   |
|    learning_rate   | 0.0003   |
|    n_updates       | 477399   |
---------------------------------
=== Iterazione IRL 867 ===
Loss reward (iter 867): 3.671847343444824
=== Iterazione IRL 868 ===
Loss reward (iter 868): 3.425334930419922
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.17e+03 |
|    critic_loss     | 2.07e+03 |
|    ent_coef        | 0.442    |
|    ent_coef_loss   | 0.0668   |
|    learning_rate   | 0.0003   |
|    n_updates       | 477699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.15e+03 |
|    critic_loss     | 1.8e+03  |
|    ent_coef        | 0.448    |
|    ent_coef_loss   | -0.296   |
|    learning_rate   | 0.0003   |
|    n_updates       | 478099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.16e+03 |
|    critic_loss     | 2.36e+03 |
|    ent_coef        | 0.459    |
|    ent_coef_loss   | 0.000999 |
|    learning_rate   | 0.0003   |
|    n_updates       | 478499   |
---------------------------------
=== Iterazione IRL 869 ===
Loss reward (iter 869): 3.4980883598327637
=== Iterazione IRL 870 ===
Loss reward (iter 870): 3.4751830101013184
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.15e+03 |
|    critic_loss     | 2.52e+03 |
|    ent_coef        | 0.463    |
|    ent_coef_loss   | -0.0998  |
|    learning_rate   | 0.0003   |
|    n_updates       | 478799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.14e+03 |
|    critic_loss     | 2.64e+03 |
|    ent_coef        | 0.465    |
|    ent_coef_loss   | -0.112   |
|    learning_rate   | 0.0003   |
|    n_updates       | 479199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.14e+03 |
|    critic_loss     | 3.24e+03 |
|    ent_coef        | 0.471    |
|    ent_coef_loss   | -0.167   |
|    learning_rate   | 0.0003   |
|    n_updates       | 479599   |
---------------------------------
=== Iterazione IRL 871 ===
Loss reward (iter 871): 3.8702118396759033
=== Iterazione IRL 872 ===
Loss reward (iter 872): 3.3040270805358887
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.16e+03 |
|    critic_loss     | 2.64e+03 |
|    ent_coef        | 0.476    |
|    ent_coef_loss   | 0.0294   |
|    learning_rate   | 0.0003   |
|    n_updates       | 479899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.15e+03 |
|    critic_loss     | 2.5e+03  |
|    ent_coef        | 0.471    |
|    ent_coef_loss   | 0.121    |
|    learning_rate   | 0.0003   |
|    n_updates       | 480299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.13e+03 |
|    critic_loss     | 1.81e+03 |
|    ent_coef        | 0.479    |
|    ent_coef_loss   | 0.17     |
|    learning_rate   | 0.0003   |
|    n_updates       | 480699   |
---------------------------------
=== Iterazione IRL 873 ===
Loss reward (iter 873): 2.1106817722320557
=== Iterazione IRL 874 ===
Loss reward (iter 874): 2.7664074897766113
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.19e+03 |
|    critic_loss     | 2.32e+03 |
|    ent_coef        | 0.478    |
|    ent_coef_loss   | 0.294    |
|    learning_rate   | 0.0003   |
|    n_updates       | 480999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.16e+03 |
|    critic_loss     | 2.09e+03 |
|    ent_coef        | 0.471    |
|    ent_coef_loss   | 0.00486  |
|    learning_rate   | 0.0003   |
|    n_updates       | 481399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.15e+03 |
|    critic_loss     | 2.42e+03 |
|    ent_coef        | 0.473    |
|    ent_coef_loss   | 0.203    |
|    learning_rate   | 0.0003   |
|    n_updates       | 481799   |
---------------------------------
=== Iterazione IRL 875 ===
Loss reward (iter 875): 3.095503807067871
=== Iterazione IRL 876 ===
Loss reward (iter 876): 2.9352993965148926
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.12e+03 |
|    critic_loss     | 2.55e+03 |
|    ent_coef        | 0.472    |
|    ent_coef_loss   | -0.225   |
|    learning_rate   | 0.0003   |
|    n_updates       | 482099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.15e+03 |
|    critic_loss     | 1.91e+03 |
|    ent_coef        | 0.46     |
|    ent_coef_loss   | -0.365   |
|    learning_rate   | 0.0003   |
|    n_updates       | 482499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.12e+03 |
|    critic_loss     | 2.22e+03 |
|    ent_coef        | 0.446    |
|    ent_coef_loss   | -0.07    |
|    learning_rate   | 0.0003   |
|    n_updates       | 482899   |
---------------------------------
=== Iterazione IRL 877 ===
Loss reward (iter 877): 4.238808631896973
=== Iterazione IRL 878 ===
Loss reward (iter 878): 4.217394828796387
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.15e+03 |
|    critic_loss     | 2.79e+03 |
|    ent_coef        | 0.439    |
|    ent_coef_loss   | -0.0245  |
|    learning_rate   | 0.0003   |
|    n_updates       | 483199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.18e+03 |
|    critic_loss     | 1.98e+03 |
|    ent_coef        | 0.451    |
|    ent_coef_loss   | -0.321   |
|    learning_rate   | 0.0003   |
|    n_updates       | 483599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.14e+03 |
|    critic_loss     | 1.96e+03 |
|    ent_coef        | 0.466    |
|    ent_coef_loss   | -0.0208  |
|    learning_rate   | 0.0003   |
|    n_updates       | 483999   |
---------------------------------
=== Iterazione IRL 879 ===
Loss reward (iter 879): 4.372044563293457
=== Iterazione IRL 880 ===
Loss reward (iter 880): 4.23507022857666
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.16e+03 |
|    critic_loss     | 2.62e+03 |
|    ent_coef        | 0.465    |
|    ent_coef_loss   | -0.137   |
|    learning_rate   | 0.0003   |
|    n_updates       | 484299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.14e+03 |
|    critic_loss     | 2.27e+03 |
|    ent_coef        | 0.462    |
|    ent_coef_loss   | 0.279    |
|    learning_rate   | 0.0003   |
|    n_updates       | 484699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.17e+03 |
|    critic_loss     | 2.6e+03  |
|    ent_coef        | 0.456    |
|    ent_coef_loss   | -0.0307  |
|    learning_rate   | 0.0003   |
|    n_updates       | 485099   |
---------------------------------
=== Iterazione IRL 881 ===
Loss reward (iter 881): 4.028438091278076
=== Iterazione IRL 882 ===
Loss reward (iter 882): 4.656955242156982
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.12e+03 |
|    critic_loss     | 2.15e+03 |
|    ent_coef        | 0.45     |
|    ent_coef_loss   | -0.198   |
|    learning_rate   | 0.0003   |
|    n_updates       | 485399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.15e+03 |
|    critic_loss     | 2.15e+03 |
|    ent_coef        | 0.45     |
|    ent_coef_loss   | 0.139    |
|    learning_rate   | 0.0003   |
|    n_updates       | 485799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.16e+03 |
|    critic_loss     | 1.92e+03 |
|    ent_coef        | 0.462    |
|    ent_coef_loss   | -0.0534  |
|    learning_rate   | 0.0003   |
|    n_updates       | 486199   |
---------------------------------
=== Iterazione IRL 883 ===
Loss reward (iter 883): 4.380044937133789
=== Iterazione IRL 884 ===
Loss reward (iter 884): 3.8539540767669678
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.13e+03 |
|    critic_loss     | 2.11e+03 |
|    ent_coef        | 0.468    |
|    ent_coef_loss   | -0.0535  |
|    learning_rate   | 0.0003   |
|    n_updates       | 486499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.18e+03 |
|    critic_loss     | 2.11e+03 |
|    ent_coef        | 0.48     |
|    ent_coef_loss   | 0.0368   |
|    learning_rate   | 0.0003   |
|    n_updates       | 486899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.14e+03 |
|    critic_loss     | 2.25e+03 |
|    ent_coef        | 0.48     |
|    ent_coef_loss   | -0.103   |
|    learning_rate   | 0.0003   |
|    n_updates       | 487299   |
---------------------------------
=== Iterazione IRL 885 ===
Loss reward (iter 885): 3.743631362915039
=== Iterazione IRL 886 ===
Loss reward (iter 886): 3.621286630630493
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.17e+03 |
|    critic_loss     | 1.98e+03 |
|    ent_coef        | 0.47     |
|    ent_coef_loss   | 0.0367   |
|    learning_rate   | 0.0003   |
|    n_updates       | 487599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.18e+03 |
|    critic_loss     | 2.2e+03  |
|    ent_coef        | 0.463    |
|    ent_coef_loss   | 0.134    |
|    learning_rate   | 0.0003   |
|    n_updates       | 487999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.15e+03 |
|    critic_loss     | 1.94e+03 |
|    ent_coef        | 0.465    |
|    ent_coef_loss   | -0.0331  |
|    learning_rate   | 0.0003   |
|    n_updates       | 488399   |
---------------------------------
=== Iterazione IRL 887 ===
Loss reward (iter 887): 3.567246913909912
=== Iterazione IRL 888 ===
Loss reward (iter 888): 3.3743999004364014
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.14e+03 |
|    critic_loss     | 1.75e+03 |
|    ent_coef        | 0.462    |
|    ent_coef_loss   | -0.075   |
|    learning_rate   | 0.0003   |
|    n_updates       | 488699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.13e+03 |
|    critic_loss     | 2.24e+03 |
|    ent_coef        | 0.457    |
|    ent_coef_loss   | -0.0409  |
|    learning_rate   | 0.0003   |
|    n_updates       | 489099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.14e+03 |
|    critic_loss     | 1.97e+03 |
|    ent_coef        | 0.463    |
|    ent_coef_loss   | -0.105   |
|    learning_rate   | 0.0003   |
|    n_updates       | 489499   |
---------------------------------
=== Iterazione IRL 889 ===
Loss reward (iter 889): 2.8247640132904053
=== Iterazione IRL 890 ===
Loss reward (iter 890): 3.752326011657715
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.15e+03 |
|    critic_loss     | 2.49e+03 |
|    ent_coef        | 0.458    |
|    ent_coef_loss   | -0.295   |
|    learning_rate   | 0.0003   |
|    n_updates       | 489799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.17e+03 |
|    critic_loss     | 2.02e+03 |
|    ent_coef        | 0.447    |
|    ent_coef_loss   | -0.0765  |
|    learning_rate   | 0.0003   |
|    n_updates       | 490199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.17e+03 |
|    critic_loss     | 2.21e+03 |
|    ent_coef        | 0.441    |
|    ent_coef_loss   | -0.0212  |
|    learning_rate   | 0.0003   |
|    n_updates       | 490599   |
---------------------------------
=== Iterazione IRL 891 ===
Loss reward (iter 891): 2.7745795249938965
=== Iterazione IRL 892 ===
Loss reward (iter 892): 3.763885736465454
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.2e+03  |
|    critic_loss     | 2.22e+03 |
|    ent_coef        | 0.452    |
|    ent_coef_loss   | 0.13     |
|    learning_rate   | 0.0003   |
|    n_updates       | 490899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.17e+03 |
|    critic_loss     | 2.18e+03 |
|    ent_coef        | 0.461    |
|    ent_coef_loss   | 0.0599   |
|    learning_rate   | 0.0003   |
|    n_updates       | 491299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.13e+03 |
|    critic_loss     | 2.29e+03 |
|    ent_coef        | 0.46     |
|    ent_coef_loss   | -0.0106  |
|    learning_rate   | 0.0003   |
|    n_updates       | 491699   |
---------------------------------
=== Iterazione IRL 893 ===
Loss reward (iter 893): 3.4574174880981445
=== Iterazione IRL 894 ===
Loss reward (iter 894): 3.5611512660980225
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.16e+03 |
|    critic_loss     | 2.01e+03 |
|    ent_coef        | 0.465    |
|    ent_coef_loss   | -0.0247  |
|    learning_rate   | 0.0003   |
|    n_updates       | 491999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.14e+03 |
|    critic_loss     | 1.8e+03  |
|    ent_coef        | 0.454    |
|    ent_coef_loss   | -0.0565  |
|    learning_rate   | 0.0003   |
|    n_updates       | 492399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.16e+03 |
|    critic_loss     | 2.04e+03 |
|    ent_coef        | 0.464    |
|    ent_coef_loss   | -0.0496  |
|    learning_rate   | 0.0003   |
|    n_updates       | 492799   |
---------------------------------
=== Iterazione IRL 895 ===
Loss reward (iter 895): 3.265024185180664
=== Iterazione IRL 896 ===
Loss reward (iter 896): 3.72072172164917
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.18e+03 |
|    critic_loss     | 1.97e+03 |
|    ent_coef        | 0.475    |
|    ent_coef_loss   | 0.0308   |
|    learning_rate   | 0.0003   |
|    n_updates       | 493099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.15e+03 |
|    critic_loss     | 1.94e+03 |
|    ent_coef        | 0.48     |
|    ent_coef_loss   | -0.00906 |
|    learning_rate   | 0.0003   |
|    n_updates       | 493499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.13e+03 |
|    critic_loss     | 2.16e+03 |
|    ent_coef        | 0.473    |
|    ent_coef_loss   | 0.0893   |
|    learning_rate   | 0.0003   |
|    n_updates       | 493899   |
---------------------------------
=== Iterazione IRL 897 ===
Loss reward (iter 897): 4.972411632537842
=== Iterazione IRL 898 ===
Loss reward (iter 898): 4.122655868530273
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.15e+03 |
|    critic_loss     | 2.24e+03 |
|    ent_coef        | 0.467    |
|    ent_coef_loss   | -0.0507  |
|    learning_rate   | 0.0003   |
|    n_updates       | 494199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.18e+03 |
|    critic_loss     | 2.53e+03 |
|    ent_coef        | 0.468    |
|    ent_coef_loss   | 0.192    |
|    learning_rate   | 0.0003   |
|    n_updates       | 494599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.14e+03 |
|    critic_loss     | 3.02e+03 |
|    ent_coef        | 0.469    |
|    ent_coef_loss   | 0.0747   |
|    learning_rate   | 0.0003   |
|    n_updates       | 494999   |
---------------------------------
=== Iterazione IRL 899 ===
Loss reward (iter 899): 4.241731643676758
=== Iterazione IRL 900 ===
Loss reward (iter 900): 4.155994415283203
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.15e+03 |
|    critic_loss     | 2.01e+03 |
|    ent_coef        | 0.466    |
|    ent_coef_loss   | -0.245   |
|    learning_rate   | 0.0003   |
|    n_updates       | 495299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.15e+03 |
|    critic_loss     | 1.78e+03 |
|    ent_coef        | 0.46     |
|    ent_coef_loss   | -0.0591  |
|    learning_rate   | 0.0003   |
|    n_updates       | 495699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.17e+03 |
|    critic_loss     | 2.1e+03  |
|    ent_coef        | 0.463    |
|    ent_coef_loss   | 0.0552   |
|    learning_rate   | 0.0003   |
|    n_updates       | 496099   |
---------------------------------
=== Iterazione IRL 901 ===
Loss reward (iter 901): 4.24211311340332
=== Iterazione IRL 902 ===
Loss reward (iter 902): 3.8004701137542725
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.16e+03 |
|    critic_loss     | 1.94e+03 |
|    ent_coef        | 0.475    |
|    ent_coef_loss   | -0.0267  |
|    learning_rate   | 0.0003   |
|    n_updates       | 496399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.16e+03 |
|    critic_loss     | 1.69e+03 |
|    ent_coef        | 0.474    |
|    ent_coef_loss   | -0.142   |
|    learning_rate   | 0.0003   |
|    n_updates       | 496799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.15e+03 |
|    critic_loss     | 2.37e+03 |
|    ent_coef        | 0.472    |
|    ent_coef_loss   | -0.114   |
|    learning_rate   | 0.0003   |
|    n_updates       | 497199   |
---------------------------------
=== Iterazione IRL 903 ===
Loss reward (iter 903): 3.378084897994995
=== Iterazione IRL 904 ===
Loss reward (iter 904): 4.540353298187256
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.16e+03 |
|    critic_loss     | 1.96e+03 |
|    ent_coef        | 0.479    |
|    ent_coef_loss   | 0.188    |
|    learning_rate   | 0.0003   |
|    n_updates       | 497499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.15e+03 |
|    critic_loss     | 2.34e+03 |
|    ent_coef        | 0.477    |
|    ent_coef_loss   | 0.206    |
|    learning_rate   | 0.0003   |
|    n_updates       | 497899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.17e+03 |
|    critic_loss     | 1.82e+03 |
|    ent_coef        | 0.483    |
|    ent_coef_loss   | 0.105    |
|    learning_rate   | 0.0003   |
|    n_updates       | 498299   |
---------------------------------
=== Iterazione IRL 905 ===
Loss reward (iter 905): 3.928776979446411
=== Iterazione IRL 906 ===
Loss reward (iter 906): 4.716662406921387
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.13e+03 |
|    critic_loss     | 1.82e+03 |
|    ent_coef        | 0.487    |
|    ent_coef_loss   | -0.0978  |
|    learning_rate   | 0.0003   |
|    n_updates       | 498599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.16e+03 |
|    critic_loss     | 2e+03    |
|    ent_coef        | 0.478    |
|    ent_coef_loss   | 0.00039  |
|    learning_rate   | 0.0003   |
|    n_updates       | 498999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.17e+03 |
|    critic_loss     | 1.83e+03 |
|    ent_coef        | 0.467    |
|    ent_coef_loss   | 0.0557   |
|    learning_rate   | 0.0003   |
|    n_updates       | 499399   |
---------------------------------
=== Iterazione IRL 907 ===
Loss reward (iter 907): 4.2819437980651855
=== Iterazione IRL 908 ===
Loss reward (iter 908): 4.119429111480713
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.16e+03 |
|    critic_loss     | 1.66e+03 |
|    ent_coef        | 0.464    |
|    ent_coef_loss   | 0.109    |
|    learning_rate   | 0.0003   |
|    n_updates       | 499699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.18e+03 |
|    critic_loss     | 2.09e+03 |
|    ent_coef        | 0.45     |
|    ent_coef_loss   | 0.0793   |
|    learning_rate   | 0.0003   |
|    n_updates       | 500099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.15e+03 |
|    critic_loss     | 2.66e+03 |
|    ent_coef        | 0.449    |
|    ent_coef_loss   | -0.116   |
|    learning_rate   | 0.0003   |
|    n_updates       | 500499   |
---------------------------------
=== Iterazione IRL 909 ===
Loss reward (iter 909): 3.5616681575775146
=== Iterazione IRL 910 ===
Loss reward (iter 910): 3.9137251377105713
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.15e+03 |
|    critic_loss     | 1.77e+03 |
|    ent_coef        | 0.453    |
|    ent_coef_loss   | 0.124    |
|    learning_rate   | 0.0003   |
|    n_updates       | 500799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.16e+03 |
|    critic_loss     | 2.18e+03 |
|    ent_coef        | 0.465    |
|    ent_coef_loss   | -0.0523  |
|    learning_rate   | 0.0003   |
|    n_updates       | 501199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.16e+03 |
|    critic_loss     | 2.36e+03 |
|    ent_coef        | 0.462    |
|    ent_coef_loss   | -0.243   |
|    learning_rate   | 0.0003   |
|    n_updates       | 501599   |
---------------------------------
=== Iterazione IRL 911 ===
Loss reward (iter 911): 3.132591485977173
=== Iterazione IRL 912 ===
Loss reward (iter 912): 3.71349835395813
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.16e+03 |
|    critic_loss     | 1.93e+03 |
|    ent_coef        | 0.449    |
|    ent_coef_loss   | -0.129   |
|    learning_rate   | 0.0003   |
|    n_updates       | 501899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.16e+03 |
|    critic_loss     | 2.17e+03 |
|    ent_coef        | 0.453    |
|    ent_coef_loss   | 0.0799   |
|    learning_rate   | 0.0003   |
|    n_updates       | 502299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.18e+03 |
|    critic_loss     | 1.99e+03 |
|    ent_coef        | 0.447    |
|    ent_coef_loss   | -0.0181  |
|    learning_rate   | 0.0003   |
|    n_updates       | 502699   |
---------------------------------
=== Iterazione IRL 913 ===
Loss reward (iter 913): 4.377318859100342
=== Iterazione IRL 914 ===
Loss reward (iter 914): 3.8780012130737305
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.18e+03 |
|    critic_loss     | 2.41e+03 |
|    ent_coef        | 0.447    |
|    ent_coef_loss   | 0.132    |
|    learning_rate   | 0.0003   |
|    n_updates       | 502999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.15e+03 |
|    critic_loss     | 1.79e+03 |
|    ent_coef        | 0.449    |
|    ent_coef_loss   | 0.0445   |
|    learning_rate   | 0.0003   |
|    n_updates       | 503399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.19e+03 |
|    critic_loss     | 2.01e+03 |
|    ent_coef        | 0.459    |
|    ent_coef_loss   | -0.0678  |
|    learning_rate   | 0.0003   |
|    n_updates       | 503799   |
---------------------------------
=== Iterazione IRL 915 ===
Loss reward (iter 915): 4.384000778198242
=== Iterazione IRL 916 ===
Loss reward (iter 916): 4.102983474731445
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.19e+03 |
|    critic_loss     | 1.8e+03  |
|    ent_coef        | 0.46     |
|    ent_coef_loss   | 0.0509   |
|    learning_rate   | 0.0003   |
|    n_updates       | 504099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.19e+03 |
|    critic_loss     | 1.94e+03 |
|    ent_coef        | 0.464    |
|    ent_coef_loss   | 0.0966   |
|    learning_rate   | 0.0003   |
|    n_updates       | 504499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.16e+03 |
|    critic_loss     | 1.74e+03 |
|    ent_coef        | 0.474    |
|    ent_coef_loss   | 0.00711  |
|    learning_rate   | 0.0003   |
|    n_updates       | 504899   |
---------------------------------
=== Iterazione IRL 917 ===
Loss reward (iter 917): 4.382371425628662
=== Iterazione IRL 918 ===
Loss reward (iter 918): 3.9436800479888916
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.18e+03 |
|    critic_loss     | 1.82e+03 |
|    ent_coef        | 0.474    |
|    ent_coef_loss   | -0.0578  |
|    learning_rate   | 0.0003   |
|    n_updates       | 505199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.17e+03 |
|    critic_loss     | 2.05e+03 |
|    ent_coef        | 0.47     |
|    ent_coef_loss   | -0.113   |
|    learning_rate   | 0.0003   |
|    n_updates       | 505599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.2e+03  |
|    critic_loss     | 2.25e+03 |
|    ent_coef        | 0.472    |
|    ent_coef_loss   | -0.178   |
|    learning_rate   | 0.0003   |
|    n_updates       | 505999   |
---------------------------------
=== Iterazione IRL 919 ===
Loss reward (iter 919): 3.693763494491577
=== Iterazione IRL 920 ===
Loss reward (iter 920): 4.060849189758301
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.19e+03 |
|    critic_loss     | 2e+03    |
|    ent_coef        | 0.472    |
|    ent_coef_loss   | -0.118   |
|    learning_rate   | 0.0003   |
|    n_updates       | 506299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.17e+03 |
|    critic_loss     | 2.07e+03 |
|    ent_coef        | 0.466    |
|    ent_coef_loss   | 0.0307   |
|    learning_rate   | 0.0003   |
|    n_updates       | 506699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.15e+03 |
|    critic_loss     | 1.61e+03 |
|    ent_coef        | 0.463    |
|    ent_coef_loss   | -0.132   |
|    learning_rate   | 0.0003   |
|    n_updates       | 507099   |
---------------------------------
=== Iterazione IRL 921 ===
Loss reward (iter 921): 3.4036786556243896
=== Iterazione IRL 922 ===
Loss reward (iter 922): 3.695518732070923
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.19e+03 |
|    critic_loss     | 3.01e+03 |
|    ent_coef        | 0.454    |
|    ent_coef_loss   | -0.0365  |
|    learning_rate   | 0.0003   |
|    n_updates       | 507399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.18e+03 |
|    critic_loss     | 1.47e+03 |
|    ent_coef        | 0.465    |
|    ent_coef_loss   | -0.16    |
|    learning_rate   | 0.0003   |
|    n_updates       | 507799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.18e+03 |
|    critic_loss     | 2.73e+03 |
|    ent_coef        | 0.459    |
|    ent_coef_loss   | -0.132   |
|    learning_rate   | 0.0003   |
|    n_updates       | 508199   |
---------------------------------
=== Iterazione IRL 923 ===
Loss reward (iter 923): 3.9777896404266357
=== Iterazione IRL 924 ===
Loss reward (iter 924): 4.167167663574219
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.18e+03 |
|    critic_loss     | 2.17e+03 |
|    ent_coef        | 0.47     |
|    ent_coef_loss   | 0.0984   |
|    learning_rate   | 0.0003   |
|    n_updates       | 508499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.2e+03  |
|    critic_loss     | 2.08e+03 |
|    ent_coef        | 0.482    |
|    ent_coef_loss   | 0.209    |
|    learning_rate   | 0.0003   |
|    n_updates       | 508899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.2e+03  |
|    critic_loss     | 2.53e+03 |
|    ent_coef        | 0.487    |
|    ent_coef_loss   | 0.181    |
|    learning_rate   | 0.0003   |
|    n_updates       | 509299   |
---------------------------------
=== Iterazione IRL 925 ===
Loss reward (iter 925): 3.820753812789917
=== Iterazione IRL 926 ===
Loss reward (iter 926): 3.242246150970459
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.17e+03 |
|    critic_loss     | 2.11e+03 |
|    ent_coef        | 0.485    |
|    ent_coef_loss   | -0.166   |
|    learning_rate   | 0.0003   |
|    n_updates       | 509599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.19e+03 |
|    critic_loss     | 2.1e+03  |
|    ent_coef        | 0.468    |
|    ent_coef_loss   | 0.0165   |
|    learning_rate   | 0.0003   |
|    n_updates       | 509999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.18e+03 |
|    critic_loss     | 2.38e+03 |
|    ent_coef        | 0.47     |
|    ent_coef_loss   | 0.0524   |
|    learning_rate   | 0.0003   |
|    n_updates       | 510399   |
---------------------------------
=== Iterazione IRL 927 ===
Loss reward (iter 927): 3.8926308155059814
=== Iterazione IRL 928 ===
Loss reward (iter 928): 3.1887731552124023
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.19e+03 |
|    critic_loss     | 2.67e+03 |
|    ent_coef        | 0.473    |
|    ent_coef_loss   | 0.19     |
|    learning_rate   | 0.0003   |
|    n_updates       | 510699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.18e+03 |
|    critic_loss     | 2.13e+03 |
|    ent_coef        | 0.471    |
|    ent_coef_loss   | 0.0676   |
|    learning_rate   | 0.0003   |
|    n_updates       | 511099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.19e+03 |
|    critic_loss     | 2.18e+03 |
|    ent_coef        | 0.473    |
|    ent_coef_loss   | -0.132   |
|    learning_rate   | 0.0003   |
|    n_updates       | 511499   |
---------------------------------
=== Iterazione IRL 929 ===
Loss reward (iter 929): 3.60744571685791
=== Iterazione IRL 930 ===
Loss reward (iter 930): 3.9414737224578857
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.2e+03  |
|    critic_loss     | 2.12e+03 |
|    ent_coef        | 0.47     |
|    ent_coef_loss   | -0.0734  |
|    learning_rate   | 0.0003   |
|    n_updates       | 511799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.22e+03 |
|    critic_loss     | 2.45e+03 |
|    ent_coef        | 0.478    |
|    ent_coef_loss   | -0.0654  |
|    learning_rate   | 0.0003   |
|    n_updates       | 512199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.19e+03 |
|    critic_loss     | 2.26e+03 |
|    ent_coef        | 0.477    |
|    ent_coef_loss   | -0.176   |
|    learning_rate   | 0.0003   |
|    n_updates       | 512599   |
---------------------------------
=== Iterazione IRL 931 ===
Loss reward (iter 931): 4.201601028442383
=== Iterazione IRL 932 ===
Loss reward (iter 932): 4.067063331604004
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.19e+03 |
|    critic_loss     | 1.52e+03 |
|    ent_coef        | 0.468    |
|    ent_coef_loss   | 0.0594   |
|    learning_rate   | 0.0003   |
|    n_updates       | 512899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.21e+03 |
|    critic_loss     | 2.09e+03 |
|    ent_coef        | 0.467    |
|    ent_coef_loss   | -0.197   |
|    learning_rate   | 0.0003   |
|    n_updates       | 513299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.2e+03  |
|    critic_loss     | 2.59e+03 |
|    ent_coef        | 0.465    |
|    ent_coef_loss   | -0.0806  |
|    learning_rate   | 0.0003   |
|    n_updates       | 513699   |
---------------------------------
=== Iterazione IRL 933 ===
Loss reward (iter 933): 3.978443145751953
=== Iterazione IRL 934 ===
Loss reward (iter 934): 4.087491512298584
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.18e+03 |
|    critic_loss     | 2.58e+03 |
|    ent_coef        | 0.471    |
|    ent_coef_loss   | 0.165    |
|    learning_rate   | 0.0003   |
|    n_updates       | 513999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.2e+03  |
|    critic_loss     | 2.24e+03 |
|    ent_coef        | 0.468    |
|    ent_coef_loss   | 0.0275   |
|    learning_rate   | 0.0003   |
|    n_updates       | 514399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.19e+03 |
|    critic_loss     | 1.7e+03  |
|    ent_coef        | 0.489    |
|    ent_coef_loss   | -0.0132  |
|    learning_rate   | 0.0003   |
|    n_updates       | 514799   |
---------------------------------
=== Iterazione IRL 935 ===
Loss reward (iter 935): 3.2650609016418457
=== Iterazione IRL 936 ===
Loss reward (iter 936): 4.34303617477417
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.16e+03 |
|    critic_loss     | 2e+03    |
|    ent_coef        | 0.485    |
|    ent_coef_loss   | -0.296   |
|    learning_rate   | 0.0003   |
|    n_updates       | 515099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.2e+03  |
|    critic_loss     | 2.56e+03 |
|    ent_coef        | 0.48     |
|    ent_coef_loss   | -0.12    |
|    learning_rate   | 0.0003   |
|    n_updates       | 515499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.17e+03 |
|    critic_loss     | 2.09e+03 |
|    ent_coef        | 0.48     |
|    ent_coef_loss   | -0.0949  |
|    learning_rate   | 0.0003   |
|    n_updates       | 515899   |
---------------------------------
=== Iterazione IRL 937 ===
Loss reward (iter 937): 2.6851274967193604
=== Iterazione IRL 938 ===
Loss reward (iter 938): 3.57985782623291
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.18e+03 |
|    critic_loss     | 1.68e+03 |
|    ent_coef        | 0.482    |
|    ent_coef_loss   | 0.0939   |
|    learning_rate   | 0.0003   |
|    n_updates       | 516199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.18e+03 |
|    critic_loss     | 2.22e+03 |
|    ent_coef        | 0.473    |
|    ent_coef_loss   | 0.042    |
|    learning_rate   | 0.0003   |
|    n_updates       | 516599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.2e+03  |
|    critic_loss     | 2.32e+03 |
|    ent_coef        | 0.475    |
|    ent_coef_loss   | -0.171   |
|    learning_rate   | 0.0003   |
|    n_updates       | 516999   |
---------------------------------
=== Iterazione IRL 939 ===
Loss reward (iter 939): 3.855668067932129
=== Iterazione IRL 940 ===
Loss reward (iter 940): 3.856938362121582
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.19e+03 |
|    critic_loss     | 2.35e+03 |
|    ent_coef        | 0.472    |
|    ent_coef_loss   | -0.0519  |
|    learning_rate   | 0.0003   |
|    n_updates       | 517299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.16e+03 |
|    critic_loss     | 1.91e+03 |
|    ent_coef        | 0.468    |
|    ent_coef_loss   | 0.0638   |
|    learning_rate   | 0.0003   |
|    n_updates       | 517699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.18e+03 |
|    critic_loss     | 2.45e+03 |
|    ent_coef        | 0.46     |
|    ent_coef_loss   | -0.245   |
|    learning_rate   | 0.0003   |
|    n_updates       | 518099   |
---------------------------------
=== Iterazione IRL 941 ===
Loss reward (iter 941): 3.7271907329559326
=== Iterazione IRL 942 ===
Loss reward (iter 942): 3.5567519664764404
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.2e+03  |
|    critic_loss     | 2.22e+03 |
|    ent_coef        | 0.46     |
|    ent_coef_loss   | 0.114    |
|    learning_rate   | 0.0003   |
|    n_updates       | 518399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.2e+03  |
|    critic_loss     | 2.06e+03 |
|    ent_coef        | 0.462    |
|    ent_coef_loss   | 0.206    |
|    learning_rate   | 0.0003   |
|    n_updates       | 518799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.18e+03 |
|    critic_loss     | 1.59e+03 |
|    ent_coef        | 0.451    |
|    ent_coef_loss   | 0.00251  |
|    learning_rate   | 0.0003   |
|    n_updates       | 519199   |
---------------------------------
=== Iterazione IRL 943 ===
Loss reward (iter 943): 2.564983367919922
=== Iterazione IRL 944 ===
Loss reward (iter 944): 3.287252902984619
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.22e+03 |
|    critic_loss     | 2.41e+03 |
|    ent_coef        | 0.442    |
|    ent_coef_loss   | 0.0241   |
|    learning_rate   | 0.0003   |
|    n_updates       | 519499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.21e+03 |
|    critic_loss     | 2.75e+03 |
|    ent_coef        | 0.447    |
|    ent_coef_loss   | -0.185   |
|    learning_rate   | 0.0003   |
|    n_updates       | 519899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.18e+03 |
|    critic_loss     | 1.82e+03 |
|    ent_coef        | 0.442    |
|    ent_coef_loss   | -0.159   |
|    learning_rate   | 0.0003   |
|    n_updates       | 520299   |
---------------------------------
=== Iterazione IRL 945 ===
Loss reward (iter 945): 3.3247780799865723
=== Iterazione IRL 946 ===
Loss reward (iter 946): 3.2166404724121094
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.17e+03 |
|    critic_loss     | 2.44e+03 |
|    ent_coef        | 0.446    |
|    ent_coef_loss   | -0.027   |
|    learning_rate   | 0.0003   |
|    n_updates       | 520599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.17e+03 |
|    critic_loss     | 1.95e+03 |
|    ent_coef        | 0.45     |
|    ent_coef_loss   | -0.101   |
|    learning_rate   | 0.0003   |
|    n_updates       | 520999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.18e+03 |
|    critic_loss     | 1.65e+03 |
|    ent_coef        | 0.45     |
|    ent_coef_loss   | -0.144   |
|    learning_rate   | 0.0003   |
|    n_updates       | 521399   |
---------------------------------
=== Iterazione IRL 947 ===
Loss reward (iter 947): 2.566263198852539
=== Iterazione IRL 948 ===
Loss reward (iter 948): 3.5516228675842285
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.19e+03 |
|    critic_loss     | 1.94e+03 |
|    ent_coef        | 0.448    |
|    ent_coef_loss   | 0.00616  |
|    learning_rate   | 0.0003   |
|    n_updates       | 521699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.22e+03 |
|    critic_loss     | 2.52e+03 |
|    ent_coef        | 0.451    |
|    ent_coef_loss   | 0.0335   |
|    learning_rate   | 0.0003   |
|    n_updates       | 522099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.23e+03 |
|    critic_loss     | 2.8e+03  |
|    ent_coef        | 0.45     |
|    ent_coef_loss   | 0.218    |
|    learning_rate   | 0.0003   |
|    n_updates       | 522499   |
---------------------------------
=== Iterazione IRL 949 ===
Loss reward (iter 949): 3.771880865097046
=== Iterazione IRL 950 ===
Loss reward (iter 950): 3.9504592418670654
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.18e+03 |
|    critic_loss     | 2.51e+03 |
|    ent_coef        | 0.451    |
|    ent_coef_loss   | 0.16     |
|    learning_rate   | 0.0003   |
|    n_updates       | 522799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.22e+03 |
|    critic_loss     | 2.62e+03 |
|    ent_coef        | 0.452    |
|    ent_coef_loss   | 0.299    |
|    learning_rate   | 0.0003   |
|    n_updates       | 523199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.2e+03  |
|    critic_loss     | 2.13e+03 |
|    ent_coef        | 0.439    |
|    ent_coef_loss   | -0.0572  |
|    learning_rate   | 0.0003   |
|    n_updates       | 523599   |
---------------------------------
=== Iterazione IRL 951 ===
Loss reward (iter 951): 3.289236545562744
=== Iterazione IRL 952 ===
Loss reward (iter 952): 3.1944448947906494
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.21e+03 |
|    critic_loss     | 2.23e+03 |
|    ent_coef        | 0.427    |
|    ent_coef_loss   | -0.199   |
|    learning_rate   | 0.0003   |
|    n_updates       | 523899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.2e+03  |
|    critic_loss     | 1.68e+03 |
|    ent_coef        | 0.435    |
|    ent_coef_loss   | -0.0754  |
|    learning_rate   | 0.0003   |
|    n_updates       | 524299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.18e+03 |
|    critic_loss     | 2.09e+03 |
|    ent_coef        | 0.448    |
|    ent_coef_loss   | -0.197   |
|    learning_rate   | 0.0003   |
|    n_updates       | 524699   |
---------------------------------
=== Iterazione IRL 953 ===
Loss reward (iter 953): 2.8041322231292725
=== Iterazione IRL 954 ===
Loss reward (iter 954): 3.3504550457000732
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.18e+03 |
|    critic_loss     | 1.45e+03 |
|    ent_coef        | 0.448    |
|    ent_coef_loss   | -0.0869  |
|    learning_rate   | 0.0003   |
|    n_updates       | 524999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.18e+03 |
|    critic_loss     | 2.15e+03 |
|    ent_coef        | 0.445    |
|    ent_coef_loss   | -0.0269  |
|    learning_rate   | 0.0003   |
|    n_updates       | 525399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.2e+03  |
|    critic_loss     | 2.33e+03 |
|    ent_coef        | 0.438    |
|    ent_coef_loss   | -0.0721  |
|    learning_rate   | 0.0003   |
|    n_updates       | 525799   |
---------------------------------
=== Iterazione IRL 955 ===
Loss reward (iter 955): 3.4543557167053223
=== Iterazione IRL 956 ===
Loss reward (iter 956): 1.8576817512512207
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.21e+03 |
|    critic_loss     | 2.42e+03 |
|    ent_coef        | 0.433    |
|    ent_coef_loss   | -0.163   |
|    learning_rate   | 0.0003   |
|    n_updates       | 526099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.23e+03 |
|    critic_loss     | 2.53e+03 |
|    ent_coef        | 0.436    |
|    ent_coef_loss   | 0.0223   |
|    learning_rate   | 0.0003   |
|    n_updates       | 526499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.23e+03 |
|    critic_loss     | 2.97e+03 |
|    ent_coef        | 0.432    |
|    ent_coef_loss   | 0.164    |
|    learning_rate   | 0.0003   |
|    n_updates       | 526899   |
---------------------------------
=== Iterazione IRL 957 ===
Loss reward (iter 957): 3.5309364795684814
=== Iterazione IRL 958 ===
Loss reward (iter 958): 4.379716396331787
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.2e+03  |
|    critic_loss     | 1.72e+03 |
|    ent_coef        | 0.427    |
|    ent_coef_loss   | 0.00631  |
|    learning_rate   | 0.0003   |
|    n_updates       | 527199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.18e+03 |
|    critic_loss     | 1.98e+03 |
|    ent_coef        | 0.431    |
|    ent_coef_loss   | -0.101   |
|    learning_rate   | 0.0003   |
|    n_updates       | 527599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.2e+03  |
|    critic_loss     | 2.58e+03 |
|    ent_coef        | 0.435    |
|    ent_coef_loss   | 0.383    |
|    learning_rate   | 0.0003   |
|    n_updates       | 527999   |
---------------------------------
=== Iterazione IRL 959 ===
Loss reward (iter 959): 4.267576694488525
=== Iterazione IRL 960 ===
Loss reward (iter 960): 4.348268508911133
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.2e+03  |
|    critic_loss     | 2.23e+03 |
|    ent_coef        | 0.442    |
|    ent_coef_loss   | -0.3     |
|    learning_rate   | 0.0003   |
|    n_updates       | 528299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.2e+03  |
|    critic_loss     | 2.01e+03 |
|    ent_coef        | 0.437    |
|    ent_coef_loss   | -0.335   |
|    learning_rate   | 0.0003   |
|    n_updates       | 528699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.22e+03 |
|    critic_loss     | 1.88e+03 |
|    ent_coef        | 0.438    |
|    ent_coef_loss   | -0.00529 |
|    learning_rate   | 0.0003   |
|    n_updates       | 529099   |
---------------------------------
=== Iterazione IRL 961 ===
Loss reward (iter 961): 3.085937976837158
=== Iterazione IRL 962 ===
Loss reward (iter 962): 3.202298402786255
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.2e+03  |
|    critic_loss     | 2.46e+03 |
|    ent_coef        | 0.445    |
|    ent_coef_loss   | 0.335    |
|    learning_rate   | 0.0003   |
|    n_updates       | 529399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.22e+03 |
|    critic_loss     | 2.15e+03 |
|    ent_coef        | 0.451    |
|    ent_coef_loss   | -0.0269  |
|    learning_rate   | 0.0003   |
|    n_updates       | 529799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.2e+03  |
|    critic_loss     | 1.74e+03 |
|    ent_coef        | 0.441    |
|    ent_coef_loss   | -0.063   |
|    learning_rate   | 0.0003   |
|    n_updates       | 530199   |
---------------------------------
=== Iterazione IRL 963 ===
Loss reward (iter 963): 3.215646743774414
=== Iterazione IRL 964 ===
Loss reward (iter 964): 3.600529193878174
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.17e+03 |
|    critic_loss     | 1.96e+03 |
|    ent_coef        | 0.437    |
|    ent_coef_loss   | -0.13    |
|    learning_rate   | 0.0003   |
|    n_updates       | 530499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.23e+03 |
|    critic_loss     | 2.03e+03 |
|    ent_coef        | 0.431    |
|    ent_coef_loss   | -0.0163  |
|    learning_rate   | 0.0003   |
|    n_updates       | 530899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.2e+03  |
|    critic_loss     | 2.14e+03 |
|    ent_coef        | 0.419    |
|    ent_coef_loss   | -0.189   |
|    learning_rate   | 0.0003   |
|    n_updates       | 531299   |
---------------------------------
=== Iterazione IRL 965 ===
Loss reward (iter 965): 3.8785603046417236
=== Iterazione IRL 966 ===
Loss reward (iter 966): 3.332796573638916
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.2e+03  |
|    critic_loss     | 2.36e+03 |
|    ent_coef        | 0.419    |
|    ent_coef_loss   | -0.0927  |
|    learning_rate   | 0.0003   |
|    n_updates       | 531599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.21e+03 |
|    critic_loss     | 2.5e+03  |
|    ent_coef        | 0.439    |
|    ent_coef_loss   | -0.0125  |
|    learning_rate   | 0.0003   |
|    n_updates       | 531999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.24e+03 |
|    critic_loss     | 2.19e+03 |
|    ent_coef        | 0.444    |
|    ent_coef_loss   | -0.0927  |
|    learning_rate   | 0.0003   |
|    n_updates       | 532399   |
---------------------------------
=== Iterazione IRL 967 ===
Loss reward (iter 967): 3.9834001064300537
=== Iterazione IRL 968 ===
Loss reward (iter 968): 3.1360831260681152
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.21e+03 |
|    critic_loss     | 1.91e+03 |
|    ent_coef        | 0.436    |
|    ent_coef_loss   | -0.208   |
|    learning_rate   | 0.0003   |
|    n_updates       | 532699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.2e+03  |
|    critic_loss     | 2e+03    |
|    ent_coef        | 0.449    |
|    ent_coef_loss   | -0.0775  |
|    learning_rate   | 0.0003   |
|    n_updates       | 533099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.18e+03 |
|    critic_loss     | 1.96e+03 |
|    ent_coef        | 0.451    |
|    ent_coef_loss   | 0.205    |
|    learning_rate   | 0.0003   |
|    n_updates       | 533499   |
---------------------------------
=== Iterazione IRL 969 ===
Loss reward (iter 969): 4.984760284423828
=== Iterazione IRL 970 ===
Loss reward (iter 970): 4.054910659790039
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.22e+03 |
|    critic_loss     | 2.13e+03 |
|    ent_coef        | 0.446    |
|    ent_coef_loss   | 0.247    |
|    learning_rate   | 0.0003   |
|    n_updates       | 533799   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | 2.21e+03  |
|    critic_loss     | 2.2e+03   |
|    ent_coef        | 0.437     |
|    ent_coef_loss   | -0.000887 |
|    learning_rate   | 0.0003    |
|    n_updates       | 534199    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.21e+03 |
|    critic_loss     | 1.66e+03 |
|    ent_coef        | 0.424    |
|    ent_coef_loss   | 0.0391   |
|    learning_rate   | 0.0003   |
|    n_updates       | 534599   |
---------------------------------
=== Iterazione IRL 971 ===
Loss reward (iter 971): 4.286590576171875
=== Iterazione IRL 972 ===
Loss reward (iter 972): 4.038779258728027
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.19e+03 |
|    critic_loss     | 1.67e+03 |
|    ent_coef        | 0.435    |
|    ent_coef_loss   | -0.193   |
|    learning_rate   | 0.0003   |
|    n_updates       | 534899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.23e+03 |
|    critic_loss     | 2e+03    |
|    ent_coef        | 0.431    |
|    ent_coef_loss   | -0.191   |
|    learning_rate   | 0.0003   |
|    n_updates       | 535299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.21e+03 |
|    critic_loss     | 1.81e+03 |
|    ent_coef        | 0.431    |
|    ent_coef_loss   | 0.149    |
|    learning_rate   | 0.0003   |
|    n_updates       | 535699   |
---------------------------------
=== Iterazione IRL 973 ===
Loss reward (iter 973): 2.9172515869140625
=== Iterazione IRL 974 ===
Loss reward (iter 974): 4.28769063949585
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.2e+03  |
|    critic_loss     | 2.07e+03 |
|    ent_coef        | 0.428    |
|    ent_coef_loss   | -0.242   |
|    learning_rate   | 0.0003   |
|    n_updates       | 535999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.22e+03 |
|    critic_loss     | 2.01e+03 |
|    ent_coef        | 0.424    |
|    ent_coef_loss   | -0.044   |
|    learning_rate   | 0.0003   |
|    n_updates       | 536399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.21e+03 |
|    critic_loss     | 2.25e+03 |
|    ent_coef        | 0.431    |
|    ent_coef_loss   | 0.0597   |
|    learning_rate   | 0.0003   |
|    n_updates       | 536799   |
---------------------------------
=== Iterazione IRL 975 ===
Loss reward (iter 975): 4.513274669647217
=== Iterazione IRL 976 ===
Loss reward (iter 976): 4.614345073699951
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.19e+03 |
|    critic_loss     | 1.8e+03  |
|    ent_coef        | 0.447    |
|    ent_coef_loss   | -0.0649  |
|    learning_rate   | 0.0003   |
|    n_updates       | 537099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.26e+03 |
|    critic_loss     | 2.5e+03  |
|    ent_coef        | 0.437    |
|    ent_coef_loss   | 0.0833   |
|    learning_rate   | 0.0003   |
|    n_updates       | 537499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.22e+03 |
|    critic_loss     | 2.77e+03 |
|    ent_coef        | 0.439    |
|    ent_coef_loss   | -0.262   |
|    learning_rate   | 0.0003   |
|    n_updates       | 537899   |
---------------------------------
=== Iterazione IRL 977 ===
Loss reward (iter 977): 5.732898235321045
=== Iterazione IRL 978 ===
Loss reward (iter 978): 6.125629425048828
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.22e+03 |
|    critic_loss     | 2.03e+03 |
|    ent_coef        | 0.433    |
|    ent_coef_loss   | 0.152    |
|    learning_rate   | 0.0003   |
|    n_updates       | 538199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.21e+03 |
|    critic_loss     | 1.79e+03 |
|    ent_coef        | 0.432    |
|    ent_coef_loss   | -0.228   |
|    learning_rate   | 0.0003   |
|    n_updates       | 538599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.21e+03 |
|    critic_loss     | 1.89e+03 |
|    ent_coef        | 0.433    |
|    ent_coef_loss   | -0.0614  |
|    learning_rate   | 0.0003   |
|    n_updates       | 538999   |
---------------------------------
=== Iterazione IRL 979 ===
Loss reward (iter 979): 5.866980075836182
=== Iterazione IRL 980 ===
Loss reward (iter 980): 4.793447494506836
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.22e+03 |
|    critic_loss     | 2.32e+03 |
|    ent_coef        | 0.43     |
|    ent_coef_loss   | -0.059   |
|    learning_rate   | 0.0003   |
|    n_updates       | 539299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.2e+03  |
|    critic_loss     | 2.23e+03 |
|    ent_coef        | 0.426    |
|    ent_coef_loss   | -0.0182  |
|    learning_rate   | 0.0003   |
|    n_updates       | 539699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.21e+03 |
|    critic_loss     | 1.83e+03 |
|    ent_coef        | 0.434    |
|    ent_coef_loss   | 0.27     |
|    learning_rate   | 0.0003   |
|    n_updates       | 540099   |
---------------------------------
=== Iterazione IRL 981 ===
Loss reward (iter 981): 4.775634765625
=== Iterazione IRL 982 ===
Loss reward (iter 982): 3.4976634979248047
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.21e+03 |
|    critic_loss     | 1.77e+03 |
|    ent_coef        | 0.428    |
|    ent_coef_loss   | -0.105   |
|    learning_rate   | 0.0003   |
|    n_updates       | 540399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.23e+03 |
|    critic_loss     | 1.83e+03 |
|    ent_coef        | 0.43     |
|    ent_coef_loss   | -0.0694  |
|    learning_rate   | 0.0003   |
|    n_updates       | 540799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.24e+03 |
|    critic_loss     | 2e+03    |
|    ent_coef        | 0.418    |
|    ent_coef_loss   | 0.281    |
|    learning_rate   | 0.0003   |
|    n_updates       | 541199   |
---------------------------------
=== Iterazione IRL 983 ===
Loss reward (iter 983): 4.7993927001953125
=== Iterazione IRL 984 ===
Loss reward (iter 984): 4.989211559295654
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.25e+03 |
|    critic_loss     | 2.35e+03 |
|    ent_coef        | 0.421    |
|    ent_coef_loss   | -0.135   |
|    learning_rate   | 0.0003   |
|    n_updates       | 541499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.22e+03 |
|    critic_loss     | 2.57e+03 |
|    ent_coef        | 0.409    |
|    ent_coef_loss   | 0.0697   |
|    learning_rate   | 0.0003   |
|    n_updates       | 541899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.21e+03 |
|    critic_loss     | 1.92e+03 |
|    ent_coef        | 0.414    |
|    ent_coef_loss   | -0.075   |
|    learning_rate   | 0.0003   |
|    n_updates       | 542299   |
---------------------------------
=== Iterazione IRL 985 ===
Loss reward (iter 985): 5.12070369720459
=== Iterazione IRL 986 ===
Loss reward (iter 986): 4.157310962677002
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.25e+03 |
|    critic_loss     | 2.17e+03 |
|    ent_coef        | 0.426    |
|    ent_coef_loss   | 0.322    |
|    learning_rate   | 0.0003   |
|    n_updates       | 542599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.25e+03 |
|    critic_loss     | 2.05e+03 |
|    ent_coef        | 0.433    |
|    ent_coef_loss   | -0.216   |
|    learning_rate   | 0.0003   |
|    n_updates       | 542999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.23e+03 |
|    critic_loss     | 1.64e+03 |
|    ent_coef        | 0.43     |
|    ent_coef_loss   | -0.0352  |
|    learning_rate   | 0.0003   |
|    n_updates       | 543399   |
---------------------------------
=== Iterazione IRL 987 ===
Loss reward (iter 987): 4.8528032302856445
=== Iterazione IRL 988 ===
Loss reward (iter 988): 4.562130451202393
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.24e+03 |
|    critic_loss     | 2.02e+03 |
|    ent_coef        | 0.429    |
|    ent_coef_loss   | 0.15     |
|    learning_rate   | 0.0003   |
|    n_updates       | 543699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.22e+03 |
|    critic_loss     | 2.58e+03 |
|    ent_coef        | 0.437    |
|    ent_coef_loss   | 0.296    |
|    learning_rate   | 0.0003   |
|    n_updates       | 544099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.24e+03 |
|    critic_loss     | 1.72e+03 |
|    ent_coef        | 0.438    |
|    ent_coef_loss   | 0.247    |
|    learning_rate   | 0.0003   |
|    n_updates       | 544499   |
---------------------------------
=== Iterazione IRL 989 ===
Loss reward (iter 989): 3.2638044357299805
=== Iterazione IRL 990 ===
Loss reward (iter 990): 3.476470947265625
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.21e+03 |
|    critic_loss     | 1.9e+03  |
|    ent_coef        | 0.438    |
|    ent_coef_loss   | -0.295   |
|    learning_rate   | 0.0003   |
|    n_updates       | 544799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.22e+03 |
|    critic_loss     | 1.97e+03 |
|    ent_coef        | 0.441    |
|    ent_coef_loss   | 0.114    |
|    learning_rate   | 0.0003   |
|    n_updates       | 545199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.22e+03 |
|    critic_loss     | 1.81e+03 |
|    ent_coef        | 0.447    |
|    ent_coef_loss   | -0.0322  |
|    learning_rate   | 0.0003   |
|    n_updates       | 545599   |
---------------------------------
=== Iterazione IRL 991 ===
Loss reward (iter 991): 3.6093180179595947
=== Iterazione IRL 992 ===
Loss reward (iter 992): 3.5033164024353027
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.23e+03 |
|    critic_loss     | 1.95e+03 |
|    ent_coef        | 0.444    |
|    ent_coef_loss   | -0.114   |
|    learning_rate   | 0.0003   |
|    n_updates       | 545899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.25e+03 |
|    critic_loss     | 2.33e+03 |
|    ent_coef        | 0.442    |
|    ent_coef_loss   | -0.0153  |
|    learning_rate   | 0.0003   |
|    n_updates       | 546299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.21e+03 |
|    critic_loss     | 1.48e+03 |
|    ent_coef        | 0.452    |
|    ent_coef_loss   | -0.19    |
|    learning_rate   | 0.0003   |
|    n_updates       | 546699   |
---------------------------------
=== Iterazione IRL 993 ===
Loss reward (iter 993): 3.3265655040740967
=== Iterazione IRL 994 ===
Loss reward (iter 994): 3.379565954208374
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.22e+03 |
|    critic_loss     | 2.28e+03 |
|    ent_coef        | 0.444    |
|    ent_coef_loss   | -0.146   |
|    learning_rate   | 0.0003   |
|    n_updates       | 546999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.23e+03 |
|    critic_loss     | 2.45e+03 |
|    ent_coef        | 0.449    |
|    ent_coef_loss   | -0.0629  |
|    learning_rate   | 0.0003   |
|    n_updates       | 547399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.2e+03  |
|    critic_loss     | 1.61e+03 |
|    ent_coef        | 0.439    |
|    ent_coef_loss   | -0.00113 |
|    learning_rate   | 0.0003   |
|    n_updates       | 547799   |
---------------------------------
=== Iterazione IRL 995 ===
Loss reward (iter 995): 2.6314492225646973
=== Iterazione IRL 996 ===
Loss reward (iter 996): 3.6372122764587402
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.23e+03 |
|    critic_loss     | 1.52e+03 |
|    ent_coef        | 0.433    |
|    ent_coef_loss   | -0.147   |
|    learning_rate   | 0.0003   |
|    n_updates       | 548099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.22e+03 |
|    critic_loss     | 1.93e+03 |
|    ent_coef        | 0.433    |
|    ent_coef_loss   | 0.346    |
|    learning_rate   | 0.0003   |
|    n_updates       | 548499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.21e+03 |
|    critic_loss     | 1.52e+03 |
|    ent_coef        | 0.433    |
|    ent_coef_loss   | 0.146    |
|    learning_rate   | 0.0003   |
|    n_updates       | 548899   |
---------------------------------
=== Iterazione IRL 997 ===
Loss reward (iter 997): 2.573269844055176
=== Iterazione IRL 998 ===
Loss reward (iter 998): 3.4785890579223633
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 2.25e+03 |
|    critic_loss     | 1.74e+03 |
|    ent_coef        | 0.428    |
|    ent_coef_loss   | -0.113   |
|    learning_rate   | 0.0003   |
|    n_updates       | 549199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.22e+03 |
|    critic_loss     | 1.58e+03 |
|    ent_coef        | 0.43     |
|    ent_coef_loss   | 0.085    |
|    learning_rate   | 0.0003   |
|    n_updates       | 549599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 2.19e+03 |
|    critic_loss     | 2.03e+03 |
|    ent_coef        | 0.436    |
|    ent_coef_loss   | -0.147   |
|    learning_rate   | 0.0003   |
|    n_updates       | 549999   |
---------------------------------
=== Iterazione IRL 999 ===
Loss reward (iter 999): 3.7860336303710938
Modello SAC salvato.
