Inizio training IRL
Using device: cuda
Using cuda device
=== Iterazione IRL 0 ===
Loss reward (iter 0): 6.200942039489746
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 143      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.89    |
|    critic_loss     | 0.0669   |
|    ent_coef        | 0.915    |
|    ent_coef_loss   | -0.291   |
|    learning_rate   | 0.0003   |
|    n_updates       | 299      |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 128      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -9.54    |
|    critic_loss     | 0.017    |
|    ent_coef        | 0.812    |
|    ent_coef_loss   | -0.679   |
|    learning_rate   | 0.0003   |
|    n_updates       | 699      |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 124      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -13.3    |
|    critic_loss     | 0.0264   |
|    ent_coef        | 0.721    |
|    ent_coef_loss   | -1.04    |
|    learning_rate   | 0.0003   |
|    n_updates       | 1099     |
---------------------------------
=== Iterazione IRL 1 ===
Loss reward (iter 1): 5.845087051391602
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -19      |
|    critic_loss     | 0.233    |
|    ent_coef        | 0.603    |
|    ent_coef_loss   | -1.64    |
|    learning_rate   | 0.0003   |
|    n_updates       | 1699     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 130      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -23.4    |
|    critic_loss     | 0.301    |
|    ent_coef        | 0.537    |
|    ent_coef_loss   | -1.96    |
|    learning_rate   | 0.0003   |
|    n_updates       | 2099     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -27.5    |
|    critic_loss     | 0.403    |
|    ent_coef        | 0.479    |
|    ent_coef_loss   | -2.07    |
|    learning_rate   | 0.0003   |
|    n_updates       | 2499     |
---------------------------------
=== Iterazione IRL 2 ===
Loss reward (iter 2): 5.066559791564941
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 149      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -34.4    |
|    critic_loss     | 0.729    |
|    ent_coef        | 0.408    |
|    ent_coef_loss   | -2.33    |
|    learning_rate   | 0.0003   |
|    n_updates       | 3099     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 130      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -40      |
|    critic_loss     | 0.926    |
|    ent_coef        | 0.371    |
|    ent_coef_loss   | -1.84    |
|    learning_rate   | 0.0003   |
|    n_updates       | 3499     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -46.1    |
|    critic_loss     | 1.17     |
|    ent_coef        | 0.344    |
|    ent_coef_loss   | -1.2     |
|    learning_rate   | 0.0003   |
|    n_updates       | 3899     |
---------------------------------
=== Iterazione IRL 3 ===
Loss reward (iter 3): 7.603559970855713
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -55.8    |
|    critic_loss     | 1.26     |
|    ent_coef        | 0.319    |
|    ent_coef_loss   | -0.554   |
|    learning_rate   | 0.0003   |
|    n_updates       | 4499     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 130      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -63.5    |
|    critic_loss     | 1.61     |
|    ent_coef        | 0.313    |
|    ent_coef_loss   | 0.0437   |
|    learning_rate   | 0.0003   |
|    n_updates       | 4899     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -72.4    |
|    critic_loss     | 1.72     |
|    ent_coef        | 0.316    |
|    ent_coef_loss   | 0.228    |
|    learning_rate   | 0.0003   |
|    n_updates       | 5299     |
---------------------------------
=== Iterazione IRL 4 ===
Loss reward (iter 4): 8.040421485900879
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -85.6    |
|    critic_loss     | 1.89     |
|    ent_coef        | 0.329    |
|    ent_coef_loss   | 0.392    |
|    learning_rate   | 0.0003   |
|    n_updates       | 5899     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 130      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -93.6    |
|    critic_loss     | 2.32     |
|    ent_coef        | 0.34     |
|    ent_coef_loss   | 0.246    |
|    learning_rate   | 0.0003   |
|    n_updates       | 6299     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -103     |
|    critic_loss     | 1.98     |
|    ent_coef        | 0.35     |
|    ent_coef_loss   | -0.0781  |
|    learning_rate   | 0.0003   |
|    n_updates       | 6699     |
---------------------------------
=== Iterazione IRL 5 ===
Loss reward (iter 5): 6.594848155975342
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -116     |
|    critic_loss     | 2.39     |
|    ent_coef        | 0.364    |
|    ent_coef_loss   | 0.0455   |
|    learning_rate   | 0.0003   |
|    n_updates       | 7299     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -125     |
|    critic_loss     | 2.33     |
|    ent_coef        | 0.374    |
|    ent_coef_loss   | 0.335    |
|    learning_rate   | 0.0003   |
|    n_updates       | 7699     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -131     |
|    critic_loss     | 2.35     |
|    ent_coef        | 0.383    |
|    ent_coef_loss   | 0.19     |
|    learning_rate   | 0.0003   |
|    n_updates       | 8099     |
---------------------------------
=== Iterazione IRL 6 ===
Loss reward (iter 6): 6.838080883026123
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -143     |
|    critic_loss     | 2.78     |
|    ent_coef        | 0.385    |
|    ent_coef_loss   | 0.0859   |
|    learning_rate   | 0.0003   |
|    n_updates       | 8699     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -152     |
|    critic_loss     | 2.65     |
|    ent_coef        | 0.377    |
|    ent_coef_loss   | 0.141    |
|    learning_rate   | 0.0003   |
|    n_updates       | 9099     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -158     |
|    critic_loss     | 3.23     |
|    ent_coef        | 0.373    |
|    ent_coef_loss   | -0.301   |
|    learning_rate   | 0.0003   |
|    n_updates       | 9499     |
---------------------------------
=== Iterazione IRL 7 ===
Loss reward (iter 7): 7.20623254776001
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -168     |
|    critic_loss     | 2.71     |
|    ent_coef        | 0.358    |
|    ent_coef_loss   | -0.199   |
|    learning_rate   | 0.0003   |
|    n_updates       | 10099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -175     |
|    critic_loss     | 2.65     |
|    ent_coef        | 0.356    |
|    ent_coef_loss   | 0.0237   |
|    learning_rate   | 0.0003   |
|    n_updates       | 10499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -182     |
|    critic_loss     | 3.42     |
|    ent_coef        | 0.379    |
|    ent_coef_loss   | 0.0599   |
|    learning_rate   | 0.0003   |
|    n_updates       | 10899    |
---------------------------------
=== Iterazione IRL 8 ===
Loss reward (iter 8): 6.806195259094238
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -192     |
|    critic_loss     | 3.04     |
|    ent_coef        | 0.4      |
|    ent_coef_loss   | 0.0634   |
|    learning_rate   | 0.0003   |
|    n_updates       | 11499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 130      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -197     |
|    critic_loss     | 3.03     |
|    ent_coef        | 0.38     |
|    ent_coef_loss   | -0.213   |
|    learning_rate   | 0.0003   |
|    n_updates       | 11899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -206     |
|    critic_loss     | 4.09     |
|    ent_coef        | 0.367    |
|    ent_coef_loss   | 0.00888  |
|    learning_rate   | 0.0003   |
|    n_updates       | 12299    |
---------------------------------
=== Iterazione IRL 9 ===
Loss reward (iter 9): 6.9995598793029785
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -210     |
|    critic_loss     | 3.63     |
|    ent_coef        | 0.354    |
|    ent_coef_loss   | -0.133   |
|    learning_rate   | 0.0003   |
|    n_updates       | 12899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -216     |
|    critic_loss     | 3.24     |
|    ent_coef        | 0.348    |
|    ent_coef_loss   | -0.0619  |
|    learning_rate   | 0.0003   |
|    n_updates       | 13299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -222     |
|    critic_loss     | 3.98     |
|    ent_coef        | 0.342    |
|    ent_coef_loss   | -0.139   |
|    learning_rate   | 0.0003   |
|    n_updates       | 13699    |
---------------------------------
=== Iterazione IRL 10 ===
Loss reward (iter 10): 6.9017014503479
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -228     |
|    critic_loss     | 4        |
|    ent_coef        | 0.322    |
|    ent_coef_loss   | 0.283    |
|    learning_rate   | 0.0003   |
|    n_updates       | 14299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -231     |
|    critic_loss     | 3.82     |
|    ent_coef        | 0.315    |
|    ent_coef_loss   | -0.131   |
|    learning_rate   | 0.0003   |
|    n_updates       | 14699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -236     |
|    critic_loss     | 3.78     |
|    ent_coef        | 0.319    |
|    ent_coef_loss   | -0.212   |
|    learning_rate   | 0.0003   |
|    n_updates       | 15099    |
---------------------------------
=== Iterazione IRL 11 ===
Loss reward (iter 11): 6.750814914703369
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -240     |
|    critic_loss     | 3.88     |
|    ent_coef        | 0.324    |
|    ent_coef_loss   | 0.188    |
|    learning_rate   | 0.0003   |
|    n_updates       | 15699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -241     |
|    critic_loss     | 4        |
|    ent_coef        | 0.312    |
|    ent_coef_loss   | 0.135    |
|    learning_rate   | 0.0003   |
|    n_updates       | 16099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -246     |
|    critic_loss     | 3.99     |
|    ent_coef        | 0.285    |
|    ent_coef_loss   | 0.00333  |
|    learning_rate   | 0.0003   |
|    n_updates       | 16499    |
---------------------------------
=== Iterazione IRL 12 ===
Loss reward (iter 12): 6.913846015930176
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -249     |
|    critic_loss     | 4.18     |
|    ent_coef        | 0.265    |
|    ent_coef_loss   | 0.228    |
|    learning_rate   | 0.0003   |
|    n_updates       | 17099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -251     |
|    critic_loss     | 4.67     |
|    ent_coef        | 0.288    |
|    ent_coef_loss   | 0.163    |
|    learning_rate   | 0.0003   |
|    n_updates       | 17499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -254     |
|    critic_loss     | 4.7      |
|    ent_coef        | 0.303    |
|    ent_coef_loss   | 0.138    |
|    learning_rate   | 0.0003   |
|    n_updates       | 17899    |
---------------------------------
=== Iterazione IRL 13 ===
Loss reward (iter 13): 6.79574728012085
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -257     |
|    critic_loss     | 4.37     |
|    ent_coef        | 0.312    |
|    ent_coef_loss   | 0.0932   |
|    learning_rate   | 0.0003   |
|    n_updates       | 18499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -260     |
|    critic_loss     | 4.83     |
|    ent_coef        | 0.3      |
|    ent_coef_loss   | -0.461   |
|    learning_rate   | 0.0003   |
|    n_updates       | 18899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -263     |
|    critic_loss     | 5.16     |
|    ent_coef        | 0.288    |
|    ent_coef_loss   | -0.347   |
|    learning_rate   | 0.0003   |
|    n_updates       | 19299    |
---------------------------------
=== Iterazione IRL 14 ===
Loss reward (iter 14): 6.551237106323242
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -266     |
|    critic_loss     | 4.47     |
|    ent_coef        | 0.274    |
|    ent_coef_loss   | -0.158   |
|    learning_rate   | 0.0003   |
|    n_updates       | 19899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -267     |
|    critic_loss     | 5.02     |
|    ent_coef        | 0.269    |
|    ent_coef_loss   | 0.0288   |
|    learning_rate   | 0.0003   |
|    n_updates       | 20299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -268     |
|    critic_loss     | 5.16     |
|    ent_coef        | 0.26     |
|    ent_coef_loss   | 0.253    |
|    learning_rate   | 0.0003   |
|    n_updates       | 20699    |
---------------------------------
=== Iterazione IRL 15 ===
Loss reward (iter 15): 6.66404390335083
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -271     |
|    critic_loss     | 5.16     |
|    ent_coef        | 0.247    |
|    ent_coef_loss   | -0.352   |
|    learning_rate   | 0.0003   |
|    n_updates       | 21299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -271     |
|    critic_loss     | 4.8      |
|    ent_coef        | 0.244    |
|    ent_coef_loss   | 0.179    |
|    learning_rate   | 0.0003   |
|    n_updates       | 21699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -271     |
|    critic_loss     | 4.53     |
|    ent_coef        | 0.241    |
|    ent_coef_loss   | 0.251    |
|    learning_rate   | 0.0003   |
|    n_updates       | 22099    |
---------------------------------
=== Iterazione IRL 16 ===
Loss reward (iter 16): 6.734122276306152
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -274     |
|    critic_loss     | 5.42     |
|    ent_coef        | 0.225    |
|    ent_coef_loss   | -0.552   |
|    learning_rate   | 0.0003   |
|    n_updates       | 22699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -274     |
|    critic_loss     | 4.64     |
|    ent_coef        | 0.216    |
|    ent_coef_loss   | 0.109    |
|    learning_rate   | 0.0003   |
|    n_updates       | 23099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -276     |
|    critic_loss     | 4.98     |
|    ent_coef        | 0.202    |
|    ent_coef_loss   | 0.036    |
|    learning_rate   | 0.0003   |
|    n_updates       | 23499    |
---------------------------------
=== Iterazione IRL 17 ===
Loss reward (iter 17): 6.692282199859619
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -274     |
|    critic_loss     | 4.57     |
|    ent_coef        | 0.193    |
|    ent_coef_loss   | -0.249   |
|    learning_rate   | 0.0003   |
|    n_updates       | 24099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -275     |
|    critic_loss     | 5.62     |
|    ent_coef        | 0.187    |
|    ent_coef_loss   | 0.0283   |
|    learning_rate   | 0.0003   |
|    n_updates       | 24499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -276     |
|    critic_loss     | 6.68     |
|    ent_coef        | 0.187    |
|    ent_coef_loss   | -0.0251  |
|    learning_rate   | 0.0003   |
|    n_updates       | 24899    |
---------------------------------
=== Iterazione IRL 18 ===
Loss reward (iter 18): 6.596960544586182
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -274     |
|    critic_loss     | 5.92     |
|    ent_coef        | 0.18     |
|    ent_coef_loss   | -0.0643  |
|    learning_rate   | 0.0003   |
|    n_updates       | 25499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -276     |
|    critic_loss     | 6.66     |
|    ent_coef        | 0.172    |
|    ent_coef_loss   | 0.183    |
|    learning_rate   | 0.0003   |
|    n_updates       | 25899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -277     |
|    critic_loss     | 5.76     |
|    ent_coef        | 0.174    |
|    ent_coef_loss   | -0.00962 |
|    learning_rate   | 0.0003   |
|    n_updates       | 26299    |
---------------------------------
=== Iterazione IRL 19 ===
Loss reward (iter 19): 6.703983306884766
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -277     |
|    critic_loss     | 5.33     |
|    ent_coef        | 0.16     |
|    ent_coef_loss   | 0.296    |
|    learning_rate   | 0.0003   |
|    n_updates       | 26899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -277     |
|    critic_loss     | 5.07     |
|    ent_coef        | 0.15     |
|    ent_coef_loss   | 0.121    |
|    learning_rate   | 0.0003   |
|    n_updates       | 27299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -278     |
|    critic_loss     | 5.27     |
|    ent_coef        | 0.144    |
|    ent_coef_loss   | 0.295    |
|    learning_rate   | 0.0003   |
|    n_updates       | 27699    |
---------------------------------
=== Iterazione IRL 20 ===
Loss reward (iter 20): 6.80211877822876
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -275     |
|    critic_loss     | 5.52     |
|    ent_coef        | 0.134    |
|    ent_coef_loss   | -0.71    |
|    learning_rate   | 0.0003   |
|    n_updates       | 28299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -275     |
|    critic_loss     | 5.36     |
|    ent_coef        | 0.13     |
|    ent_coef_loss   | 0.822    |
|    learning_rate   | 0.0003   |
|    n_updates       | 28699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -276     |
|    critic_loss     | 5.04     |
|    ent_coef        | 0.13     |
|    ent_coef_loss   | 0.644    |
|    learning_rate   | 0.0003   |
|    n_updates       | 29099    |
---------------------------------
=== Iterazione IRL 21 ===
Loss reward (iter 21): 6.437526226043701
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -275     |
|    critic_loss     | 6.93     |
|    ent_coef        | 0.132    |
|    ent_coef_loss   | 0.338    |
|    learning_rate   | 0.0003   |
|    n_updates       | 29699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -275     |
|    critic_loss     | 7.66     |
|    ent_coef        | 0.135    |
|    ent_coef_loss   | 0.165    |
|    learning_rate   | 0.0003   |
|    n_updates       | 30099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -275     |
|    critic_loss     | 5.23     |
|    ent_coef        | 0.14     |
|    ent_coef_loss   | -0.014   |
|    learning_rate   | 0.0003   |
|    n_updates       | 30499    |
---------------------------------
=== Iterazione IRL 22 ===
Loss reward (iter 22): 7.308168888092041
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -273     |
|    critic_loss     | 6.42     |
|    ent_coef        | 0.148    |
|    ent_coef_loss   | 0.175    |
|    learning_rate   | 0.0003   |
|    n_updates       | 31099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -276     |
|    critic_loss     | 5.9      |
|    ent_coef        | 0.153    |
|    ent_coef_loss   | 0.159    |
|    learning_rate   | 0.0003   |
|    n_updates       | 31499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -273     |
|    critic_loss     | 6.1      |
|    ent_coef        | 0.16     |
|    ent_coef_loss   | 0.0827   |
|    learning_rate   | 0.0003   |
|    n_updates       | 31899    |
---------------------------------
=== Iterazione IRL 23 ===
Loss reward (iter 23): 7.1219916343688965
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -273     |
|    critic_loss     | 6.47     |
|    ent_coef        | 0.163    |
|    ent_coef_loss   | -0.299   |
|    learning_rate   | 0.0003   |
|    n_updates       | 32499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -272     |
|    critic_loss     | 4.8      |
|    ent_coef        | 0.158    |
|    ent_coef_loss   | -0.353   |
|    learning_rate   | 0.0003   |
|    n_updates       | 32899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -273     |
|    critic_loss     | 6.87     |
|    ent_coef        | 0.153    |
|    ent_coef_loss   | 0.476    |
|    learning_rate   | 0.0003   |
|    n_updates       | 33299    |
---------------------------------
=== Iterazione IRL 24 ===
Loss reward (iter 24): 6.489364147186279
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -269     |
|    critic_loss     | 7.48     |
|    ent_coef        | 0.153    |
|    ent_coef_loss   | -0.176   |
|    learning_rate   | 0.0003   |
|    n_updates       | 33899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -269     |
|    critic_loss     | 5.84     |
|    ent_coef        | 0.149    |
|    ent_coef_loss   | 0.786    |
|    learning_rate   | 0.0003   |
|    n_updates       | 34299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -267     |
|    critic_loss     | 6.02     |
|    ent_coef        | 0.14     |
|    ent_coef_loss   | 0.292    |
|    learning_rate   | 0.0003   |
|    n_updates       | 34699    |
---------------------------------
=== Iterazione IRL 25 ===
Loss reward (iter 25): 6.361607074737549
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -268     |
|    critic_loss     | 5.89     |
|    ent_coef        | 0.132    |
|    ent_coef_loss   | 0.169    |
|    learning_rate   | 0.0003   |
|    n_updates       | 35299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -267     |
|    critic_loss     | 6.37     |
|    ent_coef        | 0.146    |
|    ent_coef_loss   | -0.086   |
|    learning_rate   | 0.0003   |
|    n_updates       | 35699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -266     |
|    critic_loss     | 6.7      |
|    ent_coef        | 0.157    |
|    ent_coef_loss   | 0.535    |
|    learning_rate   | 0.0003   |
|    n_updates       | 36099    |
---------------------------------
=== Iterazione IRL 26 ===
Loss reward (iter 26): 6.239951133728027
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -265     |
|    critic_loss     | 6.5      |
|    ent_coef        | 0.165    |
|    ent_coef_loss   | 0.786    |
|    learning_rate   | 0.0003   |
|    n_updates       | 36699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -265     |
|    critic_loss     | 6.26     |
|    ent_coef        | 0.162    |
|    ent_coef_loss   | 0.0596   |
|    learning_rate   | 0.0003   |
|    n_updates       | 37099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -265     |
|    critic_loss     | 6.13     |
|    ent_coef        | 0.159    |
|    ent_coef_loss   | 0.167    |
|    learning_rate   | 0.0003   |
|    n_updates       | 37499    |
---------------------------------
=== Iterazione IRL 27 ===
Loss reward (iter 27): 5.989346504211426
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -264     |
|    critic_loss     | 7.5      |
|    ent_coef        | 0.152    |
|    ent_coef_loss   | -0.264   |
|    learning_rate   | 0.0003   |
|    n_updates       | 38099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -262     |
|    critic_loss     | 7.2      |
|    ent_coef        | 0.147    |
|    ent_coef_loss   | -0.244   |
|    learning_rate   | 0.0003   |
|    n_updates       | 38499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -262     |
|    critic_loss     | 7.47     |
|    ent_coef        | 0.145    |
|    ent_coef_loss   | -0.12    |
|    learning_rate   | 0.0003   |
|    n_updates       | 38899    |
---------------------------------
=== Iterazione IRL 28 ===
Loss reward (iter 28): 5.876407623291016
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -259     |
|    critic_loss     | 7.86     |
|    ent_coef        | 0.143    |
|    ent_coef_loss   | -0.365   |
|    learning_rate   | 0.0003   |
|    n_updates       | 39499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -259     |
|    critic_loss     | 6.07     |
|    ent_coef        | 0.139    |
|    ent_coef_loss   | -0.0877  |
|    learning_rate   | 0.0003   |
|    n_updates       | 39899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -260     |
|    critic_loss     | 7.65     |
|    ent_coef        | 0.138    |
|    ent_coef_loss   | -0.0734  |
|    learning_rate   | 0.0003   |
|    n_updates       | 40299    |
---------------------------------
=== Iterazione IRL 29 ===
Loss reward (iter 29): 5.844511032104492
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -255     |
|    critic_loss     | 6.04     |
|    ent_coef        | 0.137    |
|    ent_coef_loss   | -0.154   |
|    learning_rate   | 0.0003   |
|    n_updates       | 40899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -255     |
|    critic_loss     | 5.36     |
|    ent_coef        | 0.138    |
|    ent_coef_loss   | 0.685    |
|    learning_rate   | 0.0003   |
|    n_updates       | 41299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -254     |
|    critic_loss     | 7.29     |
|    ent_coef        | 0.139    |
|    ent_coef_loss   | 0.529    |
|    learning_rate   | 0.0003   |
|    n_updates       | 41699    |
---------------------------------
=== Iterazione IRL 30 ===
Loss reward (iter 30): 5.935800552368164
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -253     |
|    critic_loss     | 7.07     |
|    ent_coef        | 0.14     |
|    ent_coef_loss   | -0.682   |
|    learning_rate   | 0.0003   |
|    n_updates       | 42299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -251     |
|    critic_loss     | 6.63     |
|    ent_coef        | 0.136    |
|    ent_coef_loss   | 0.23     |
|    learning_rate   | 0.0003   |
|    n_updates       | 42699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -252     |
|    critic_loss     | 6.71     |
|    ent_coef        | 0.13     |
|    ent_coef_loss   | 0.445    |
|    learning_rate   | 0.0003   |
|    n_updates       | 43099    |
---------------------------------
=== Iterazione IRL 31 ===
Loss reward (iter 31): 5.385963439941406
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -249     |
|    critic_loss     | 6.79     |
|    ent_coef        | 0.132    |
|    ent_coef_loss   | 0.194    |
|    learning_rate   | 0.0003   |
|    n_updates       | 43699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -248     |
|    critic_loss     | 7.35     |
|    ent_coef        | 0.129    |
|    ent_coef_loss   | 0.101    |
|    learning_rate   | 0.0003   |
|    n_updates       | 44099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -248     |
|    critic_loss     | 7.44     |
|    ent_coef        | 0.126    |
|    ent_coef_loss   | 0.771    |
|    learning_rate   | 0.0003   |
|    n_updates       | 44499    |
---------------------------------
=== Iterazione IRL 32 ===
Loss reward (iter 32): 6.884857177734375
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -244     |
|    critic_loss     | 6.8      |
|    ent_coef        | 0.131    |
|    ent_coef_loss   | 0.0982   |
|    learning_rate   | 0.0003   |
|    n_updates       | 45099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -243     |
|    critic_loss     | 7.02     |
|    ent_coef        | 0.13     |
|    ent_coef_loss   | -0.92    |
|    learning_rate   | 0.0003   |
|    n_updates       | 45499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -245     |
|    critic_loss     | 7.19     |
|    ent_coef        | 0.136    |
|    ent_coef_loss   | 0.724    |
|    learning_rate   | 0.0003   |
|    n_updates       | 45899    |
---------------------------------
=== Iterazione IRL 33 ===
Loss reward (iter 33): 7.162128925323486
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -242     |
|    critic_loss     | 8.73     |
|    ent_coef        | 0.154    |
|    ent_coef_loss   | -0.0796  |
|    learning_rate   | 0.0003   |
|    n_updates       | 46499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -243     |
|    critic_loss     | 7.5      |
|    ent_coef        | 0.16     |
|    ent_coef_loss   | 0.0968   |
|    learning_rate   | 0.0003   |
|    n_updates       | 46899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -239     |
|    critic_loss     | 8.1      |
|    ent_coef        | 0.161    |
|    ent_coef_loss   | 0.223    |
|    learning_rate   | 0.0003   |
|    n_updates       | 47299    |
---------------------------------
=== Iterazione IRL 34 ===
Loss reward (iter 34): 6.828193664550781
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -238     |
|    critic_loss     | 8.32     |
|    ent_coef        | 0.167    |
|    ent_coef_loss   | -0.0367  |
|    learning_rate   | 0.0003   |
|    n_updates       | 47899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -237     |
|    critic_loss     | 6.87     |
|    ent_coef        | 0.168    |
|    ent_coef_loss   | 0.164    |
|    learning_rate   | 0.0003   |
|    n_updates       | 48299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -238     |
|    critic_loss     | 7.15     |
|    ent_coef        | 0.165    |
|    ent_coef_loss   | -0.264   |
|    learning_rate   | 0.0003   |
|    n_updates       | 48699    |
---------------------------------
=== Iterazione IRL 35 ===
Loss reward (iter 35): 6.990331649780273
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -237     |
|    critic_loss     | 8.2      |
|    ent_coef        | 0.165    |
|    ent_coef_loss   | 0.694    |
|    learning_rate   | 0.0003   |
|    n_updates       | 49299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -236     |
|    critic_loss     | 8.11     |
|    ent_coef        | 0.161    |
|    ent_coef_loss   | 0.111    |
|    learning_rate   | 0.0003   |
|    n_updates       | 49699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -233     |
|    critic_loss     | 6.45     |
|    ent_coef        | 0.157    |
|    ent_coef_loss   | -0.482   |
|    learning_rate   | 0.0003   |
|    n_updates       | 50099    |
---------------------------------
=== Iterazione IRL 36 ===
Loss reward (iter 36): 6.623815059661865
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -234     |
|    critic_loss     | 8.83     |
|    ent_coef        | 0.154    |
|    ent_coef_loss   | -0.398   |
|    learning_rate   | 0.0003   |
|    n_updates       | 50699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -232     |
|    critic_loss     | 8.63     |
|    ent_coef        | 0.154    |
|    ent_coef_loss   | -0.301   |
|    learning_rate   | 0.0003   |
|    n_updates       | 51099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -232     |
|    critic_loss     | 7.28     |
|    ent_coef        | 0.152    |
|    ent_coef_loss   | -0.362   |
|    learning_rate   | 0.0003   |
|    n_updates       | 51499    |
---------------------------------
=== Iterazione IRL 37 ===
Loss reward (iter 37): 6.677615642547607
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -230     |
|    critic_loss     | 7.54     |
|    ent_coef        | 0.151    |
|    ent_coef_loss   | 0.368    |
|    learning_rate   | 0.0003   |
|    n_updates       | 52099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -227     |
|    critic_loss     | 6.32     |
|    ent_coef        | 0.148    |
|    ent_coef_loss   | -0.339   |
|    learning_rate   | 0.0003   |
|    n_updates       | 52499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -230     |
|    critic_loss     | 8.59     |
|    ent_coef        | 0.144    |
|    ent_coef_loss   | 0.205    |
|    learning_rate   | 0.0003   |
|    n_updates       | 52899    |
---------------------------------
=== Iterazione IRL 38 ===
Loss reward (iter 38): 6.72064733505249
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -228     |
|    critic_loss     | 8.84     |
|    ent_coef        | 0.137    |
|    ent_coef_loss   | 0.169    |
|    learning_rate   | 0.0003   |
|    n_updates       | 53499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -227     |
|    critic_loss     | 8.1      |
|    ent_coef        | 0.136    |
|    ent_coef_loss   | -0.19    |
|    learning_rate   | 0.0003   |
|    n_updates       | 53899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -225     |
|    critic_loss     | 8.96     |
|    ent_coef        | 0.133    |
|    ent_coef_loss   | -0.446   |
|    learning_rate   | 0.0003   |
|    n_updates       | 54299    |
---------------------------------
=== Iterazione IRL 39 ===
Loss reward (iter 39): 6.723384857177734
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -224     |
|    critic_loss     | 10.2     |
|    ent_coef        | 0.13     |
|    ent_coef_loss   | 0.814    |
|    learning_rate   | 0.0003   |
|    n_updates       | 54899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -222     |
|    critic_loss     | 7.27     |
|    ent_coef        | 0.129    |
|    ent_coef_loss   | -0.182   |
|    learning_rate   | 0.0003   |
|    n_updates       | 55299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -222     |
|    critic_loss     | 8.7      |
|    ent_coef        | 0.126    |
|    ent_coef_loss   | 0.178    |
|    learning_rate   | 0.0003   |
|    n_updates       | 55699    |
---------------------------------
=== Iterazione IRL 40 ===
Loss reward (iter 40): 6.6138529777526855
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -222     |
|    critic_loss     | 10.3     |
|    ent_coef        | 0.122    |
|    ent_coef_loss   | 0.317    |
|    learning_rate   | 0.0003   |
|    n_updates       | 56299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -220     |
|    critic_loss     | 10.5     |
|    ent_coef        | 0.119    |
|    ent_coef_loss   | -0.738   |
|    learning_rate   | 0.0003   |
|    n_updates       | 56699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -218     |
|    critic_loss     | 8.63     |
|    ent_coef        | 0.119    |
|    ent_coef_loss   | 0.425    |
|    learning_rate   | 0.0003   |
|    n_updates       | 57099    |
---------------------------------
=== Iterazione IRL 41 ===
Loss reward (iter 41): 6.658719062805176
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -217     |
|    critic_loss     | 10.8     |
|    ent_coef        | 0.118    |
|    ent_coef_loss   | -0.825   |
|    learning_rate   | 0.0003   |
|    n_updates       | 57699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -217     |
|    critic_loss     | 10.1     |
|    ent_coef        | 0.12     |
|    ent_coef_loss   | 0.181    |
|    learning_rate   | 0.0003   |
|    n_updates       | 58099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -215     |
|    critic_loss     | 10.2     |
|    ent_coef        | 0.123    |
|    ent_coef_loss   | -0.677   |
|    learning_rate   | 0.0003   |
|    n_updates       | 58499    |
---------------------------------
=== Iterazione IRL 42 ===
Loss reward (iter 42): 6.470555782318115
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -211     |
|    critic_loss     | 7.68     |
|    ent_coef        | 0.12     |
|    ent_coef_loss   | -0.309   |
|    learning_rate   | 0.0003   |
|    n_updates       | 59099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -211     |
|    critic_loss     | 7.97     |
|    ent_coef        | 0.118    |
|    ent_coef_loss   | -0.624   |
|    learning_rate   | 0.0003   |
|    n_updates       | 59499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -211     |
|    critic_loss     | 9.78     |
|    ent_coef        | 0.123    |
|    ent_coef_loss   | 0.263    |
|    learning_rate   | 0.0003   |
|    n_updates       | 59899    |
---------------------------------
=== Iterazione IRL 43 ===
Loss reward (iter 43): 6.628857135772705
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -212     |
|    critic_loss     | 10.6     |
|    ent_coef        | 0.122    |
|    ent_coef_loss   | 0.165    |
|    learning_rate   | 0.0003   |
|    n_updates       | 60499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -208     |
|    critic_loss     | 8.93     |
|    ent_coef        | 0.122    |
|    ent_coef_loss   | -0.309   |
|    learning_rate   | 0.0003   |
|    n_updates       | 60899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -208     |
|    critic_loss     | 7.63     |
|    ent_coef        | 0.121    |
|    ent_coef_loss   | -0.45    |
|    learning_rate   | 0.0003   |
|    n_updates       | 61299    |
---------------------------------
=== Iterazione IRL 44 ===
Loss reward (iter 44): 5.904921531677246
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -207     |
|    critic_loss     | 9.36     |
|    ent_coef        | 0.123    |
|    ent_coef_loss   | -0.545   |
|    learning_rate   | 0.0003   |
|    n_updates       | 61899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -206     |
|    critic_loss     | 8.97     |
|    ent_coef        | 0.123    |
|    ent_coef_loss   | 0.0761   |
|    learning_rate   | 0.0003   |
|    n_updates       | 62299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -206     |
|    critic_loss     | 9.21     |
|    ent_coef        | 0.125    |
|    ent_coef_loss   | -0.5     |
|    learning_rate   | 0.0003   |
|    n_updates       | 62699    |
---------------------------------
=== Iterazione IRL 45 ===
Loss reward (iter 45): 5.983726978302002
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -204     |
|    critic_loss     | 8.92     |
|    ent_coef        | 0.125    |
|    ent_coef_loss   | -0.949   |
|    learning_rate   | 0.0003   |
|    n_updates       | 63299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -201     |
|    critic_loss     | 9.61     |
|    ent_coef        | 0.123    |
|    ent_coef_loss   | -0.245   |
|    learning_rate   | 0.0003   |
|    n_updates       | 63699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -201     |
|    critic_loss     | 8.46     |
|    ent_coef        | 0.122    |
|    ent_coef_loss   | 0.648    |
|    learning_rate   | 0.0003   |
|    n_updates       | 64099    |
---------------------------------
=== Iterazione IRL 46 ===
Loss reward (iter 46): 6.418949604034424
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -202     |
|    critic_loss     | 10.2     |
|    ent_coef        | 0.123    |
|    ent_coef_loss   | -0.237   |
|    learning_rate   | 0.0003   |
|    n_updates       | 64699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -198     |
|    critic_loss     | 10.3     |
|    ent_coef        | 0.117    |
|    ent_coef_loss   | -0.328   |
|    learning_rate   | 0.0003   |
|    n_updates       | 65099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -198     |
|    critic_loss     | 9.59     |
|    ent_coef        | 0.117    |
|    ent_coef_loss   | -0.923   |
|    learning_rate   | 0.0003   |
|    n_updates       | 65499    |
---------------------------------
=== Iterazione IRL 47 ===
Loss reward (iter 47): 6.150280475616455
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -196     |
|    critic_loss     | 10.4     |
|    ent_coef        | 0.11     |
|    ent_coef_loss   | 0.185    |
|    learning_rate   | 0.0003   |
|    n_updates       | 66099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -197     |
|    critic_loss     | 10.3     |
|    ent_coef        | 0.107    |
|    ent_coef_loss   | 0.338    |
|    learning_rate   | 0.0003   |
|    n_updates       | 66499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -196     |
|    critic_loss     | 10.2     |
|    ent_coef        | 0.102    |
|    ent_coef_loss   | -0.751   |
|    learning_rate   | 0.0003   |
|    n_updates       | 66899    |
---------------------------------
=== Iterazione IRL 48 ===
Loss reward (iter 48): 5.6815948486328125
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -192     |
|    critic_loss     | 9.47     |
|    ent_coef        | 0.0994   |
|    ent_coef_loss   | 0.876    |
|    learning_rate   | 0.0003   |
|    n_updates       | 67499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -191     |
|    critic_loss     | 10.1     |
|    ent_coef        | 0.108    |
|    ent_coef_loss   | 0.462    |
|    learning_rate   | 0.0003   |
|    n_updates       | 67899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -190     |
|    critic_loss     | 9.93     |
|    ent_coef        | 0.116    |
|    ent_coef_loss   | -0.889   |
|    learning_rate   | 0.0003   |
|    n_updates       | 68299    |
---------------------------------
=== Iterazione IRL 49 ===
Loss reward (iter 49): 5.924587249755859
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -192     |
|    critic_loss     | 12       |
|    ent_coef        | 0.125    |
|    ent_coef_loss   | 1.08     |
|    learning_rate   | 0.0003   |
|    n_updates       | 68899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -189     |
|    critic_loss     | 10.4     |
|    ent_coef        | 0.126    |
|    ent_coef_loss   | 0.0412   |
|    learning_rate   | 0.0003   |
|    n_updates       | 69299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -188     |
|    critic_loss     | 10.9     |
|    ent_coef        | 0.126    |
|    ent_coef_loss   | -0.87    |
|    learning_rate   | 0.0003   |
|    n_updates       | 69699    |
---------------------------------
=== Iterazione IRL 50 ===
Loss reward (iter 50): 5.480456829071045
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -186     |
|    critic_loss     | 9.17     |
|    ent_coef        | 0.128    |
|    ent_coef_loss   | 0.126    |
|    learning_rate   | 0.0003   |
|    n_updates       | 70299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -185     |
|    critic_loss     | 10.4     |
|    ent_coef        | 0.127    |
|    ent_coef_loss   | 0.056    |
|    learning_rate   | 0.0003   |
|    n_updates       | 70699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -188     |
|    critic_loss     | 10.3     |
|    ent_coef        | 0.124    |
|    ent_coef_loss   | 0.0563   |
|    learning_rate   | 0.0003   |
|    n_updates       | 71099    |
---------------------------------
=== Iterazione IRL 51 ===
Loss reward (iter 51): 6.287662029266357
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -185     |
|    critic_loss     | 8.6      |
|    ent_coef        | 0.12     |
|    ent_coef_loss   | -0.0592  |
|    learning_rate   | 0.0003   |
|    n_updates       | 71699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -183     |
|    critic_loss     | 10.2     |
|    ent_coef        | 0.119    |
|    ent_coef_loss   | 0.407    |
|    learning_rate   | 0.0003   |
|    n_updates       | 72099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -181     |
|    critic_loss     | 9.67     |
|    ent_coef        | 0.119    |
|    ent_coef_loss   | -0.813   |
|    learning_rate   | 0.0003   |
|    n_updates       | 72499    |
---------------------------------
=== Iterazione IRL 52 ===
Loss reward (iter 52): 6.419118881225586
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -182     |
|    critic_loss     | 8.16     |
|    ent_coef        | 0.118    |
|    ent_coef_loss   | -0.263   |
|    learning_rate   | 0.0003   |
|    n_updates       | 73099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -184     |
|    critic_loss     | 11       |
|    ent_coef        | 0.115    |
|    ent_coef_loss   | -0.122   |
|    learning_rate   | 0.0003   |
|    n_updates       | 73499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -181     |
|    critic_loss     | 9.71     |
|    ent_coef        | 0.115    |
|    ent_coef_loss   | 0.509    |
|    learning_rate   | 0.0003   |
|    n_updates       | 73899    |
---------------------------------
=== Iterazione IRL 53 ===
Loss reward (iter 53): 6.252817153930664
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -179     |
|    critic_loss     | 10.2     |
|    ent_coef        | 0.117    |
|    ent_coef_loss   | -0.203   |
|    learning_rate   | 0.0003   |
|    n_updates       | 74499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -177     |
|    critic_loss     | 9.63     |
|    ent_coef        | 0.117    |
|    ent_coef_loss   | -0.439   |
|    learning_rate   | 0.0003   |
|    n_updates       | 74899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -176     |
|    critic_loss     | 9.49     |
|    ent_coef        | 0.117    |
|    ent_coef_loss   | 0.248    |
|    learning_rate   | 0.0003   |
|    n_updates       | 75299    |
---------------------------------
=== Iterazione IRL 54 ===
Loss reward (iter 54): 6.394525527954102
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -180     |
|    critic_loss     | 10.4     |
|    ent_coef        | 0.117    |
|    ent_coef_loss   | -0.483   |
|    learning_rate   | 0.0003   |
|    n_updates       | 75899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -176     |
|    critic_loss     | 9.27     |
|    ent_coef        | 0.116    |
|    ent_coef_loss   | -0.361   |
|    learning_rate   | 0.0003   |
|    n_updates       | 76299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -174     |
|    critic_loss     | 9.07     |
|    ent_coef        | 0.119    |
|    ent_coef_loss   | -0.609   |
|    learning_rate   | 0.0003   |
|    n_updates       | 76699    |
---------------------------------
=== Iterazione IRL 55 ===
Loss reward (iter 55): 6.519174098968506
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -175     |
|    critic_loss     | 10.1     |
|    ent_coef        | 0.117    |
|    ent_coef_loss   | -0.0202  |
|    learning_rate   | 0.0003   |
|    n_updates       | 77299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -175     |
|    critic_loss     | 9.82     |
|    ent_coef        | 0.114    |
|    ent_coef_loss   | 0.138    |
|    learning_rate   | 0.0003   |
|    n_updates       | 77699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -174     |
|    critic_loss     | 9.4      |
|    ent_coef        | 0.113    |
|    ent_coef_loss   | -0.0066  |
|    learning_rate   | 0.0003   |
|    n_updates       | 78099    |
---------------------------------
=== Iterazione IRL 56 ===
Loss reward (iter 56): 6.178219318389893
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -172     |
|    critic_loss     | 8.24     |
|    ent_coef        | 0.114    |
|    ent_coef_loss   | 0.11     |
|    learning_rate   | 0.0003   |
|    n_updates       | 78699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -172     |
|    critic_loss     | 8.54     |
|    ent_coef        | 0.116    |
|    ent_coef_loss   | 0.0277   |
|    learning_rate   | 0.0003   |
|    n_updates       | 79099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -170     |
|    critic_loss     | 7.84     |
|    ent_coef        | 0.114    |
|    ent_coef_loss   | -0.0614  |
|    learning_rate   | 0.0003   |
|    n_updates       | 79499    |
---------------------------------
=== Iterazione IRL 57 ===
Loss reward (iter 57): 6.106719970703125
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -169     |
|    critic_loss     | 6.92     |
|    ent_coef        | 0.114    |
|    ent_coef_loss   | -0.147   |
|    learning_rate   | 0.0003   |
|    n_updates       | 80099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -169     |
|    critic_loss     | 9.35     |
|    ent_coef        | 0.111    |
|    ent_coef_loss   | 0.0232   |
|    learning_rate   | 0.0003   |
|    n_updates       | 80499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -168     |
|    critic_loss     | 8.55     |
|    ent_coef        | 0.109    |
|    ent_coef_loss   | 0.0747   |
|    learning_rate   | 0.0003   |
|    n_updates       | 80899    |
---------------------------------
=== Iterazione IRL 58 ===
Loss reward (iter 58): 6.170285224914551
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -168     |
|    critic_loss     | 8.98     |
|    ent_coef        | 0.111    |
|    ent_coef_loss   | -0.283   |
|    learning_rate   | 0.0003   |
|    n_updates       | 81499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -167     |
|    critic_loss     | 8.26     |
|    ent_coef        | 0.108    |
|    ent_coef_loss   | 0.165    |
|    learning_rate   | 0.0003   |
|    n_updates       | 81899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -167     |
|    critic_loss     | 7.41     |
|    ent_coef        | 0.109    |
|    ent_coef_loss   | 0.261    |
|    learning_rate   | 0.0003   |
|    n_updates       | 82299    |
---------------------------------
=== Iterazione IRL 59 ===
Loss reward (iter 59): 5.82390832901001
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -167     |
|    critic_loss     | 8.74     |
|    ent_coef        | 0.105    |
|    ent_coef_loss   | 0.0904   |
|    learning_rate   | 0.0003   |
|    n_updates       | 82899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -166     |
|    critic_loss     | 7.41     |
|    ent_coef        | 0.107    |
|    ent_coef_loss   | 0.144    |
|    learning_rate   | 0.0003   |
|    n_updates       | 83299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -167     |
|    critic_loss     | 8.65     |
|    ent_coef        | 0.109    |
|    ent_coef_loss   | 0.691    |
|    learning_rate   | 0.0003   |
|    n_updates       | 83699    |
---------------------------------
=== Iterazione IRL 60 ===
Loss reward (iter 60): 5.751716136932373
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -165     |
|    critic_loss     | 7.62     |
|    ent_coef        | 0.113    |
|    ent_coef_loss   | -0.186   |
|    learning_rate   | 0.0003   |
|    n_updates       | 84299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -164     |
|    critic_loss     | 7.9      |
|    ent_coef        | 0.117    |
|    ent_coef_loss   | -0.428   |
|    learning_rate   | 0.0003   |
|    n_updates       | 84699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -165     |
|    critic_loss     | 6.74     |
|    ent_coef        | 0.113    |
|    ent_coef_loss   | 0.417    |
|    learning_rate   | 0.0003   |
|    n_updates       | 85099    |
---------------------------------
=== Iterazione IRL 61 ===
Loss reward (iter 61): 5.68312406539917
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -165     |
|    critic_loss     | 8.63     |
|    ent_coef        | 0.121    |
|    ent_coef_loss   | -0.0813  |
|    learning_rate   | 0.0003   |
|    n_updates       | 85699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -164     |
|    critic_loss     | 6.66     |
|    ent_coef        | 0.129    |
|    ent_coef_loss   | -0.453   |
|    learning_rate   | 0.0003   |
|    n_updates       | 86099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -164     |
|    critic_loss     | 6.24     |
|    ent_coef        | 0.13     |
|    ent_coef_loss   | 0.225    |
|    learning_rate   | 0.0003   |
|    n_updates       | 86499    |
---------------------------------
=== Iterazione IRL 62 ===
Loss reward (iter 62): 6.29292631149292
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -166     |
|    critic_loss     | 7.35     |
|    ent_coef        | 0.13     |
|    ent_coef_loss   | 0.676    |
|    learning_rate   | 0.0003   |
|    n_updates       | 87099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -164     |
|    critic_loss     | 7.78     |
|    ent_coef        | 0.124    |
|    ent_coef_loss   | -0.267   |
|    learning_rate   | 0.0003   |
|    n_updates       | 87499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -164     |
|    critic_loss     | 6.32     |
|    ent_coef        | 0.122    |
|    ent_coef_loss   | -0.479   |
|    learning_rate   | 0.0003   |
|    n_updates       | 87899    |
---------------------------------
=== Iterazione IRL 63 ===
Loss reward (iter 63): 6.385756969451904
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -163     |
|    critic_loss     | 8.35     |
|    ent_coef        | 0.118    |
|    ent_coef_loss   | -0.537   |
|    learning_rate   | 0.0003   |
|    n_updates       | 88499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -163     |
|    critic_loss     | 7.71     |
|    ent_coef        | 0.118    |
|    ent_coef_loss   | 0.178    |
|    learning_rate   | 0.0003   |
|    n_updates       | 88899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -165     |
|    critic_loss     | 7.19     |
|    ent_coef        | 0.117    |
|    ent_coef_loss   | -0.592   |
|    learning_rate   | 0.0003   |
|    n_updates       | 89299    |
---------------------------------
=== Iterazione IRL 64 ===
Loss reward (iter 64): 6.2860517501831055
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -162     |
|    critic_loss     | 7.26     |
|    ent_coef        | 0.112    |
|    ent_coef_loss   | 0.258    |
|    learning_rate   | 0.0003   |
|    n_updates       | 89899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -164     |
|    critic_loss     | 7.78     |
|    ent_coef        | 0.114    |
|    ent_coef_loss   | 0.53     |
|    learning_rate   | 0.0003   |
|    n_updates       | 90299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -163     |
|    critic_loss     | 7.07     |
|    ent_coef        | 0.113    |
|    ent_coef_loss   | 0.0781   |
|    learning_rate   | 0.0003   |
|    n_updates       | 90699    |
---------------------------------
=== Iterazione IRL 65 ===
Loss reward (iter 65): 6.290550231933594
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -163     |
|    critic_loss     | 7.38     |
|    ent_coef        | 0.12     |
|    ent_coef_loss   | 0.619    |
|    learning_rate   | 0.0003   |
|    n_updates       | 91299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -161     |
|    critic_loss     | 7.2      |
|    ent_coef        | 0.122    |
|    ent_coef_loss   | -0.0752  |
|    learning_rate   | 0.0003   |
|    n_updates       | 91699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -162     |
|    critic_loss     | 7.25     |
|    ent_coef        | 0.122    |
|    ent_coef_loss   | 0.612    |
|    learning_rate   | 0.0003   |
|    n_updates       | 92099    |
---------------------------------
=== Iterazione IRL 66 ===
Loss reward (iter 66): 6.530412673950195
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -163     |
|    critic_loss     | 6.98     |
|    ent_coef        | 0.128    |
|    ent_coef_loss   | 0.0205   |
|    learning_rate   | 0.0003   |
|    n_updates       | 92699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -163     |
|    critic_loss     | 6.38     |
|    ent_coef        | 0.131    |
|    ent_coef_loss   | 0.172    |
|    learning_rate   | 0.0003   |
|    n_updates       | 93099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -163     |
|    critic_loss     | 6.89     |
|    ent_coef        | 0.131    |
|    ent_coef_loss   | -0.18    |
|    learning_rate   | 0.0003   |
|    n_updates       | 93499    |
---------------------------------
=== Iterazione IRL 67 ===
Loss reward (iter 67): 6.407974720001221
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -162     |
|    critic_loss     | 6.41     |
|    ent_coef        | 0.133    |
|    ent_coef_loss   | -0.945   |
|    learning_rate   | 0.0003   |
|    n_updates       | 94099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -160     |
|    critic_loss     | 5.48     |
|    ent_coef        | 0.132    |
|    ent_coef_loss   | 0.525    |
|    learning_rate   | 0.0003   |
|    n_updates       | 94499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -163     |
|    critic_loss     | 7.5      |
|    ent_coef        | 0.13     |
|    ent_coef_loss   | 0.854    |
|    learning_rate   | 0.0003   |
|    n_updates       | 94899    |
---------------------------------
=== Iterazione IRL 68 ===
Loss reward (iter 68): 6.064176559448242
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -163     |
|    critic_loss     | 8.01     |
|    ent_coef        | 0.123    |
|    ent_coef_loss   | -0.158   |
|    learning_rate   | 0.0003   |
|    n_updates       | 95499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -162     |
|    critic_loss     | 6.55     |
|    ent_coef        | 0.12     |
|    ent_coef_loss   | 0.804    |
|    learning_rate   | 0.0003   |
|    n_updates       | 95899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -163     |
|    critic_loss     | 6.57     |
|    ent_coef        | 0.121    |
|    ent_coef_loss   | 0.0084   |
|    learning_rate   | 0.0003   |
|    n_updates       | 96299    |
---------------------------------
=== Iterazione IRL 69 ===
Loss reward (iter 69): 6.239553928375244
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -162     |
|    critic_loss     | 6.07     |
|    ent_coef        | 0.122    |
|    ent_coef_loss   | -0.551   |
|    learning_rate   | 0.0003   |
|    n_updates       | 96899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -163     |
|    critic_loss     | 6.55     |
|    ent_coef        | 0.122    |
|    ent_coef_loss   | -0.477   |
|    learning_rate   | 0.0003   |
|    n_updates       | 97299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -165     |
|    critic_loss     | 7.02     |
|    ent_coef        | 0.123    |
|    ent_coef_loss   | 0.237    |
|    learning_rate   | 0.0003   |
|    n_updates       | 97699    |
---------------------------------
=== Iterazione IRL 70 ===
Loss reward (iter 70): 6.361224174499512
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -164     |
|    critic_loss     | 6.05     |
|    ent_coef        | 0.123    |
|    ent_coef_loss   | -0.247   |
|    learning_rate   | 0.0003   |
|    n_updates       | 98299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -162     |
|    critic_loss     | 6.28     |
|    ent_coef        | 0.129    |
|    ent_coef_loss   | -0.165   |
|    learning_rate   | 0.0003   |
|    n_updates       | 98699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -165     |
|    critic_loss     | 6.3      |
|    ent_coef        | 0.128    |
|    ent_coef_loss   | -0.0466  |
|    learning_rate   | 0.0003   |
|    n_updates       | 99099    |
---------------------------------
=== Iterazione IRL 71 ===
Loss reward (iter 71): 6.347405910491943
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -164     |
|    critic_loss     | 5.46     |
|    ent_coef        | 0.129    |
|    ent_coef_loss   | -0.0526  |
|    learning_rate   | 0.0003   |
|    n_updates       | 99699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -164     |
|    critic_loss     | 5.69     |
|    ent_coef        | 0.127    |
|    ent_coef_loss   | -0.0299  |
|    learning_rate   | 0.0003   |
|    n_updates       | 100099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -164     |
|    critic_loss     | 4.78     |
|    ent_coef        | 0.128    |
|    ent_coef_loss   | 0.0239   |
|    learning_rate   | 0.0003   |
|    n_updates       | 100499   |
---------------------------------
=== Iterazione IRL 72 ===
Loss reward (iter 72): 6.006427764892578
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -164     |
|    critic_loss     | 3.84     |
|    ent_coef        | 0.127    |
|    ent_coef_loss   | 0.242    |
|    learning_rate   | 0.0003   |
|    n_updates       | 101099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -166     |
|    critic_loss     | 6.09     |
|    ent_coef        | 0.13     |
|    ent_coef_loss   | -0.226   |
|    learning_rate   | 0.0003   |
|    n_updates       | 101499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -165     |
|    critic_loss     | 6.44     |
|    ent_coef        | 0.132    |
|    ent_coef_loss   | -0.0708  |
|    learning_rate   | 0.0003   |
|    n_updates       | 101899   |
---------------------------------
=== Iterazione IRL 73 ===
Loss reward (iter 73): 5.638018608093262
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -165     |
|    critic_loss     | 5.99     |
|    ent_coef        | 0.135    |
|    ent_coef_loss   | -0.423   |
|    learning_rate   | 0.0003   |
|    n_updates       | 102499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -166     |
|    critic_loss     | 6.11     |
|    ent_coef        | 0.133    |
|    ent_coef_loss   | 0.292    |
|    learning_rate   | 0.0003   |
|    n_updates       | 102899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -166     |
|    critic_loss     | 5.13     |
|    ent_coef        | 0.136    |
|    ent_coef_loss   | 0.0698   |
|    learning_rate   | 0.0003   |
|    n_updates       | 103299   |
---------------------------------
=== Iterazione IRL 74 ===
Loss reward (iter 74): 5.9295477867126465
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -166     |
|    critic_loss     | 5.05     |
|    ent_coef        | 0.142    |
|    ent_coef_loss   | -0.0733  |
|    learning_rate   | 0.0003   |
|    n_updates       | 103899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -168     |
|    critic_loss     | 5.16     |
|    ent_coef        | 0.141    |
|    ent_coef_loss   | 0.228    |
|    learning_rate   | 0.0003   |
|    n_updates       | 104299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -167     |
|    critic_loss     | 4.3      |
|    ent_coef        | 0.136    |
|    ent_coef_loss   | -0.289   |
|    learning_rate   | 0.0003   |
|    n_updates       | 104699   |
---------------------------------
=== Iterazione IRL 75 ===
Loss reward (iter 75): 5.755847930908203
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -169     |
|    critic_loss     | 5.08     |
|    ent_coef        | 0.136    |
|    ent_coef_loss   | -0.477   |
|    learning_rate   | 0.0003   |
|    n_updates       | 105299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -170     |
|    critic_loss     | 4.93     |
|    ent_coef        | 0.136    |
|    ent_coef_loss   | -0.442   |
|    learning_rate   | 0.0003   |
|    n_updates       | 105699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -170     |
|    critic_loss     | 4.35     |
|    ent_coef        | 0.139    |
|    ent_coef_loss   | 0.773    |
|    learning_rate   | 0.0003   |
|    n_updates       | 106099   |
---------------------------------
=== Iterazione IRL 76 ===
Loss reward (iter 76): 5.994476318359375
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -170     |
|    critic_loss     | 4.77     |
|    ent_coef        | 0.145    |
|    ent_coef_loss   | 0.0259   |
|    learning_rate   | 0.0003   |
|    n_updates       | 106699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -171     |
|    critic_loss     | 4.85     |
|    ent_coef        | 0.147    |
|    ent_coef_loss   | -0.675   |
|    learning_rate   | 0.0003   |
|    n_updates       | 107099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -171     |
|    critic_loss     | 4.6      |
|    ent_coef        | 0.149    |
|    ent_coef_loss   | 0.0502   |
|    learning_rate   | 0.0003   |
|    n_updates       | 107499   |
---------------------------------
=== Iterazione IRL 77 ===
Loss reward (iter 77): 5.7334370613098145
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -173     |
|    critic_loss     | 5.08     |
|    ent_coef        | 0.148    |
|    ent_coef_loss   | -0.407   |
|    learning_rate   | 0.0003   |
|    n_updates       | 108099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -172     |
|    critic_loss     | 3.94     |
|    ent_coef        | 0.148    |
|    ent_coef_loss   | 0.261    |
|    learning_rate   | 0.0003   |
|    n_updates       | 108499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -174     |
|    critic_loss     | 5.19     |
|    ent_coef        | 0.148    |
|    ent_coef_loss   | 0.396    |
|    learning_rate   | 0.0003   |
|    n_updates       | 108899   |
---------------------------------
=== Iterazione IRL 78 ===
Loss reward (iter 78): 6.0304179191589355
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -175     |
|    critic_loss     | 4.33     |
|    ent_coef        | 0.149    |
|    ent_coef_loss   | -0.397   |
|    learning_rate   | 0.0003   |
|    n_updates       | 109499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -175     |
|    critic_loss     | 4.92     |
|    ent_coef        | 0.152    |
|    ent_coef_loss   | -0.522   |
|    learning_rate   | 0.0003   |
|    n_updates       | 109899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -175     |
|    critic_loss     | 4.75     |
|    ent_coef        | 0.152    |
|    ent_coef_loss   | 0.374    |
|    learning_rate   | 0.0003   |
|    n_updates       | 110299   |
---------------------------------
=== Iterazione IRL 79 ===
Loss reward (iter 79): 6.047119140625
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -176     |
|    critic_loss     | 4.67     |
|    ent_coef        | 0.151    |
|    ent_coef_loss   | 0.00384  |
|    learning_rate   | 0.0003   |
|    n_updates       | 110899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -176     |
|    critic_loss     | 4.12     |
|    ent_coef        | 0.152    |
|    ent_coef_loss   | -0.386   |
|    learning_rate   | 0.0003   |
|    n_updates       | 111299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -177     |
|    critic_loss     | 5.11     |
|    ent_coef        | 0.153    |
|    ent_coef_loss   | -0.595   |
|    learning_rate   | 0.0003   |
|    n_updates       | 111699   |
---------------------------------
=== Iterazione IRL 80 ===
Loss reward (iter 80): 5.87067985534668
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -179     |
|    critic_loss     | 4.07     |
|    ent_coef        | 0.153    |
|    ent_coef_loss   | 0.0431   |
|    learning_rate   | 0.0003   |
|    n_updates       | 112299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -179     |
|    critic_loss     | 4.27     |
|    ent_coef        | 0.153    |
|    ent_coef_loss   | -0.36    |
|    learning_rate   | 0.0003   |
|    n_updates       | 112699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -180     |
|    critic_loss     | 4.33     |
|    ent_coef        | 0.155    |
|    ent_coef_loss   | 0.142    |
|    learning_rate   | 0.0003   |
|    n_updates       | 113099   |
---------------------------------
=== Iterazione IRL 81 ===
Loss reward (iter 81): 5.531382083892822
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -180     |
|    critic_loss     | 4.75     |
|    ent_coef        | 0.155    |
|    ent_coef_loss   | -0.0434  |
|    learning_rate   | 0.0003   |
|    n_updates       | 113699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -181     |
|    critic_loss     | 5.03     |
|    ent_coef        | 0.154    |
|    ent_coef_loss   | -0.342   |
|    learning_rate   | 0.0003   |
|    n_updates       | 114099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -181     |
|    critic_loss     | 3.25     |
|    ent_coef        | 0.153    |
|    ent_coef_loss   | -0.377   |
|    learning_rate   | 0.0003   |
|    n_updates       | 114499   |
---------------------------------
=== Iterazione IRL 82 ===
Loss reward (iter 82): 5.459230422973633
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -180     |
|    critic_loss     | 4.42     |
|    ent_coef        | 0.151    |
|    ent_coef_loss   | 0.326    |
|    learning_rate   | 0.0003   |
|    n_updates       | 115099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -182     |
|    critic_loss     | 3.33     |
|    ent_coef        | 0.152    |
|    ent_coef_loss   | -0.539   |
|    learning_rate   | 0.0003   |
|    n_updates       | 115499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -182     |
|    critic_loss     | 4.72     |
|    ent_coef        | 0.148    |
|    ent_coef_loss   | 0.279    |
|    learning_rate   | 0.0003   |
|    n_updates       | 115899   |
---------------------------------
=== Iterazione IRL 83 ===
Loss reward (iter 83): 5.378539085388184
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -183     |
|    critic_loss     | 3.81     |
|    ent_coef        | 0.145    |
|    ent_coef_loss   | -0.126   |
|    learning_rate   | 0.0003   |
|    n_updates       | 116499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -183     |
|    critic_loss     | 4.53     |
|    ent_coef        | 0.142    |
|    ent_coef_loss   | -0.195   |
|    learning_rate   | 0.0003   |
|    n_updates       | 116899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -182     |
|    critic_loss     | 4.19     |
|    ent_coef        | 0.141    |
|    ent_coef_loss   | 0.188    |
|    learning_rate   | 0.0003   |
|    n_updates       | 117299   |
---------------------------------
=== Iterazione IRL 84 ===
Loss reward (iter 84): 6.21786642074585
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -185     |
|    critic_loss     | 5.01     |
|    ent_coef        | 0.136    |
|    ent_coef_loss   | -0.166   |
|    learning_rate   | 0.0003   |
|    n_updates       | 117899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -184     |
|    critic_loss     | 4.06     |
|    ent_coef        | 0.137    |
|    ent_coef_loss   | 0.33     |
|    learning_rate   | 0.0003   |
|    n_updates       | 118299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -184     |
|    critic_loss     | 4.57     |
|    ent_coef        | 0.135    |
|    ent_coef_loss   | -0.154   |
|    learning_rate   | 0.0003   |
|    n_updates       | 118699   |
---------------------------------
=== Iterazione IRL 85 ===
Loss reward (iter 85): 6.162248134613037
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -184     |
|    critic_loss     | 5.01     |
|    ent_coef        | 0.133    |
|    ent_coef_loss   | -0.129   |
|    learning_rate   | 0.0003   |
|    n_updates       | 119299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -184     |
|    critic_loss     | 3.39     |
|    ent_coef        | 0.131    |
|    ent_coef_loss   | -0.0256  |
|    learning_rate   | 0.0003   |
|    n_updates       | 119699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -185     |
|    critic_loss     | 4.85     |
|    ent_coef        | 0.131    |
|    ent_coef_loss   | -0.229   |
|    learning_rate   | 0.0003   |
|    n_updates       | 120099   |
---------------------------------
=== Iterazione IRL 86 ===
Loss reward (iter 86): 6.087630271911621
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -183     |
|    critic_loss     | 4        |
|    ent_coef        | 0.133    |
|    ent_coef_loss   | -0.378   |
|    learning_rate   | 0.0003   |
|    n_updates       | 120699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -182     |
|    critic_loss     | 4.42     |
|    ent_coef        | 0.129    |
|    ent_coef_loss   | 0.375    |
|    learning_rate   | 0.0003   |
|    n_updates       | 121099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -183     |
|    critic_loss     | 3.75     |
|    ent_coef        | 0.129    |
|    ent_coef_loss   | 0.014    |
|    learning_rate   | 0.0003   |
|    n_updates       | 121499   |
---------------------------------
=== Iterazione IRL 87 ===
Loss reward (iter 87): 5.704883575439453
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -184     |
|    critic_loss     | 3.91     |
|    ent_coef        | 0.121    |
|    ent_coef_loss   | -0.512   |
|    learning_rate   | 0.0003   |
|    n_updates       | 122099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -183     |
|    critic_loss     | 4.33     |
|    ent_coef        | 0.118    |
|    ent_coef_loss   | -0.0336  |
|    learning_rate   | 0.0003   |
|    n_updates       | 122499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -181     |
|    critic_loss     | 4.77     |
|    ent_coef        | 0.115    |
|    ent_coef_loss   | 0.18     |
|    learning_rate   | 0.0003   |
|    n_updates       | 122899   |
---------------------------------
=== Iterazione IRL 88 ===
Loss reward (iter 88): 6.182289123535156
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -182     |
|    critic_loss     | 5        |
|    ent_coef        | 0.105    |
|    ent_coef_loss   | 0.367    |
|    learning_rate   | 0.0003   |
|    n_updates       | 123499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -181     |
|    critic_loss     | 4.64     |
|    ent_coef        | 0.102    |
|    ent_coef_loss   | -0.175   |
|    learning_rate   | 0.0003   |
|    n_updates       | 123899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -180     |
|    critic_loss     | 5.04     |
|    ent_coef        | 0.1      |
|    ent_coef_loss   | 0.391    |
|    learning_rate   | 0.0003   |
|    n_updates       | 124299   |
---------------------------------
=== Iterazione IRL 89 ===
Loss reward (iter 89): 5.8497419357299805
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -179     |
|    critic_loss     | 5.23     |
|    ent_coef        | 0.101    |
|    ent_coef_loss   | 0.826    |
|    learning_rate   | 0.0003   |
|    n_updates       | 124899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -178     |
|    critic_loss     | 5.11     |
|    ent_coef        | 0.102    |
|    ent_coef_loss   | 0.0244   |
|    learning_rate   | 0.0003   |
|    n_updates       | 125299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -178     |
|    critic_loss     | 4.65     |
|    ent_coef        | 0.098    |
|    ent_coef_loss   | -0.139   |
|    learning_rate   | 0.0003   |
|    n_updates       | 125699   |
---------------------------------
=== Iterazione IRL 90 ===
Loss reward (iter 90): 5.782105445861816
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -176     |
|    critic_loss     | 5.08     |
|    ent_coef        | 0.0984   |
|    ent_coef_loss   | -0.0676  |
|    learning_rate   | 0.0003   |
|    n_updates       | 126299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -175     |
|    critic_loss     | 5.52     |
|    ent_coef        | 0.104    |
|    ent_coef_loss   | -0.117   |
|    learning_rate   | 0.0003   |
|    n_updates       | 126699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -175     |
|    critic_loss     | 5.34     |
|    ent_coef        | 0.105    |
|    ent_coef_loss   | 0.764    |
|    learning_rate   | 0.0003   |
|    n_updates       | 127099   |
---------------------------------
=== Iterazione IRL 91 ===
Loss reward (iter 91): 5.949409008026123
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -173     |
|    critic_loss     | 6.05     |
|    ent_coef        | 0.103    |
|    ent_coef_loss   | -0.299   |
|    learning_rate   | 0.0003   |
|    n_updates       | 127699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -173     |
|    critic_loss     | 4.36     |
|    ent_coef        | 0.101    |
|    ent_coef_loss   | 0.373    |
|    learning_rate   | 0.0003   |
|    n_updates       | 128099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -170     |
|    critic_loss     | 5.93     |
|    ent_coef        | 0.101    |
|    ent_coef_loss   | -0.457   |
|    learning_rate   | 0.0003   |
|    n_updates       | 128499   |
---------------------------------
=== Iterazione IRL 92 ===
Loss reward (iter 92): 6.425497055053711
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -172     |
|    critic_loss     | 6.48     |
|    ent_coef        | 0.099    |
|    ent_coef_loss   | -0.523   |
|    learning_rate   | 0.0003   |
|    n_updates       | 129099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -170     |
|    critic_loss     | 6.17     |
|    ent_coef        | 0.0957   |
|    ent_coef_loss   | 0.136    |
|    learning_rate   | 0.0003   |
|    n_updates       | 129499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -169     |
|    critic_loss     | 6.1      |
|    ent_coef        | 0.0929   |
|    ent_coef_loss   | -0.132   |
|    learning_rate   | 0.0003   |
|    n_updates       | 129899   |
---------------------------------
=== Iterazione IRL 93 ===
Loss reward (iter 93): 6.593518257141113
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -167     |
|    critic_loss     | 6.18     |
|    ent_coef        | 0.0974   |
|    ent_coef_loss   | -0.106   |
|    learning_rate   | 0.0003   |
|    n_updates       | 130499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -166     |
|    critic_loss     | 5.7      |
|    ent_coef        | 0.0944   |
|    ent_coef_loss   | -0.0786  |
|    learning_rate   | 0.0003   |
|    n_updates       | 130899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -165     |
|    critic_loss     | 6.07     |
|    ent_coef        | 0.0962   |
|    ent_coef_loss   | 0.00272  |
|    learning_rate   | 0.0003   |
|    n_updates       | 131299   |
---------------------------------
=== Iterazione IRL 94 ===
Loss reward (iter 94): 6.043160915374756
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -164     |
|    critic_loss     | 8.31     |
|    ent_coef        | 0.0987   |
|    ent_coef_loss   | 0.161    |
|    learning_rate   | 0.0003   |
|    n_updates       | 131899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -163     |
|    critic_loss     | 8.64     |
|    ent_coef        | 0.101    |
|    ent_coef_loss   | 0.966    |
|    learning_rate   | 0.0003   |
|    n_updates       | 132299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -162     |
|    critic_loss     | 6.25     |
|    ent_coef        | 0.103    |
|    ent_coef_loss   | -0.724   |
|    learning_rate   | 0.0003   |
|    n_updates       | 132699   |
---------------------------------
=== Iterazione IRL 95 ===
Loss reward (iter 95): 5.630095958709717
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -162     |
|    critic_loss     | 6.72     |
|    ent_coef        | 0.105    |
|    ent_coef_loss   | -0.0615  |
|    learning_rate   | 0.0003   |
|    n_updates       | 133299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -163     |
|    critic_loss     | 6.75     |
|    ent_coef        | 0.103    |
|    ent_coef_loss   | 0.509    |
|    learning_rate   | 0.0003   |
|    n_updates       | 133699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -162     |
|    critic_loss     | 9.01     |
|    ent_coef        | 0.106    |
|    ent_coef_loss   | 0.472    |
|    learning_rate   | 0.0003   |
|    n_updates       | 134099   |
---------------------------------
=== Iterazione IRL 96 ===
Loss reward (iter 96): 5.207292079925537
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -162     |
|    critic_loss     | 5.76     |
|    ent_coef        | 0.104    |
|    ent_coef_loss   | -0.859   |
|    learning_rate   | 0.0003   |
|    n_updates       | 134699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -161     |
|    critic_loss     | 6.93     |
|    ent_coef        | 0.0999   |
|    ent_coef_loss   | 1.24     |
|    learning_rate   | 0.0003   |
|    n_updates       | 135099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -160     |
|    critic_loss     | 6.67     |
|    ent_coef        | 0.0983   |
|    ent_coef_loss   | 0.577    |
|    learning_rate   | 0.0003   |
|    n_updates       | 135499   |
---------------------------------
=== Iterazione IRL 97 ===
Loss reward (iter 97): 5.955221652984619
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -160     |
|    critic_loss     | 7.33     |
|    ent_coef        | 0.101    |
|    ent_coef_loss   | -0.352   |
|    learning_rate   | 0.0003   |
|    n_updates       | 136099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -158     |
|    critic_loss     | 8.12     |
|    ent_coef        | 0.0991   |
|    ent_coef_loss   | 0.156    |
|    learning_rate   | 0.0003   |
|    n_updates       | 136499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -157     |
|    critic_loss     | 7.21     |
|    ent_coef        | 0.105    |
|    ent_coef_loss   | -0.0747  |
|    learning_rate   | 0.0003   |
|    n_updates       | 136899   |
---------------------------------
=== Iterazione IRL 98 ===
Loss reward (iter 98): 6.516605854034424
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -155     |
|    critic_loss     | 7.77     |
|    ent_coef        | 0.108    |
|    ent_coef_loss   | 1.1      |
|    learning_rate   | 0.0003   |
|    n_updates       | 137499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -156     |
|    critic_loss     | 8.2      |
|    ent_coef        | 0.113    |
|    ent_coef_loss   | -0.0444  |
|    learning_rate   | 0.0003   |
|    n_updates       | 137899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -157     |
|    critic_loss     | 6.49     |
|    ent_coef        | 0.114    |
|    ent_coef_loss   | 0.383    |
|    learning_rate   | 0.0003   |
|    n_updates       | 138299   |
---------------------------------
=== Iterazione IRL 99 ===
Loss reward (iter 99): 6.093486309051514
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -154     |
|    critic_loss     | 7.05     |
|    ent_coef        | 0.111    |
|    ent_coef_loss   | 0.947    |
|    learning_rate   | 0.0003   |
|    n_updates       | 138899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -153     |
|    critic_loss     | 9.04     |
|    ent_coef        | 0.11     |
|    ent_coef_loss   | -0.322   |
|    learning_rate   | 0.0003   |
|    n_updates       | 139299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -151     |
|    critic_loss     | 7.93     |
|    ent_coef        | 0.107    |
|    ent_coef_loss   | -0.0741  |
|    learning_rate   | 0.0003   |
|    n_updates       | 139699   |
---------------------------------
=== Iterazione IRL 100 ===
Loss reward (iter 100): 6.130993843078613
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -152     |
|    critic_loss     | 7.36     |
|    ent_coef        | 0.111    |
|    ent_coef_loss   | -0.0261  |
|    learning_rate   | 0.0003   |
|    n_updates       | 140299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -151     |
|    critic_loss     | 9.53     |
|    ent_coef        | 0.11     |
|    ent_coef_loss   | -0.443   |
|    learning_rate   | 0.0003   |
|    n_updates       | 140699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -150     |
|    critic_loss     | 7.33     |
|    ent_coef        | 0.105    |
|    ent_coef_loss   | 0.458    |
|    learning_rate   | 0.0003   |
|    n_updates       | 141099   |
---------------------------------
=== Iterazione IRL 101 ===
Loss reward (iter 101): 5.607321739196777
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -148     |
|    critic_loss     | 7.23     |
|    ent_coef        | 0.105    |
|    ent_coef_loss   | 0.2      |
|    learning_rate   | 0.0003   |
|    n_updates       | 141699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -149     |
|    critic_loss     | 5.63     |
|    ent_coef        | 0.102    |
|    ent_coef_loss   | 0.653    |
|    learning_rate   | 0.0003   |
|    n_updates       | 142099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -148     |
|    critic_loss     | 7.28     |
|    ent_coef        | 0.0984   |
|    ent_coef_loss   | 0.475    |
|    learning_rate   | 0.0003   |
|    n_updates       | 142499   |
---------------------------------
=== Iterazione IRL 102 ===
Loss reward (iter 102): 5.4710612297058105
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -147     |
|    critic_loss     | 6.83     |
|    ent_coef        | 0.0947   |
|    ent_coef_loss   | -0.742   |
|    learning_rate   | 0.0003   |
|    n_updates       | 143099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -144     |
|    critic_loss     | 10.3     |
|    ent_coef        | 0.0932   |
|    ent_coef_loss   | 0.509    |
|    learning_rate   | 0.0003   |
|    n_updates       | 143499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -145     |
|    critic_loss     | 5.92     |
|    ent_coef        | 0.0928   |
|    ent_coef_loss   | -0.0998  |
|    learning_rate   | 0.0003   |
|    n_updates       | 143899   |
---------------------------------
=== Iterazione IRL 103 ===
Loss reward (iter 103): 5.586386680603027
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -142     |
|    critic_loss     | 8.63     |
|    ent_coef        | 0.0914   |
|    ent_coef_loss   | 0.114    |
|    learning_rate   | 0.0003   |
|    n_updates       | 144499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -143     |
|    critic_loss     | 7.14     |
|    ent_coef        | 0.0933   |
|    ent_coef_loss   | -0.179   |
|    learning_rate   | 0.0003   |
|    n_updates       | 144899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -140     |
|    critic_loss     | 6.69     |
|    ent_coef        | 0.0951   |
|    ent_coef_loss   | 0.349    |
|    learning_rate   | 0.0003   |
|    n_updates       | 145299   |
---------------------------------
=== Iterazione IRL 104 ===
Loss reward (iter 104): 5.475876331329346
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -143     |
|    critic_loss     | 7.16     |
|    ent_coef        | 0.0963   |
|    ent_coef_loss   | -0.929   |
|    learning_rate   | 0.0003   |
|    n_updates       | 145899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -142     |
|    critic_loss     | 6.76     |
|    ent_coef        | 0.0991   |
|    ent_coef_loss   | 0.671    |
|    learning_rate   | 0.0003   |
|    n_updates       | 146299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -140     |
|    critic_loss     | 7.13     |
|    ent_coef        | 0.103    |
|    ent_coef_loss   | -0.224   |
|    learning_rate   | 0.0003   |
|    n_updates       | 146699   |
---------------------------------
=== Iterazione IRL 105 ===
Loss reward (iter 105): 5.406124591827393
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -139     |
|    critic_loss     | 9.63     |
|    ent_coef        | 0.1      |
|    ent_coef_loss   | 0.187    |
|    learning_rate   | 0.0003   |
|    n_updates       | 147299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -140     |
|    critic_loss     | 7.33     |
|    ent_coef        | 0.0959   |
|    ent_coef_loss   | 0.0752   |
|    learning_rate   | 0.0003   |
|    n_updates       | 147699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -135     |
|    critic_loss     | 8.79     |
|    ent_coef        | 0.0932   |
|    ent_coef_loss   | 0.201    |
|    learning_rate   | 0.0003   |
|    n_updates       | 148099   |
---------------------------------
=== Iterazione IRL 106 ===
Loss reward (iter 106): 5.225178241729736
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -135     |
|    critic_loss     | 8.37     |
|    ent_coef        | 0.0935   |
|    ent_coef_loss   | 0.373    |
|    learning_rate   | 0.0003   |
|    n_updates       | 148699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -136     |
|    critic_loss     | 8.1      |
|    ent_coef        | 0.0936   |
|    ent_coef_loss   | -0.203   |
|    learning_rate   | 0.0003   |
|    n_updates       | 149099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -136     |
|    critic_loss     | 6.22     |
|    ent_coef        | 0.0929   |
|    ent_coef_loss   | -0.462   |
|    learning_rate   | 0.0003   |
|    n_updates       | 149499   |
---------------------------------
=== Iterazione IRL 107 ===
Loss reward (iter 107): 5.219798564910889
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -136     |
|    critic_loss     | 7.87     |
|    ent_coef        | 0.0878   |
|    ent_coef_loss   | 0.421    |
|    learning_rate   | 0.0003   |
|    n_updates       | 150099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -133     |
|    critic_loss     | 7.8      |
|    ent_coef        | 0.0868   |
|    ent_coef_loss   | -1.07    |
|    learning_rate   | 0.0003   |
|    n_updates       | 150499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -133     |
|    critic_loss     | 8.28     |
|    ent_coef        | 0.0862   |
|    ent_coef_loss   | 0.208    |
|    learning_rate   | 0.0003   |
|    n_updates       | 150899   |
---------------------------------
=== Iterazione IRL 108 ===
Loss reward (iter 108): 5.4271697998046875
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -131     |
|    critic_loss     | 7.68     |
|    ent_coef        | 0.0876   |
|    ent_coef_loss   | 0.556    |
|    learning_rate   | 0.0003   |
|    n_updates       | 151499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -130     |
|    critic_loss     | 7.73     |
|    ent_coef        | 0.0875   |
|    ent_coef_loss   | -0.358   |
|    learning_rate   | 0.0003   |
|    n_updates       | 151899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -130     |
|    critic_loss     | 7.49     |
|    ent_coef        | 0.0876   |
|    ent_coef_loss   | -0.314   |
|    learning_rate   | 0.0003   |
|    n_updates       | 152299   |
---------------------------------
=== Iterazione IRL 109 ===
Loss reward (iter 109): 5.549091339111328
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -129     |
|    critic_loss     | 6.96     |
|    ent_coef        | 0.0895   |
|    ent_coef_loss   | -0.545   |
|    learning_rate   | 0.0003   |
|    n_updates       | 152899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -128     |
|    critic_loss     | 9.37     |
|    ent_coef        | 0.0902   |
|    ent_coef_loss   | -0.424   |
|    learning_rate   | 0.0003   |
|    n_updates       | 153299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -128     |
|    critic_loss     | 7.32     |
|    ent_coef        | 0.0885   |
|    ent_coef_loss   | -0.52    |
|    learning_rate   | 0.0003   |
|    n_updates       | 153699   |
---------------------------------
=== Iterazione IRL 110 ===
Loss reward (iter 110): 5.563042640686035
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -128     |
|    critic_loss     | 6.86     |
|    ent_coef        | 0.0894   |
|    ent_coef_loss   | 0.145    |
|    learning_rate   | 0.0003   |
|    n_updates       | 154299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -126     |
|    critic_loss     | 8.36     |
|    ent_coef        | 0.092    |
|    ent_coef_loss   | -0.269   |
|    learning_rate   | 0.0003   |
|    n_updates       | 154699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -124     |
|    critic_loss     | 7.19     |
|    ent_coef        | 0.0919   |
|    ent_coef_loss   | -0.67    |
|    learning_rate   | 0.0003   |
|    n_updates       | 155099   |
---------------------------------
=== Iterazione IRL 111 ===
Loss reward (iter 111): 5.483314514160156
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -123     |
|    critic_loss     | 8.51     |
|    ent_coef        | 0.0903   |
|    ent_coef_loss   | -0.509   |
|    learning_rate   | 0.0003   |
|    n_updates       | 155699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -124     |
|    critic_loss     | 7.37     |
|    ent_coef        | 0.0916   |
|    ent_coef_loss   | 0.133    |
|    learning_rate   | 0.0003   |
|    n_updates       | 156099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -122     |
|    critic_loss     | 7.39     |
|    ent_coef        | 0.091    |
|    ent_coef_loss   | -0.262   |
|    learning_rate   | 0.0003   |
|    n_updates       | 156499   |
---------------------------------
=== Iterazione IRL 112 ===
Loss reward (iter 112): 5.308445453643799
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -121     |
|    critic_loss     | 7.58     |
|    ent_coef        | 0.0946   |
|    ent_coef_loss   | 0.344    |
|    learning_rate   | 0.0003   |
|    n_updates       | 157099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -120     |
|    critic_loss     | 10       |
|    ent_coef        | 0.0943   |
|    ent_coef_loss   | -0.479   |
|    learning_rate   | 0.0003   |
|    n_updates       | 157499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -121     |
|    critic_loss     | 6.88     |
|    ent_coef        | 0.0981   |
|    ent_coef_loss   | 0.799    |
|    learning_rate   | 0.0003   |
|    n_updates       | 157899   |
---------------------------------
=== Iterazione IRL 113 ===
Loss reward (iter 113): 4.9893388748168945
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -120     |
|    critic_loss     | 6.77     |
|    ent_coef        | 0.0975   |
|    ent_coef_loss   | 0.0723   |
|    learning_rate   | 0.0003   |
|    n_updates       | 158499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -117     |
|    critic_loss     | 7.82     |
|    ent_coef        | 0.0943   |
|    ent_coef_loss   | -0.388   |
|    learning_rate   | 0.0003   |
|    n_updates       | 158899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -116     |
|    critic_loss     | 8.35     |
|    ent_coef        | 0.0925   |
|    ent_coef_loss   | 0.0891   |
|    learning_rate   | 0.0003   |
|    n_updates       | 159299   |
---------------------------------
=== Iterazione IRL 114 ===
Loss reward (iter 114): 4.787063121795654
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -115     |
|    critic_loss     | 8.65     |
|    ent_coef        | 0.091    |
|    ent_coef_loss   | -0.615   |
|    learning_rate   | 0.0003   |
|    n_updates       | 159899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -114     |
|    critic_loss     | 7.81     |
|    ent_coef        | 0.0889   |
|    ent_coef_loss   | 0.187    |
|    learning_rate   | 0.0003   |
|    n_updates       | 160299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -114     |
|    critic_loss     | 8.61     |
|    ent_coef        | 0.0882   |
|    ent_coef_loss   | -0.126   |
|    learning_rate   | 0.0003   |
|    n_updates       | 160699   |
---------------------------------
=== Iterazione IRL 115 ===
Loss reward (iter 115): 4.317055702209473
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -112     |
|    critic_loss     | 9.45     |
|    ent_coef        | 0.0853   |
|    ent_coef_loss   | -0.374   |
|    learning_rate   | 0.0003   |
|    n_updates       | 161299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -110     |
|    critic_loss     | 8.49     |
|    ent_coef        | 0.0842   |
|    ent_coef_loss   | -0.57    |
|    learning_rate   | 0.0003   |
|    n_updates       | 161699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -111     |
|    critic_loss     | 8.35     |
|    ent_coef        | 0.0826   |
|    ent_coef_loss   | -0.927   |
|    learning_rate   | 0.0003   |
|    n_updates       | 162099   |
---------------------------------
=== Iterazione IRL 116 ===
Loss reward (iter 116): 3.0777223110198975
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -109     |
|    critic_loss     | 7.96     |
|    ent_coef        | 0.0886   |
|    ent_coef_loss   | 0.495    |
|    learning_rate   | 0.0003   |
|    n_updates       | 162699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -106     |
|    critic_loss     | 9.77     |
|    ent_coef        | 0.0916   |
|    ent_coef_loss   | -0.148   |
|    learning_rate   | 0.0003   |
|    n_updates       | 163099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -106     |
|    critic_loss     | 9.29     |
|    ent_coef        | 0.0913   |
|    ent_coef_loss   | 0.527    |
|    learning_rate   | 0.0003   |
|    n_updates       | 163499   |
---------------------------------
=== Iterazione IRL 117 ===
Loss reward (iter 117): 4.579246997833252
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -105     |
|    critic_loss     | 8.8      |
|    ent_coef        | 0.0887   |
|    ent_coef_loss   | 0.102    |
|    learning_rate   | 0.0003   |
|    n_updates       | 164099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -105     |
|    critic_loss     | 7.92     |
|    ent_coef        | 0.0873   |
|    ent_coef_loss   | 0.912    |
|    learning_rate   | 0.0003   |
|    n_updates       | 164499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -103     |
|    critic_loss     | 9.92     |
|    ent_coef        | 0.0863   |
|    ent_coef_loss   | -0.864   |
|    learning_rate   | 0.0003   |
|    n_updates       | 164899   |
---------------------------------
=== Iterazione IRL 118 ===
Loss reward (iter 118): 4.280139923095703
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -103     |
|    critic_loss     | 10       |
|    ent_coef        | 0.0879   |
|    ent_coef_loss   | -0.539   |
|    learning_rate   | 0.0003   |
|    n_updates       | 165499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -102     |
|    critic_loss     | 10.8     |
|    ent_coef        | 0.086    |
|    ent_coef_loss   | 0.239    |
|    learning_rate   | 0.0003   |
|    n_updates       | 165899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -98.2    |
|    critic_loss     | 9.88     |
|    ent_coef        | 0.0838   |
|    ent_coef_loss   | -1.03    |
|    learning_rate   | 0.0003   |
|    n_updates       | 166299   |
---------------------------------
=== Iterazione IRL 119 ===
Loss reward (iter 119): 4.770151138305664
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -98.2    |
|    critic_loss     | 11.8     |
|    ent_coef        | 0.0855   |
|    ent_coef_loss   | 0.0142   |
|    learning_rate   | 0.0003   |
|    n_updates       | 166899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -96      |
|    critic_loss     | 11.4     |
|    ent_coef        | 0.0918   |
|    ent_coef_loss   | 0.648    |
|    learning_rate   | 0.0003   |
|    n_updates       | 167299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -96.5    |
|    critic_loss     | 12.9     |
|    ent_coef        | 0.0945   |
|    ent_coef_loss   | 0.449    |
|    learning_rate   | 0.0003   |
|    n_updates       | 167699   |
---------------------------------
=== Iterazione IRL 120 ===
Loss reward (iter 120): 5.454914569854736
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -94.1    |
|    critic_loss     | 11.2     |
|    ent_coef        | 0.101    |
|    ent_coef_loss   | 0.244    |
|    learning_rate   | 0.0003   |
|    n_updates       | 168299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -93.1    |
|    critic_loss     | 10.2     |
|    ent_coef        | 0.102    |
|    ent_coef_loss   | 0.44     |
|    learning_rate   | 0.0003   |
|    n_updates       | 168699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -90.3    |
|    critic_loss     | 12.7     |
|    ent_coef        | 0.103    |
|    ent_coef_loss   | 0.511    |
|    learning_rate   | 0.0003   |
|    n_updates       | 169099   |
---------------------------------
=== Iterazione IRL 121 ===
Loss reward (iter 121): 5.572192192077637
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -91      |
|    critic_loss     | 10.9     |
|    ent_coef        | 0.106    |
|    ent_coef_loss   | 0.437    |
|    learning_rate   | 0.0003   |
|    n_updates       | 169699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -89      |
|    critic_loss     | 11.7     |
|    ent_coef        | 0.103    |
|    ent_coef_loss   | -0.472   |
|    learning_rate   | 0.0003   |
|    n_updates       | 170099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -87.8    |
|    critic_loss     | 11.3     |
|    ent_coef        | 0.107    |
|    ent_coef_loss   | -0.0714  |
|    learning_rate   | 0.0003   |
|    n_updates       | 170499   |
---------------------------------
=== Iterazione IRL 122 ===
Loss reward (iter 122): 5.775091648101807
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -88.7    |
|    critic_loss     | 12.9     |
|    ent_coef        | 0.111    |
|    ent_coef_loss   | 0.673    |
|    learning_rate   | 0.0003   |
|    n_updates       | 171099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -86.6    |
|    critic_loss     | 11.9     |
|    ent_coef        | 0.112    |
|    ent_coef_loss   | 0.273    |
|    learning_rate   | 0.0003   |
|    n_updates       | 171499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -85.8    |
|    critic_loss     | 13.9     |
|    ent_coef        | 0.109    |
|    ent_coef_loss   | 0.375    |
|    learning_rate   | 0.0003   |
|    n_updates       | 171899   |
---------------------------------
=== Iterazione IRL 123 ===
Loss reward (iter 123): 6.139379024505615
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -85.3    |
|    critic_loss     | 11.8     |
|    ent_coef        | 0.109    |
|    ent_coef_loss   | 0.949    |
|    learning_rate   | 0.0003   |
|    n_updates       | 172499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -85.5    |
|    critic_loss     | 10.8     |
|    ent_coef        | 0.11     |
|    ent_coef_loss   | 0.00484  |
|    learning_rate   | 0.0003   |
|    n_updates       | 172899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -84.1    |
|    critic_loss     | 12.3     |
|    ent_coef        | 0.104    |
|    ent_coef_loss   | -0.145   |
|    learning_rate   | 0.0003   |
|    n_updates       | 173299   |
---------------------------------
=== Iterazione IRL 124 ===
Loss reward (iter 124): 5.68770694732666
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -83.9    |
|    critic_loss     | 10.4     |
|    ent_coef        | 0.106    |
|    ent_coef_loss   | 0.274    |
|    learning_rate   | 0.0003   |
|    n_updates       | 173899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -83.6    |
|    critic_loss     | 13       |
|    ent_coef        | 0.107    |
|    ent_coef_loss   | -0.191   |
|    learning_rate   | 0.0003   |
|    n_updates       | 174299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -80.9    |
|    critic_loss     | 13       |
|    ent_coef        | 0.105    |
|    ent_coef_loss   | -0.0793  |
|    learning_rate   | 0.0003   |
|    n_updates       | 174699   |
---------------------------------
=== Iterazione IRL 125 ===
Loss reward (iter 125): 5.730883598327637
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -80.8    |
|    critic_loss     | 14.6     |
|    ent_coef        | 0.113    |
|    ent_coef_loss   | -0.105   |
|    learning_rate   | 0.0003   |
|    n_updates       | 175299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -82.6    |
|    critic_loss     | 10.5     |
|    ent_coef        | 0.109    |
|    ent_coef_loss   | -0.0554  |
|    learning_rate   | 0.0003   |
|    n_updates       | 175699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -80      |
|    critic_loss     | 13.3     |
|    ent_coef        | 0.109    |
|    ent_coef_loss   | 0.358    |
|    learning_rate   | 0.0003   |
|    n_updates       | 176099   |
---------------------------------
=== Iterazione IRL 126 ===
Loss reward (iter 126): 5.274345874786377
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -78.3    |
|    critic_loss     | 12.4     |
|    ent_coef        | 0.109    |
|    ent_coef_loss   | 0.562    |
|    learning_rate   | 0.0003   |
|    n_updates       | 176699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -78.1    |
|    critic_loss     | 13.8     |
|    ent_coef        | 0.11     |
|    ent_coef_loss   | 0.194    |
|    learning_rate   | 0.0003   |
|    n_updates       | 177099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -79      |
|    critic_loss     | 11.4     |
|    ent_coef        | 0.112    |
|    ent_coef_loss   | 0.257    |
|    learning_rate   | 0.0003   |
|    n_updates       | 177499   |
---------------------------------
=== Iterazione IRL 127 ===
Loss reward (iter 127): 5.252516746520996
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -78      |
|    critic_loss     | 15.1     |
|    ent_coef        | 0.111    |
|    ent_coef_loss   | 0.013    |
|    learning_rate   | 0.0003   |
|    n_updates       | 178099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -77.7    |
|    critic_loss     | 13.8     |
|    ent_coef        | 0.113    |
|    ent_coef_loss   | 0.483    |
|    learning_rate   | 0.0003   |
|    n_updates       | 178499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -79.3    |
|    critic_loss     | 11.3     |
|    ent_coef        | 0.11     |
|    ent_coef_loss   | 0.205    |
|    learning_rate   | 0.0003   |
|    n_updates       | 178899   |
---------------------------------
=== Iterazione IRL 128 ===
Loss reward (iter 128): 4.9501776695251465
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -74.8    |
|    critic_loss     | 11.4     |
|    ent_coef        | 0.111    |
|    ent_coef_loss   | -0.633   |
|    learning_rate   | 0.0003   |
|    n_updates       | 179499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -76.2    |
|    critic_loss     | 12.4     |
|    ent_coef        | 0.111    |
|    ent_coef_loss   | 0.875    |
|    learning_rate   | 0.0003   |
|    n_updates       | 179899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -74.9    |
|    critic_loss     | 12.5     |
|    ent_coef        | 0.108    |
|    ent_coef_loss   | -0.582   |
|    learning_rate   | 0.0003   |
|    n_updates       | 180299   |
---------------------------------
=== Iterazione IRL 129 ===
Loss reward (iter 129): 5.2430853843688965
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -76.1    |
|    critic_loss     | 13.2     |
|    ent_coef        | 0.11     |
|    ent_coef_loss   | 0.131    |
|    learning_rate   | 0.0003   |
|    n_updates       | 180899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -74.7    |
|    critic_loss     | 13.2     |
|    ent_coef        | 0.11     |
|    ent_coef_loss   | 0.302    |
|    learning_rate   | 0.0003   |
|    n_updates       | 181299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -73.9    |
|    critic_loss     | 12.8     |
|    ent_coef        | 0.11     |
|    ent_coef_loss   | -0.418   |
|    learning_rate   | 0.0003   |
|    n_updates       | 181699   |
---------------------------------
=== Iterazione IRL 130 ===
Loss reward (iter 130): 5.0379767417907715
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -72.9    |
|    critic_loss     | 11.5     |
|    ent_coef        | 0.112    |
|    ent_coef_loss   | 0.287    |
|    learning_rate   | 0.0003   |
|    n_updates       | 182299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -72.4    |
|    critic_loss     | 12.5     |
|    ent_coef        | 0.111    |
|    ent_coef_loss   | 0.676    |
|    learning_rate   | 0.0003   |
|    n_updates       | 182699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -72.8    |
|    critic_loss     | 14.3     |
|    ent_coef        | 0.11     |
|    ent_coef_loss   | -0.157   |
|    learning_rate   | 0.0003   |
|    n_updates       | 183099   |
---------------------------------
=== Iterazione IRL 131 ===
Loss reward (iter 131): 5.212589263916016
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -73.6    |
|    critic_loss     | 11.9     |
|    ent_coef        | 0.109    |
|    ent_coef_loss   | -0.0433  |
|    learning_rate   | 0.0003   |
|    n_updates       | 183699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -73.1    |
|    critic_loss     | 11.1     |
|    ent_coef        | 0.108    |
|    ent_coef_loss   | -0.155   |
|    learning_rate   | 0.0003   |
|    n_updates       | 184099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -74      |
|    critic_loss     | 13.7     |
|    ent_coef        | 0.107    |
|    ent_coef_loss   | -0.076   |
|    learning_rate   | 0.0003   |
|    n_updates       | 184499   |
---------------------------------
=== Iterazione IRL 132 ===
Loss reward (iter 132): 5.700280666351318
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -73.3    |
|    critic_loss     | 11.5     |
|    ent_coef        | 0.105    |
|    ent_coef_loss   | 0.116    |
|    learning_rate   | 0.0003   |
|    n_updates       | 185099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -67.7    |
|    critic_loss     | 12.2     |
|    ent_coef        | 0.109    |
|    ent_coef_loss   | -0.404   |
|    learning_rate   | 0.0003   |
|    n_updates       | 185499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -70      |
|    critic_loss     | 13.4     |
|    ent_coef        | 0.108    |
|    ent_coef_loss   | 0.935    |
|    learning_rate   | 0.0003   |
|    n_updates       | 185899   |
---------------------------------
=== Iterazione IRL 133 ===
Loss reward (iter 133): 5.3822197914123535
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -67.3    |
|    critic_loss     | 12       |
|    ent_coef        | 0.106    |
|    ent_coef_loss   | 0.16     |
|    learning_rate   | 0.0003   |
|    n_updates       | 186499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -69.1    |
|    critic_loss     | 14       |
|    ent_coef        | 0.108    |
|    ent_coef_loss   | 0.227    |
|    learning_rate   | 0.0003   |
|    n_updates       | 186899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -66.5    |
|    critic_loss     | 11.4     |
|    ent_coef        | 0.109    |
|    ent_coef_loss   | -0.279   |
|    learning_rate   | 0.0003   |
|    n_updates       | 187299   |
---------------------------------
=== Iterazione IRL 134 ===
Loss reward (iter 134): 5.572124481201172
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -66.4    |
|    critic_loss     | 14.3     |
|    ent_coef        | 0.109    |
|    ent_coef_loss   | -0.0373  |
|    learning_rate   | 0.0003   |
|    n_updates       | 187899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -65.3    |
|    critic_loss     | 11.5     |
|    ent_coef        | 0.107    |
|    ent_coef_loss   | -0.218   |
|    learning_rate   | 0.0003   |
|    n_updates       | 188299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -67.9    |
|    critic_loss     | 12.6     |
|    ent_coef        | 0.103    |
|    ent_coef_loss   | 0.253    |
|    learning_rate   | 0.0003   |
|    n_updates       | 188699   |
---------------------------------
=== Iterazione IRL 135 ===
Loss reward (iter 135): 5.640013694763184
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -65.2    |
|    critic_loss     | 10.4     |
|    ent_coef        | 0.105    |
|    ent_coef_loss   | 0.0917   |
|    learning_rate   | 0.0003   |
|    n_updates       | 189299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -65.2    |
|    critic_loss     | 10.3     |
|    ent_coef        | 0.105    |
|    ent_coef_loss   | 0.0731   |
|    learning_rate   | 0.0003   |
|    n_updates       | 189699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -62.6    |
|    critic_loss     | 12.6     |
|    ent_coef        | 0.102    |
|    ent_coef_loss   | -0.14    |
|    learning_rate   | 0.0003   |
|    n_updates       | 190099   |
---------------------------------
=== Iterazione IRL 136 ===
Loss reward (iter 136): 5.036219596862793
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -62.1    |
|    critic_loss     | 13.5     |
|    ent_coef        | 0.106    |
|    ent_coef_loss   | -0.443   |
|    learning_rate   | 0.0003   |
|    n_updates       | 190699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -64.2    |
|    critic_loss     | 15.4     |
|    ent_coef        | 0.109    |
|    ent_coef_loss   | 0.676    |
|    learning_rate   | 0.0003   |
|    n_updates       | 191099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -61      |
|    critic_loss     | 16.1     |
|    ent_coef        | 0.113    |
|    ent_coef_loss   | 0.312    |
|    learning_rate   | 0.0003   |
|    n_updates       | 191499   |
---------------------------------
=== Iterazione IRL 137 ===
Loss reward (iter 137): 5.052477836608887
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -60.7    |
|    critic_loss     | 15.4     |
|    ent_coef        | 0.109    |
|    ent_coef_loss   | -0.0341  |
|    learning_rate   | 0.0003   |
|    n_updates       | 192099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -63.2    |
|    critic_loss     | 12.4     |
|    ent_coef        | 0.107    |
|    ent_coef_loss   | -0.34    |
|    learning_rate   | 0.0003   |
|    n_updates       | 192499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -60.6    |
|    critic_loss     | 14.1     |
|    ent_coef        | 0.105    |
|    ent_coef_loss   | 0.185    |
|    learning_rate   | 0.0003   |
|    n_updates       | 192899   |
---------------------------------
=== Iterazione IRL 138 ===
Loss reward (iter 138): 5.153844356536865
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -57.1    |
|    critic_loss     | 17.1     |
|    ent_coef        | 0.103    |
|    ent_coef_loss   | 0.471    |
|    learning_rate   | 0.0003   |
|    n_updates       | 193499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -57.7    |
|    critic_loss     | 15.1     |
|    ent_coef        | 0.103    |
|    ent_coef_loss   | -0.392   |
|    learning_rate   | 0.0003   |
|    n_updates       | 193899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -56.9    |
|    critic_loss     | 15.6     |
|    ent_coef        | 0.105    |
|    ent_coef_loss   | -0.103   |
|    learning_rate   | 0.0003   |
|    n_updates       | 194299   |
---------------------------------
=== Iterazione IRL 139 ===
Loss reward (iter 139): 5.069002151489258
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -57.2    |
|    critic_loss     | 17.3     |
|    ent_coef        | 0.102    |
|    ent_coef_loss   | -0.0605  |
|    learning_rate   | 0.0003   |
|    n_updates       | 194899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -53.3    |
|    critic_loss     | 16.4     |
|    ent_coef        | 0.104    |
|    ent_coef_loss   | 0.01     |
|    learning_rate   | 0.0003   |
|    n_updates       | 195299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -54.9    |
|    critic_loss     | 17.1     |
|    ent_coef        | 0.0982   |
|    ent_coef_loss   | 0.304    |
|    learning_rate   | 0.0003   |
|    n_updates       | 195699   |
---------------------------------
=== Iterazione IRL 140 ===
Loss reward (iter 140): 5.418689250946045
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -55.4    |
|    critic_loss     | 16.3     |
|    ent_coef        | 0.0985   |
|    ent_coef_loss   | 0.274    |
|    learning_rate   | 0.0003   |
|    n_updates       | 196299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -50.3    |
|    critic_loss     | 13.7     |
|    ent_coef        | 0.0983   |
|    ent_coef_loss   | 0.637    |
|    learning_rate   | 0.0003   |
|    n_updates       | 196699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -53.2    |
|    critic_loss     | 15       |
|    ent_coef        | 0.0971   |
|    ent_coef_loss   | -0.131   |
|    learning_rate   | 0.0003   |
|    n_updates       | 197099   |
---------------------------------
=== Iterazione IRL 141 ===
Loss reward (iter 141): 4.631668567657471
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -51.4    |
|    critic_loss     | 15.1     |
|    ent_coef        | 0.0993   |
|    ent_coef_loss   | 0.5      |
|    learning_rate   | 0.0003   |
|    n_updates       | 197699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -50.4    |
|    critic_loss     | 16.4     |
|    ent_coef        | 0.101    |
|    ent_coef_loss   | 0.426    |
|    learning_rate   | 0.0003   |
|    n_updates       | 198099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -48.7    |
|    critic_loss     | 16.9     |
|    ent_coef        | 0.105    |
|    ent_coef_loss   | 0.946    |
|    learning_rate   | 0.0003   |
|    n_updates       | 198499   |
---------------------------------
=== Iterazione IRL 142 ===
Loss reward (iter 142): 4.933174133300781
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -48      |
|    critic_loss     | 15.5     |
|    ent_coef        | 0.112    |
|    ent_coef_loss   | 0.25     |
|    learning_rate   | 0.0003   |
|    n_updates       | 199099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -46.5    |
|    critic_loss     | 18.7     |
|    ent_coef        | 0.112    |
|    ent_coef_loss   | 0.132    |
|    learning_rate   | 0.0003   |
|    n_updates       | 199499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -47.7    |
|    critic_loss     | 17.7     |
|    ent_coef        | 0.111    |
|    ent_coef_loss   | -0.52    |
|    learning_rate   | 0.0003   |
|    n_updates       | 199899   |
---------------------------------
=== Iterazione IRL 143 ===
Loss reward (iter 143): 5.423242092132568
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -45.1    |
|    critic_loss     | 16.4     |
|    ent_coef        | 0.115    |
|    ent_coef_loss   | 0.823    |
|    learning_rate   | 0.0003   |
|    n_updates       | 200499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -46      |
|    critic_loss     | 15.6     |
|    ent_coef        | 0.114    |
|    ent_coef_loss   | -0.0045  |
|    learning_rate   | 0.0003   |
|    n_updates       | 200899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -43.3    |
|    critic_loss     | 17.8     |
|    ent_coef        | 0.113    |
|    ent_coef_loss   | -0.381   |
|    learning_rate   | 0.0003   |
|    n_updates       | 201299   |
---------------------------------
=== Iterazione IRL 144 ===
Loss reward (iter 144): 5.24427604675293
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -43.4    |
|    critic_loss     | 21.9     |
|    ent_coef        | 0.109    |
|    ent_coef_loss   | -0.725   |
|    learning_rate   | 0.0003   |
|    n_updates       | 201899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -42.8    |
|    critic_loss     | 17.7     |
|    ent_coef        | 0.109    |
|    ent_coef_loss   | -0.153   |
|    learning_rate   | 0.0003   |
|    n_updates       | 202299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -43.1    |
|    critic_loss     | 11.3     |
|    ent_coef        | 0.106    |
|    ent_coef_loss   | 0.45     |
|    learning_rate   | 0.0003   |
|    n_updates       | 202699   |
---------------------------------
=== Iterazione IRL 145 ===
Loss reward (iter 145): 5.41959285736084
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -40.4    |
|    critic_loss     | 15.3     |
|    ent_coef        | 0.107    |
|    ent_coef_loss   | 0.0469   |
|    learning_rate   | 0.0003   |
|    n_updates       | 203299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -37.9    |
|    critic_loss     | 14.2     |
|    ent_coef        | 0.108    |
|    ent_coef_loss   | -0.112   |
|    learning_rate   | 0.0003   |
|    n_updates       | 203699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -41.4    |
|    critic_loss     | 15.2     |
|    ent_coef        | 0.106    |
|    ent_coef_loss   | 0.238    |
|    learning_rate   | 0.0003   |
|    n_updates       | 204099   |
---------------------------------
=== Iterazione IRL 146 ===
Loss reward (iter 146): 5.42595100402832
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -37.7    |
|    critic_loss     | 13.7     |
|    ent_coef        | 0.111    |
|    ent_coef_loss   | -0.0942  |
|    learning_rate   | 0.0003   |
|    n_updates       | 204699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -37.2    |
|    critic_loss     | 15.5     |
|    ent_coef        | 0.112    |
|    ent_coef_loss   | -0.467   |
|    learning_rate   | 0.0003   |
|    n_updates       | 205099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -36.3    |
|    critic_loss     | 17.4     |
|    ent_coef        | 0.114    |
|    ent_coef_loss   | 0.0949   |
|    learning_rate   | 0.0003   |
|    n_updates       | 205499   |
---------------------------------
=== Iterazione IRL 147 ===
Loss reward (iter 147): 4.6196160316467285
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -34.8    |
|    critic_loss     | 18.8     |
|    ent_coef        | 0.112    |
|    ent_coef_loss   | -0.0444  |
|    learning_rate   | 0.0003   |
|    n_updates       | 206099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -36.2    |
|    critic_loss     | 16.6     |
|    ent_coef        | 0.113    |
|    ent_coef_loss   | 0.206    |
|    learning_rate   | 0.0003   |
|    n_updates       | 206499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -33      |
|    critic_loss     | 18.2     |
|    ent_coef        | 0.111    |
|    ent_coef_loss   | -0.172   |
|    learning_rate   | 0.0003   |
|    n_updates       | 206899   |
---------------------------------
=== Iterazione IRL 148 ===
Loss reward (iter 148): 4.269218444824219
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -31.2    |
|    critic_loss     | 16       |
|    ent_coef        | 0.11     |
|    ent_coef_loss   | -0.0996  |
|    learning_rate   | 0.0003   |
|    n_updates       | 207499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -30.9    |
|    critic_loss     | 16.5     |
|    ent_coef        | 0.11     |
|    ent_coef_loss   | 0.488    |
|    learning_rate   | 0.0003   |
|    n_updates       | 207899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -31.6    |
|    critic_loss     | 16.7     |
|    ent_coef        | 0.108    |
|    ent_coef_loss   | -0.748   |
|    learning_rate   | 0.0003   |
|    n_updates       | 208299   |
---------------------------------
=== Iterazione IRL 149 ===
Loss reward (iter 149): 5.158115386962891
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -30.4    |
|    critic_loss     | 15.3     |
|    ent_coef        | 0.111    |
|    ent_coef_loss   | 0.118    |
|    learning_rate   | 0.0003   |
|    n_updates       | 208899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -26.2    |
|    critic_loss     | 17.6     |
|    ent_coef        | 0.111    |
|    ent_coef_loss   | 0.427    |
|    learning_rate   | 0.0003   |
|    n_updates       | 209299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -29.8    |
|    critic_loss     | 15.5     |
|    ent_coef        | 0.11     |
|    ent_coef_loss   | -0.165   |
|    learning_rate   | 0.0003   |
|    n_updates       | 209699   |
---------------------------------
=== Iterazione IRL 150 ===
Loss reward (iter 150): 5.646352291107178
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -26.1    |
|    critic_loss     | 16.8     |
|    ent_coef        | 0.111    |
|    ent_coef_loss   | 0.875    |
|    learning_rate   | 0.0003   |
|    n_updates       | 210299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -26.6    |
|    critic_loss     | 22.3     |
|    ent_coef        | 0.111    |
|    ent_coef_loss   | -0.0316  |
|    learning_rate   | 0.0003   |
|    n_updates       | 210699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -25.6    |
|    critic_loss     | 18.7     |
|    ent_coef        | 0.115    |
|    ent_coef_loss   | -0.398   |
|    learning_rate   | 0.0003   |
|    n_updates       | 211099   |
---------------------------------
=== Iterazione IRL 151 ===
Loss reward (iter 151): 5.640293121337891
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -23.1    |
|    critic_loss     | 22.5     |
|    ent_coef        | 0.116    |
|    ent_coef_loss   | 0.34     |
|    learning_rate   | 0.0003   |
|    n_updates       | 211699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -22.9    |
|    critic_loss     | 24       |
|    ent_coef        | 0.116    |
|    ent_coef_loss   | 0.282    |
|    learning_rate   | 0.0003   |
|    n_updates       | 212099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -18.1    |
|    critic_loss     | 20.5     |
|    ent_coef        | 0.117    |
|    ent_coef_loss   | -0.246   |
|    learning_rate   | 0.0003   |
|    n_updates       | 212499   |
---------------------------------
=== Iterazione IRL 152 ===
Loss reward (iter 152): 4.972824573516846
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -20.9    |
|    critic_loss     | 17.6     |
|    ent_coef        | 0.118    |
|    ent_coef_loss   | -0.246   |
|    learning_rate   | 0.0003   |
|    n_updates       | 213099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -20.2    |
|    critic_loss     | 20.3     |
|    ent_coef        | 0.121    |
|    ent_coef_loss   | 0.2      |
|    learning_rate   | 0.0003   |
|    n_updates       | 213499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -21.7    |
|    critic_loss     | 22.6     |
|    ent_coef        | 0.122    |
|    ent_coef_loss   | -0.0707  |
|    learning_rate   | 0.0003   |
|    n_updates       | 213899   |
---------------------------------
=== Iterazione IRL 153 ===
Loss reward (iter 153): 5.522670745849609
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -18.2    |
|    critic_loss     | 20.7     |
|    ent_coef        | 0.117    |
|    ent_coef_loss   | 0.0794   |
|    learning_rate   | 0.0003   |
|    n_updates       | 214499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -16.4    |
|    critic_loss     | 15.7     |
|    ent_coef        | 0.115    |
|    ent_coef_loss   | 0.189    |
|    learning_rate   | 0.0003   |
|    n_updates       | 214899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -16.6    |
|    critic_loss     | 24.1     |
|    ent_coef        | 0.112    |
|    ent_coef_loss   | 0.0115   |
|    learning_rate   | 0.0003   |
|    n_updates       | 215299   |
---------------------------------
=== Iterazione IRL 154 ===
Loss reward (iter 154): 5.037020206451416
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -15      |
|    critic_loss     | 23.6     |
|    ent_coef        | 0.112    |
|    ent_coef_loss   | -0.597   |
|    learning_rate   | 0.0003   |
|    n_updates       | 215899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.9    |
|    critic_loss     | 21.3     |
|    ent_coef        | 0.111    |
|    ent_coef_loss   | 0.631    |
|    learning_rate   | 0.0003   |
|    n_updates       | 216299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -14.5    |
|    critic_loss     | 17.9     |
|    ent_coef        | 0.113    |
|    ent_coef_loss   | -0.389   |
|    learning_rate   | 0.0003   |
|    n_updates       | 216699   |
---------------------------------
=== Iterazione IRL 155 ===
Loss reward (iter 155): 5.08911657333374
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.62    |
|    critic_loss     | 21.2     |
|    ent_coef        | 0.112    |
|    ent_coef_loss   | -0.514   |
|    learning_rate   | 0.0003   |
|    n_updates       | 217299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.3    |
|    critic_loss     | 19       |
|    ent_coef        | 0.108    |
|    ent_coef_loss   | -1.06    |
|    learning_rate   | 0.0003   |
|    n_updates       | 217699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -12.6    |
|    critic_loss     | 22.3     |
|    ent_coef        | 0.107    |
|    ent_coef_loss   | -0.383   |
|    learning_rate   | 0.0003   |
|    n_updates       | 218099   |
---------------------------------
=== Iterazione IRL 156 ===
Loss reward (iter 156): 5.076066970825195
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.9    |
|    critic_loss     | 17       |
|    ent_coef        | 0.112    |
|    ent_coef_loss   | 0.564    |
|    learning_rate   | 0.0003   |
|    n_updates       | 218699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.2    |
|    critic_loss     | 22.6     |
|    ent_coef        | 0.113    |
|    ent_coef_loss   | 0.912    |
|    learning_rate   | 0.0003   |
|    n_updates       | 219099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -6.61    |
|    critic_loss     | 22.8     |
|    ent_coef        | 0.116    |
|    ent_coef_loss   | -0.219   |
|    learning_rate   | 0.0003   |
|    n_updates       | 219499   |
---------------------------------
=== Iterazione IRL 157 ===
Loss reward (iter 157): 4.385179042816162
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.9     |
|    critic_loss     | 24.3     |
|    ent_coef        | 0.117    |
|    ent_coef_loss   | 0.214    |
|    learning_rate   | 0.0003   |
|    n_updates       | 220099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -9.08    |
|    critic_loss     | 22.2     |
|    ent_coef        | 0.116    |
|    ent_coef_loss   | -0.0822  |
|    learning_rate   | 0.0003   |
|    n_updates       | 220499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -2.85    |
|    critic_loss     | 25.3     |
|    ent_coef        | 0.121    |
|    ent_coef_loss   | 0.376    |
|    learning_rate   | 0.0003   |
|    n_updates       | 220899   |
---------------------------------
=== Iterazione IRL 158 ===
Loss reward (iter 158): 4.051207542419434
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.79    |
|    critic_loss     | 23.7     |
|    ent_coef        | 0.128    |
|    ent_coef_loss   | -0.334   |
|    learning_rate   | 0.0003   |
|    n_updates       | 221499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.09    |
|    critic_loss     | 24.3     |
|    ent_coef        | 0.127    |
|    ent_coef_loss   | 0.594    |
|    learning_rate   | 0.0003   |
|    n_updates       | 221899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -6.3     |
|    critic_loss     | 24.9     |
|    ent_coef        | 0.129    |
|    ent_coef_loss   | 0.0996   |
|    learning_rate   | 0.0003   |
|    n_updates       | 222299   |
---------------------------------
=== Iterazione IRL 159 ===
Loss reward (iter 159): 4.293042182922363
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.68    |
|    critic_loss     | 21.2     |
|    ent_coef        | 0.13     |
|    ent_coef_loss   | 0.0883   |
|    learning_rate   | 0.0003   |
|    n_updates       | 222899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.47    |
|    critic_loss     | 22.2     |
|    ent_coef        | 0.126    |
|    ent_coef_loss   | -0.576   |
|    learning_rate   | 0.0003   |
|    n_updates       | 223299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -2.02    |
|    critic_loss     | 23.1     |
|    ent_coef        | 0.12     |
|    ent_coef_loss   | -1.17    |
|    learning_rate   | 0.0003   |
|    n_updates       | 223699   |
---------------------------------
=== Iterazione IRL 160 ===
Loss reward (iter 160): 4.8166117668151855
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.71    |
|    critic_loss     | 19.3     |
|    ent_coef        | 0.121    |
|    ent_coef_loss   | -0.263   |
|    learning_rate   | 0.0003   |
|    n_updates       | 224299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.1     |
|    critic_loss     | 21.3     |
|    ent_coef        | 0.118    |
|    ent_coef_loss   | -0.171   |
|    learning_rate   | 0.0003   |
|    n_updates       | 224699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.65     |
|    critic_loss     | 24.4     |
|    ent_coef        | 0.119    |
|    ent_coef_loss   | 0.601    |
|    learning_rate   | 0.0003   |
|    n_updates       | 225099   |
---------------------------------
=== Iterazione IRL 161 ===
Loss reward (iter 161): 4.6504011154174805
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -1.1     |
|    critic_loss     | 25.9     |
|    ent_coef        | 0.119    |
|    ent_coef_loss   | 0.406    |
|    learning_rate   | 0.0003   |
|    n_updates       | 225699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 2.35     |
|    critic_loss     | 21.8     |
|    ent_coef        | 0.122    |
|    ent_coef_loss   | 0.258    |
|    learning_rate   | 0.0003   |
|    n_updates       | 226099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 1.81     |
|    critic_loss     | 21.3     |
|    ent_coef        | 0.121    |
|    ent_coef_loss   | -0.223   |
|    learning_rate   | 0.0003   |
|    n_updates       | 226499   |
---------------------------------
=== Iterazione IRL 162 ===
Loss reward (iter 162): 5.5260233879089355
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 3.14     |
|    critic_loss     | 26.5     |
|    ent_coef        | 0.125    |
|    ent_coef_loss   | 0.459    |
|    learning_rate   | 0.0003   |
|    n_updates       | 227099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 4.44     |
|    critic_loss     | 26.2     |
|    ent_coef        | 0.123    |
|    ent_coef_loss   | 0.378    |
|    learning_rate   | 0.0003   |
|    n_updates       | 227499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 6.43     |
|    critic_loss     | 28.4     |
|    ent_coef        | 0.121    |
|    ent_coef_loss   | 0.491    |
|    learning_rate   | 0.0003   |
|    n_updates       | 227899   |
---------------------------------
=== Iterazione IRL 163 ===
Loss reward (iter 163): 4.825428485870361
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 7.76     |
|    critic_loss     | 29.3     |
|    ent_coef        | 0.119    |
|    ent_coef_loss   | 0.0939   |
|    learning_rate   | 0.0003   |
|    n_updates       | 228499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 5.83     |
|    critic_loss     | 31.1     |
|    ent_coef        | 0.121    |
|    ent_coef_loss   | -0.578   |
|    learning_rate   | 0.0003   |
|    n_updates       | 228899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 9.54     |
|    critic_loss     | 34.4     |
|    ent_coef        | 0.116    |
|    ent_coef_loss   | 0.418    |
|    learning_rate   | 0.0003   |
|    n_updates       | 229299   |
---------------------------------
=== Iterazione IRL 164 ===
Loss reward (iter 164): 4.843104362487793
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 7.75     |
|    critic_loss     | 24.2     |
|    ent_coef        | 0.119    |
|    ent_coef_loss   | -0.0824  |
|    learning_rate   | 0.0003   |
|    n_updates       | 229899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 8.88     |
|    critic_loss     | 27.7     |
|    ent_coef        | 0.114    |
|    ent_coef_loss   | -0.0209  |
|    learning_rate   | 0.0003   |
|    n_updates       | 230299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 9.1      |
|    critic_loss     | 21.5     |
|    ent_coef        | 0.119    |
|    ent_coef_loss   | 0.802    |
|    learning_rate   | 0.0003   |
|    n_updates       | 230699   |
---------------------------------
=== Iterazione IRL 165 ===
Loss reward (iter 165): 4.410655975341797
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 13.2     |
|    critic_loss     | 25.3     |
|    ent_coef        | 0.121    |
|    ent_coef_loss   | 0.245    |
|    learning_rate   | 0.0003   |
|    n_updates       | 231299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 9.75     |
|    critic_loss     | 22.5     |
|    ent_coef        | 0.121    |
|    ent_coef_loss   | 0.622    |
|    learning_rate   | 0.0003   |
|    n_updates       | 231699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 14.5     |
|    critic_loss     | 26.9     |
|    ent_coef        | 0.123    |
|    ent_coef_loss   | 0.0338   |
|    learning_rate   | 0.0003   |
|    n_updates       | 232099   |
---------------------------------
=== Iterazione IRL 166 ===
Loss reward (iter 166): 4.6926774978637695
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 16.4     |
|    critic_loss     | 28.9     |
|    ent_coef        | 0.119    |
|    ent_coef_loss   | 0.189    |
|    learning_rate   | 0.0003   |
|    n_updates       | 232699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 14.1     |
|    critic_loss     | 31       |
|    ent_coef        | 0.119    |
|    ent_coef_loss   | -0.644   |
|    learning_rate   | 0.0003   |
|    n_updates       | 233099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 17.8     |
|    critic_loss     | 28.9     |
|    ent_coef        | 0.116    |
|    ent_coef_loss   | -0.00406 |
|    learning_rate   | 0.0003   |
|    n_updates       | 233499   |
---------------------------------
=== Iterazione IRL 167 ===
Loss reward (iter 167): 4.043720722198486
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 13.3     |
|    critic_loss     | 24.4     |
|    ent_coef        | 0.116    |
|    ent_coef_loss   | -0.376   |
|    learning_rate   | 0.0003   |
|    n_updates       | 234099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 19.8     |
|    critic_loss     | 30.8     |
|    ent_coef        | 0.117    |
|    ent_coef_loss   | 0.111    |
|    learning_rate   | 0.0003   |
|    n_updates       | 234499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 19.7     |
|    critic_loss     | 27.8     |
|    ent_coef        | 0.115    |
|    ent_coef_loss   | 1.02     |
|    learning_rate   | 0.0003   |
|    n_updates       | 234899   |
---------------------------------
=== Iterazione IRL 168 ===
Loss reward (iter 168): 3.9004223346710205
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 19.2     |
|    critic_loss     | 26       |
|    ent_coef        | 0.113    |
|    ent_coef_loss   | -0.449   |
|    learning_rate   | 0.0003   |
|    n_updates       | 235499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 20.4     |
|    critic_loss     | 24       |
|    ent_coef        | 0.113    |
|    ent_coef_loss   | 0.992    |
|    learning_rate   | 0.0003   |
|    n_updates       | 235899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 24.8     |
|    critic_loss     | 37.4     |
|    ent_coef        | 0.115    |
|    ent_coef_loss   | 0.794    |
|    learning_rate   | 0.0003   |
|    n_updates       | 236299   |
---------------------------------
=== Iterazione IRL 169 ===
Loss reward (iter 169): 4.752245903015137
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 21.8     |
|    critic_loss     | 29.7     |
|    ent_coef        | 0.117    |
|    ent_coef_loss   | 0.749    |
|    learning_rate   | 0.0003   |
|    n_updates       | 236899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 22.5     |
|    critic_loss     | 30.9     |
|    ent_coef        | 0.121    |
|    ent_coef_loss   | -0.716   |
|    learning_rate   | 0.0003   |
|    n_updates       | 237299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 28       |
|    critic_loss     | 29.6     |
|    ent_coef        | 0.117    |
|    ent_coef_loss   | -0.0195  |
|    learning_rate   | 0.0003   |
|    n_updates       | 237699   |
---------------------------------
=== Iterazione IRL 170 ===
Loss reward (iter 170): 4.631716728210449
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 28.6     |
|    critic_loss     | 33.4     |
|    ent_coef        | 0.121    |
|    ent_coef_loss   | 0.258    |
|    learning_rate   | 0.0003   |
|    n_updates       | 238299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 24.6     |
|    critic_loss     | 32.3     |
|    ent_coef        | 0.121    |
|    ent_coef_loss   | -0.0146  |
|    learning_rate   | 0.0003   |
|    n_updates       | 238699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 27.5     |
|    critic_loss     | 27.2     |
|    ent_coef        | 0.12     |
|    ent_coef_loss   | -0.0356  |
|    learning_rate   | 0.0003   |
|    n_updates       | 239099   |
---------------------------------
=== Iterazione IRL 171 ===
Loss reward (iter 171): 4.993208885192871
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 32.7     |
|    critic_loss     | 29.4     |
|    ent_coef        | 0.124    |
|    ent_coef_loss   | 0.916    |
|    learning_rate   | 0.0003   |
|    n_updates       | 239699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 32.6     |
|    critic_loss     | 29       |
|    ent_coef        | 0.121    |
|    ent_coef_loss   | -0.229   |
|    learning_rate   | 0.0003   |
|    n_updates       | 240099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 32.9     |
|    critic_loss     | 30       |
|    ent_coef        | 0.118    |
|    ent_coef_loss   | 0.18     |
|    learning_rate   | 0.0003   |
|    n_updates       | 240499   |
---------------------------------
=== Iterazione IRL 172 ===
Loss reward (iter 172): 4.639653205871582
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 35.2     |
|    critic_loss     | 32.6     |
|    ent_coef        | 0.122    |
|    ent_coef_loss   | 0.354    |
|    learning_rate   | 0.0003   |
|    n_updates       | 241099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 34.7     |
|    critic_loss     | 31.6     |
|    ent_coef        | 0.121    |
|    ent_coef_loss   | 0.173    |
|    learning_rate   | 0.0003   |
|    n_updates       | 241499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 34.3     |
|    critic_loss     | 28.6     |
|    ent_coef        | 0.119    |
|    ent_coef_loss   | -0.0807  |
|    learning_rate   | 0.0003   |
|    n_updates       | 241899   |
---------------------------------
=== Iterazione IRL 173 ===
Loss reward (iter 173): 4.493599891662598
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 36.3     |
|    critic_loss     | 30.2     |
|    ent_coef        | 0.122    |
|    ent_coef_loss   | -0.409   |
|    learning_rate   | 0.0003   |
|    n_updates       | 242499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 36       |
|    critic_loss     | 34.3     |
|    ent_coef        | 0.117    |
|    ent_coef_loss   | -0.296   |
|    learning_rate   | 0.0003   |
|    n_updates       | 242899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 36.8     |
|    critic_loss     | 29.6     |
|    ent_coef        | 0.115    |
|    ent_coef_loss   | -0.408   |
|    learning_rate   | 0.0003   |
|    n_updates       | 243299   |
---------------------------------
=== Iterazione IRL 174 ===
Loss reward (iter 174): 5.3967204093933105
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 38.8     |
|    critic_loss     | 28.3     |
|    ent_coef        | 0.113    |
|    ent_coef_loss   | -0.376   |
|    learning_rate   | 0.0003   |
|    n_updates       | 243899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 39.3     |
|    critic_loss     | 35       |
|    ent_coef        | 0.111    |
|    ent_coef_loss   | -0.6     |
|    learning_rate   | 0.0003   |
|    n_updates       | 244299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 40.1     |
|    critic_loss     | 33.7     |
|    ent_coef        | 0.11     |
|    ent_coef_loss   | 0.0645   |
|    learning_rate   | 0.0003   |
|    n_updates       | 244699   |
---------------------------------
=== Iterazione IRL 175 ===
Loss reward (iter 175): 5.186673641204834
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 41.8     |
|    critic_loss     | 33.7     |
|    ent_coef        | 0.115    |
|    ent_coef_loss   | -0.283   |
|    learning_rate   | 0.0003   |
|    n_updates       | 245299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 43.8     |
|    critic_loss     | 32.5     |
|    ent_coef        | 0.111    |
|    ent_coef_loss   | 0.0893   |
|    learning_rate   | 0.0003   |
|    n_updates       | 245699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 43.6     |
|    critic_loss     | 29.3     |
|    ent_coef        | 0.114    |
|    ent_coef_loss   | 0.165    |
|    learning_rate   | 0.0003   |
|    n_updates       | 246099   |
---------------------------------
=== Iterazione IRL 176 ===
Loss reward (iter 176): 4.429042339324951
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 43.9     |
|    critic_loss     | 32.7     |
|    ent_coef        | 0.116    |
|    ent_coef_loss   | 0.647    |
|    learning_rate   | 0.0003   |
|    n_updates       | 246699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 44.3     |
|    critic_loss     | 32.2     |
|    ent_coef        | 0.117    |
|    ent_coef_loss   | -0.65    |
|    learning_rate   | 0.0003   |
|    n_updates       | 247099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 47.8     |
|    critic_loss     | 35.4     |
|    ent_coef        | 0.121    |
|    ent_coef_loss   | 0.0221   |
|    learning_rate   | 0.0003   |
|    n_updates       | 247499   |
---------------------------------
=== Iterazione IRL 177 ===
Loss reward (iter 177): 4.762231826782227
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 49.8     |
|    critic_loss     | 38.9     |
|    ent_coef        | 0.125    |
|    ent_coef_loss   | -0.184   |
|    learning_rate   | 0.0003   |
|    n_updates       | 248099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 50.9     |
|    critic_loss     | 33.8     |
|    ent_coef        | 0.127    |
|    ent_coef_loss   | 0.937    |
|    learning_rate   | 0.0003   |
|    n_updates       | 248499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 53.1     |
|    critic_loss     | 34.5     |
|    ent_coef        | 0.127    |
|    ent_coef_loss   | 0.555    |
|    learning_rate   | 0.0003   |
|    n_updates       | 248899   |
---------------------------------
=== Iterazione IRL 178 ===
Loss reward (iter 178): 4.4784016609191895
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 53.4     |
|    critic_loss     | 31.5     |
|    ent_coef        | 0.125    |
|    ent_coef_loss   | 0.154    |
|    learning_rate   | 0.0003   |
|    n_updates       | 249499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 56.6     |
|    critic_loss     | 38.2     |
|    ent_coef        | 0.126    |
|    ent_coef_loss   | 0.8      |
|    learning_rate   | 0.0003   |
|    n_updates       | 249899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 52.6     |
|    critic_loss     | 32.2     |
|    ent_coef        | 0.128    |
|    ent_coef_loss   | 0.451    |
|    learning_rate   | 0.0003   |
|    n_updates       | 250299   |
---------------------------------
=== Iterazione IRL 179 ===
Loss reward (iter 179): 4.313115119934082
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 56.5     |
|    critic_loss     | 31.5     |
|    ent_coef        | 0.124    |
|    ent_coef_loss   | -0.0173  |
|    learning_rate   | 0.0003   |
|    n_updates       | 250899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 58.9     |
|    critic_loss     | 33.4     |
|    ent_coef        | 0.124    |
|    ent_coef_loss   | 0.269    |
|    learning_rate   | 0.0003   |
|    n_updates       | 251299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 57.9     |
|    critic_loss     | 30.9     |
|    ent_coef        | 0.121    |
|    ent_coef_loss   | -0.691   |
|    learning_rate   | 0.0003   |
|    n_updates       | 251699   |
---------------------------------
=== Iterazione IRL 180 ===
Loss reward (iter 180): 4.707118511199951
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 58       |
|    critic_loss     | 32.5     |
|    ent_coef        | 0.121    |
|    ent_coef_loss   | 0.103    |
|    learning_rate   | 0.0003   |
|    n_updates       | 252299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 62.8     |
|    critic_loss     | 35.2     |
|    ent_coef        | 0.114    |
|    ent_coef_loss   | -1.17    |
|    learning_rate   | 0.0003   |
|    n_updates       | 252699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 63.5     |
|    critic_loss     | 40.7     |
|    ent_coef        | 0.112    |
|    ent_coef_loss   | 0.138    |
|    learning_rate   | 0.0003   |
|    n_updates       | 253099   |
---------------------------------
=== Iterazione IRL 181 ===
Loss reward (iter 181): 4.711270332336426
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 65.6     |
|    critic_loss     | 25.3     |
|    ent_coef        | 0.113    |
|    ent_coef_loss   | -0.653   |
|    learning_rate   | 0.0003   |
|    n_updates       | 253699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 64.9     |
|    critic_loss     | 34.4     |
|    ent_coef        | 0.112    |
|    ent_coef_loss   | -0.443   |
|    learning_rate   | 0.0003   |
|    n_updates       | 254099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 68       |
|    critic_loss     | 30.1     |
|    ent_coef        | 0.109    |
|    ent_coef_loss   | 0.011    |
|    learning_rate   | 0.0003   |
|    n_updates       | 254499   |
---------------------------------
=== Iterazione IRL 182 ===
Loss reward (iter 182): 4.505121231079102
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 65.6     |
|    critic_loss     | 36.6     |
|    ent_coef        | 0.108    |
|    ent_coef_loss   | 0.106    |
|    learning_rate   | 0.0003   |
|    n_updates       | 255099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 68.3     |
|    critic_loss     | 38.8     |
|    ent_coef        | 0.109    |
|    ent_coef_loss   | 0.0609   |
|    learning_rate   | 0.0003   |
|    n_updates       | 255499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 70.2     |
|    critic_loss     | 32.7     |
|    ent_coef        | 0.106    |
|    ent_coef_loss   | -0.997   |
|    learning_rate   | 0.0003   |
|    n_updates       | 255899   |
---------------------------------
=== Iterazione IRL 183 ===
Loss reward (iter 183): 4.705239772796631
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 70.5     |
|    critic_loss     | 38.5     |
|    ent_coef        | 0.105    |
|    ent_coef_loss   | -0.558   |
|    learning_rate   | 0.0003   |
|    n_updates       | 256499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 76.7     |
|    critic_loss     | 45.1     |
|    ent_coef        | 0.111    |
|    ent_coef_loss   | 1.08     |
|    learning_rate   | 0.0003   |
|    n_updates       | 256899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 71.4     |
|    critic_loss     | 40.2     |
|    ent_coef        | 0.113    |
|    ent_coef_loss   | -0.00167 |
|    learning_rate   | 0.0003   |
|    n_updates       | 257299   |
---------------------------------
=== Iterazione IRL 184 ===
Loss reward (iter 184): 5.053005695343018
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 77.6     |
|    critic_loss     | 40.9     |
|    ent_coef        | 0.114    |
|    ent_coef_loss   | 0.733    |
|    learning_rate   | 0.0003   |
|    n_updates       | 257899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 79       |
|    critic_loss     | 40.1     |
|    ent_coef        | 0.115    |
|    ent_coef_loss   | -0.601   |
|    learning_rate   | 0.0003   |
|    n_updates       | 258299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 79.2     |
|    critic_loss     | 46.5     |
|    ent_coef        | 0.117    |
|    ent_coef_loss   | -0.305   |
|    learning_rate   | 0.0003   |
|    n_updates       | 258699   |
---------------------------------
=== Iterazione IRL 185 ===
Loss reward (iter 185): 4.614757537841797
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 77.6     |
|    critic_loss     | 40.3     |
|    ent_coef        | 0.117    |
|    ent_coef_loss   | 0.0109   |
|    learning_rate   | 0.0003   |
|    n_updates       | 259299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 77.2     |
|    critic_loss     | 35.8     |
|    ent_coef        | 0.119    |
|    ent_coef_loss   | 0.146    |
|    learning_rate   | 0.0003   |
|    n_updates       | 259699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 79.4     |
|    critic_loss     | 41.2     |
|    ent_coef        | 0.12     |
|    ent_coef_loss   | -0.411   |
|    learning_rate   | 0.0003   |
|    n_updates       | 260099   |
---------------------------------
=== Iterazione IRL 186 ===
Loss reward (iter 186): 4.661918640136719
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 84.1     |
|    critic_loss     | 47.3     |
|    ent_coef        | 0.12     |
|    ent_coef_loss   | 0.133    |
|    learning_rate   | 0.0003   |
|    n_updates       | 260699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 81.6     |
|    critic_loss     | 44.8     |
|    ent_coef        | 0.119    |
|    ent_coef_loss   | -0.015   |
|    learning_rate   | 0.0003   |
|    n_updates       | 261099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 82.7     |
|    critic_loss     | 38.5     |
|    ent_coef        | 0.119    |
|    ent_coef_loss   | -0.191   |
|    learning_rate   | 0.0003   |
|    n_updates       | 261499   |
---------------------------------
=== Iterazione IRL 187 ===
Loss reward (iter 187): 5.730907917022705
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 82.6     |
|    critic_loss     | 45.2     |
|    ent_coef        | 0.123    |
|    ent_coef_loss   | 0.399    |
|    learning_rate   | 0.0003   |
|    n_updates       | 262099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 83.8     |
|    critic_loss     | 39.7     |
|    ent_coef        | 0.127    |
|    ent_coef_loss   | 0.302    |
|    learning_rate   | 0.0003   |
|    n_updates       | 262499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 89.7     |
|    critic_loss     | 43.6     |
|    ent_coef        | 0.128    |
|    ent_coef_loss   | 0.325    |
|    learning_rate   | 0.0003   |
|    n_updates       | 262899   |
---------------------------------
=== Iterazione IRL 188 ===
Loss reward (iter 188): 4.72090482711792
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 90.3     |
|    critic_loss     | 43.6     |
|    ent_coef        | 0.128    |
|    ent_coef_loss   | -0.0557  |
|    learning_rate   | 0.0003   |
|    n_updates       | 263499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 90       |
|    critic_loss     | 46.1     |
|    ent_coef        | 0.129    |
|    ent_coef_loss   | 0.0545   |
|    learning_rate   | 0.0003   |
|    n_updates       | 263899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 88.9     |
|    critic_loss     | 42.4     |
|    ent_coef        | 0.13     |
|    ent_coef_loss   | 0.061    |
|    learning_rate   | 0.0003   |
|    n_updates       | 264299   |
---------------------------------
=== Iterazione IRL 189 ===
Loss reward (iter 189): 5.094696998596191
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 89.5     |
|    critic_loss     | 49.5     |
|    ent_coef        | 0.124    |
|    ent_coef_loss   | 0.076    |
|    learning_rate   | 0.0003   |
|    n_updates       | 264899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 90.8     |
|    critic_loss     | 45.9     |
|    ent_coef        | 0.126    |
|    ent_coef_loss   | -0.079   |
|    learning_rate   | 0.0003   |
|    n_updates       | 265299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 92.9     |
|    critic_loss     | 45.2     |
|    ent_coef        | 0.123    |
|    ent_coef_loss   | 0.198    |
|    learning_rate   | 0.0003   |
|    n_updates       | 265699   |
---------------------------------
=== Iterazione IRL 190 ===
Loss reward (iter 190): 4.832936763763428
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 98.2     |
|    critic_loss     | 43.6     |
|    ent_coef        | 0.119    |
|    ent_coef_loss   | -0.131   |
|    learning_rate   | 0.0003   |
|    n_updates       | 266299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 97.4     |
|    critic_loss     | 40.6     |
|    ent_coef        | 0.118    |
|    ent_coef_loss   | 0.175    |
|    learning_rate   | 0.0003   |
|    n_updates       | 266699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 97.7     |
|    critic_loss     | 40.6     |
|    ent_coef        | 0.115    |
|    ent_coef_loss   | -0.286   |
|    learning_rate   | 0.0003   |
|    n_updates       | 267099   |
---------------------------------
=== Iterazione IRL 191 ===
Loss reward (iter 191): 4.614933490753174
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 97.4     |
|    critic_loss     | 35.4     |
|    ent_coef        | 0.122    |
|    ent_coef_loss   | 0.753    |
|    learning_rate   | 0.0003   |
|    n_updates       | 267699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 99.4     |
|    critic_loss     | 58.3     |
|    ent_coef        | 0.121    |
|    ent_coef_loss   | 0.232    |
|    learning_rate   | 0.0003   |
|    n_updates       | 268099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 102      |
|    critic_loss     | 46.8     |
|    ent_coef        | 0.117    |
|    ent_coef_loss   | 0.242    |
|    learning_rate   | 0.0003   |
|    n_updates       | 268499   |
---------------------------------
=== Iterazione IRL 192 ===
Loss reward (iter 192): 3.514812469482422
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 101      |
|    critic_loss     | 49.6     |
|    ent_coef        | 0.125    |
|    ent_coef_loss   | 0.584    |
|    learning_rate   | 0.0003   |
|    n_updates       | 269099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 103      |
|    critic_loss     | 48.4     |
|    ent_coef        | 0.124    |
|    ent_coef_loss   | -0.283   |
|    learning_rate   | 0.0003   |
|    n_updates       | 269499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 102      |
|    critic_loss     | 44.8     |
|    ent_coef        | 0.125    |
|    ent_coef_loss   | 0.225    |
|    learning_rate   | 0.0003   |
|    n_updates       | 269899   |
---------------------------------
=== Iterazione IRL 193 ===
Loss reward (iter 193): 4.120217800140381
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 104      |
|    critic_loss     | 49.9     |
|    ent_coef        | 0.117    |
|    ent_coef_loss   | -0.402   |
|    learning_rate   | 0.0003   |
|    n_updates       | 270499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 107      |
|    critic_loss     | 53       |
|    ent_coef        | 0.113    |
|    ent_coef_loss   | -0.302   |
|    learning_rate   | 0.0003   |
|    n_updates       | 270899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 105      |
|    critic_loss     | 39.7     |
|    ent_coef        | 0.115    |
|    ent_coef_loss   | -0.207   |
|    learning_rate   | 0.0003   |
|    n_updates       | 271299   |
---------------------------------
=== Iterazione IRL 194 ===
Loss reward (iter 194): 4.681530475616455
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 107      |
|    critic_loss     | 46.9     |
|    ent_coef        | 0.116    |
|    ent_coef_loss   | 0.299    |
|    learning_rate   | 0.0003   |
|    n_updates       | 271899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 111      |
|    critic_loss     | 48.3     |
|    ent_coef        | 0.118    |
|    ent_coef_loss   | 0.314    |
|    learning_rate   | 0.0003   |
|    n_updates       | 272299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 113      |
|    critic_loss     | 45.7     |
|    ent_coef        | 0.12     |
|    ent_coef_loss   | -0.326   |
|    learning_rate   | 0.0003   |
|    n_updates       | 272699   |
---------------------------------
=== Iterazione IRL 195 ===
Loss reward (iter 195): 5.534881114959717
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 113      |
|    critic_loss     | 49.4     |
|    ent_coef        | 0.124    |
|    ent_coef_loss   | 0.0846   |
|    learning_rate   | 0.0003   |
|    n_updates       | 273299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 113      |
|    critic_loss     | 49.3     |
|    ent_coef        | 0.125    |
|    ent_coef_loss   | -0.0908  |
|    learning_rate   | 0.0003   |
|    n_updates       | 273699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 117      |
|    critic_loss     | 40.8     |
|    ent_coef        | 0.123    |
|    ent_coef_loss   | -0.254   |
|    learning_rate   | 0.0003   |
|    n_updates       | 274099   |
---------------------------------
=== Iterazione IRL 196 ===
Loss reward (iter 196): 4.855727195739746
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 114      |
|    critic_loss     | 46.5     |
|    ent_coef        | 0.123    |
|    ent_coef_loss   | 0.609    |
|    learning_rate   | 0.0003   |
|    n_updates       | 274699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 113      |
|    critic_loss     | 40.5     |
|    ent_coef        | 0.117    |
|    ent_coef_loss   | -0.374   |
|    learning_rate   | 0.0003   |
|    n_updates       | 275099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 119      |
|    critic_loss     | 55       |
|    ent_coef        | 0.115    |
|    ent_coef_loss   | 0.225    |
|    learning_rate   | 0.0003   |
|    n_updates       | 275499   |
---------------------------------
=== Iterazione IRL 197 ===
Loss reward (iter 197): 4.3333916664123535
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 122      |
|    critic_loss     | 51.9     |
|    ent_coef        | 0.116    |
|    ent_coef_loss   | 0.00242  |
|    learning_rate   | 0.0003   |
|    n_updates       | 276099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 118      |
|    critic_loss     | 39.9     |
|    ent_coef        | 0.117    |
|    ent_coef_loss   | -0.941   |
|    learning_rate   | 0.0003   |
|    n_updates       | 276499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 121      |
|    critic_loss     | 47.2     |
|    ent_coef        | 0.115    |
|    ent_coef_loss   | -0.111   |
|    learning_rate   | 0.0003   |
|    n_updates       | 276899   |
---------------------------------
=== Iterazione IRL 198 ===
Loss reward (iter 198): 4.630361557006836
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 122      |
|    critic_loss     | 50.1     |
|    ent_coef        | 0.12     |
|    ent_coef_loss   | 0.558    |
|    learning_rate   | 0.0003   |
|    n_updates       | 277499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 122      |
|    critic_loss     | 48.7     |
|    ent_coef        | 0.118    |
|    ent_coef_loss   | -0.245   |
|    learning_rate   | 0.0003   |
|    n_updates       | 277899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 126      |
|    critic_loss     | 58.7     |
|    ent_coef        | 0.116    |
|    ent_coef_loss   | -0.233   |
|    learning_rate   | 0.0003   |
|    n_updates       | 278299   |
---------------------------------
=== Iterazione IRL 199 ===
Loss reward (iter 199): 4.170300483703613
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 128      |
|    critic_loss     | 46       |
|    ent_coef        | 0.119    |
|    ent_coef_loss   | 0.12     |
|    learning_rate   | 0.0003   |
|    n_updates       | 278899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 124      |
|    critic_loss     | 52.5     |
|    ent_coef        | 0.119    |
|    ent_coef_loss   | -0.39    |
|    learning_rate   | 0.0003   |
|    n_updates       | 279299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 125      |
|    critic_loss     | 41.6     |
|    ent_coef        | 0.117    |
|    ent_coef_loss   | -0.53    |
|    learning_rate   | 0.0003   |
|    n_updates       | 279699   |
---------------------------------
=== Iterazione IRL 200 ===
Loss reward (iter 200): 4.32380485534668
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 125      |
|    critic_loss     | 46.7     |
|    ent_coef        | 0.114    |
|    ent_coef_loss   | -1.23    |
|    learning_rate   | 0.0003   |
|    n_updates       | 280299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 128      |
|    critic_loss     | 53.3     |
|    ent_coef        | 0.113    |
|    ent_coef_loss   | -0.427   |
|    learning_rate   | 0.0003   |
|    n_updates       | 280699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 129      |
|    critic_loss     | 57.8     |
|    ent_coef        | 0.113    |
|    ent_coef_loss   | -0.101   |
|    learning_rate   | 0.0003   |
|    n_updates       | 281099   |
---------------------------------
=== Iterazione IRL 201 ===
Loss reward (iter 201): 4.670671463012695
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 134      |
|    critic_loss     | 57.6     |
|    ent_coef        | 0.111    |
|    ent_coef_loss   | -0.106   |
|    learning_rate   | 0.0003   |
|    n_updates       | 281699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 134      |
|    critic_loss     | 50.1     |
|    ent_coef        | 0.113    |
|    ent_coef_loss   | -0.0694  |
|    learning_rate   | 0.0003   |
|    n_updates       | 282099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 136      |
|    critic_loss     | 54.3     |
|    ent_coef        | 0.121    |
|    ent_coef_loss   | -0.346   |
|    learning_rate   | 0.0003   |
|    n_updates       | 282499   |
---------------------------------
=== Iterazione IRL 202 ===
Loss reward (iter 202): 3.1366095542907715
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 138      |
|    critic_loss     | 64.5     |
|    ent_coef        | 0.117    |
|    ent_coef_loss   | 0.182    |
|    learning_rate   | 0.0003   |
|    n_updates       | 283099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 138      |
|    critic_loss     | 49.6     |
|    ent_coef        | 0.116    |
|    ent_coef_loss   | -0.539   |
|    learning_rate   | 0.0003   |
|    n_updates       | 283499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 143      |
|    critic_loss     | 58.2     |
|    ent_coef        | 0.116    |
|    ent_coef_loss   | 0.407    |
|    learning_rate   | 0.0003   |
|    n_updates       | 283899   |
---------------------------------
=== Iterazione IRL 203 ===
Loss reward (iter 203): 3.224436044692993
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 142      |
|    critic_loss     | 59.1     |
|    ent_coef        | 0.117    |
|    ent_coef_loss   | 0.644    |
|    learning_rate   | 0.0003   |
|    n_updates       | 284499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 139      |
|    critic_loss     | 67.7     |
|    ent_coef        | 0.109    |
|    ent_coef_loss   | 0.734    |
|    learning_rate   | 0.0003   |
|    n_updates       | 284899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 145      |
|    critic_loss     | 70.4     |
|    ent_coef        | 0.11     |
|    ent_coef_loss   | 0.00834  |
|    learning_rate   | 0.0003   |
|    n_updates       | 285299   |
---------------------------------
=== Iterazione IRL 204 ===
Loss reward (iter 204): 3.710146188735962
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 144      |
|    critic_loss     | 57.5     |
|    ent_coef        | 0.114    |
|    ent_coef_loss   | -0.0205  |
|    learning_rate   | 0.0003   |
|    n_updates       | 285899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 144      |
|    critic_loss     | 54.5     |
|    ent_coef        | 0.115    |
|    ent_coef_loss   | -0.0414  |
|    learning_rate   | 0.0003   |
|    n_updates       | 286299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 151      |
|    critic_loss     | 67.7     |
|    ent_coef        | 0.118    |
|    ent_coef_loss   | -0.235   |
|    learning_rate   | 0.0003   |
|    n_updates       | 286699   |
---------------------------------
=== Iterazione IRL 205 ===
Loss reward (iter 205): 4.765049934387207
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 151      |
|    critic_loss     | 72.4     |
|    ent_coef        | 0.116    |
|    ent_coef_loss   | 0.0104   |
|    learning_rate   | 0.0003   |
|    n_updates       | 287299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 153      |
|    critic_loss     | 64.1     |
|    ent_coef        | 0.124    |
|    ent_coef_loss   | -0.0145  |
|    learning_rate   | 0.0003   |
|    n_updates       | 287699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 151      |
|    critic_loss     | 60.9     |
|    ent_coef        | 0.123    |
|    ent_coef_loss   | 0.257    |
|    learning_rate   | 0.0003   |
|    n_updates       | 288099   |
---------------------------------
=== Iterazione IRL 206 ===
Loss reward (iter 206): 3.757927894592285
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 155      |
|    critic_loss     | 73.5     |
|    ent_coef        | 0.131    |
|    ent_coef_loss   | -0.247   |
|    learning_rate   | 0.0003   |
|    n_updates       | 288699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 136      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 158      |
|    critic_loss     | 77.3     |
|    ent_coef        | 0.129    |
|    ent_coef_loss   | 0.394    |
|    learning_rate   | 0.0003   |
|    n_updates       | 289099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 156      |
|    critic_loss     | 72.3     |
|    ent_coef        | 0.134    |
|    ent_coef_loss   | 0.149    |
|    learning_rate   | 0.0003   |
|    n_updates       | 289499   |
---------------------------------
=== Iterazione IRL 207 ===
Loss reward (iter 207): 4.291468143463135
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 157      |
|    critic_loss     | 62.8     |
|    ent_coef        | 0.138    |
|    ent_coef_loss   | 0.254    |
|    learning_rate   | 0.0003   |
|    n_updates       | 290099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 156      |
|    critic_loss     | 70       |
|    ent_coef        | 0.141    |
|    ent_coef_loss   | -0.752   |
|    learning_rate   | 0.0003   |
|    n_updates       | 290499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 164      |
|    critic_loss     | 71.6     |
|    ent_coef        | 0.14     |
|    ent_coef_loss   | 0.299    |
|    learning_rate   | 0.0003   |
|    n_updates       | 290899   |
---------------------------------
=== Iterazione IRL 208 ===
Loss reward (iter 208): 4.375865936279297
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 162      |
|    critic_loss     | 67.5     |
|    ent_coef        | 0.145    |
|    ent_coef_loss   | 0.376    |
|    learning_rate   | 0.0003   |
|    n_updates       | 291499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 167      |
|    critic_loss     | 80.3     |
|    ent_coef        | 0.143    |
|    ent_coef_loss   | -0.103   |
|    learning_rate   | 0.0003   |
|    n_updates       | 291899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 166      |
|    critic_loss     | 84.3     |
|    ent_coef        | 0.145    |
|    ent_coef_loss   | -0.623   |
|    learning_rate   | 0.0003   |
|    n_updates       | 292299   |
---------------------------------
=== Iterazione IRL 209 ===
Loss reward (iter 209): 3.9243524074554443
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 164      |
|    critic_loss     | 86.5     |
|    ent_coef        | 0.153    |
|    ent_coef_loss   | 0.4      |
|    learning_rate   | 0.0003   |
|    n_updates       | 292899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 165      |
|    critic_loss     | 72.8     |
|    ent_coef        | 0.145    |
|    ent_coef_loss   | -0.847   |
|    learning_rate   | 0.0003   |
|    n_updates       | 293299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 168      |
|    critic_loss     | 81.9     |
|    ent_coef        | 0.145    |
|    ent_coef_loss   | -0.43    |
|    learning_rate   | 0.0003   |
|    n_updates       | 293699   |
---------------------------------
=== Iterazione IRL 210 ===
Loss reward (iter 210): 3.6987452507019043
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 172      |
|    critic_loss     | 67.9     |
|    ent_coef        | 0.149    |
|    ent_coef_loss   | 0.233    |
|    learning_rate   | 0.0003   |
|    n_updates       | 294299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 172      |
|    critic_loss     | 81.9     |
|    ent_coef        | 0.149    |
|    ent_coef_loss   | 1.05     |
|    learning_rate   | 0.0003   |
|    n_updates       | 294699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 173      |
|    critic_loss     | 78.3     |
|    ent_coef        | 0.148    |
|    ent_coef_loss   | 0.295    |
|    learning_rate   | 0.0003   |
|    n_updates       | 295099   |
---------------------------------
=== Iterazione IRL 211 ===
Loss reward (iter 211): 3.6468400955200195
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 176      |
|    critic_loss     | 72.1     |
|    ent_coef        | 0.147    |
|    ent_coef_loss   | -0.0249  |
|    learning_rate   | 0.0003   |
|    n_updates       | 295699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 174      |
|    critic_loss     | 88.2     |
|    ent_coef        | 0.148    |
|    ent_coef_loss   | 0.641    |
|    learning_rate   | 0.0003   |
|    n_updates       | 296099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 174      |
|    critic_loss     | 81       |
|    ent_coef        | 0.145    |
|    ent_coef_loss   | -0.49    |
|    learning_rate   | 0.0003   |
|    n_updates       | 296499   |
---------------------------------
=== Iterazione IRL 212 ===
Loss reward (iter 212): 3.900969982147217
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 178      |
|    critic_loss     | 67.8     |
|    ent_coef        | 0.145    |
|    ent_coef_loss   | -0.128   |
|    learning_rate   | 0.0003   |
|    n_updates       | 297099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 184      |
|    critic_loss     | 85.3     |
|    ent_coef        | 0.143    |
|    ent_coef_loss   | -0.0598  |
|    learning_rate   | 0.0003   |
|    n_updates       | 297499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 179      |
|    critic_loss     | 73       |
|    ent_coef        | 0.141    |
|    ent_coef_loss   | 0.62     |
|    learning_rate   | 0.0003   |
|    n_updates       | 297899   |
---------------------------------
=== Iterazione IRL 213 ===
Loss reward (iter 213): 3.926778793334961
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 181      |
|    critic_loss     | 77.3     |
|    ent_coef        | 0.14     |
|    ent_coef_loss   | 0.346    |
|    learning_rate   | 0.0003   |
|    n_updates       | 298499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 186      |
|    critic_loss     | 72.3     |
|    ent_coef        | 0.142    |
|    ent_coef_loss   | -0.543   |
|    learning_rate   | 0.0003   |
|    n_updates       | 298899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 182      |
|    critic_loss     | 78.4     |
|    ent_coef        | 0.144    |
|    ent_coef_loss   | 0.497    |
|    learning_rate   | 0.0003   |
|    n_updates       | 299299   |
---------------------------------
=== Iterazione IRL 214 ===
Loss reward (iter 214): 3.9528560638427734
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 184      |
|    critic_loss     | 73.6     |
|    ent_coef        | 0.137    |
|    ent_coef_loss   | 0.851    |
|    learning_rate   | 0.0003   |
|    n_updates       | 299899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 189      |
|    critic_loss     | 88.7     |
|    ent_coef        | 0.14     |
|    ent_coef_loss   | -0.748   |
|    learning_rate   | 0.0003   |
|    n_updates       | 300299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 190      |
|    critic_loss     | 79.6     |
|    ent_coef        | 0.136    |
|    ent_coef_loss   | -0.699   |
|    learning_rate   | 0.0003   |
|    n_updates       | 300699   |
---------------------------------
=== Iterazione IRL 215 ===
Loss reward (iter 215): 2.7477047443389893
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 193      |
|    critic_loss     | 94       |
|    ent_coef        | 0.139    |
|    ent_coef_loss   | -0.442   |
|    learning_rate   | 0.0003   |
|    n_updates       | 301299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 190      |
|    critic_loss     | 80.1     |
|    ent_coef        | 0.134    |
|    ent_coef_loss   | 0.234    |
|    learning_rate   | 0.0003   |
|    n_updates       | 301699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 195      |
|    critic_loss     | 98.3     |
|    ent_coef        | 0.129    |
|    ent_coef_loss   | 0.196    |
|    learning_rate   | 0.0003   |
|    n_updates       | 302099   |
---------------------------------
=== Iterazione IRL 216 ===
Loss reward (iter 216): 3.3786914348602295
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 197      |
|    critic_loss     | 86.8     |
|    ent_coef        | 0.133    |
|    ent_coef_loss   | -0.254   |
|    learning_rate   | 0.0003   |
|    n_updates       | 302699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 200      |
|    critic_loss     | 105      |
|    ent_coef        | 0.131    |
|    ent_coef_loss   | 0.231    |
|    learning_rate   | 0.0003   |
|    n_updates       | 303099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 194      |
|    critic_loss     | 80.2     |
|    ent_coef        | 0.131    |
|    ent_coef_loss   | -0.529   |
|    learning_rate   | 0.0003   |
|    n_updates       | 303499   |
---------------------------------
=== Iterazione IRL 217 ===
Loss reward (iter 217): 3.413721799850464
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 202      |
|    critic_loss     | 105      |
|    ent_coef        | 0.128    |
|    ent_coef_loss   | -0.378   |
|    learning_rate   | 0.0003   |
|    n_updates       | 304099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 207      |
|    critic_loss     | 75.9     |
|    ent_coef        | 0.125    |
|    ent_coef_loss   | 0.35     |
|    learning_rate   | 0.0003   |
|    n_updates       | 304499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 202      |
|    critic_loss     | 104      |
|    ent_coef        | 0.126    |
|    ent_coef_loss   | -0.186   |
|    learning_rate   | 0.0003   |
|    n_updates       | 304899   |
---------------------------------
=== Iterazione IRL 218 ===
Loss reward (iter 218): 4.544299125671387
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 199      |
|    critic_loss     | 83.7     |
|    ent_coef        | 0.133    |
|    ent_coef_loss   | -0.11    |
|    learning_rate   | 0.0003   |
|    n_updates       | 305499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 206      |
|    critic_loss     | 77.4     |
|    ent_coef        | 0.135    |
|    ent_coef_loss   | -0.253   |
|    learning_rate   | 0.0003   |
|    n_updates       | 305899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 204      |
|    critic_loss     | 109      |
|    ent_coef        | 0.127    |
|    ent_coef_loss   | -0.258   |
|    learning_rate   | 0.0003   |
|    n_updates       | 306299   |
---------------------------------
=== Iterazione IRL 219 ===
Loss reward (iter 219): 4.971415996551514
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 213      |
|    critic_loss     | 102      |
|    ent_coef        | 0.123    |
|    ent_coef_loss   | 0.0893   |
|    learning_rate   | 0.0003   |
|    n_updates       | 306899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 216      |
|    critic_loss     | 109      |
|    ent_coef        | 0.123    |
|    ent_coef_loss   | 0.767    |
|    learning_rate   | 0.0003   |
|    n_updates       | 307299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 209      |
|    critic_loss     | 113      |
|    ent_coef        | 0.129    |
|    ent_coef_loss   | 0.268    |
|    learning_rate   | 0.0003   |
|    n_updates       | 307699   |
---------------------------------
=== Iterazione IRL 220 ===
Loss reward (iter 220): 4.254390239715576
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 219      |
|    critic_loss     | 106      |
|    ent_coef        | 0.132    |
|    ent_coef_loss   | -0.171   |
|    learning_rate   | 0.0003   |
|    n_updates       | 308299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 213      |
|    critic_loss     | 91       |
|    ent_coef        | 0.134    |
|    ent_coef_loss   | 0.665    |
|    learning_rate   | 0.0003   |
|    n_updates       | 308699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 216      |
|    critic_loss     | 96.5     |
|    ent_coef        | 0.141    |
|    ent_coef_loss   | 0.983    |
|    learning_rate   | 0.0003   |
|    n_updates       | 309099   |
---------------------------------
=== Iterazione IRL 221 ===
Loss reward (iter 221): 5.236379623413086
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 221      |
|    critic_loss     | 145      |
|    ent_coef        | 0.137    |
|    ent_coef_loss   | -0.491   |
|    learning_rate   | 0.0003   |
|    n_updates       | 309699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 218      |
|    critic_loss     | 110      |
|    ent_coef        | 0.139    |
|    ent_coef_loss   | 0.618    |
|    learning_rate   | 0.0003   |
|    n_updates       | 310099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 223      |
|    critic_loss     | 106      |
|    ent_coef        | 0.14     |
|    ent_coef_loss   | 0.578    |
|    learning_rate   | 0.0003   |
|    n_updates       | 310499   |
---------------------------------
=== Iterazione IRL 222 ===
Loss reward (iter 222): 5.101088523864746
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 220      |
|    critic_loss     | 114      |
|    ent_coef        | 0.146    |
|    ent_coef_loss   | -0.302   |
|    learning_rate   | 0.0003   |
|    n_updates       | 311099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 226      |
|    critic_loss     | 114      |
|    ent_coef        | 0.14     |
|    ent_coef_loss   | -0.0105  |
|    learning_rate   | 0.0003   |
|    n_updates       | 311499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 223      |
|    critic_loss     | 97.1     |
|    ent_coef        | 0.14     |
|    ent_coef_loss   | -0.514   |
|    learning_rate   | 0.0003   |
|    n_updates       | 311899   |
---------------------------------
=== Iterazione IRL 223 ===
Loss reward (iter 223): 5.061643600463867
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 231      |
|    critic_loss     | 95       |
|    ent_coef        | 0.139    |
|    ent_coef_loss   | -0.183   |
|    learning_rate   | 0.0003   |
|    n_updates       | 312499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 235      |
|    critic_loss     | 108      |
|    ent_coef        | 0.142    |
|    ent_coef_loss   | -0.0403  |
|    learning_rate   | 0.0003   |
|    n_updates       | 312899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 229      |
|    critic_loss     | 89.3     |
|    ent_coef        | 0.146    |
|    ent_coef_loss   | -0.635   |
|    learning_rate   | 0.0003   |
|    n_updates       | 313299   |
---------------------------------
=== Iterazione IRL 224 ===
Loss reward (iter 224): 5.142442226409912
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 234      |
|    critic_loss     | 91.8     |
|    ent_coef        | 0.144    |
|    ent_coef_loss   | -0.658   |
|    learning_rate   | 0.0003   |
|    n_updates       | 313899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 230      |
|    critic_loss     | 116      |
|    ent_coef        | 0.148    |
|    ent_coef_loss   | 0.134    |
|    learning_rate   | 0.0003   |
|    n_updates       | 314299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 240      |
|    critic_loss     | 131      |
|    ent_coef        | 0.15     |
|    ent_coef_loss   | 0.759    |
|    learning_rate   | 0.0003   |
|    n_updates       | 314699   |
---------------------------------
=== Iterazione IRL 225 ===
Loss reward (iter 225): 4.515864372253418
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 244      |
|    critic_loss     | 90.8     |
|    ent_coef        | 0.149    |
|    ent_coef_loss   | 0.188    |
|    learning_rate   | 0.0003   |
|    n_updates       | 315299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 232      |
|    critic_loss     | 91.4     |
|    ent_coef        | 0.153    |
|    ent_coef_loss   | -0.277   |
|    learning_rate   | 0.0003   |
|    n_updates       | 315699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 235      |
|    critic_loss     | 90.6     |
|    ent_coef        | 0.152    |
|    ent_coef_loss   | -0.175   |
|    learning_rate   | 0.0003   |
|    n_updates       | 316099   |
---------------------------------
=== Iterazione IRL 226 ===
Loss reward (iter 226): 4.989248752593994
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 242      |
|    critic_loss     | 115      |
|    ent_coef        | 0.159    |
|    ent_coef_loss   | -0.276   |
|    learning_rate   | 0.0003   |
|    n_updates       | 316699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 239      |
|    critic_loss     | 135      |
|    ent_coef        | 0.157    |
|    ent_coef_loss   | -0.00743 |
|    learning_rate   | 0.0003   |
|    n_updates       | 317099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 245      |
|    critic_loss     | 127      |
|    ent_coef        | 0.158    |
|    ent_coef_loss   | -0.504   |
|    learning_rate   | 0.0003   |
|    n_updates       | 317499   |
---------------------------------
=== Iterazione IRL 227 ===
Loss reward (iter 227): 3.1043851375579834
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 243      |
|    critic_loss     | 104      |
|    ent_coef        | 0.151    |
|    ent_coef_loss   | -0.0491  |
|    learning_rate   | 0.0003   |
|    n_updates       | 318099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 244      |
|    critic_loss     | 122      |
|    ent_coef        | 0.154    |
|    ent_coef_loss   | 0.731    |
|    learning_rate   | 0.0003   |
|    n_updates       | 318499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 240      |
|    critic_loss     | 94.5     |
|    ent_coef        | 0.155    |
|    ent_coef_loss   | -0.468   |
|    learning_rate   | 0.0003   |
|    n_updates       | 318899   |
---------------------------------
=== Iterazione IRL 228 ===
Loss reward (iter 228): 4.640478610992432
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 248      |
|    critic_loss     | 102      |
|    ent_coef        | 0.151    |
|    ent_coef_loss   | -0.288   |
|    learning_rate   | 0.0003   |
|    n_updates       | 319499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 246      |
|    critic_loss     | 88.3     |
|    ent_coef        | 0.15     |
|    ent_coef_loss   | 0.0951   |
|    learning_rate   | 0.0003   |
|    n_updates       | 319899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 249      |
|    critic_loss     | 106      |
|    ent_coef        | 0.149    |
|    ent_coef_loss   | -0.088   |
|    learning_rate   | 0.0003   |
|    n_updates       | 320299   |
---------------------------------
=== Iterazione IRL 229 ===
Loss reward (iter 229): 4.831289768218994
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 249      |
|    critic_loss     | 83.1     |
|    ent_coef        | 0.162    |
|    ent_coef_loss   | -0.464   |
|    learning_rate   | 0.0003   |
|    n_updates       | 320899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 251      |
|    critic_loss     | 104      |
|    ent_coef        | 0.16     |
|    ent_coef_loss   | -0.711   |
|    learning_rate   | 0.0003   |
|    n_updates       | 321299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 256      |
|    critic_loss     | 115      |
|    ent_coef        | 0.158    |
|    ent_coef_loss   | -0.522   |
|    learning_rate   | 0.0003   |
|    n_updates       | 321699   |
---------------------------------
=== Iterazione IRL 230 ===
Loss reward (iter 230): 3.887577533721924
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 257      |
|    critic_loss     | 96.3     |
|    ent_coef        | 0.152    |
|    ent_coef_loss   | 0.143    |
|    learning_rate   | 0.0003   |
|    n_updates       | 322299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 260      |
|    critic_loss     | 110      |
|    ent_coef        | 0.154    |
|    ent_coef_loss   | -0.547   |
|    learning_rate   | 0.0003   |
|    n_updates       | 322699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 255      |
|    critic_loss     | 106      |
|    ent_coef        | 0.148    |
|    ent_coef_loss   | -0.0117  |
|    learning_rate   | 0.0003   |
|    n_updates       | 323099   |
---------------------------------
=== Iterazione IRL 231 ===
Loss reward (iter 231): 3.238762617111206
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 256      |
|    critic_loss     | 103      |
|    ent_coef        | 0.153    |
|    ent_coef_loss   | -0.299   |
|    learning_rate   | 0.0003   |
|    n_updates       | 323699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 254      |
|    critic_loss     | 95.6     |
|    ent_coef        | 0.156    |
|    ent_coef_loss   | -0.393   |
|    learning_rate   | 0.0003   |
|    n_updates       | 324099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 263      |
|    critic_loss     | 108      |
|    ent_coef        | 0.159    |
|    ent_coef_loss   | -0.102   |
|    learning_rate   | 0.0003   |
|    n_updates       | 324499   |
---------------------------------
=== Iterazione IRL 232 ===
Loss reward (iter 232): 4.640512466430664
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 261      |
|    critic_loss     | 79.1     |
|    ent_coef        | 0.159    |
|    ent_coef_loss   | 0.207    |
|    learning_rate   | 0.0003   |
|    n_updates       | 325099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 259      |
|    critic_loss     | 98.1     |
|    ent_coef        | 0.159    |
|    ent_coef_loss   | -0.312   |
|    learning_rate   | 0.0003   |
|    n_updates       | 325499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 266      |
|    critic_loss     | 113      |
|    ent_coef        | 0.156    |
|    ent_coef_loss   | -0.183   |
|    learning_rate   | 0.0003   |
|    n_updates       | 325899   |
---------------------------------
=== Iterazione IRL 233 ===
Loss reward (iter 233): 3.336240768432617
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 265      |
|    critic_loss     | 107      |
|    ent_coef        | 0.154    |
|    ent_coef_loss   | 0.52     |
|    learning_rate   | 0.0003   |
|    n_updates       | 326499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 268      |
|    critic_loss     | 136      |
|    ent_coef        | 0.157    |
|    ent_coef_loss   | -0.102   |
|    learning_rate   | 0.0003   |
|    n_updates       | 326899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 266      |
|    critic_loss     | 101      |
|    ent_coef        | 0.158    |
|    ent_coef_loss   | 0.698    |
|    learning_rate   | 0.0003   |
|    n_updates       | 327299   |
---------------------------------
=== Iterazione IRL 234 ===
Loss reward (iter 234): 3.554443597793579
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 269      |
|    critic_loss     | 117      |
|    ent_coef        | 0.159    |
|    ent_coef_loss   | -0.456   |
|    learning_rate   | 0.0003   |
|    n_updates       | 327899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 270      |
|    critic_loss     | 113      |
|    ent_coef        | 0.156    |
|    ent_coef_loss   | -0.555   |
|    learning_rate   | 0.0003   |
|    n_updates       | 328299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 273      |
|    critic_loss     | 113      |
|    ent_coef        | 0.157    |
|    ent_coef_loss   | 0.271    |
|    learning_rate   | 0.0003   |
|    n_updates       | 328699   |
---------------------------------
=== Iterazione IRL 235 ===
Loss reward (iter 235): 4.913506031036377
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 272      |
|    critic_loss     | 125      |
|    ent_coef        | 0.152    |
|    ent_coef_loss   | -0.118   |
|    learning_rate   | 0.0003   |
|    n_updates       | 329299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 282      |
|    critic_loss     | 138      |
|    ent_coef        | 0.149    |
|    ent_coef_loss   | -0.396   |
|    learning_rate   | 0.0003   |
|    n_updates       | 329699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 276      |
|    critic_loss     | 123      |
|    ent_coef        | 0.152    |
|    ent_coef_loss   | -0.376   |
|    learning_rate   | 0.0003   |
|    n_updates       | 330099   |
---------------------------------
=== Iterazione IRL 236 ===
Loss reward (iter 236): 4.130808353424072
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 277      |
|    critic_loss     | 140      |
|    ent_coef        | 0.161    |
|    ent_coef_loss   | -0.201   |
|    learning_rate   | 0.0003   |
|    n_updates       | 330699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 276      |
|    critic_loss     | 133      |
|    ent_coef        | 0.165    |
|    ent_coef_loss   | 0.0138   |
|    learning_rate   | 0.0003   |
|    n_updates       | 331099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 275      |
|    critic_loss     | 102      |
|    ent_coef        | 0.161    |
|    ent_coef_loss   | -0.304   |
|    learning_rate   | 0.0003   |
|    n_updates       | 331499   |
---------------------------------
=== Iterazione IRL 237 ===
Loss reward (iter 237): 4.232487201690674
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 276      |
|    critic_loss     | 117      |
|    ent_coef        | 0.157    |
|    ent_coef_loss   | 0.692    |
|    learning_rate   | 0.0003   |
|    n_updates       | 332099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 279      |
|    critic_loss     | 125      |
|    ent_coef        | 0.159    |
|    ent_coef_loss   | 0.371    |
|    learning_rate   | 0.0003   |
|    n_updates       | 332499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 282      |
|    critic_loss     | 127      |
|    ent_coef        | 0.158    |
|    ent_coef_loss   | -0.334   |
|    learning_rate   | 0.0003   |
|    n_updates       | 332899   |
---------------------------------
=== Iterazione IRL 238 ===
Loss reward (iter 238): 2.8754706382751465
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 279      |
|    critic_loss     | 108      |
|    ent_coef        | 0.161    |
|    ent_coef_loss   | -0.16    |
|    learning_rate   | 0.0003   |
|    n_updates       | 333499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 289      |
|    critic_loss     | 117      |
|    ent_coef        | 0.164    |
|    ent_coef_loss   | -0.146   |
|    learning_rate   | 0.0003   |
|    n_updates       | 333899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 291      |
|    critic_loss     | 128      |
|    ent_coef        | 0.161    |
|    ent_coef_loss   | 0.127    |
|    learning_rate   | 0.0003   |
|    n_updates       | 334299   |
---------------------------------
=== Iterazione IRL 239 ===
Loss reward (iter 239): 4.26054573059082
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 287      |
|    critic_loss     | 115      |
|    ent_coef        | 0.166    |
|    ent_coef_loss   | -0.735   |
|    learning_rate   | 0.0003   |
|    n_updates       | 334899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 299      |
|    critic_loss     | 146      |
|    ent_coef        | 0.159    |
|    ent_coef_loss   | 0.783    |
|    learning_rate   | 0.0003   |
|    n_updates       | 335299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 287      |
|    critic_loss     | 122      |
|    ent_coef        | 0.161    |
|    ent_coef_loss   | -0.221   |
|    learning_rate   | 0.0003   |
|    n_updates       | 335699   |
---------------------------------
=== Iterazione IRL 240 ===
Loss reward (iter 240): 3.2910008430480957
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 294      |
|    critic_loss     | 121      |
|    ent_coef        | 0.163    |
|    ent_coef_loss   | 0.109    |
|    learning_rate   | 0.0003   |
|    n_updates       | 336299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 286      |
|    critic_loss     | 143      |
|    ent_coef        | 0.158    |
|    ent_coef_loss   | -0.143   |
|    learning_rate   | 0.0003   |
|    n_updates       | 336699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 291      |
|    critic_loss     | 140      |
|    ent_coef        | 0.152    |
|    ent_coef_loss   | -0.0664  |
|    learning_rate   | 0.0003   |
|    n_updates       | 337099   |
---------------------------------
=== Iterazione IRL 241 ===
Loss reward (iter 241): 4.574850082397461
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 305      |
|    critic_loss     | 127      |
|    ent_coef        | 0.157    |
|    ent_coef_loss   | 0.101    |
|    learning_rate   | 0.0003   |
|    n_updates       | 337699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 297      |
|    critic_loss     | 130      |
|    ent_coef        | 0.157    |
|    ent_coef_loss   | 0.156    |
|    learning_rate   | 0.0003   |
|    n_updates       | 338099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 299      |
|    critic_loss     | 103      |
|    ent_coef        | 0.16     |
|    ent_coef_loss   | -0.433   |
|    learning_rate   | 0.0003   |
|    n_updates       | 338499   |
---------------------------------
=== Iterazione IRL 242 ===
Loss reward (iter 242): 4.089075088500977
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 306      |
|    critic_loss     | 118      |
|    ent_coef        | 0.157    |
|    ent_coef_loss   | 0.0975   |
|    learning_rate   | 0.0003   |
|    n_updates       | 339099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 309      |
|    critic_loss     | 112      |
|    ent_coef        | 0.154    |
|    ent_coef_loss   | 0.495    |
|    learning_rate   | 0.0003   |
|    n_updates       | 339499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 305      |
|    critic_loss     | 141      |
|    ent_coef        | 0.151    |
|    ent_coef_loss   | -0.178   |
|    learning_rate   | 0.0003   |
|    n_updates       | 339899   |
---------------------------------
=== Iterazione IRL 243 ===
Loss reward (iter 243): 3.563201904296875
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 300      |
|    critic_loss     | 113      |
|    ent_coef        | 0.152    |
|    ent_coef_loss   | 0.133    |
|    learning_rate   | 0.0003   |
|    n_updates       | 340499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 309      |
|    critic_loss     | 133      |
|    ent_coef        | 0.153    |
|    ent_coef_loss   | 0.424    |
|    learning_rate   | 0.0003   |
|    n_updates       | 340899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 305      |
|    critic_loss     | 130      |
|    ent_coef        | 0.158    |
|    ent_coef_loss   | -0.0605  |
|    learning_rate   | 0.0003   |
|    n_updates       | 341299   |
---------------------------------
=== Iterazione IRL 244 ===
Loss reward (iter 244): 4.405409812927246
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 311      |
|    critic_loss     | 156      |
|    ent_coef        | 0.155    |
|    ent_coef_loss   | -0.0983  |
|    learning_rate   | 0.0003   |
|    n_updates       | 341899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 314      |
|    critic_loss     | 148      |
|    ent_coef        | 0.157    |
|    ent_coef_loss   | 0.198    |
|    learning_rate   | 0.0003   |
|    n_updates       | 342299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 313      |
|    critic_loss     | 152      |
|    ent_coef        | 0.153    |
|    ent_coef_loss   | 0.242    |
|    learning_rate   | 0.0003   |
|    n_updates       | 342699   |
---------------------------------
=== Iterazione IRL 245 ===
Loss reward (iter 245): 4.020366191864014
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 313      |
|    critic_loss     | 148      |
|    ent_coef        | 0.158    |
|    ent_coef_loss   | 0.856    |
|    learning_rate   | 0.0003   |
|    n_updates       | 343299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 313      |
|    critic_loss     | 148      |
|    ent_coef        | 0.158    |
|    ent_coef_loss   | 0.0504   |
|    learning_rate   | 0.0003   |
|    n_updates       | 343699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 311      |
|    critic_loss     | 146      |
|    ent_coef        | 0.15     |
|    ent_coef_loss   | 0.139    |
|    learning_rate   | 0.0003   |
|    n_updates       | 344099   |
---------------------------------
=== Iterazione IRL 246 ===
Loss reward (iter 246): 4.801732540130615
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 318      |
|    critic_loss     | 144      |
|    ent_coef        | 0.149    |
|    ent_coef_loss   | -0.409   |
|    learning_rate   | 0.0003   |
|    n_updates       | 344699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 318      |
|    critic_loss     | 128      |
|    ent_coef        | 0.155    |
|    ent_coef_loss   | 0.176    |
|    learning_rate   | 0.0003   |
|    n_updates       | 345099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 319      |
|    critic_loss     | 115      |
|    ent_coef        | 0.157    |
|    ent_coef_loss   | 1.09     |
|    learning_rate   | 0.0003   |
|    n_updates       | 345499   |
---------------------------------
=== Iterazione IRL 247 ===
Loss reward (iter 247): 4.486748695373535
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 325      |
|    critic_loss     | 128      |
|    ent_coef        | 0.153    |
|    ent_coef_loss   | 0.164    |
|    learning_rate   | 0.0003   |
|    n_updates       | 346099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 323      |
|    critic_loss     | 154      |
|    ent_coef        | 0.162    |
|    ent_coef_loss   | -0.341   |
|    learning_rate   | 0.0003   |
|    n_updates       | 346499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 320      |
|    critic_loss     | 136      |
|    ent_coef        | 0.164    |
|    ent_coef_loss   | 0.524    |
|    learning_rate   | 0.0003   |
|    n_updates       | 346899   |
---------------------------------
=== Iterazione IRL 248 ===
Loss reward (iter 248): 4.669652938842773
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 322      |
|    critic_loss     | 191      |
|    ent_coef        | 0.16     |
|    ent_coef_loss   | -0.226   |
|    learning_rate   | 0.0003   |
|    n_updates       | 347499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 322      |
|    critic_loss     | 119      |
|    ent_coef        | 0.16     |
|    ent_coef_loss   | 0.254    |
|    learning_rate   | 0.0003   |
|    n_updates       | 347899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 327      |
|    critic_loss     | 144      |
|    ent_coef        | 0.167    |
|    ent_coef_loss   | 0.411    |
|    learning_rate   | 0.0003   |
|    n_updates       | 348299   |
---------------------------------
=== Iterazione IRL 249 ===
Loss reward (iter 249): 4.137938022613525
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 327      |
|    critic_loss     | 147      |
|    ent_coef        | 0.162    |
|    ent_coef_loss   | -0.141   |
|    learning_rate   | 0.0003   |
|    n_updates       | 348899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 329      |
|    critic_loss     | 128      |
|    ent_coef        | 0.171    |
|    ent_coef_loss   | 0.0836   |
|    learning_rate   | 0.0003   |
|    n_updates       | 349299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 324      |
|    critic_loss     | 130      |
|    ent_coef        | 0.17     |
|    ent_coef_loss   | 0.0184   |
|    learning_rate   | 0.0003   |
|    n_updates       | 349699   |
---------------------------------
=== Iterazione IRL 250 ===
Loss reward (iter 250): 4.468989372253418
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 334      |
|    critic_loss     | 127      |
|    ent_coef        | 0.167    |
|    ent_coef_loss   | -0.297   |
|    learning_rate   | 0.0003   |
|    n_updates       | 350299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 334      |
|    critic_loss     | 163      |
|    ent_coef        | 0.165    |
|    ent_coef_loss   | -0.329   |
|    learning_rate   | 0.0003   |
|    n_updates       | 350699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 332      |
|    critic_loss     | 150      |
|    ent_coef        | 0.162    |
|    ent_coef_loss   | -0.359   |
|    learning_rate   | 0.0003   |
|    n_updates       | 351099   |
---------------------------------
=== Iterazione IRL 251 ===
Loss reward (iter 251): 5.225739479064941
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 334      |
|    critic_loss     | 144      |
|    ent_coef        | 0.163    |
|    ent_coef_loss   | 0.0937   |
|    learning_rate   | 0.0003   |
|    n_updates       | 351699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 338      |
|    critic_loss     | 145      |
|    ent_coef        | 0.164    |
|    ent_coef_loss   | 0.187    |
|    learning_rate   | 0.0003   |
|    n_updates       | 352099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 339      |
|    critic_loss     | 163      |
|    ent_coef        | 0.158    |
|    ent_coef_loss   | -0.497   |
|    learning_rate   | 0.0003   |
|    n_updates       | 352499   |
---------------------------------
=== Iterazione IRL 252 ===
Loss reward (iter 252): 4.603240966796875
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 339      |
|    critic_loss     | 126      |
|    ent_coef        | 0.149    |
|    ent_coef_loss   | -0.388   |
|    learning_rate   | 0.0003   |
|    n_updates       | 353099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 334      |
|    critic_loss     | 153      |
|    ent_coef        | 0.15     |
|    ent_coef_loss   | 1.14     |
|    learning_rate   | 0.0003   |
|    n_updates       | 353499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 337      |
|    critic_loss     | 139      |
|    ent_coef        | 0.159    |
|    ent_coef_loss   | 0.399    |
|    learning_rate   | 0.0003   |
|    n_updates       | 353899   |
---------------------------------
=== Iterazione IRL 253 ===
Loss reward (iter 253): 4.462172508239746
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 335      |
|    critic_loss     | 118      |
|    ent_coef        | 0.159    |
|    ent_coef_loss   | -0.451   |
|    learning_rate   | 0.0003   |
|    n_updates       | 354499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 339      |
|    critic_loss     | 161      |
|    ent_coef        | 0.163    |
|    ent_coef_loss   | -0.806   |
|    learning_rate   | 0.0003   |
|    n_updates       | 354899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 347      |
|    critic_loss     | 151      |
|    ent_coef        | 0.164    |
|    ent_coef_loss   | -0.191   |
|    learning_rate   | 0.0003   |
|    n_updates       | 355299   |
---------------------------------
=== Iterazione IRL 254 ===
Loss reward (iter 254): 4.557864189147949
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 341      |
|    critic_loss     | 154      |
|    ent_coef        | 0.154    |
|    ent_coef_loss   | 0.229    |
|    learning_rate   | 0.0003   |
|    n_updates       | 355899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 347      |
|    critic_loss     | 138      |
|    ent_coef        | 0.145    |
|    ent_coef_loss   | 0.0488   |
|    learning_rate   | 0.0003   |
|    n_updates       | 356299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 347      |
|    critic_loss     | 162      |
|    ent_coef        | 0.141    |
|    ent_coef_loss   | -0.415   |
|    learning_rate   | 0.0003   |
|    n_updates       | 356699   |
---------------------------------
=== Iterazione IRL 255 ===
Loss reward (iter 255): 4.780691146850586
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 344      |
|    critic_loss     | 123      |
|    ent_coef        | 0.135    |
|    ent_coef_loss   | -0.201   |
|    learning_rate   | 0.0003   |
|    n_updates       | 357299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 350      |
|    critic_loss     | 171      |
|    ent_coef        | 0.141    |
|    ent_coef_loss   | -0.00939 |
|    learning_rate   | 0.0003   |
|    n_updates       | 357699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 346      |
|    critic_loss     | 155      |
|    ent_coef        | 0.142    |
|    ent_coef_loss   | -0.194   |
|    learning_rate   | 0.0003   |
|    n_updates       | 358099   |
---------------------------------
=== Iterazione IRL 256 ===
Loss reward (iter 256): 4.59188985824585
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 351      |
|    critic_loss     | 154      |
|    ent_coef        | 0.146    |
|    ent_coef_loss   | -0.517   |
|    learning_rate   | 0.0003   |
|    n_updates       | 358699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 354      |
|    critic_loss     | 162      |
|    ent_coef        | 0.145    |
|    ent_coef_loss   | 0.722    |
|    learning_rate   | 0.0003   |
|    n_updates       | 359099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 352      |
|    critic_loss     | 155      |
|    ent_coef        | 0.151    |
|    ent_coef_loss   | 0.74     |
|    learning_rate   | 0.0003   |
|    n_updates       | 359499   |
---------------------------------
=== Iterazione IRL 257 ===
Loss reward (iter 257): 4.322219371795654
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 353      |
|    critic_loss     | 139      |
|    ent_coef        | 0.147    |
|    ent_coef_loss   | 0.252    |
|    learning_rate   | 0.0003   |
|    n_updates       | 360099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 353      |
|    critic_loss     | 157      |
|    ent_coef        | 0.154    |
|    ent_coef_loss   | 0.33     |
|    learning_rate   | 0.0003   |
|    n_updates       | 360499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 357      |
|    critic_loss     | 169      |
|    ent_coef        | 0.164    |
|    ent_coef_loss   | -0.122   |
|    learning_rate   | 0.0003   |
|    n_updates       | 360899   |
---------------------------------
=== Iterazione IRL 258 ===
Loss reward (iter 258): 3.1434078216552734
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 353      |
|    critic_loss     | 130      |
|    ent_coef        | 0.16     |
|    ent_coef_loss   | -0.232   |
|    learning_rate   | 0.0003   |
|    n_updates       | 361499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 363      |
|    critic_loss     | 147      |
|    ent_coef        | 0.165    |
|    ent_coef_loss   | 0.448    |
|    learning_rate   | 0.0003   |
|    n_updates       | 361899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 358      |
|    critic_loss     | 173      |
|    ent_coef        | 0.167    |
|    ent_coef_loss   | 0.91     |
|    learning_rate   | 0.0003   |
|    n_updates       | 362299   |
---------------------------------
=== Iterazione IRL 259 ===
Loss reward (iter 259): 3.4324774742126465
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 359      |
|    critic_loss     | 148      |
|    ent_coef        | 0.163    |
|    ent_coef_loss   | 0.115    |
|    learning_rate   | 0.0003   |
|    n_updates       | 362899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 362      |
|    critic_loss     | 178      |
|    ent_coef        | 0.162    |
|    ent_coef_loss   | 0.131    |
|    learning_rate   | 0.0003   |
|    n_updates       | 363299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 357      |
|    critic_loss     | 139      |
|    ent_coef        | 0.155    |
|    ent_coef_loss   | -0.301   |
|    learning_rate   | 0.0003   |
|    n_updates       | 363699   |
---------------------------------
=== Iterazione IRL 260 ===
Loss reward (iter 260): 4.758149147033691
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 363      |
|    critic_loss     | 169      |
|    ent_coef        | 0.147    |
|    ent_coef_loss   | -0.528   |
|    learning_rate   | 0.0003   |
|    n_updates       | 364299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 366      |
|    critic_loss     | 127      |
|    ent_coef        | 0.145    |
|    ent_coef_loss   | 0.347    |
|    learning_rate   | 0.0003   |
|    n_updates       | 364699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 362      |
|    critic_loss     | 152      |
|    ent_coef        | 0.147    |
|    ent_coef_loss   | -0.0643  |
|    learning_rate   | 0.0003   |
|    n_updates       | 365099   |
---------------------------------
=== Iterazione IRL 261 ===
Loss reward (iter 261): 3.1324501037597656
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 361      |
|    critic_loss     | 125      |
|    ent_coef        | 0.158    |
|    ent_coef_loss   | 0.431    |
|    learning_rate   | 0.0003   |
|    n_updates       | 365699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 376      |
|    critic_loss     | 189      |
|    ent_coef        | 0.169    |
|    ent_coef_loss   | -0.0643  |
|    learning_rate   | 0.0003   |
|    n_updates       | 366099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 356      |
|    critic_loss     | 147      |
|    ent_coef        | 0.175    |
|    ent_coef_loss   | -0.00754 |
|    learning_rate   | 0.0003   |
|    n_updates       | 366499   |
---------------------------------
=== Iterazione IRL 262 ===
Loss reward (iter 262): 4.704324245452881
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 371      |
|    critic_loss     | 161      |
|    ent_coef        | 0.179    |
|    ent_coef_loss   | 0.0534   |
|    learning_rate   | 0.0003   |
|    n_updates       | 367099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 373      |
|    critic_loss     | 145      |
|    ent_coef        | 0.184    |
|    ent_coef_loss   | -0.0853  |
|    learning_rate   | 0.0003   |
|    n_updates       | 367499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 364      |
|    critic_loss     | 133      |
|    ent_coef        | 0.189    |
|    ent_coef_loss   | 0.376    |
|    learning_rate   | 0.0003   |
|    n_updates       | 367899   |
---------------------------------
=== Iterazione IRL 263 ===
Loss reward (iter 263): 4.418033599853516
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 377      |
|    critic_loss     | 197      |
|    ent_coef        | 0.195    |
|    ent_coef_loss   | 0.287    |
|    learning_rate   | 0.0003   |
|    n_updates       | 368499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 377      |
|    critic_loss     | 157      |
|    ent_coef        | 0.186    |
|    ent_coef_loss   | 0.00751  |
|    learning_rate   | 0.0003   |
|    n_updates       | 368899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 380      |
|    critic_loss     | 171      |
|    ent_coef        | 0.185    |
|    ent_coef_loss   | -0.458   |
|    learning_rate   | 0.0003   |
|    n_updates       | 369299   |
---------------------------------
=== Iterazione IRL 264 ===
Loss reward (iter 264): 5.319720268249512
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 369      |
|    critic_loss     | 192      |
|    ent_coef        | 0.175    |
|    ent_coef_loss   | -0.21    |
|    learning_rate   | 0.0003   |
|    n_updates       | 369899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 381      |
|    critic_loss     | 160      |
|    ent_coef        | 0.173    |
|    ent_coef_loss   | 0.0115   |
|    learning_rate   | 0.0003   |
|    n_updates       | 370299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 374      |
|    critic_loss     | 143      |
|    ent_coef        | 0.172    |
|    ent_coef_loss   | -0.275   |
|    learning_rate   | 0.0003   |
|    n_updates       | 370699   |
---------------------------------
=== Iterazione IRL 265 ===
Loss reward (iter 265): 4.173259258270264
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 382      |
|    critic_loss     | 175      |
|    ent_coef        | 0.175    |
|    ent_coef_loss   | 0.0689   |
|    learning_rate   | 0.0003   |
|    n_updates       | 371299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 378      |
|    critic_loss     | 159      |
|    ent_coef        | 0.179    |
|    ent_coef_loss   | -0.253   |
|    learning_rate   | 0.0003   |
|    n_updates       | 371699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 381      |
|    critic_loss     | 178      |
|    ent_coef        | 0.186    |
|    ent_coef_loss   | -0.281   |
|    learning_rate   | 0.0003   |
|    n_updates       | 372099   |
---------------------------------
=== Iterazione IRL 266 ===
Loss reward (iter 266): 4.709696292877197
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 375      |
|    critic_loss     | 136      |
|    ent_coef        | 0.178    |
|    ent_coef_loss   | -0.335   |
|    learning_rate   | 0.0003   |
|    n_updates       | 372699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 375      |
|    critic_loss     | 153      |
|    ent_coef        | 0.176    |
|    ent_coef_loss   | 0.721    |
|    learning_rate   | 0.0003   |
|    n_updates       | 373099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 379      |
|    critic_loss     | 163      |
|    ent_coef        | 0.179    |
|    ent_coef_loss   | -0.0234  |
|    learning_rate   | 0.0003   |
|    n_updates       | 373499   |
---------------------------------
=== Iterazione IRL 267 ===
Loss reward (iter 267): 4.821660995483398
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 382      |
|    critic_loss     | 164      |
|    ent_coef        | 0.185    |
|    ent_coef_loss   | -0.146   |
|    learning_rate   | 0.0003   |
|    n_updates       | 374099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 385      |
|    critic_loss     | 161      |
|    ent_coef        | 0.188    |
|    ent_coef_loss   | -0.56    |
|    learning_rate   | 0.0003   |
|    n_updates       | 374499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 389      |
|    critic_loss     | 162      |
|    ent_coef        | 0.191    |
|    ent_coef_loss   | 0.22     |
|    learning_rate   | 0.0003   |
|    n_updates       | 374899   |
---------------------------------
=== Iterazione IRL 268 ===
Loss reward (iter 268): 5.285508632659912
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 384      |
|    critic_loss     | 215      |
|    ent_coef        | 0.181    |
|    ent_coef_loss   | 0.25     |
|    learning_rate   | 0.0003   |
|    n_updates       | 375499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 385      |
|    critic_loss     | 139      |
|    ent_coef        | 0.178    |
|    ent_coef_loss   | 0.35     |
|    learning_rate   | 0.0003   |
|    n_updates       | 375899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 378      |
|    critic_loss     | 171      |
|    ent_coef        | 0.183    |
|    ent_coef_loss   | 0.285    |
|    learning_rate   | 0.0003   |
|    n_updates       | 376299   |
---------------------------------
=== Iterazione IRL 269 ===
Loss reward (iter 269): 4.742214202880859
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 386      |
|    critic_loss     | 147      |
|    ent_coef        | 0.187    |
|    ent_coef_loss   | 0.00536  |
|    learning_rate   | 0.0003   |
|    n_updates       | 376899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 389      |
|    critic_loss     | 154      |
|    ent_coef        | 0.182    |
|    ent_coef_loss   | 0.0264   |
|    learning_rate   | 0.0003   |
|    n_updates       | 377299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 381      |
|    critic_loss     | 146      |
|    ent_coef        | 0.191    |
|    ent_coef_loss   | 0.251    |
|    learning_rate   | 0.0003   |
|    n_updates       | 377699   |
---------------------------------
=== Iterazione IRL 270 ===
Loss reward (iter 270): 5.0507402420043945
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 392      |
|    critic_loss     | 194      |
|    ent_coef        | 0.187    |
|    ent_coef_loss   | 0.141    |
|    learning_rate   | 0.0003   |
|    n_updates       | 378299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 401      |
|    critic_loss     | 175      |
|    ent_coef        | 0.193    |
|    ent_coef_loss   | -0.0466  |
|    learning_rate   | 0.0003   |
|    n_updates       | 378699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 393      |
|    critic_loss     | 150      |
|    ent_coef        | 0.197    |
|    ent_coef_loss   | -0.146   |
|    learning_rate   | 0.0003   |
|    n_updates       | 379099   |
---------------------------------
=== Iterazione IRL 271 ===
Loss reward (iter 271): 4.839644432067871
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 395      |
|    critic_loss     | 163      |
|    ent_coef        | 0.19     |
|    ent_coef_loss   | -0.295   |
|    learning_rate   | 0.0003   |
|    n_updates       | 379699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 388      |
|    critic_loss     | 146      |
|    ent_coef        | 0.186    |
|    ent_coef_loss   | -0.311   |
|    learning_rate   | 0.0003   |
|    n_updates       | 380099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 396      |
|    critic_loss     | 145      |
|    ent_coef        | 0.183    |
|    ent_coef_loss   | -0.0405  |
|    learning_rate   | 0.0003   |
|    n_updates       | 380499   |
---------------------------------
=== Iterazione IRL 272 ===
Loss reward (iter 272): 5.030557632446289
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 386      |
|    critic_loss     | 170      |
|    ent_coef        | 0.183    |
|    ent_coef_loss   | 0.0168   |
|    learning_rate   | 0.0003   |
|    n_updates       | 381099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 392      |
|    critic_loss     | 169      |
|    ent_coef        | 0.189    |
|    ent_coef_loss   | 0.33     |
|    learning_rate   | 0.0003   |
|    n_updates       | 381499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 391      |
|    critic_loss     | 158      |
|    ent_coef        | 0.185    |
|    ent_coef_loss   | -0.0779  |
|    learning_rate   | 0.0003   |
|    n_updates       | 381899   |
---------------------------------
=== Iterazione IRL 273 ===
Loss reward (iter 273): 4.71718168258667
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 390      |
|    critic_loss     | 169      |
|    ent_coef        | 0.178    |
|    ent_coef_loss   | -0.179   |
|    learning_rate   | 0.0003   |
|    n_updates       | 382499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 395      |
|    critic_loss     | 144      |
|    ent_coef        | 0.185    |
|    ent_coef_loss   | 0.264    |
|    learning_rate   | 0.0003   |
|    n_updates       | 382899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 396      |
|    critic_loss     | 180      |
|    ent_coef        | 0.187    |
|    ent_coef_loss   | 0.49     |
|    learning_rate   | 0.0003   |
|    n_updates       | 383299   |
---------------------------------
=== Iterazione IRL 274 ===
Loss reward (iter 274): 3.963609218597412
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 402      |
|    critic_loss     | 188      |
|    ent_coef        | 0.184    |
|    ent_coef_loss   | -0.0998  |
|    learning_rate   | 0.0003   |
|    n_updates       | 383899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 400      |
|    critic_loss     | 172      |
|    ent_coef        | 0.183    |
|    ent_coef_loss   | -0.53    |
|    learning_rate   | 0.0003   |
|    n_updates       | 384299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 399      |
|    critic_loss     | 156      |
|    ent_coef        | 0.181    |
|    ent_coef_loss   | 0.0499   |
|    learning_rate   | 0.0003   |
|    n_updates       | 384699   |
---------------------------------
=== Iterazione IRL 275 ===
Loss reward (iter 275): 5.123106479644775
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 403      |
|    critic_loss     | 183      |
|    ent_coef        | 0.184    |
|    ent_coef_loss   | -0.232   |
|    learning_rate   | 0.0003   |
|    n_updates       | 385299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 399      |
|    critic_loss     | 197      |
|    ent_coef        | 0.175    |
|    ent_coef_loss   | 0.632    |
|    learning_rate   | 0.0003   |
|    n_updates       | 385699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 407      |
|    critic_loss     | 184      |
|    ent_coef        | 0.176    |
|    ent_coef_loss   | -0.156   |
|    learning_rate   | 0.0003   |
|    n_updates       | 386099   |
---------------------------------
=== Iterazione IRL 276 ===
Loss reward (iter 276): 4.862382888793945
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 407      |
|    critic_loss     | 188      |
|    ent_coef        | 0.176    |
|    ent_coef_loss   | 0.151    |
|    learning_rate   | 0.0003   |
|    n_updates       | 386699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 400      |
|    critic_loss     | 163      |
|    ent_coef        | 0.179    |
|    ent_coef_loss   | -0.115   |
|    learning_rate   | 0.0003   |
|    n_updates       | 387099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 409      |
|    critic_loss     | 133      |
|    ent_coef        | 0.181    |
|    ent_coef_loss   | -0.362   |
|    learning_rate   | 0.0003   |
|    n_updates       | 387499   |
---------------------------------
=== Iterazione IRL 277 ===
Loss reward (iter 277): 4.006521224975586
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 406      |
|    critic_loss     | 186      |
|    ent_coef        | 0.176    |
|    ent_coef_loss   | -0.454   |
|    learning_rate   | 0.0003   |
|    n_updates       | 388099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 409      |
|    critic_loss     | 188      |
|    ent_coef        | 0.184    |
|    ent_coef_loss   | -0.115   |
|    learning_rate   | 0.0003   |
|    n_updates       | 388499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 405      |
|    critic_loss     | 184      |
|    ent_coef        | 0.188    |
|    ent_coef_loss   | -0.036   |
|    learning_rate   | 0.0003   |
|    n_updates       | 388899   |
---------------------------------
=== Iterazione IRL 278 ===
Loss reward (iter 278): 4.615606784820557
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 409      |
|    critic_loss     | 164      |
|    ent_coef        | 0.181    |
|    ent_coef_loss   | 0.169    |
|    learning_rate   | 0.0003   |
|    n_updates       | 389499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 412      |
|    critic_loss     | 147      |
|    ent_coef        | 0.186    |
|    ent_coef_loss   | 0.237    |
|    learning_rate   | 0.0003   |
|    n_updates       | 389899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 405      |
|    critic_loss     | 150      |
|    ent_coef        | 0.184    |
|    ent_coef_loss   | -0.0534  |
|    learning_rate   | 0.0003   |
|    n_updates       | 390299   |
---------------------------------
=== Iterazione IRL 279 ===
Loss reward (iter 279): 4.5928473472595215
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 406      |
|    critic_loss     | 163      |
|    ent_coef        | 0.195    |
|    ent_coef_loss   | 0.904    |
|    learning_rate   | 0.0003   |
|    n_updates       | 390899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 404      |
|    critic_loss     | 138      |
|    ent_coef        | 0.201    |
|    ent_coef_loss   | 0.148    |
|    learning_rate   | 0.0003   |
|    n_updates       | 391299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 408      |
|    critic_loss     | 184      |
|    ent_coef        | 0.203    |
|    ent_coef_loss   | -0.101   |
|    learning_rate   | 0.0003   |
|    n_updates       | 391699   |
---------------------------------
=== Iterazione IRL 280 ===
Loss reward (iter 280): 4.455181121826172
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 414      |
|    critic_loss     | 150      |
|    ent_coef        | 0.21     |
|    ent_coef_loss   | -0.34    |
|    learning_rate   | 0.0003   |
|    n_updates       | 392299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 407      |
|    critic_loss     | 183      |
|    ent_coef        | 0.21     |
|    ent_coef_loss   | -0.358   |
|    learning_rate   | 0.0003   |
|    n_updates       | 392699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 415      |
|    critic_loss     | 174      |
|    ent_coef        | 0.209    |
|    ent_coef_loss   | -0.108   |
|    learning_rate   | 0.0003   |
|    n_updates       | 393099   |
---------------------------------
=== Iterazione IRL 281 ===
Loss reward (iter 281): 4.388738632202148
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 411      |
|    critic_loss     | 160      |
|    ent_coef        | 0.207    |
|    ent_coef_loss   | 0.217    |
|    learning_rate   | 0.0003   |
|    n_updates       | 393699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 415      |
|    critic_loss     | 191      |
|    ent_coef        | 0.206    |
|    ent_coef_loss   | 0.063    |
|    learning_rate   | 0.0003   |
|    n_updates       | 394099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 413      |
|    critic_loss     | 170      |
|    ent_coef        | 0.205    |
|    ent_coef_loss   | -0.488   |
|    learning_rate   | 0.0003   |
|    n_updates       | 394499   |
---------------------------------
=== Iterazione IRL 282 ===
Loss reward (iter 282): 3.978970527648926
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 415      |
|    critic_loss     | 154      |
|    ent_coef        | 0.22     |
|    ent_coef_loss   | -0.517   |
|    learning_rate   | 0.0003   |
|    n_updates       | 395099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 422      |
|    critic_loss     | 197      |
|    ent_coef        | 0.226    |
|    ent_coef_loss   | 0.201    |
|    learning_rate   | 0.0003   |
|    n_updates       | 395499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 413      |
|    critic_loss     | 180      |
|    ent_coef        | 0.218    |
|    ent_coef_loss   | -0.553   |
|    learning_rate   | 0.0003   |
|    n_updates       | 395899   |
---------------------------------
=== Iterazione IRL 283 ===
Loss reward (iter 283): 4.547358989715576
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 424      |
|    critic_loss     | 169      |
|    ent_coef        | 0.22     |
|    ent_coef_loss   | -0.169   |
|    learning_rate   | 0.0003   |
|    n_updates       | 396499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 414      |
|    critic_loss     | 139      |
|    ent_coef        | 0.224    |
|    ent_coef_loss   | -0.597   |
|    learning_rate   | 0.0003   |
|    n_updates       | 396899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 421      |
|    critic_loss     | 131      |
|    ent_coef        | 0.222    |
|    ent_coef_loss   | -0.452   |
|    learning_rate   | 0.0003   |
|    n_updates       | 397299   |
---------------------------------
=== Iterazione IRL 284 ===
Loss reward (iter 284): 3.0548787117004395
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 430      |
|    critic_loss     | 165      |
|    ent_coef        | 0.208    |
|    ent_coef_loss   | -0.271   |
|    learning_rate   | 0.0003   |
|    n_updates       | 397899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 416      |
|    critic_loss     | 143      |
|    ent_coef        | 0.203    |
|    ent_coef_loss   | -0.589   |
|    learning_rate   | 0.0003   |
|    n_updates       | 398299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 425      |
|    critic_loss     | 158      |
|    ent_coef        | 0.207    |
|    ent_coef_loss   | 0.00966  |
|    learning_rate   | 0.0003   |
|    n_updates       | 398699   |
---------------------------------
=== Iterazione IRL 285 ===
Loss reward (iter 285): 4.408590793609619
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 422      |
|    critic_loss     | 196      |
|    ent_coef        | 0.201    |
|    ent_coef_loss   | -0.232   |
|    learning_rate   | 0.0003   |
|    n_updates       | 399299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 427      |
|    critic_loss     | 161      |
|    ent_coef        | 0.198    |
|    ent_coef_loss   | -0.455   |
|    learning_rate   | 0.0003   |
|    n_updates       | 399699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 425      |
|    critic_loss     | 181      |
|    ent_coef        | 0.2      |
|    ent_coef_loss   | 0.116    |
|    learning_rate   | 0.0003   |
|    n_updates       | 400099   |
---------------------------------
=== Iterazione IRL 286 ===
Loss reward (iter 286): 4.608365535736084
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 431      |
|    critic_loss     | 178      |
|    ent_coef        | 0.196    |
|    ent_coef_loss   | 0.603    |
|    learning_rate   | 0.0003   |
|    n_updates       | 400699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 421      |
|    critic_loss     | 147      |
|    ent_coef        | 0.202    |
|    ent_coef_loss   | 0.272    |
|    learning_rate   | 0.0003   |
|    n_updates       | 401099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 428      |
|    critic_loss     | 197      |
|    ent_coef        | 0.207    |
|    ent_coef_loss   | 0.305    |
|    learning_rate   | 0.0003   |
|    n_updates       | 401499   |
---------------------------------
=== Iterazione IRL 287 ===
Loss reward (iter 287): 3.612149477005005
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 436      |
|    critic_loss     | 189      |
|    ent_coef        | 0.207    |
|    ent_coef_loss   | 0.158    |
|    learning_rate   | 0.0003   |
|    n_updates       | 402099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 433      |
|    critic_loss     | 167      |
|    ent_coef        | 0.201    |
|    ent_coef_loss   | -0.0218  |
|    learning_rate   | 0.0003   |
|    n_updates       | 402499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 435      |
|    critic_loss     | 177      |
|    ent_coef        | 0.203    |
|    ent_coef_loss   | 0.0674   |
|    learning_rate   | 0.0003   |
|    n_updates       | 402899   |
---------------------------------
=== Iterazione IRL 288 ===
Loss reward (iter 288): 4.299084663391113
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 437      |
|    critic_loss     | 183      |
|    ent_coef        | 0.214    |
|    ent_coef_loss   | 0.0101   |
|    learning_rate   | 0.0003   |
|    n_updates       | 403499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 436      |
|    critic_loss     | 180      |
|    ent_coef        | 0.217    |
|    ent_coef_loss   | -0.27    |
|    learning_rate   | 0.0003   |
|    n_updates       | 403899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 435      |
|    critic_loss     | 197      |
|    ent_coef        | 0.218    |
|    ent_coef_loss   | 0.181    |
|    learning_rate   | 0.0003   |
|    n_updates       | 404299   |
---------------------------------
=== Iterazione IRL 289 ===
Loss reward (iter 289): 2.8586130142211914
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 440      |
|    critic_loss     | 167      |
|    ent_coef        | 0.215    |
|    ent_coef_loss   | -0.121   |
|    learning_rate   | 0.0003   |
|    n_updates       | 404899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 439      |
|    critic_loss     | 200      |
|    ent_coef        | 0.217    |
|    ent_coef_loss   | 0.16     |
|    learning_rate   | 0.0003   |
|    n_updates       | 405299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 442      |
|    critic_loss     | 179      |
|    ent_coef        | 0.209    |
|    ent_coef_loss   | -0.074   |
|    learning_rate   | 0.0003   |
|    n_updates       | 405699   |
---------------------------------
=== Iterazione IRL 290 ===
Loss reward (iter 290): 3.9657280445098877
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 438      |
|    critic_loss     | 231      |
|    ent_coef        | 0.21     |
|    ent_coef_loss   | 0.0484   |
|    learning_rate   | 0.0003   |
|    n_updates       | 406299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 439      |
|    critic_loss     | 183      |
|    ent_coef        | 0.212    |
|    ent_coef_loss   | -0.404   |
|    learning_rate   | 0.0003   |
|    n_updates       | 406699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 441      |
|    critic_loss     | 174      |
|    ent_coef        | 0.207    |
|    ent_coef_loss   | -0.0377  |
|    learning_rate   | 0.0003   |
|    n_updates       | 407099   |
---------------------------------
=== Iterazione IRL 291 ===
Loss reward (iter 291): 4.352526664733887
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 441      |
|    critic_loss     | 203      |
|    ent_coef        | 0.207    |
|    ent_coef_loss   | 0.372    |
|    learning_rate   | 0.0003   |
|    n_updates       | 407699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 434      |
|    critic_loss     | 211      |
|    ent_coef        | 0.201    |
|    ent_coef_loss   | -0.603   |
|    learning_rate   | 0.0003   |
|    n_updates       | 408099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 440      |
|    critic_loss     | 194      |
|    ent_coef        | 0.201    |
|    ent_coef_loss   | 0.115    |
|    learning_rate   | 0.0003   |
|    n_updates       | 408499   |
---------------------------------
=== Iterazione IRL 292 ===
Loss reward (iter 292): 5.215384483337402
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 439      |
|    critic_loss     | 182      |
|    ent_coef        | 0.209    |
|    ent_coef_loss   | -0.251   |
|    learning_rate   | 0.0003   |
|    n_updates       | 409099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 446      |
|    critic_loss     | 233      |
|    ent_coef        | 0.208    |
|    ent_coef_loss   | -0.0801  |
|    learning_rate   | 0.0003   |
|    n_updates       | 409499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 439      |
|    critic_loss     | 162      |
|    ent_coef        | 0.201    |
|    ent_coef_loss   | -0.19    |
|    learning_rate   | 0.0003   |
|    n_updates       | 409899   |
---------------------------------
=== Iterazione IRL 293 ===
Loss reward (iter 293): 4.378427505493164
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 445      |
|    critic_loss     | 171      |
|    ent_coef        | 0.2      |
|    ent_coef_loss   | 0.0177   |
|    learning_rate   | 0.0003   |
|    n_updates       | 410499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 450      |
|    critic_loss     | 158      |
|    ent_coef        | 0.195    |
|    ent_coef_loss   | -0.279   |
|    learning_rate   | 0.0003   |
|    n_updates       | 410899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 448      |
|    critic_loss     | 185      |
|    ent_coef        | 0.195    |
|    ent_coef_loss   | 0.137    |
|    learning_rate   | 0.0003   |
|    n_updates       | 411299   |
---------------------------------
=== Iterazione IRL 294 ===
Loss reward (iter 294): 4.188628673553467
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 448      |
|    critic_loss     | 227      |
|    ent_coef        | 0.199    |
|    ent_coef_loss   | -0.455   |
|    learning_rate   | 0.0003   |
|    n_updates       | 411899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 454      |
|    critic_loss     | 195      |
|    ent_coef        | 0.202    |
|    ent_coef_loss   | 0.229    |
|    learning_rate   | 0.0003   |
|    n_updates       | 412299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 452      |
|    critic_loss     | 256      |
|    ent_coef        | 0.2      |
|    ent_coef_loss   | -0.0393  |
|    learning_rate   | 0.0003   |
|    n_updates       | 412699   |
---------------------------------
=== Iterazione IRL 295 ===
Loss reward (iter 295): 4.600363731384277
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 454      |
|    critic_loss     | 208      |
|    ent_coef        | 0.197    |
|    ent_coef_loss   | 0.408    |
|    learning_rate   | 0.0003   |
|    n_updates       | 413299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 453      |
|    critic_loss     | 161      |
|    ent_coef        | 0.2      |
|    ent_coef_loss   | -0.318   |
|    learning_rate   | 0.0003   |
|    n_updates       | 413699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 455      |
|    critic_loss     | 170      |
|    ent_coef        | 0.197    |
|    ent_coef_loss   | 0.271    |
|    learning_rate   | 0.0003   |
|    n_updates       | 414099   |
---------------------------------
=== Iterazione IRL 296 ===
Loss reward (iter 296): 5.155937671661377
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 451      |
|    critic_loss     | 185      |
|    ent_coef        | 0.204    |
|    ent_coef_loss   | 0.00807  |
|    learning_rate   | 0.0003   |
|    n_updates       | 414699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 450      |
|    critic_loss     | 205      |
|    ent_coef        | 0.203    |
|    ent_coef_loss   | 0.1      |
|    learning_rate   | 0.0003   |
|    n_updates       | 415099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 458      |
|    critic_loss     | 196      |
|    ent_coef        | 0.203    |
|    ent_coef_loss   | 0.447    |
|    learning_rate   | 0.0003   |
|    n_updates       | 415499   |
---------------------------------
=== Iterazione IRL 297 ===
Loss reward (iter 297): 4.346371173858643
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 461      |
|    critic_loss     | 359      |
|    ent_coef        | 0.2      |
|    ent_coef_loss   | 0.275    |
|    learning_rate   | 0.0003   |
|    n_updates       | 416099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 460      |
|    critic_loss     | 187      |
|    ent_coef        | 0.203    |
|    ent_coef_loss   | -0.532   |
|    learning_rate   | 0.0003   |
|    n_updates       | 416499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 458      |
|    critic_loss     | 195      |
|    ent_coef        | 0.199    |
|    ent_coef_loss   | -0.38    |
|    learning_rate   | 0.0003   |
|    n_updates       | 416899   |
---------------------------------
=== Iterazione IRL 298 ===
Loss reward (iter 298): 3.904141664505005
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 456      |
|    critic_loss     | 174      |
|    ent_coef        | 0.201    |
|    ent_coef_loss   | -0.241   |
|    learning_rate   | 0.0003   |
|    n_updates       | 417499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 467      |
|    critic_loss     | 227      |
|    ent_coef        | 0.2      |
|    ent_coef_loss   | 0.0428   |
|    learning_rate   | 0.0003   |
|    n_updates       | 417899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 466      |
|    critic_loss     | 187      |
|    ent_coef        | 0.201    |
|    ent_coef_loss   | -0.305   |
|    learning_rate   | 0.0003   |
|    n_updates       | 418299   |
---------------------------------
=== Iterazione IRL 299 ===
Loss reward (iter 299): 4.109030246734619
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 466      |
|    critic_loss     | 219      |
|    ent_coef        | 0.191    |
|    ent_coef_loss   | 0.135    |
|    learning_rate   | 0.0003   |
|    n_updates       | 418899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 458      |
|    critic_loss     | 210      |
|    ent_coef        | 0.198    |
|    ent_coef_loss   | 0.167    |
|    learning_rate   | 0.0003   |
|    n_updates       | 419299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 466      |
|    critic_loss     | 224      |
|    ent_coef        | 0.202    |
|    ent_coef_loss   | -0.0962  |
|    learning_rate   | 0.0003   |
|    n_updates       | 419699   |
---------------------------------
=== Iterazione IRL 300 ===
Loss reward (iter 300): 3.726778984069824
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 461      |
|    critic_loss     | 168      |
|    ent_coef        | 0.213    |
|    ent_coef_loss   | -0.346   |
|    learning_rate   | 0.0003   |
|    n_updates       | 420299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 475      |
|    critic_loss     | 203      |
|    ent_coef        | 0.203    |
|    ent_coef_loss   | 0.0672   |
|    learning_rate   | 0.0003   |
|    n_updates       | 420699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 470      |
|    critic_loss     | 201      |
|    ent_coef        | 0.208    |
|    ent_coef_loss   | 0.498    |
|    learning_rate   | 0.0003   |
|    n_updates       | 421099   |
---------------------------------
=== Iterazione IRL 301 ===
Loss reward (iter 301): 4.15215539932251
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 474      |
|    critic_loss     | 246      |
|    ent_coef        | 0.207    |
|    ent_coef_loss   | -0.124   |
|    learning_rate   | 0.0003   |
|    n_updates       | 421699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 478      |
|    critic_loss     | 234      |
|    ent_coef        | 0.214    |
|    ent_coef_loss   | 0.132    |
|    learning_rate   | 0.0003   |
|    n_updates       | 422099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 464      |
|    critic_loss     | 206      |
|    ent_coef        | 0.213    |
|    ent_coef_loss   | -0.48    |
|    learning_rate   | 0.0003   |
|    n_updates       | 422499   |
---------------------------------
=== Iterazione IRL 302 ===
Loss reward (iter 302): 2.9743778705596924
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 156      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 473      |
|    critic_loss     | 206      |
|    ent_coef        | 0.207    |
|    ent_coef_loss   | -0.188   |
|    learning_rate   | 0.0003   |
|    n_updates       | 423099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 468      |
|    critic_loss     | 162      |
|    ent_coef        | 0.214    |
|    ent_coef_loss   | -0.225   |
|    learning_rate   | 0.0003   |
|    n_updates       | 423499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 471      |
|    critic_loss     | 191      |
|    ent_coef        | 0.219    |
|    ent_coef_loss   | 0.227    |
|    learning_rate   | 0.0003   |
|    n_updates       | 423899   |
---------------------------------
=== Iterazione IRL 303 ===
Loss reward (iter 303): 3.8334641456604004
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 473      |
|    critic_loss     | 222      |
|    ent_coef        | 0.213    |
|    ent_coef_loss   | -0.12    |
|    learning_rate   | 0.0003   |
|    n_updates       | 424499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 479      |
|    critic_loss     | 240      |
|    ent_coef        | 0.211    |
|    ent_coef_loss   | -0.264   |
|    learning_rate   | 0.0003   |
|    n_updates       | 424899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 474      |
|    critic_loss     | 185      |
|    ent_coef        | 0.214    |
|    ent_coef_loss   | 0.0699   |
|    learning_rate   | 0.0003   |
|    n_updates       | 425299   |
---------------------------------
=== Iterazione IRL 304 ===
Loss reward (iter 304): 4.625607967376709
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 481      |
|    critic_loss     | 251      |
|    ent_coef        | 0.219    |
|    ent_coef_loss   | 0.287    |
|    learning_rate   | 0.0003   |
|    n_updates       | 425899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 473      |
|    critic_loss     | 226      |
|    ent_coef        | 0.215    |
|    ent_coef_loss   | 0.296    |
|    learning_rate   | 0.0003   |
|    n_updates       | 426299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 477      |
|    critic_loss     | 218      |
|    ent_coef        | 0.213    |
|    ent_coef_loss   | 0.399    |
|    learning_rate   | 0.0003   |
|    n_updates       | 426699   |
---------------------------------
=== Iterazione IRL 305 ===
Loss reward (iter 305): 4.596492290496826
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 478      |
|    critic_loss     | 201      |
|    ent_coef        | 0.204    |
|    ent_coef_loss   | 0.0914   |
|    learning_rate   | 0.0003   |
|    n_updates       | 427299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 478      |
|    critic_loss     | 206      |
|    ent_coef        | 0.208    |
|    ent_coef_loss   | 0.0434   |
|    learning_rate   | 0.0003   |
|    n_updates       | 427699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 484      |
|    critic_loss     | 195      |
|    ent_coef        | 0.203    |
|    ent_coef_loss   | -0.351   |
|    learning_rate   | 0.0003   |
|    n_updates       | 428099   |
---------------------------------
=== Iterazione IRL 306 ===
Loss reward (iter 306): 4.114544868469238
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 488      |
|    critic_loss     | 266      |
|    ent_coef        | 0.202    |
|    ent_coef_loss   | -0.0383  |
|    learning_rate   | 0.0003   |
|    n_updates       | 428699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 480      |
|    critic_loss     | 207      |
|    ent_coef        | 0.205    |
|    ent_coef_loss   | -0.115   |
|    learning_rate   | 0.0003   |
|    n_updates       | 429099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 483      |
|    critic_loss     | 212      |
|    ent_coef        | 0.208    |
|    ent_coef_loss   | 0.0259   |
|    learning_rate   | 0.0003   |
|    n_updates       | 429499   |
---------------------------------
=== Iterazione IRL 307 ===
Loss reward (iter 307): 4.13926887512207
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 474      |
|    critic_loss     | 210      |
|    ent_coef        | 0.203    |
|    ent_coef_loss   | -0.735   |
|    learning_rate   | 0.0003   |
|    n_updates       | 430099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 485      |
|    critic_loss     | 209      |
|    ent_coef        | 0.203    |
|    ent_coef_loss   | -0.152   |
|    learning_rate   | 0.0003   |
|    n_updates       | 430499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 485      |
|    critic_loss     | 187      |
|    ent_coef        | 0.207    |
|    ent_coef_loss   | 0.279    |
|    learning_rate   | 0.0003   |
|    n_updates       | 430899   |
---------------------------------
=== Iterazione IRL 308 ===
Loss reward (iter 308): 3.5820329189300537
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 491      |
|    critic_loss     | 229      |
|    ent_coef        | 0.203    |
|    ent_coef_loss   | 0.645    |
|    learning_rate   | 0.0003   |
|    n_updates       | 431499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 485      |
|    critic_loss     | 374      |
|    ent_coef        | 0.203    |
|    ent_coef_loss   | -0.0483  |
|    learning_rate   | 0.0003   |
|    n_updates       | 431899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 492      |
|    critic_loss     | 166      |
|    ent_coef        | 0.202    |
|    ent_coef_loss   | 0.269    |
|    learning_rate   | 0.0003   |
|    n_updates       | 432299   |
---------------------------------
=== Iterazione IRL 309 ===
Loss reward (iter 309): 3.837460994720459
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 492      |
|    critic_loss     | 215      |
|    ent_coef        | 0.205    |
|    ent_coef_loss   | -0.0436  |
|    learning_rate   | 0.0003   |
|    n_updates       | 432899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 493      |
|    critic_loss     | 196      |
|    ent_coef        | 0.208    |
|    ent_coef_loss   | -0.0971  |
|    learning_rate   | 0.0003   |
|    n_updates       | 433299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 495      |
|    critic_loss     | 223      |
|    ent_coef        | 0.21     |
|    ent_coef_loss   | 0.316    |
|    learning_rate   | 0.0003   |
|    n_updates       | 433699   |
---------------------------------
=== Iterazione IRL 310 ===
Loss reward (iter 310): 4.493089199066162
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 492      |
|    critic_loss     | 193      |
|    ent_coef        | 0.207    |
|    ent_coef_loss   | -0.263   |
|    learning_rate   | 0.0003   |
|    n_updates       | 434299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 491      |
|    critic_loss     | 245      |
|    ent_coef        | 0.211    |
|    ent_coef_loss   | 0.299    |
|    learning_rate   | 0.0003   |
|    n_updates       | 434699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 492      |
|    critic_loss     | 220      |
|    ent_coef        | 0.213    |
|    ent_coef_loss   | 0.203    |
|    learning_rate   | 0.0003   |
|    n_updates       | 435099   |
---------------------------------
=== Iterazione IRL 311 ===
Loss reward (iter 311): 4.441251277923584
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 497      |
|    critic_loss     | 208      |
|    ent_coef        | 0.204    |
|    ent_coef_loss   | -0.0968  |
|    learning_rate   | 0.0003   |
|    n_updates       | 435699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 494      |
|    critic_loss     | 201      |
|    ent_coef        | 0.201    |
|    ent_coef_loss   | -0.113   |
|    learning_rate   | 0.0003   |
|    n_updates       | 436099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 491      |
|    critic_loss     | 166      |
|    ent_coef        | 0.199    |
|    ent_coef_loss   | -0.681   |
|    learning_rate   | 0.0003   |
|    n_updates       | 436499   |
---------------------------------
=== Iterazione IRL 312 ===
Loss reward (iter 312): 4.217838287353516
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 504      |
|    critic_loss     | 236      |
|    ent_coef        | 0.206    |
|    ent_coef_loss   | 0.0564   |
|    learning_rate   | 0.0003   |
|    n_updates       | 437099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 491      |
|    critic_loss     | 202      |
|    ent_coef        | 0.21     |
|    ent_coef_loss   | -0.201   |
|    learning_rate   | 0.0003   |
|    n_updates       | 437499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 491      |
|    critic_loss     | 232      |
|    ent_coef        | 0.212    |
|    ent_coef_loss   | 0.123    |
|    learning_rate   | 0.0003   |
|    n_updates       | 437899   |
---------------------------------
=== Iterazione IRL 313 ===
Loss reward (iter 313): 4.657234191894531
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 498      |
|    critic_loss     | 225      |
|    ent_coef        | 0.211    |
|    ent_coef_loss   | -0.0405  |
|    learning_rate   | 0.0003   |
|    n_updates       | 438499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 491      |
|    critic_loss     | 197      |
|    ent_coef        | 0.209    |
|    ent_coef_loss   | -0.363   |
|    learning_rate   | 0.0003   |
|    n_updates       | 438899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 494      |
|    critic_loss     | 197      |
|    ent_coef        | 0.208    |
|    ent_coef_loss   | -0.0774  |
|    learning_rate   | 0.0003   |
|    n_updates       | 439299   |
---------------------------------
=== Iterazione IRL 314 ===
Loss reward (iter 314): 4.796629428863525
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 496      |
|    critic_loss     | 191      |
|    ent_coef        | 0.218    |
|    ent_coef_loss   | 0.182    |
|    learning_rate   | 0.0003   |
|    n_updates       | 439899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 496      |
|    critic_loss     | 221      |
|    ent_coef        | 0.218    |
|    ent_coef_loss   | 0.501    |
|    learning_rate   | 0.0003   |
|    n_updates       | 440299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 495      |
|    critic_loss     | 234      |
|    ent_coef        | 0.219    |
|    ent_coef_loss   | 0.169    |
|    learning_rate   | 0.0003   |
|    n_updates       | 440699   |
---------------------------------
=== Iterazione IRL 315 ===
Loss reward (iter 315): 4.255774974822998
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 502      |
|    critic_loss     | 216      |
|    ent_coef        | 0.221    |
|    ent_coef_loss   | 0.59     |
|    learning_rate   | 0.0003   |
|    n_updates       | 441299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 497      |
|    critic_loss     | 202      |
|    ent_coef        | 0.225    |
|    ent_coef_loss   | -0.269   |
|    learning_rate   | 0.0003   |
|    n_updates       | 441699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 494      |
|    critic_loss     | 197      |
|    ent_coef        | 0.221    |
|    ent_coef_loss   | 0.0539   |
|    learning_rate   | 0.0003   |
|    n_updates       | 442099   |
---------------------------------
=== Iterazione IRL 316 ===
Loss reward (iter 316): 3.6512069702148438
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 501      |
|    critic_loss     | 246      |
|    ent_coef        | 0.231    |
|    ent_coef_loss   | 0.128    |
|    learning_rate   | 0.0003   |
|    n_updates       | 442699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 500      |
|    critic_loss     | 217      |
|    ent_coef        | 0.228    |
|    ent_coef_loss   | 0.0597   |
|    learning_rate   | 0.0003   |
|    n_updates       | 443099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 504      |
|    critic_loss     | 184      |
|    ent_coef        | 0.226    |
|    ent_coef_loss   | -0.365   |
|    learning_rate   | 0.0003   |
|    n_updates       | 443499   |
---------------------------------
=== Iterazione IRL 317 ===
Loss reward (iter 317): 3.7837436199188232
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 504      |
|    critic_loss     | 191      |
|    ent_coef        | 0.219    |
|    ent_coef_loss   | 0.0271   |
|    learning_rate   | 0.0003   |
|    n_updates       | 444099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 500      |
|    critic_loss     | 200      |
|    ent_coef        | 0.227    |
|    ent_coef_loss   | 0.0794   |
|    learning_rate   | 0.0003   |
|    n_updates       | 444499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 496      |
|    critic_loss     | 203      |
|    ent_coef        | 0.232    |
|    ent_coef_loss   | 0.0907   |
|    learning_rate   | 0.0003   |
|    n_updates       | 444899   |
---------------------------------
=== Iterazione IRL 318 ===
Loss reward (iter 318): 3.1921348571777344
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 496      |
|    critic_loss     | 195      |
|    ent_coef        | 0.231    |
|    ent_coef_loss   | 0.167    |
|    learning_rate   | 0.0003   |
|    n_updates       | 445499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 503      |
|    critic_loss     | 209      |
|    ent_coef        | 0.231    |
|    ent_coef_loss   | -0.234   |
|    learning_rate   | 0.0003   |
|    n_updates       | 445899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 505      |
|    critic_loss     | 238      |
|    ent_coef        | 0.234    |
|    ent_coef_loss   | -0.0319  |
|    learning_rate   | 0.0003   |
|    n_updates       | 446299   |
---------------------------------
=== Iterazione IRL 319 ===
Loss reward (iter 319): 2.6978917121887207
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 501      |
|    critic_loss     | 243      |
|    ent_coef        | 0.24     |
|    ent_coef_loss   | 0.293    |
|    learning_rate   | 0.0003   |
|    n_updates       | 446899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 500      |
|    critic_loss     | 179      |
|    ent_coef        | 0.238    |
|    ent_coef_loss   | -0.293   |
|    learning_rate   | 0.0003   |
|    n_updates       | 447299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 506      |
|    critic_loss     | 232      |
|    ent_coef        | 0.235    |
|    ent_coef_loss   | 0.166    |
|    learning_rate   | 0.0003   |
|    n_updates       | 447699   |
---------------------------------
=== Iterazione IRL 320 ===
Loss reward (iter 320): 2.8784070014953613
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 503      |
|    critic_loss     | 212      |
|    ent_coef        | 0.231    |
|    ent_coef_loss   | 0.575    |
|    learning_rate   | 0.0003   |
|    n_updates       | 448299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 499      |
|    critic_loss     | 201      |
|    ent_coef        | 0.225    |
|    ent_coef_loss   | -0.122   |
|    learning_rate   | 0.0003   |
|    n_updates       | 448699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 506      |
|    critic_loss     | 229      |
|    ent_coef        | 0.221    |
|    ent_coef_loss   | -0.0368  |
|    learning_rate   | 0.0003   |
|    n_updates       | 449099   |
---------------------------------
=== Iterazione IRL 321 ===
Loss reward (iter 321): 3.672478675842285
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 512      |
|    critic_loss     | 229      |
|    ent_coef        | 0.222    |
|    ent_coef_loss   | -0.14    |
|    learning_rate   | 0.0003   |
|    n_updates       | 449699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 509      |
|    critic_loss     | 192      |
|    ent_coef        | 0.223    |
|    ent_coef_loss   | -0.347   |
|    learning_rate   | 0.0003   |
|    n_updates       | 450099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 515      |
|    critic_loss     | 223      |
|    ent_coef        | 0.219    |
|    ent_coef_loss   | 0.134    |
|    learning_rate   | 0.0003   |
|    n_updates       | 450499   |
---------------------------------
=== Iterazione IRL 322 ===
Loss reward (iter 322): 3.2267634868621826
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 516      |
|    critic_loss     | 219      |
|    ent_coef        | 0.227    |
|    ent_coef_loss   | 0.277    |
|    learning_rate   | 0.0003   |
|    n_updates       | 451099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 515      |
|    critic_loss     | 213      |
|    ent_coef        | 0.225    |
|    ent_coef_loss   | 0.245    |
|    learning_rate   | 0.0003   |
|    n_updates       | 451499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 515      |
|    critic_loss     | 222      |
|    ent_coef        | 0.226    |
|    ent_coef_loss   | -0.154   |
|    learning_rate   | 0.0003   |
|    n_updates       | 451899   |
---------------------------------
=== Iterazione IRL 323 ===
Loss reward (iter 323): 3.024552822113037
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 514      |
|    critic_loss     | 209      |
|    ent_coef        | 0.223    |
|    ent_coef_loss   | 0.197    |
|    learning_rate   | 0.0003   |
|    n_updates       | 452499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 517      |
|    critic_loss     | 182      |
|    ent_coef        | 0.221    |
|    ent_coef_loss   | 0.0456   |
|    learning_rate   | 0.0003   |
|    n_updates       | 452899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 511      |
|    critic_loss     | 244      |
|    ent_coef        | 0.229    |
|    ent_coef_loss   | -0.0183  |
|    learning_rate   | 0.0003   |
|    n_updates       | 453299   |
---------------------------------
=== Iterazione IRL 324 ===
Loss reward (iter 324): 3.1380815505981445
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 522      |
|    critic_loss     | 250      |
|    ent_coef        | 0.232    |
|    ent_coef_loss   | 0.264    |
|    learning_rate   | 0.0003   |
|    n_updates       | 453899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 523      |
|    critic_loss     | 216      |
|    ent_coef        | 0.24     |
|    ent_coef_loss   | -0.0272  |
|    learning_rate   | 0.0003   |
|    n_updates       | 454299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 521      |
|    critic_loss     | 205      |
|    ent_coef        | 0.234    |
|    ent_coef_loss   | -0.145   |
|    learning_rate   | 0.0003   |
|    n_updates       | 454699   |
---------------------------------
=== Iterazione IRL 325 ===
Loss reward (iter 325): 2.449301242828369
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 522      |
|    critic_loss     | 240      |
|    ent_coef        | 0.225    |
|    ent_coef_loss   | -0.189   |
|    learning_rate   | 0.0003   |
|    n_updates       | 455299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 520      |
|    critic_loss     | 253      |
|    ent_coef        | 0.227    |
|    ent_coef_loss   | -0.00164 |
|    learning_rate   | 0.0003   |
|    n_updates       | 455699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 520      |
|    critic_loss     | 194      |
|    ent_coef        | 0.216    |
|    ent_coef_loss   | 0.079    |
|    learning_rate   | 0.0003   |
|    n_updates       | 456099   |
---------------------------------
=== Iterazione IRL 326 ===
Loss reward (iter 326): 3.216470956802368
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 515      |
|    critic_loss     | 228      |
|    ent_coef        | 0.23     |
|    ent_coef_loss   | 0.114    |
|    learning_rate   | 0.0003   |
|    n_updates       | 456699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 518      |
|    critic_loss     | 231      |
|    ent_coef        | 0.228    |
|    ent_coef_loss   | -0.419   |
|    learning_rate   | 0.0003   |
|    n_updates       | 457099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 527      |
|    critic_loss     | 219      |
|    ent_coef        | 0.227    |
|    ent_coef_loss   | -0.201   |
|    learning_rate   | 0.0003   |
|    n_updates       | 457499   |
---------------------------------
=== Iterazione IRL 327 ===
Loss reward (iter 327): 3.610529899597168
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 523      |
|    critic_loss     | 248      |
|    ent_coef        | 0.226    |
|    ent_coef_loss   | 0.154    |
|    learning_rate   | 0.0003   |
|    n_updates       | 458099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 527      |
|    critic_loss     | 187      |
|    ent_coef        | 0.222    |
|    ent_coef_loss   | -0.271   |
|    learning_rate   | 0.0003   |
|    n_updates       | 458499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 528      |
|    critic_loss     | 214      |
|    ent_coef        | 0.226    |
|    ent_coef_loss   | 0.311    |
|    learning_rate   | 0.0003   |
|    n_updates       | 458899   |
---------------------------------
=== Iterazione IRL 328 ===
Loss reward (iter 328): 2.354288339614868
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 528      |
|    critic_loss     | 255      |
|    ent_coef        | 0.222    |
|    ent_coef_loss   | -0.168   |
|    learning_rate   | 0.0003   |
|    n_updates       | 459499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 531      |
|    critic_loss     | 247      |
|    ent_coef        | 0.227    |
|    ent_coef_loss   | 0.242    |
|    learning_rate   | 0.0003   |
|    n_updates       | 459899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 523      |
|    critic_loss     | 205      |
|    ent_coef        | 0.221    |
|    ent_coef_loss   | -0.21    |
|    learning_rate   | 0.0003   |
|    n_updates       | 460299   |
---------------------------------
=== Iterazione IRL 329 ===
Loss reward (iter 329): 4.299227237701416
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 529      |
|    critic_loss     | 266      |
|    ent_coef        | 0.216    |
|    ent_coef_loss   | 0.0919   |
|    learning_rate   | 0.0003   |
|    n_updates       | 460899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 535      |
|    critic_loss     | 252      |
|    ent_coef        | 0.223    |
|    ent_coef_loss   | 0.378    |
|    learning_rate   | 0.0003   |
|    n_updates       | 461299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 527      |
|    critic_loss     | 194      |
|    ent_coef        | 0.224    |
|    ent_coef_loss   | -0.147   |
|    learning_rate   | 0.0003   |
|    n_updates       | 461699   |
---------------------------------
=== Iterazione IRL 330 ===
Loss reward (iter 330): 4.0821685791015625
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 532      |
|    critic_loss     | 204      |
|    ent_coef        | 0.214    |
|    ent_coef_loss   | -0.151   |
|    learning_rate   | 0.0003   |
|    n_updates       | 462299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 531      |
|    critic_loss     | 239      |
|    ent_coef        | 0.22     |
|    ent_coef_loss   | 0.332    |
|    learning_rate   | 0.0003   |
|    n_updates       | 462699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 538      |
|    critic_loss     | 309      |
|    ent_coef        | 0.232    |
|    ent_coef_loss   | 0.0065   |
|    learning_rate   | 0.0003   |
|    n_updates       | 463099   |
---------------------------------
=== Iterazione IRL 331 ===
Loss reward (iter 331): 3.696345806121826
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 534      |
|    critic_loss     | 230      |
|    ent_coef        | 0.237    |
|    ent_coef_loss   | -0.514   |
|    learning_rate   | 0.0003   |
|    n_updates       | 463699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 539      |
|    critic_loss     | 315      |
|    ent_coef        | 0.236    |
|    ent_coef_loss   | 0.395    |
|    learning_rate   | 0.0003   |
|    n_updates       | 464099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 534      |
|    critic_loss     | 239      |
|    ent_coef        | 0.233    |
|    ent_coef_loss   | -0.13    |
|    learning_rate   | 0.0003   |
|    n_updates       | 464499   |
---------------------------------
=== Iterazione IRL 332 ===
Loss reward (iter 332): 4.488205432891846
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 540      |
|    critic_loss     | 327      |
|    ent_coef        | 0.239    |
|    ent_coef_loss   | 0.0989   |
|    learning_rate   | 0.0003   |
|    n_updates       | 465099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 532      |
|    critic_loss     | 297      |
|    ent_coef        | 0.233    |
|    ent_coef_loss   | -0.376   |
|    learning_rate   | 0.0003   |
|    n_updates       | 465499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 552      |
|    critic_loss     | 291      |
|    ent_coef        | 0.228    |
|    ent_coef_loss   | 0.538    |
|    learning_rate   | 0.0003   |
|    n_updates       | 465899   |
---------------------------------
=== Iterazione IRL 333 ===
Loss reward (iter 333): 4.675755977630615
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 537      |
|    critic_loss     | 286      |
|    ent_coef        | 0.236    |
|    ent_coef_loss   | -0.152   |
|    learning_rate   | 0.0003   |
|    n_updates       | 466499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 530      |
|    critic_loss     | 216      |
|    ent_coef        | 0.24     |
|    ent_coef_loss   | 0.233    |
|    learning_rate   | 0.0003   |
|    n_updates       | 466899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 552      |
|    critic_loss     | 311      |
|    ent_coef        | 0.235    |
|    ent_coef_loss   | 0.254    |
|    learning_rate   | 0.0003   |
|    n_updates       | 467299   |
---------------------------------
=== Iterazione IRL 334 ===
Loss reward (iter 334): 5.177440166473389
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 544      |
|    critic_loss     | 270      |
|    ent_coef        | 0.236    |
|    ent_coef_loss   | -0.0961  |
|    learning_rate   | 0.0003   |
|    n_updates       | 467899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 537      |
|    critic_loss     | 287      |
|    ent_coef        | 0.231    |
|    ent_coef_loss   | 0.17     |
|    learning_rate   | 0.0003   |
|    n_updates       | 468299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 553      |
|    critic_loss     | 290      |
|    ent_coef        | 0.226    |
|    ent_coef_loss   | 0.44     |
|    learning_rate   | 0.0003   |
|    n_updates       | 468699   |
---------------------------------
=== Iterazione IRL 335 ===
Loss reward (iter 335): 4.631235122680664
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 544      |
|    critic_loss     | 255      |
|    ent_coef        | 0.242    |
|    ent_coef_loss   | 0.171    |
|    learning_rate   | 0.0003   |
|    n_updates       | 469299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 543      |
|    critic_loss     | 261      |
|    ent_coef        | 0.244    |
|    ent_coef_loss   | -0.193   |
|    learning_rate   | 0.0003   |
|    n_updates       | 469699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 544      |
|    critic_loss     | 308      |
|    ent_coef        | 0.256    |
|    ent_coef_loss   | -0.433   |
|    learning_rate   | 0.0003   |
|    n_updates       | 470099   |
---------------------------------
=== Iterazione IRL 336 ===
Loss reward (iter 336): 4.946693420410156
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 537      |
|    critic_loss     | 347      |
|    ent_coef        | 0.265    |
|    ent_coef_loss   | -0.0409  |
|    learning_rate   | 0.0003   |
|    n_updates       | 470699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 556      |
|    critic_loss     | 273      |
|    ent_coef        | 0.267    |
|    ent_coef_loss   | -0.163   |
|    learning_rate   | 0.0003   |
|    n_updates       | 471099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 544      |
|    critic_loss     | 374      |
|    ent_coef        | 0.268    |
|    ent_coef_loss   | 0.0286   |
|    learning_rate   | 0.0003   |
|    n_updates       | 471499   |
---------------------------------
=== Iterazione IRL 337 ===
Loss reward (iter 337): 3.587894916534424
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 546      |
|    critic_loss     | 311      |
|    ent_coef        | 0.285    |
|    ent_coef_loss   | -0.347   |
|    learning_rate   | 0.0003   |
|    n_updates       | 472099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 543      |
|    critic_loss     | 259      |
|    ent_coef        | 0.29     |
|    ent_coef_loss   | 0.0217   |
|    learning_rate   | 0.0003   |
|    n_updates       | 472499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 548      |
|    critic_loss     | 332      |
|    ent_coef        | 0.291    |
|    ent_coef_loss   | -0.0822  |
|    learning_rate   | 0.0003   |
|    n_updates       | 472899   |
---------------------------------
=== Iterazione IRL 338 ===
Loss reward (iter 338): 0.01562941074371338
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 561      |
|    critic_loss     | 343      |
|    ent_coef        | 0.295    |
|    ent_coef_loss   | 0.214    |
|    learning_rate   | 0.0003   |
|    n_updates       | 473499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 547      |
|    critic_loss     | 356      |
|    ent_coef        | 0.301    |
|    ent_coef_loss   | -0.153   |
|    learning_rate   | 0.0003   |
|    n_updates       | 473899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 550      |
|    critic_loss     | 284      |
|    ent_coef        | 0.293    |
|    ent_coef_loss   | -0.197   |
|    learning_rate   | 0.0003   |
|    n_updates       | 474299   |
---------------------------------
=== Iterazione IRL 339 ===
Loss reward (iter 339): 1.198828935623169
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 554      |
|    critic_loss     | 394      |
|    ent_coef        | 0.295    |
|    ent_coef_loss   | 0.175    |
|    learning_rate   | 0.0003   |
|    n_updates       | 474899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 562      |
|    critic_loss     | 298      |
|    ent_coef        | 0.295    |
|    ent_coef_loss   | -0.131   |
|    learning_rate   | 0.0003   |
|    n_updates       | 475299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 551      |
|    critic_loss     | 332      |
|    ent_coef        | 0.313    |
|    ent_coef_loss   | 0.358    |
|    learning_rate   | 0.0003   |
|    n_updates       | 475699   |
---------------------------------
=== Iterazione IRL 340 ===
Loss reward (iter 340): 3.6440744400024414
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 557      |
|    critic_loss     | 345      |
|    ent_coef        | 0.341    |
|    ent_coef_loss   | -0.0972  |
|    learning_rate   | 0.0003   |
|    n_updates       | 476299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 549      |
|    critic_loss     | 305      |
|    ent_coef        | 0.349    |
|    ent_coef_loss   | 0.341    |
|    learning_rate   | 0.0003   |
|    n_updates       | 476699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 554      |
|    critic_loss     | 306      |
|    ent_coef        | 0.353    |
|    ent_coef_loss   | 0.0195   |
|    learning_rate   | 0.0003   |
|    n_updates       | 477099   |
---------------------------------
=== Iterazione IRL 341 ===
Loss reward (iter 341): 3.2510480880737305
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 558      |
|    critic_loss     | 305      |
|    ent_coef        | 0.351    |
|    ent_coef_loss   | -0.329   |
|    learning_rate   | 0.0003   |
|    n_updates       | 477699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 555      |
|    critic_loss     | 281      |
|    ent_coef        | 0.347    |
|    ent_coef_loss   | -0.527   |
|    learning_rate   | 0.0003   |
|    n_updates       | 478099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 554      |
|    critic_loss     | 239      |
|    ent_coef        | 0.34     |
|    ent_coef_loss   | -0.338   |
|    learning_rate   | 0.0003   |
|    n_updates       | 478499   |
---------------------------------
=== Iterazione IRL 342 ===
Loss reward (iter 342): 5.431642055511475
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 552      |
|    critic_loss     | 302      |
|    ent_coef        | 0.335    |
|    ent_coef_loss   | -0.0808  |
|    learning_rate   | 0.0003   |
|    n_updates       | 479099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 555      |
|    critic_loss     | 370      |
|    ent_coef        | 0.32     |
|    ent_coef_loss   | -0.141   |
|    learning_rate   | 0.0003   |
|    n_updates       | 479499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 566      |
|    critic_loss     | 315      |
|    ent_coef        | 0.317    |
|    ent_coef_loss   | 0.298    |
|    learning_rate   | 0.0003   |
|    n_updates       | 479899   |
---------------------------------
=== Iterazione IRL 343 ===
Loss reward (iter 343): 5.800426483154297
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 574      |
|    critic_loss     | 351      |
|    ent_coef        | 0.297    |
|    ent_coef_loss   | 0.519    |
|    learning_rate   | 0.0003   |
|    n_updates       | 480499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 560      |
|    critic_loss     | 289      |
|    ent_coef        | 0.298    |
|    ent_coef_loss   | -0.158   |
|    learning_rate   | 0.0003   |
|    n_updates       | 480899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 561      |
|    critic_loss     | 298      |
|    ent_coef        | 0.305    |
|    ent_coef_loss   | -0.259   |
|    learning_rate   | 0.0003   |
|    n_updates       | 481299   |
---------------------------------
=== Iterazione IRL 344 ===
Loss reward (iter 344): 4.302689552307129
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 549      |
|    critic_loss     | 254      |
|    ent_coef        | 0.283    |
|    ent_coef_loss   | -0.143   |
|    learning_rate   | 0.0003   |
|    n_updates       | 481899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 563      |
|    critic_loss     | 335      |
|    ent_coef        | 0.284    |
|    ent_coef_loss   | 0.0167   |
|    learning_rate   | 0.0003   |
|    n_updates       | 482299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 562      |
|    critic_loss     | 329      |
|    ent_coef        | 0.28     |
|    ent_coef_loss   | 0.00323  |
|    learning_rate   | 0.0003   |
|    n_updates       | 482699   |
---------------------------------
=== Iterazione IRL 345 ===
Loss reward (iter 345): 4.590507984161377
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 565      |
|    critic_loss     | 382      |
|    ent_coef        | 0.274    |
|    ent_coef_loss   | 0.338    |
|    learning_rate   | 0.0003   |
|    n_updates       | 483299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 576      |
|    critic_loss     | 428      |
|    ent_coef        | 0.286    |
|    ent_coef_loss   | -0.336   |
|    learning_rate   | 0.0003   |
|    n_updates       | 483699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 570      |
|    critic_loss     | 319      |
|    ent_coef        | 0.282    |
|    ent_coef_loss   | 0.03     |
|    learning_rate   | 0.0003   |
|    n_updates       | 484099   |
---------------------------------
=== Iterazione IRL 346 ===
Loss reward (iter 346): 5.377799034118652
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 564      |
|    critic_loss     | 283      |
|    ent_coef        | 0.277    |
|    ent_coef_loss   | 0.0904   |
|    learning_rate   | 0.0003   |
|    n_updates       | 484699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 566      |
|    critic_loss     | 338      |
|    ent_coef        | 0.281    |
|    ent_coef_loss   | -0.0786  |
|    learning_rate   | 0.0003   |
|    n_updates       | 485099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 567      |
|    critic_loss     | 275      |
|    ent_coef        | 0.277    |
|    ent_coef_loss   | -0.251   |
|    learning_rate   | 0.0003   |
|    n_updates       | 485499   |
---------------------------------
=== Iterazione IRL 347 ===
Loss reward (iter 347): 4.318608283996582
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 568      |
|    critic_loss     | 339      |
|    ent_coef        | 0.29     |
|    ent_coef_loss   | -0.252   |
|    learning_rate   | 0.0003   |
|    n_updates       | 486099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 578      |
|    critic_loss     | 312      |
|    ent_coef        | 0.28     |
|    ent_coef_loss   | 0.0593   |
|    learning_rate   | 0.0003   |
|    n_updates       | 486499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 562      |
|    critic_loss     | 268      |
|    ent_coef        | 0.275    |
|    ent_coef_loss   | -0.267   |
|    learning_rate   | 0.0003   |
|    n_updates       | 486899   |
---------------------------------
=== Iterazione IRL 348 ===
Loss reward (iter 348): 4.789116859436035
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 584      |
|    critic_loss     | 404      |
|    ent_coef        | 0.271    |
|    ent_coef_loss   | -0.111   |
|    learning_rate   | 0.0003   |
|    n_updates       | 487499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 588      |
|    critic_loss     | 381      |
|    ent_coef        | 0.275    |
|    ent_coef_loss   | 0.419    |
|    learning_rate   | 0.0003   |
|    n_updates       | 487899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 581      |
|    critic_loss     | 363      |
|    ent_coef        | 0.275    |
|    ent_coef_loss   | 0.41     |
|    learning_rate   | 0.0003   |
|    n_updates       | 488299   |
---------------------------------
=== Iterazione IRL 349 ===
Loss reward (iter 349): 4.483903408050537
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 577      |
|    critic_loss     | 384      |
|    ent_coef        | 0.27     |
|    ent_coef_loss   | -0.0867  |
|    learning_rate   | 0.0003   |
|    n_updates       | 488899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 585      |
|    critic_loss     | 374      |
|    ent_coef        | 0.274    |
|    ent_coef_loss   | -0.0576  |
|    learning_rate   | 0.0003   |
|    n_updates       | 489299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 585      |
|    critic_loss     | 377      |
|    ent_coef        | 0.27     |
|    ent_coef_loss   | 0.0342   |
|    learning_rate   | 0.0003   |
|    n_updates       | 489699   |
---------------------------------
=== Iterazione IRL 350 ===
Loss reward (iter 350): 4.1179633140563965
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 579      |
|    critic_loss     | 360      |
|    ent_coef        | 0.264    |
|    ent_coef_loss   | 0.331    |
|    learning_rate   | 0.0003   |
|    n_updates       | 490299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 580      |
|    critic_loss     | 313      |
|    ent_coef        | 0.266    |
|    ent_coef_loss   | 0.157    |
|    learning_rate   | 0.0003   |
|    n_updates       | 490699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 578      |
|    critic_loss     | 299      |
|    ent_coef        | 0.271    |
|    ent_coef_loss   | -0.00378 |
|    learning_rate   | 0.0003   |
|    n_updates       | 491099   |
---------------------------------
=== Iterazione IRL 351 ===
Loss reward (iter 351): 4.6854963302612305
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 592      |
|    critic_loss     | 318      |
|    ent_coef        | 0.269    |
|    ent_coef_loss   | 0.127    |
|    learning_rate   | 0.0003   |
|    n_updates       | 491699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 582      |
|    critic_loss     | 328      |
|    ent_coef        | 0.278    |
|    ent_coef_loss   | -0.22    |
|    learning_rate   | 0.0003   |
|    n_updates       | 492099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 596      |
|    critic_loss     | 389      |
|    ent_coef        | 0.274    |
|    ent_coef_loss   | 0.0839   |
|    learning_rate   | 0.0003   |
|    n_updates       | 492499   |
---------------------------------
=== Iterazione IRL 352 ===
Loss reward (iter 352): 4.416955471038818
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 597      |
|    critic_loss     | 376      |
|    ent_coef        | 0.28     |
|    ent_coef_loss   | -0.113   |
|    learning_rate   | 0.0003   |
|    n_updates       | 493099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 580      |
|    critic_loss     | 292      |
|    ent_coef        | 0.285    |
|    ent_coef_loss   | 0.323    |
|    learning_rate   | 0.0003   |
|    n_updates       | 493499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 593      |
|    critic_loss     | 293      |
|    ent_coef        | 0.295    |
|    ent_coef_loss   | -0.157   |
|    learning_rate   | 0.0003   |
|    n_updates       | 493899   |
---------------------------------
=== Iterazione IRL 353 ===
Loss reward (iter 353): 5.215303897857666
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 581      |
|    critic_loss     | 335      |
|    ent_coef        | 0.282    |
|    ent_coef_loss   | -0.35    |
|    learning_rate   | 0.0003   |
|    n_updates       | 494499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 584      |
|    critic_loss     | 297      |
|    ent_coef        | 0.283    |
|    ent_coef_loss   | -0.349   |
|    learning_rate   | 0.0003   |
|    n_updates       | 494899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 585      |
|    critic_loss     | 341      |
|    ent_coef        | 0.292    |
|    ent_coef_loss   | -0.144   |
|    learning_rate   | 0.0003   |
|    n_updates       | 495299   |
---------------------------------
=== Iterazione IRL 354 ===
Loss reward (iter 354): 3.978001356124878
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 592      |
|    critic_loss     | 411      |
|    ent_coef        | 0.291    |
|    ent_coef_loss   | -0.129   |
|    learning_rate   | 0.0003   |
|    n_updates       | 495899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 588      |
|    critic_loss     | 374      |
|    ent_coef        | 0.283    |
|    ent_coef_loss   | -0.0861  |
|    learning_rate   | 0.0003   |
|    n_updates       | 496299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 592      |
|    critic_loss     | 336      |
|    ent_coef        | 0.287    |
|    ent_coef_loss   | 0.267    |
|    learning_rate   | 0.0003   |
|    n_updates       | 496699   |
---------------------------------
=== Iterazione IRL 355 ===
Loss reward (iter 355): 4.471024990081787
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 601      |
|    critic_loss     | 331      |
|    ent_coef        | 0.286    |
|    ent_coef_loss   | 0.0632   |
|    learning_rate   | 0.0003   |
|    n_updates       | 497299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 578      |
|    critic_loss     | 325      |
|    ent_coef        | 0.29     |
|    ent_coef_loss   | -0.323   |
|    learning_rate   | 0.0003   |
|    n_updates       | 497699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 588      |
|    critic_loss     | 323      |
|    ent_coef        | 0.289    |
|    ent_coef_loss   | -0.431   |
|    learning_rate   | 0.0003   |
|    n_updates       | 498099   |
---------------------------------
=== Iterazione IRL 356 ===
Loss reward (iter 356): 4.07068395614624
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 590      |
|    critic_loss     | 334      |
|    ent_coef        | 0.286    |
|    ent_coef_loss   | -0.106   |
|    learning_rate   | 0.0003   |
|    n_updates       | 498699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 615      |
|    critic_loss     | 365      |
|    ent_coef        | 0.286    |
|    ent_coef_loss   | 0.328    |
|    learning_rate   | 0.0003   |
|    n_updates       | 499099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 597      |
|    critic_loss     | 465      |
|    ent_coef        | 0.288    |
|    ent_coef_loss   | 0.00893  |
|    learning_rate   | 0.0003   |
|    n_updates       | 499499   |
---------------------------------
=== Iterazione IRL 357 ===
Loss reward (iter 357): 3.819547176361084
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 599      |
|    critic_loss     | 391      |
|    ent_coef        | 0.29     |
|    ent_coef_loss   | -0.0399  |
|    learning_rate   | 0.0003   |
|    n_updates       | 500099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 606      |
|    critic_loss     | 364      |
|    ent_coef        | 0.289    |
|    ent_coef_loss   | -0.114   |
|    learning_rate   | 0.0003   |
|    n_updates       | 500499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 599      |
|    critic_loss     | 399      |
|    ent_coef        | 0.278    |
|    ent_coef_loss   | -0.102   |
|    learning_rate   | 0.0003   |
|    n_updates       | 500899   |
---------------------------------
=== Iterazione IRL 358 ===
Loss reward (iter 358): 4.27129602432251
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 620      |
|    critic_loss     | 385      |
|    ent_coef        | 0.274    |
|    ent_coef_loss   | -0.133   |
|    learning_rate   | 0.0003   |
|    n_updates       | 501499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 604      |
|    critic_loss     | 358      |
|    ent_coef        | 0.268    |
|    ent_coef_loss   | -0.388   |
|    learning_rate   | 0.0003   |
|    n_updates       | 501899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 608      |
|    critic_loss     | 375      |
|    ent_coef        | 0.274    |
|    ent_coef_loss   | 0.0743   |
|    learning_rate   | 0.0003   |
|    n_updates       | 502299   |
---------------------------------
=== Iterazione IRL 359 ===
Loss reward (iter 359): 2.4378490447998047
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 612      |
|    critic_loss     | 340      |
|    ent_coef        | 0.268    |
|    ent_coef_loss   | -0.584   |
|    learning_rate   | 0.0003   |
|    n_updates       | 502899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 611      |
|    critic_loss     | 393      |
|    ent_coef        | 0.275    |
|    ent_coef_loss   | 0.219    |
|    learning_rate   | 0.0003   |
|    n_updates       | 503299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 609      |
|    critic_loss     | 406      |
|    ent_coef        | 0.284    |
|    ent_coef_loss   | 0.0517   |
|    learning_rate   | 0.0003   |
|    n_updates       | 503699   |
---------------------------------
=== Iterazione IRL 360 ===
Loss reward (iter 360): 3.9083921909332275
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 611      |
|    critic_loss     | 334      |
|    ent_coef        | 0.286    |
|    ent_coef_loss   | 0.0864   |
|    learning_rate   | 0.0003   |
|    n_updates       | 504299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 611      |
|    critic_loss     | 453      |
|    ent_coef        | 0.28     |
|    ent_coef_loss   | 0.38     |
|    learning_rate   | 0.0003   |
|    n_updates       | 504699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 622      |
|    critic_loss     | 465      |
|    ent_coef        | 0.284    |
|    ent_coef_loss   | -0.476   |
|    learning_rate   | 0.0003   |
|    n_updates       | 505099   |
---------------------------------
=== Iterazione IRL 361 ===
Loss reward (iter 361): 2.8484394550323486
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 625      |
|    critic_loss     | 469      |
|    ent_coef        | 0.286    |
|    ent_coef_loss   | 0.0256   |
|    learning_rate   | 0.0003   |
|    n_updates       | 505699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 616      |
|    critic_loss     | 431      |
|    ent_coef        | 0.279    |
|    ent_coef_loss   | 0.11     |
|    learning_rate   | 0.0003   |
|    n_updates       | 506099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 636      |
|    critic_loss     | 488      |
|    ent_coef        | 0.284    |
|    ent_coef_loss   | 0.348    |
|    learning_rate   | 0.0003   |
|    n_updates       | 506499   |
---------------------------------
=== Iterazione IRL 362 ===
Loss reward (iter 362): 3.350942611694336
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 632      |
|    critic_loss     | 512      |
|    ent_coef        | 0.282    |
|    ent_coef_loss   | -0.0842  |
|    learning_rate   | 0.0003   |
|    n_updates       | 507099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 626      |
|    critic_loss     | 479      |
|    ent_coef        | 0.287    |
|    ent_coef_loss   | -0.529   |
|    learning_rate   | 0.0003   |
|    n_updates       | 507499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 631      |
|    critic_loss     | 490      |
|    ent_coef        | 0.285    |
|    ent_coef_loss   | 0.18     |
|    learning_rate   | 0.0003   |
|    n_updates       | 507899   |
---------------------------------
=== Iterazione IRL 363 ===
Loss reward (iter 363): 3.4648642539978027
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 622      |
|    critic_loss     | 330      |
|    ent_coef        | 0.283    |
|    ent_coef_loss   | -0.151   |
|    learning_rate   | 0.0003   |
|    n_updates       | 508499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 631      |
|    critic_loss     | 489      |
|    ent_coef        | 0.279    |
|    ent_coef_loss   | 0.14     |
|    learning_rate   | 0.0003   |
|    n_updates       | 508899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 623      |
|    critic_loss     | 460      |
|    ent_coef        | 0.28     |
|    ent_coef_loss   | -0.0333  |
|    learning_rate   | 0.0003   |
|    n_updates       | 509299   |
---------------------------------
=== Iterazione IRL 364 ===
Loss reward (iter 364): 3.0354819297790527
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 634      |
|    critic_loss     | 522      |
|    ent_coef        | 0.285    |
|    ent_coef_loss   | 0.507    |
|    learning_rate   | 0.0003   |
|    n_updates       | 509899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 630      |
|    critic_loss     | 419      |
|    ent_coef        | 0.294    |
|    ent_coef_loss   | -0.189   |
|    learning_rate   | 0.0003   |
|    n_updates       | 510299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 643      |
|    critic_loss     | 548      |
|    ent_coef        | 0.291    |
|    ent_coef_loss   | 0.0979   |
|    learning_rate   | 0.0003   |
|    n_updates       | 510699   |
---------------------------------
=== Iterazione IRL 365 ===
Loss reward (iter 365): 3.5055017471313477
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 641      |
|    critic_loss     | 396      |
|    ent_coef        | 0.29     |
|    ent_coef_loss   | -0.497   |
|    learning_rate   | 0.0003   |
|    n_updates       | 511299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 625      |
|    critic_loss     | 441      |
|    ent_coef        | 0.295    |
|    ent_coef_loss   | 0.0224   |
|    learning_rate   | 0.0003   |
|    n_updates       | 511699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 636      |
|    critic_loss     | 411      |
|    ent_coef        | 0.286    |
|    ent_coef_loss   | 0.19     |
|    learning_rate   | 0.0003   |
|    n_updates       | 512099   |
---------------------------------
=== Iterazione IRL 366 ===
Loss reward (iter 366): 0.8317992687225342
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 637      |
|    critic_loss     | 385      |
|    ent_coef        | 0.288    |
|    ent_coef_loss   | 0.199    |
|    learning_rate   | 0.0003   |
|    n_updates       | 512699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 633      |
|    critic_loss     | 340      |
|    ent_coef        | 0.287    |
|    ent_coef_loss   | 0.278    |
|    learning_rate   | 0.0003   |
|    n_updates       | 513099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 631      |
|    critic_loss     | 403      |
|    ent_coef        | 0.283    |
|    ent_coef_loss   | -0.0692  |
|    learning_rate   | 0.0003   |
|    n_updates       | 513499   |
---------------------------------
=== Iterazione IRL 367 ===
Loss reward (iter 367): 4.004737854003906
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 646      |
|    critic_loss     | 421      |
|    ent_coef        | 0.286    |
|    ent_coef_loss   | 0.368    |
|    learning_rate   | 0.0003   |
|    n_updates       | 514099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 639      |
|    critic_loss     | 396      |
|    ent_coef        | 0.29     |
|    ent_coef_loss   | -0.047   |
|    learning_rate   | 0.0003   |
|    n_updates       | 514499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 648      |
|    critic_loss     | 497      |
|    ent_coef        | 0.29     |
|    ent_coef_loss   | -0.203   |
|    learning_rate   | 0.0003   |
|    n_updates       | 514899   |
---------------------------------
=== Iterazione IRL 368 ===
Loss reward (iter 368): 3.787515163421631
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 631      |
|    critic_loss     | 374      |
|    ent_coef        | 0.286    |
|    ent_coef_loss   | -0.759   |
|    learning_rate   | 0.0003   |
|    n_updates       | 515499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 635      |
|    critic_loss     | 346      |
|    ent_coef        | 0.277    |
|    ent_coef_loss   | 0.0284   |
|    learning_rate   | 0.0003   |
|    n_updates       | 515899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 637      |
|    critic_loss     | 449      |
|    ent_coef        | 0.271    |
|    ent_coef_loss   | -0.562   |
|    learning_rate   | 0.0003   |
|    n_updates       | 516299   |
---------------------------------
=== Iterazione IRL 369 ===
Loss reward (iter 369): 4.23486328125
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | 647       |
|    critic_loss     | 409       |
|    ent_coef        | 0.28      |
|    ent_coef_loss   | -8.29e-05 |
|    learning_rate   | 0.0003    |
|    n_updates       | 516899    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 633      |
|    critic_loss     | 411      |
|    ent_coef        | 0.281    |
|    ent_coef_loss   | 0.0402   |
|    learning_rate   | 0.0003   |
|    n_updates       | 517299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 645      |
|    critic_loss     | 455      |
|    ent_coef        | 0.287    |
|    ent_coef_loss   | 0.301    |
|    learning_rate   | 0.0003   |
|    n_updates       | 517699   |
---------------------------------
=== Iterazione IRL 370 ===
Loss reward (iter 370): 1.2262296676635742
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 653      |
|    critic_loss     | 405      |
|    ent_coef        | 0.28     |
|    ent_coef_loss   | 0.0905   |
|    learning_rate   | 0.0003   |
|    n_updates       | 518299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 656      |
|    critic_loss     | 407      |
|    ent_coef        | 0.282    |
|    ent_coef_loss   | -0.0764  |
|    learning_rate   | 0.0003   |
|    n_updates       | 518699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 647      |
|    critic_loss     | 431      |
|    ent_coef        | 0.281    |
|    ent_coef_loss   | -0.418   |
|    learning_rate   | 0.0003   |
|    n_updates       | 519099   |
---------------------------------
=== Iterazione IRL 371 ===
Loss reward (iter 371): 4.239975929260254
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 642      |
|    critic_loss     | 383      |
|    ent_coef        | 0.269    |
|    ent_coef_loss   | 0.16     |
|    learning_rate   | 0.0003   |
|    n_updates       | 519699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 675      |
|    critic_loss     | 526      |
|    ent_coef        | 0.269    |
|    ent_coef_loss   | 0.264    |
|    learning_rate   | 0.0003   |
|    n_updates       | 520099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 659      |
|    critic_loss     | 396      |
|    ent_coef        | 0.272    |
|    ent_coef_loss   | 0.123    |
|    learning_rate   | 0.0003   |
|    n_updates       | 520499   |
---------------------------------
=== Iterazione IRL 372 ===
Loss reward (iter 372): 3.7829463481903076
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 656      |
|    critic_loss     | 436      |
|    ent_coef        | 0.279    |
|    ent_coef_loss   | 0.086    |
|    learning_rate   | 0.0003   |
|    n_updates       | 521099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 654      |
|    critic_loss     | 391      |
|    ent_coef        | 0.275    |
|    ent_coef_loss   | 0.124    |
|    learning_rate   | 0.0003   |
|    n_updates       | 521499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 654      |
|    critic_loss     | 429      |
|    ent_coef        | 0.272    |
|    ent_coef_loss   | 0.211    |
|    learning_rate   | 0.0003   |
|    n_updates       | 521899   |
---------------------------------
=== Iterazione IRL 373 ===
Loss reward (iter 373): 4.297946453094482
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 659      |
|    critic_loss     | 382      |
|    ent_coef        | 0.278    |
|    ent_coef_loss   | 0.602    |
|    learning_rate   | 0.0003   |
|    n_updates       | 522499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 666      |
|    critic_loss     | 404      |
|    ent_coef        | 0.281    |
|    ent_coef_loss   | 0.181    |
|    learning_rate   | 0.0003   |
|    n_updates       | 522899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 660      |
|    critic_loss     | 401      |
|    ent_coef        | 0.278    |
|    ent_coef_loss   | 0.33     |
|    learning_rate   | 0.0003   |
|    n_updates       | 523299   |
---------------------------------
=== Iterazione IRL 374 ===
Loss reward (iter 374): 7.023612022399902
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 662      |
|    critic_loss     | 384      |
|    ent_coef        | 0.268    |
|    ent_coef_loss   | -0.253   |
|    learning_rate   | 0.0003   |
|    n_updates       | 523899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 670      |
|    critic_loss     | 416      |
|    ent_coef        | 0.266    |
|    ent_coef_loss   | -0.143   |
|    learning_rate   | 0.0003   |
|    n_updates       | 524299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 656      |
|    critic_loss     | 413      |
|    ent_coef        | 0.274    |
|    ent_coef_loss   | 0.0928   |
|    learning_rate   | 0.0003   |
|    n_updates       | 524699   |
---------------------------------
=== Iterazione IRL 375 ===
Loss reward (iter 375): 4.957883358001709
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 665      |
|    critic_loss     | 480      |
|    ent_coef        | 0.272    |
|    ent_coef_loss   | -0.353   |
|    learning_rate   | 0.0003   |
|    n_updates       | 525299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 655      |
|    critic_loss     | 389      |
|    ent_coef        | 0.268    |
|    ent_coef_loss   | 0.105    |
|    learning_rate   | 0.0003   |
|    n_updates       | 525699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 675      |
|    critic_loss     | 507      |
|    ent_coef        | 0.274    |
|    ent_coef_loss   | 0.206    |
|    learning_rate   | 0.0003   |
|    n_updates       | 526099   |
---------------------------------
=== Iterazione IRL 376 ===
Loss reward (iter 376): 5.286698341369629
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 666      |
|    critic_loss     | 440      |
|    ent_coef        | 0.28     |
|    ent_coef_loss   | 0.236    |
|    learning_rate   | 0.0003   |
|    n_updates       | 526699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 663      |
|    critic_loss     | 369      |
|    ent_coef        | 0.274    |
|    ent_coef_loss   | 0.0212   |
|    learning_rate   | 0.0003   |
|    n_updates       | 527099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 669      |
|    critic_loss     | 379      |
|    ent_coef        | 0.279    |
|    ent_coef_loss   | 0.0269   |
|    learning_rate   | 0.0003   |
|    n_updates       | 527499   |
---------------------------------
=== Iterazione IRL 377 ===
Loss reward (iter 377): 5.001432418823242
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 655      |
|    critic_loss     | 393      |
|    ent_coef        | 0.273    |
|    ent_coef_loss   | 0.134    |
|    learning_rate   | 0.0003   |
|    n_updates       | 528099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 671      |
|    critic_loss     | 367      |
|    ent_coef        | 0.278    |
|    ent_coef_loss   | -0.186   |
|    learning_rate   | 0.0003   |
|    n_updates       | 528499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 670      |
|    critic_loss     | 379      |
|    ent_coef        | 0.28     |
|    ent_coef_loss   | -0.158   |
|    learning_rate   | 0.0003   |
|    n_updates       | 528899   |
---------------------------------
=== Iterazione IRL 378 ===
Loss reward (iter 378): 4.932106018066406
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 680      |
|    critic_loss     | 420      |
|    ent_coef        | 0.277    |
|    ent_coef_loss   | -0.0431  |
|    learning_rate   | 0.0003   |
|    n_updates       | 529499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 659      |
|    critic_loss     | 376      |
|    ent_coef        | 0.281    |
|    ent_coef_loss   | -0.0428  |
|    learning_rate   | 0.0003   |
|    n_updates       | 529899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 666      |
|    critic_loss     | 500      |
|    ent_coef        | 0.28     |
|    ent_coef_loss   | -0.0272  |
|    learning_rate   | 0.0003   |
|    n_updates       | 530299   |
---------------------------------
=== Iterazione IRL 379 ===
Loss reward (iter 379): 4.011574745178223
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 665      |
|    critic_loss     | 392      |
|    ent_coef        | 0.285    |
|    ent_coef_loss   | 0.194    |
|    learning_rate   | 0.0003   |
|    n_updates       | 530899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 669      |
|    critic_loss     | 453      |
|    ent_coef        | 0.291    |
|    ent_coef_loss   | -0.0721  |
|    learning_rate   | 0.0003   |
|    n_updates       | 531299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 671      |
|    critic_loss     | 354      |
|    ent_coef        | 0.287    |
|    ent_coef_loss   | -0.102   |
|    learning_rate   | 0.0003   |
|    n_updates       | 531699   |
---------------------------------
=== Iterazione IRL 380 ===
Loss reward (iter 380): 5.796081066131592
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 668      |
|    critic_loss     | 494      |
|    ent_coef        | 0.294    |
|    ent_coef_loss   | 0.108    |
|    learning_rate   | 0.0003   |
|    n_updates       | 532299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 675      |
|    critic_loss     | 401      |
|    ent_coef        | 0.295    |
|    ent_coef_loss   | -0.375   |
|    learning_rate   | 0.0003   |
|    n_updates       | 532699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 687      |
|    critic_loss     | 449      |
|    ent_coef        | 0.285    |
|    ent_coef_loss   | 0.422    |
|    learning_rate   | 0.0003   |
|    n_updates       | 533099   |
---------------------------------
=== Iterazione IRL 381 ===
Loss reward (iter 381): 4.716253280639648
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 684      |
|    critic_loss     | 565      |
|    ent_coef        | 0.289    |
|    ent_coef_loss   | -0.0894  |
|    learning_rate   | 0.0003   |
|    n_updates       | 533699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 682      |
|    critic_loss     | 439      |
|    ent_coef        | 0.286    |
|    ent_coef_loss   | 0.26     |
|    learning_rate   | 0.0003   |
|    n_updates       | 534099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 676      |
|    critic_loss     | 386      |
|    ent_coef        | 0.284    |
|    ent_coef_loss   | -0.0595  |
|    learning_rate   | 0.0003   |
|    n_updates       | 534499   |
---------------------------------
=== Iterazione IRL 382 ===
Loss reward (iter 382): 4.526981830596924
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 665      |
|    critic_loss     | 373      |
|    ent_coef        | 0.286    |
|    ent_coef_loss   | -0.523   |
|    learning_rate   | 0.0003   |
|    n_updates       | 535099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 682      |
|    critic_loss     | 490      |
|    ent_coef        | 0.285    |
|    ent_coef_loss   | -0.619   |
|    learning_rate   | 0.0003   |
|    n_updates       | 535499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 677      |
|    critic_loss     | 518      |
|    ent_coef        | 0.289    |
|    ent_coef_loss   | -0.191   |
|    learning_rate   | 0.0003   |
|    n_updates       | 535899   |
---------------------------------
=== Iterazione IRL 383 ===
Loss reward (iter 383): 4.375011444091797
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 667      |
|    critic_loss     | 430      |
|    ent_coef        | 0.292    |
|    ent_coef_loss   | -0.177   |
|    learning_rate   | 0.0003   |
|    n_updates       | 536499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 696      |
|    critic_loss     | 573      |
|    ent_coef        | 0.291    |
|    ent_coef_loss   | 0.047    |
|    learning_rate   | 0.0003   |
|    n_updates       | 536899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 685      |
|    critic_loss     | 450      |
|    ent_coef        | 0.293    |
|    ent_coef_loss   | 0.0463   |
|    learning_rate   | 0.0003   |
|    n_updates       | 537299   |
---------------------------------
=== Iterazione IRL 384 ===
Loss reward (iter 384): 4.756243705749512
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 692      |
|    critic_loss     | 490      |
|    ent_coef        | 0.287    |
|    ent_coef_loss   | -0.0908  |
|    learning_rate   | 0.0003   |
|    n_updates       | 537899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 688      |
|    critic_loss     | 423      |
|    ent_coef        | 0.287    |
|    ent_coef_loss   | -0.44    |
|    learning_rate   | 0.0003   |
|    n_updates       | 538299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 676      |
|    critic_loss     | 432      |
|    ent_coef        | 0.281    |
|    ent_coef_loss   | 0.336    |
|    learning_rate   | 0.0003   |
|    n_updates       | 538699   |
---------------------------------
=== Iterazione IRL 385 ===
Loss reward (iter 385): 4.680933475494385
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 695      |
|    critic_loss     | 461      |
|    ent_coef        | 0.289    |
|    ent_coef_loss   | 0.157    |
|    learning_rate   | 0.0003   |
|    n_updates       | 539299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 683      |
|    critic_loss     | 445      |
|    ent_coef        | 0.283    |
|    ent_coef_loss   | -0.331   |
|    learning_rate   | 0.0003   |
|    n_updates       | 539699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 689      |
|    critic_loss     | 484      |
|    ent_coef        | 0.281    |
|    ent_coef_loss   | 0.228    |
|    learning_rate   | 0.0003   |
|    n_updates       | 540099   |
---------------------------------
=== Iterazione IRL 386 ===
Loss reward (iter 386): 3.726137399673462
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 685      |
|    critic_loss     | 427      |
|    ent_coef        | 0.283    |
|    ent_coef_loss   | 0.0565   |
|    learning_rate   | 0.0003   |
|    n_updates       | 540699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 696      |
|    critic_loss     | 473      |
|    ent_coef        | 0.282    |
|    ent_coef_loss   | -0.25    |
|    learning_rate   | 0.0003   |
|    n_updates       | 541099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 698      |
|    critic_loss     | 708      |
|    ent_coef        | 0.287    |
|    ent_coef_loss   | -0.156   |
|    learning_rate   | 0.0003   |
|    n_updates       | 541499   |
---------------------------------
=== Iterazione IRL 387 ===
Loss reward (iter 387): 4.500095367431641
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 685      |
|    critic_loss     | 368      |
|    ent_coef        | 0.283    |
|    ent_coef_loss   | 0.745    |
|    learning_rate   | 0.0003   |
|    n_updates       | 542099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 690      |
|    critic_loss     | 455      |
|    ent_coef        | 0.291    |
|    ent_coef_loss   | 0.00112  |
|    learning_rate   | 0.0003   |
|    n_updates       | 542499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 673      |
|    critic_loss     | 406      |
|    ent_coef        | 0.286    |
|    ent_coef_loss   | 0.0726   |
|    learning_rate   | 0.0003   |
|    n_updates       | 542899   |
---------------------------------
=== Iterazione IRL 388 ===
Loss reward (iter 388): 5.296214580535889
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 677      |
|    critic_loss     | 389      |
|    ent_coef        | 0.293    |
|    ent_coef_loss   | 0.167    |
|    learning_rate   | 0.0003   |
|    n_updates       | 543499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 700      |
|    critic_loss     | 414      |
|    ent_coef        | 0.284    |
|    ent_coef_loss   | 0.102    |
|    learning_rate   | 0.0003   |
|    n_updates       | 543899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 685      |
|    critic_loss     | 458      |
|    ent_coef        | 0.283    |
|    ent_coef_loss   | -0.357   |
|    learning_rate   | 0.0003   |
|    n_updates       | 544299   |
---------------------------------
=== Iterazione IRL 389 ===
Loss reward (iter 389): 3.8966245651245117
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 699      |
|    critic_loss     | 457      |
|    ent_coef        | 0.275    |
|    ent_coef_loss   | 0.308    |
|    learning_rate   | 0.0003   |
|    n_updates       | 544899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 707      |
|    critic_loss     | 594      |
|    ent_coef        | 0.279    |
|    ent_coef_loss   | 0.123    |
|    learning_rate   | 0.0003   |
|    n_updates       | 545299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 705      |
|    critic_loss     | 490      |
|    ent_coef        | 0.287    |
|    ent_coef_loss   | 0.279    |
|    learning_rate   | 0.0003   |
|    n_updates       | 545699   |
---------------------------------
=== Iterazione IRL 390 ===
Loss reward (iter 390): 4.223464012145996
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 696      |
|    critic_loss     | 497      |
|    ent_coef        | 0.284    |
|    ent_coef_loss   | -0.391   |
|    learning_rate   | 0.0003   |
|    n_updates       | 546299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 688      |
|    critic_loss     | 552      |
|    ent_coef        | 0.281    |
|    ent_coef_loss   | -0.151   |
|    learning_rate   | 0.0003   |
|    n_updates       | 546699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 701      |
|    critic_loss     | 470      |
|    ent_coef        | 0.278    |
|    ent_coef_loss   | -0.428   |
|    learning_rate   | 0.0003   |
|    n_updates       | 547099   |
---------------------------------
=== Iterazione IRL 391 ===
Loss reward (iter 391): 4.206547737121582
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 724      |
|    critic_loss     | 686      |
|    ent_coef        | 0.294    |
|    ent_coef_loss   | 0.381    |
|    learning_rate   | 0.0003   |
|    n_updates       | 547699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 691      |
|    critic_loss     | 484      |
|    ent_coef        | 0.288    |
|    ent_coef_loss   | -0.0826  |
|    learning_rate   | 0.0003   |
|    n_updates       | 548099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 699      |
|    critic_loss     | 459      |
|    ent_coef        | 0.28     |
|    ent_coef_loss   | -0.467   |
|    learning_rate   | 0.0003   |
|    n_updates       | 548499   |
---------------------------------
=== Iterazione IRL 392 ===
Loss reward (iter 392): 4.5566935539245605
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 701      |
|    critic_loss     | 546      |
|    ent_coef        | 0.281    |
|    ent_coef_loss   | 0.238    |
|    learning_rate   | 0.0003   |
|    n_updates       | 549099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 719      |
|    critic_loss     | 462      |
|    ent_coef        | 0.287    |
|    ent_coef_loss   | 0.759    |
|    learning_rate   | 0.0003   |
|    n_updates       | 549499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 712      |
|    critic_loss     | 672      |
|    ent_coef        | 0.296    |
|    ent_coef_loss   | -0.0425  |
|    learning_rate   | 0.0003   |
|    n_updates       | 549899   |
---------------------------------
=== Iterazione IRL 393 ===
Loss reward (iter 393): 4.242688179016113
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 697      |
|    critic_loss     | 532      |
|    ent_coef        | 0.292    |
|    ent_coef_loss   | 0.381    |
|    learning_rate   | 0.0003   |
|    n_updates       | 550499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 704      |
|    critic_loss     | 468      |
|    ent_coef        | 0.293    |
|    ent_coef_loss   | -0.143   |
|    learning_rate   | 0.0003   |
|    n_updates       | 550899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 692      |
|    critic_loss     | 507      |
|    ent_coef        | 0.294    |
|    ent_coef_loss   | -0.36    |
|    learning_rate   | 0.0003   |
|    n_updates       | 551299   |
---------------------------------
=== Iterazione IRL 394 ===
Loss reward (iter 394): 4.388130187988281
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 719      |
|    critic_loss     | 593      |
|    ent_coef        | 0.282    |
|    ent_coef_loss   | 0.11     |
|    learning_rate   | 0.0003   |
|    n_updates       | 551899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 703      |
|    critic_loss     | 613      |
|    ent_coef        | 0.292    |
|    ent_coef_loss   | -0.164   |
|    learning_rate   | 0.0003   |
|    n_updates       | 552299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 701      |
|    critic_loss     | 556      |
|    ent_coef        | 0.299    |
|    ent_coef_loss   | -0.557   |
|    learning_rate   | 0.0003   |
|    n_updates       | 552699   |
---------------------------------
=== Iterazione IRL 395 ===
Loss reward (iter 395): 3.601933717727661
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 700      |
|    critic_loss     | 536      |
|    ent_coef        | 0.289    |
|    ent_coef_loss   | 0.0618   |
|    learning_rate   | 0.0003   |
|    n_updates       | 553299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 701      |
|    critic_loss     | 409      |
|    ent_coef        | 0.293    |
|    ent_coef_loss   | -0.217   |
|    learning_rate   | 0.0003   |
|    n_updates       | 553699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 715      |
|    critic_loss     | 453      |
|    ent_coef        | 0.296    |
|    ent_coef_loss   | 0.167    |
|    learning_rate   | 0.0003   |
|    n_updates       | 554099   |
---------------------------------
=== Iterazione IRL 396 ===
Loss reward (iter 396): 2.838876724243164
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 694      |
|    critic_loss     | 666      |
|    ent_coef        | 0.317    |
|    ent_coef_loss   | 0.0506   |
|    learning_rate   | 0.0003   |
|    n_updates       | 554699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 717      |
|    critic_loss     | 506      |
|    ent_coef        | 0.325    |
|    ent_coef_loss   | 0.0624   |
|    learning_rate   | 0.0003   |
|    n_updates       | 555099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 703      |
|    critic_loss     | 466      |
|    ent_coef        | 0.35     |
|    ent_coef_loss   | -0.41    |
|    learning_rate   | 0.0003   |
|    n_updates       | 555499   |
---------------------------------
=== Iterazione IRL 397 ===
Loss reward (iter 397): 2.9905879497528076
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 695      |
|    critic_loss     | 644      |
|    ent_coef        | 0.389    |
|    ent_coef_loss   | 0.345    |
|    learning_rate   | 0.0003   |
|    n_updates       | 556099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 693      |
|    critic_loss     | 559      |
|    ent_coef        | 0.408    |
|    ent_coef_loss   | 0.033    |
|    learning_rate   | 0.0003   |
|    n_updates       | 556499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 726      |
|    critic_loss     | 664      |
|    ent_coef        | 0.413    |
|    ent_coef_loss   | 0.0891   |
|    learning_rate   | 0.0003   |
|    n_updates       | 556899   |
---------------------------------
=== Iterazione IRL 398 ===
Loss reward (iter 398): 3.023970365524292
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 708      |
|    critic_loss     | 726      |
|    ent_coef        | 0.392    |
|    ent_coef_loss   | 0.00853  |
|    learning_rate   | 0.0003   |
|    n_updates       | 557499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 713      |
|    critic_loss     | 798      |
|    ent_coef        | 0.373    |
|    ent_coef_loss   | -0.0726  |
|    learning_rate   | 0.0003   |
|    n_updates       | 557899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 708      |
|    critic_loss     | 619      |
|    ent_coef        | 0.359    |
|    ent_coef_loss   | 0.45     |
|    learning_rate   | 0.0003   |
|    n_updates       | 558299   |
---------------------------------
=== Iterazione IRL 399 ===
Loss reward (iter 399): 1.9700987339019775
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 706      |
|    critic_loss     | 587      |
|    ent_coef        | 0.352    |
|    ent_coef_loss   | 0.222    |
|    learning_rate   | 0.0003   |
|    n_updates       | 558899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 702      |
|    critic_loss     | 551      |
|    ent_coef        | 0.341    |
|    ent_coef_loss   | -0.194   |
|    learning_rate   | 0.0003   |
|    n_updates       | 559299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 715      |
|    critic_loss     | 597      |
|    ent_coef        | 0.321    |
|    ent_coef_loss   | -0.288   |
|    learning_rate   | 0.0003   |
|    n_updates       | 559699   |
---------------------------------
=== Iterazione IRL 400 ===
Loss reward (iter 400): 2.113064765930176
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 701      |
|    critic_loss     | 617      |
|    ent_coef        | 0.318    |
|    ent_coef_loss   | 0.092    |
|    learning_rate   | 0.0003   |
|    n_updates       | 560299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 709      |
|    critic_loss     | 517      |
|    ent_coef        | 0.313    |
|    ent_coef_loss   | -0.0364  |
|    learning_rate   | 0.0003   |
|    n_updates       | 560699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 715      |
|    critic_loss     | 578      |
|    ent_coef        | 0.303    |
|    ent_coef_loss   | -0.0376  |
|    learning_rate   | 0.0003   |
|    n_updates       | 561099   |
---------------------------------
=== Iterazione IRL 401 ===
Loss reward (iter 401): 4.216094970703125
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 709      |
|    critic_loss     | 576      |
|    ent_coef        | 0.298    |
|    ent_coef_loss   | 0.218    |
|    learning_rate   | 0.0003   |
|    n_updates       | 561699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 716      |
|    critic_loss     | 769      |
|    ent_coef        | 0.295    |
|    ent_coef_loss   | 0.0518   |
|    learning_rate   | 0.0003   |
|    n_updates       | 562099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 718      |
|    critic_loss     | 612      |
|    ent_coef        | 0.299    |
|    ent_coef_loss   | -0.197   |
|    learning_rate   | 0.0003   |
|    n_updates       | 562499   |
---------------------------------
=== Iterazione IRL 402 ===
Loss reward (iter 402): 5.677557468414307
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 717      |
|    critic_loss     | 674      |
|    ent_coef        | 0.309    |
|    ent_coef_loss   | 0.159    |
|    learning_rate   | 0.0003   |
|    n_updates       | 563099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 726      |
|    critic_loss     | 677      |
|    ent_coef        | 0.306    |
|    ent_coef_loss   | -0.0693  |
|    learning_rate   | 0.0003   |
|    n_updates       | 563499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 714      |
|    critic_loss     | 601      |
|    ent_coef        | 0.316    |
|    ent_coef_loss   | -0.126   |
|    learning_rate   | 0.0003   |
|    n_updates       | 563899   |
---------------------------------
=== Iterazione IRL 403 ===
Loss reward (iter 403): 5.38308048248291
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 725      |
|    critic_loss     | 526      |
|    ent_coef        | 0.313    |
|    ent_coef_loss   | -0.0172  |
|    learning_rate   | 0.0003   |
|    n_updates       | 564499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 706      |
|    critic_loss     | 628      |
|    ent_coef        | 0.317    |
|    ent_coef_loss   | 0.149    |
|    learning_rate   | 0.0003   |
|    n_updates       | 564899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 726      |
|    critic_loss     | 629      |
|    ent_coef        | 0.331    |
|    ent_coef_loss   | -0.071   |
|    learning_rate   | 0.0003   |
|    n_updates       | 565299   |
---------------------------------
=== Iterazione IRL 404 ===
Loss reward (iter 404): 4.766838073730469
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 740      |
|    critic_loss     | 772      |
|    ent_coef        | 0.321    |
|    ent_coef_loss   | -0.262   |
|    learning_rate   | 0.0003   |
|    n_updates       | 565899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 741      |
|    critic_loss     | 731      |
|    ent_coef        | 0.322    |
|    ent_coef_loss   | 0.317    |
|    learning_rate   | 0.0003   |
|    n_updates       | 566299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 718      |
|    critic_loss     | 579      |
|    ent_coef        | 0.315    |
|    ent_coef_loss   | -0.335   |
|    learning_rate   | 0.0003   |
|    n_updates       | 566699   |
---------------------------------
=== Iterazione IRL 405 ===
Loss reward (iter 405): 5.082299709320068
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 722      |
|    critic_loss     | 665      |
|    ent_coef        | 0.322    |
|    ent_coef_loss   | -0.129   |
|    learning_rate   | 0.0003   |
|    n_updates       | 567299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 728      |
|    critic_loss     | 558      |
|    ent_coef        | 0.314    |
|    ent_coef_loss   | -0.136   |
|    learning_rate   | 0.0003   |
|    n_updates       | 567699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 728      |
|    critic_loss     | 857      |
|    ent_coef        | 0.316    |
|    ent_coef_loss   | 0.349    |
|    learning_rate   | 0.0003   |
|    n_updates       | 568099   |
---------------------------------
=== Iterazione IRL 406 ===
Loss reward (iter 406): 5.231032848358154
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 728      |
|    critic_loss     | 671      |
|    ent_coef        | 0.316    |
|    ent_coef_loss   | 0.056    |
|    learning_rate   | 0.0003   |
|    n_updates       | 568699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 734      |
|    critic_loss     | 632      |
|    ent_coef        | 0.327    |
|    ent_coef_loss   | 0.00842  |
|    learning_rate   | 0.0003   |
|    n_updates       | 569099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 717      |
|    critic_loss     | 566      |
|    ent_coef        | 0.339    |
|    ent_coef_loss   | 0.0333   |
|    learning_rate   | 0.0003   |
|    n_updates       | 569499   |
---------------------------------
=== Iterazione IRL 407 ===
Loss reward (iter 407): 4.813924789428711
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 735      |
|    critic_loss     | 669      |
|    ent_coef        | 0.337    |
|    ent_coef_loss   | 0.36     |
|    learning_rate   | 0.0003   |
|    n_updates       | 570099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 728      |
|    critic_loss     | 690      |
|    ent_coef        | 0.338    |
|    ent_coef_loss   | 0.0222   |
|    learning_rate   | 0.0003   |
|    n_updates       | 570499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 728      |
|    critic_loss     | 599      |
|    ent_coef        | 0.328    |
|    ent_coef_loss   | -0.209   |
|    learning_rate   | 0.0003   |
|    n_updates       | 570899   |
---------------------------------
=== Iterazione IRL 408 ===
Loss reward (iter 408): 4.836230278015137
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 714      |
|    critic_loss     | 500      |
|    ent_coef        | 0.322    |
|    ent_coef_loss   | -0.0538  |
|    learning_rate   | 0.0003   |
|    n_updates       | 571499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 739      |
|    critic_loss     | 641      |
|    ent_coef        | 0.329    |
|    ent_coef_loss   | 0.193    |
|    learning_rate   | 0.0003   |
|    n_updates       | 571899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 734      |
|    critic_loss     | 849      |
|    ent_coef        | 0.321    |
|    ent_coef_loss   | -0.0956  |
|    learning_rate   | 0.0003   |
|    n_updates       | 572299   |
---------------------------------
=== Iterazione IRL 409 ===
Loss reward (iter 409): 4.196333408355713
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 717      |
|    critic_loss     | 585      |
|    ent_coef        | 0.325    |
|    ent_coef_loss   | -0.191   |
|    learning_rate   | 0.0003   |
|    n_updates       | 572899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 739      |
|    critic_loss     | 698      |
|    ent_coef        | 0.335    |
|    ent_coef_loss   | -0.0662  |
|    learning_rate   | 0.0003   |
|    n_updates       | 573299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 747      |
|    critic_loss     | 649      |
|    ent_coef        | 0.33     |
|    ent_coef_loss   | -0.0637  |
|    learning_rate   | 0.0003   |
|    n_updates       | 573699   |
---------------------------------
=== Iterazione IRL 410 ===
Loss reward (iter 410): 3.9185068607330322
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 724      |
|    critic_loss     | 601      |
|    ent_coef        | 0.33     |
|    ent_coef_loss   | 0.203    |
|    learning_rate   | 0.0003   |
|    n_updates       | 574299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 727      |
|    critic_loss     | 562      |
|    ent_coef        | 0.333    |
|    ent_coef_loss   | -0.00114 |
|    learning_rate   | 0.0003   |
|    n_updates       | 574699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 739      |
|    critic_loss     | 727      |
|    ent_coef        | 0.334    |
|    ent_coef_loss   | 0.136    |
|    learning_rate   | 0.0003   |
|    n_updates       | 575099   |
---------------------------------
=== Iterazione IRL 411 ===
Loss reward (iter 411): 4.569066047668457
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 746      |
|    critic_loss     | 591      |
|    ent_coef        | 0.338    |
|    ent_coef_loss   | 0.207    |
|    learning_rate   | 0.0003   |
|    n_updates       | 575699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 725      |
|    critic_loss     | 604      |
|    ent_coef        | 0.336    |
|    ent_coef_loss   | -0.072   |
|    learning_rate   | 0.0003   |
|    n_updates       | 576099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 758      |
|    critic_loss     | 794      |
|    ent_coef        | 0.334    |
|    ent_coef_loss   | 0.278    |
|    learning_rate   | 0.0003   |
|    n_updates       | 576499   |
---------------------------------
=== Iterazione IRL 412 ===
Loss reward (iter 412): 4.792590618133545
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 724      |
|    critic_loss     | 509      |
|    ent_coef        | 0.344    |
|    ent_coef_loss   | -0.253   |
|    learning_rate   | 0.0003   |
|    n_updates       | 577099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 726      |
|    critic_loss     | 692      |
|    ent_coef        | 0.342    |
|    ent_coef_loss   | -0.0404  |
|    learning_rate   | 0.0003   |
|    n_updates       | 577499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 733      |
|    critic_loss     | 600      |
|    ent_coef        | 0.346    |
|    ent_coef_loss   | -0.286   |
|    learning_rate   | 0.0003   |
|    n_updates       | 577899   |
---------------------------------
=== Iterazione IRL 413 ===
Loss reward (iter 413): 4.954850196838379
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 731      |
|    critic_loss     | 524      |
|    ent_coef        | 0.342    |
|    ent_coef_loss   | 0.0596   |
|    learning_rate   | 0.0003   |
|    n_updates       | 578499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 747      |
|    critic_loss     | 653      |
|    ent_coef        | 0.342    |
|    ent_coef_loss   | 0.191    |
|    learning_rate   | 0.0003   |
|    n_updates       | 578899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 748      |
|    critic_loss     | 621      |
|    ent_coef        | 0.339    |
|    ent_coef_loss   | -0.0687  |
|    learning_rate   | 0.0003   |
|    n_updates       | 579299   |
---------------------------------
=== Iterazione IRL 414 ===
Loss reward (iter 414): 5.441793441772461
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 760      |
|    critic_loss     | 770      |
|    ent_coef        | 0.347    |
|    ent_coef_loss   | 0.052    |
|    learning_rate   | 0.0003   |
|    n_updates       | 579899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 743      |
|    critic_loss     | 776      |
|    ent_coef        | 0.35     |
|    ent_coef_loss   | -0.154   |
|    learning_rate   | 0.0003   |
|    n_updates       | 580299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 758      |
|    critic_loss     | 949      |
|    ent_coef        | 0.356    |
|    ent_coef_loss   | 0.346    |
|    learning_rate   | 0.0003   |
|    n_updates       | 580699   |
---------------------------------
=== Iterazione IRL 415 ===
Loss reward (iter 415): 4.548628330230713
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 758      |
|    critic_loss     | 704      |
|    ent_coef        | 0.357    |
|    ent_coef_loss   | 0.0516   |
|    learning_rate   | 0.0003   |
|    n_updates       | 581299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 747      |
|    critic_loss     | 752      |
|    ent_coef        | 0.355    |
|    ent_coef_loss   | 0.156    |
|    learning_rate   | 0.0003   |
|    n_updates       | 581699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 746      |
|    critic_loss     | 731      |
|    ent_coef        | 0.355    |
|    ent_coef_loss   | -0.0413  |
|    learning_rate   | 0.0003   |
|    n_updates       | 582099   |
---------------------------------
=== Iterazione IRL 416 ===
Loss reward (iter 416): 4.750124931335449
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 748      |
|    critic_loss     | 882      |
|    ent_coef        | 0.35     |
|    ent_coef_loss   | 0.0337   |
|    learning_rate   | 0.0003   |
|    n_updates       | 582699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 751      |
|    critic_loss     | 617      |
|    ent_coef        | 0.345    |
|    ent_coef_loss   | -0.0504  |
|    learning_rate   | 0.0003   |
|    n_updates       | 583099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 759      |
|    critic_loss     | 722      |
|    ent_coef        | 0.341    |
|    ent_coef_loss   | -0.213   |
|    learning_rate   | 0.0003   |
|    n_updates       | 583499   |
---------------------------------
=== Iterazione IRL 417 ===
Loss reward (iter 417): 4.93556022644043
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 761      |
|    critic_loss     | 610      |
|    ent_coef        | 0.339    |
|    ent_coef_loss   | 0.337    |
|    learning_rate   | 0.0003   |
|    n_updates       | 584099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 762      |
|    critic_loss     | 821      |
|    ent_coef        | 0.327    |
|    ent_coef_loss   | -0.185   |
|    learning_rate   | 0.0003   |
|    n_updates       | 584499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 775      |
|    critic_loss     | 824      |
|    ent_coef        | 0.333    |
|    ent_coef_loss   | 0.136    |
|    learning_rate   | 0.0003   |
|    n_updates       | 584899   |
---------------------------------
=== Iterazione IRL 418 ===
Loss reward (iter 418): 4.979894161224365
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 752      |
|    critic_loss     | 692      |
|    ent_coef        | 0.34     |
|    ent_coef_loss   | -0.238   |
|    learning_rate   | 0.0003   |
|    n_updates       | 585499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 763      |
|    critic_loss     | 792      |
|    ent_coef        | 0.346    |
|    ent_coef_loss   | -0.115   |
|    learning_rate   | 0.0003   |
|    n_updates       | 585899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 757      |
|    critic_loss     | 823      |
|    ent_coef        | 0.343    |
|    ent_coef_loss   | 0.0696   |
|    learning_rate   | 0.0003   |
|    n_updates       | 586299   |
---------------------------------
=== Iterazione IRL 419 ===
Loss reward (iter 419): 4.613070964813232
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 758      |
|    critic_loss     | 946      |
|    ent_coef        | 0.345    |
|    ent_coef_loss   | 0.115    |
|    learning_rate   | 0.0003   |
|    n_updates       | 586899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 767      |
|    critic_loss     | 806      |
|    ent_coef        | 0.331    |
|    ent_coef_loss   | -0.0824  |
|    learning_rate   | 0.0003   |
|    n_updates       | 587299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 767      |
|    critic_loss     | 736      |
|    ent_coef        | 0.32     |
|    ent_coef_loss   | 0.171    |
|    learning_rate   | 0.0003   |
|    n_updates       | 587699   |
---------------------------------
=== Iterazione IRL 420 ===
Loss reward (iter 420): 5.053418159484863
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 755      |
|    critic_loss     | 626      |
|    ent_coef        | 0.33     |
|    ent_coef_loss   | 0.379    |
|    learning_rate   | 0.0003   |
|    n_updates       | 588299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 758      |
|    critic_loss     | 683      |
|    ent_coef        | 0.333    |
|    ent_coef_loss   | 0.228    |
|    learning_rate   | 0.0003   |
|    n_updates       | 588699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 750      |
|    critic_loss     | 747      |
|    ent_coef        | 0.334    |
|    ent_coef_loss   | -0.283   |
|    learning_rate   | 0.0003   |
|    n_updates       | 589099   |
---------------------------------
=== Iterazione IRL 421 ===
Loss reward (iter 421): 5.220092296600342
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 773      |
|    critic_loss     | 781      |
|    ent_coef        | 0.325    |
|    ent_coef_loss   | 0.13     |
|    learning_rate   | 0.0003   |
|    n_updates       | 589699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 775      |
|    critic_loss     | 735      |
|    ent_coef        | 0.324    |
|    ent_coef_loss   | -0.144   |
|    learning_rate   | 0.0003   |
|    n_updates       | 590099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 759      |
|    critic_loss     | 773      |
|    ent_coef        | 0.335    |
|    ent_coef_loss   | 0.478    |
|    learning_rate   | 0.0003   |
|    n_updates       | 590499   |
---------------------------------
=== Iterazione IRL 422 ===
Loss reward (iter 422): 4.6602020263671875
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 766      |
|    critic_loss     | 611      |
|    ent_coef        | 0.328    |
|    ent_coef_loss   | -0.00364 |
|    learning_rate   | 0.0003   |
|    n_updates       | 591099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 761      |
|    critic_loss     | 772      |
|    ent_coef        | 0.337    |
|    ent_coef_loss   | 0.0263   |
|    learning_rate   | 0.0003   |
|    n_updates       | 591499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 765      |
|    critic_loss     | 746      |
|    ent_coef        | 0.336    |
|    ent_coef_loss   | 0.126    |
|    learning_rate   | 0.0003   |
|    n_updates       | 591899   |
---------------------------------
=== Iterazione IRL 423 ===
Loss reward (iter 423): 4.206416606903076
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 767      |
|    critic_loss     | 765      |
|    ent_coef        | 0.341    |
|    ent_coef_loss   | 0.233    |
|    learning_rate   | 0.0003   |
|    n_updates       | 592499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 762      |
|    critic_loss     | 611      |
|    ent_coef        | 0.336    |
|    ent_coef_loss   | 0.218    |
|    learning_rate   | 0.0003   |
|    n_updates       | 592899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 767      |
|    critic_loss     | 846      |
|    ent_coef        | 0.333    |
|    ent_coef_loss   | -0.243   |
|    learning_rate   | 0.0003   |
|    n_updates       | 593299   |
---------------------------------
=== Iterazione IRL 424 ===
Loss reward (iter 424): 4.496265888214111
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 768      |
|    critic_loss     | 1.01e+03 |
|    ent_coef        | 0.346    |
|    ent_coef_loss   | 0.186    |
|    learning_rate   | 0.0003   |
|    n_updates       | 593899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 793      |
|    critic_loss     | 895      |
|    ent_coef        | 0.343    |
|    ent_coef_loss   | 0.577    |
|    learning_rate   | 0.0003   |
|    n_updates       | 594299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 767      |
|    critic_loss     | 896      |
|    ent_coef        | 0.331    |
|    ent_coef_loss   | -0.474   |
|    learning_rate   | 0.0003   |
|    n_updates       | 594699   |
---------------------------------
=== Iterazione IRL 425 ===
Loss reward (iter 425): 5.499669075012207
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 780      |
|    critic_loss     | 745      |
|    ent_coef        | 0.333    |
|    ent_coef_loss   | -0.162   |
|    learning_rate   | 0.0003   |
|    n_updates       | 595299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 764      |
|    critic_loss     | 691      |
|    ent_coef        | 0.331    |
|    ent_coef_loss   | -0.0443  |
|    learning_rate   | 0.0003   |
|    n_updates       | 595699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 748      |
|    critic_loss     | 640      |
|    ent_coef        | 0.334    |
|    ent_coef_loss   | -0.227   |
|    learning_rate   | 0.0003   |
|    n_updates       | 596099   |
---------------------------------
=== Iterazione IRL 426 ===
Loss reward (iter 426): 5.129359722137451
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 781      |
|    critic_loss     | 780      |
|    ent_coef        | 0.337    |
|    ent_coef_loss   | 0.214    |
|    learning_rate   | 0.0003   |
|    n_updates       | 596699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 777      |
|    critic_loss     | 1.09e+03 |
|    ent_coef        | 0.344    |
|    ent_coef_loss   | 0.337    |
|    learning_rate   | 0.0003   |
|    n_updates       | 597099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 812      |
|    critic_loss     | 972      |
|    ent_coef        | 0.344    |
|    ent_coef_loss   | 0.0889   |
|    learning_rate   | 0.0003   |
|    n_updates       | 597499   |
---------------------------------
=== Iterazione IRL 427 ===
Loss reward (iter 427): 5.040562629699707
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 786      |
|    critic_loss     | 981      |
|    ent_coef        | 0.341    |
|    ent_coef_loss   | -0.188   |
|    learning_rate   | 0.0003   |
|    n_updates       | 598099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 776      |
|    critic_loss     | 831      |
|    ent_coef        | 0.331    |
|    ent_coef_loss   | 0.215    |
|    learning_rate   | 0.0003   |
|    n_updates       | 598499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 776      |
|    critic_loss     | 1.08e+03 |
|    ent_coef        | 0.337    |
|    ent_coef_loss   | -0.0969  |
|    learning_rate   | 0.0003   |
|    n_updates       | 598899   |
---------------------------------
=== Iterazione IRL 428 ===
Loss reward (iter 428): 4.0106377601623535
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 790      |
|    critic_loss     | 866      |
|    ent_coef        | 0.347    |
|    ent_coef_loss   | 0.212    |
|    learning_rate   | 0.0003   |
|    n_updates       | 599499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 766      |
|    critic_loss     | 733      |
|    ent_coef        | 0.343    |
|    ent_coef_loss   | 0.0049   |
|    learning_rate   | 0.0003   |
|    n_updates       | 599899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 799      |
|    critic_loss     | 1.1e+03  |
|    ent_coef        | 0.343    |
|    ent_coef_loss   | 0.00839  |
|    learning_rate   | 0.0003   |
|    n_updates       | 600299   |
---------------------------------
=== Iterazione IRL 429 ===
Loss reward (iter 429): 5.604259490966797
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 778      |
|    critic_loss     | 804      |
|    ent_coef        | 0.353    |
|    ent_coef_loss   | 0.254    |
|    learning_rate   | 0.0003   |
|    n_updates       | 600899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 772      |
|    critic_loss     | 845      |
|    ent_coef        | 0.363    |
|    ent_coef_loss   | 0.00434  |
|    learning_rate   | 0.0003   |
|    n_updates       | 601299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 780      |
|    critic_loss     | 802      |
|    ent_coef        | 0.353    |
|    ent_coef_loss   | -0.0555  |
|    learning_rate   | 0.0003   |
|    n_updates       | 601699   |
---------------------------------
=== Iterazione IRL 430 ===
Loss reward (iter 430): 5.2270989418029785
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 789      |
|    critic_loss     | 928      |
|    ent_coef        | 0.345    |
|    ent_coef_loss   | -0.0456  |
|    learning_rate   | 0.0003   |
|    n_updates       | 602299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 776      |
|    critic_loss     | 850      |
|    ent_coef        | 0.334    |
|    ent_coef_loss   | 0.121    |
|    learning_rate   | 0.0003   |
|    n_updates       | 602699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 778      |
|    critic_loss     | 795      |
|    ent_coef        | 0.347    |
|    ent_coef_loss   | -0.0101  |
|    learning_rate   | 0.0003   |
|    n_updates       | 603099   |
---------------------------------
=== Iterazione IRL 431 ===
Loss reward (iter 431): 4.673959732055664
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 794      |
|    critic_loss     | 812      |
|    ent_coef        | 0.347    |
|    ent_coef_loss   | 0.000436 |
|    learning_rate   | 0.0003   |
|    n_updates       | 603699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 812      |
|    critic_loss     | 846      |
|    ent_coef        | 0.35     |
|    ent_coef_loss   | 0.11     |
|    learning_rate   | 0.0003   |
|    n_updates       | 604099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 782      |
|    critic_loss     | 745      |
|    ent_coef        | 0.344    |
|    ent_coef_loss   | -0.251   |
|    learning_rate   | 0.0003   |
|    n_updates       | 604499   |
---------------------------------
=== Iterazione IRL 432 ===
Loss reward (iter 432): 5.570425987243652
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 788      |
|    critic_loss     | 740      |
|    ent_coef        | 0.349    |
|    ent_coef_loss   | 0.0861   |
|    learning_rate   | 0.0003   |
|    n_updates       | 605099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 779      |
|    critic_loss     | 754      |
|    ent_coef        | 0.347    |
|    ent_coef_loss   | -0.237   |
|    learning_rate   | 0.0003   |
|    n_updates       | 605499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 777      |
|    critic_loss     | 622      |
|    ent_coef        | 0.352    |
|    ent_coef_loss   | 0.0482   |
|    learning_rate   | 0.0003   |
|    n_updates       | 605899   |
---------------------------------
=== Iterazione IRL 433 ===
Loss reward (iter 433): 5.2093505859375
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 786      |
|    critic_loss     | 833      |
|    ent_coef        | 0.343    |
|    ent_coef_loss   | 0.275    |
|    learning_rate   | 0.0003   |
|    n_updates       | 606499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 782      |
|    critic_loss     | 794      |
|    ent_coef        | 0.347    |
|    ent_coef_loss   | -0.105   |
|    learning_rate   | 0.0003   |
|    n_updates       | 606899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 788      |
|    critic_loss     | 1.17e+03 |
|    ent_coef        | 0.338    |
|    ent_coef_loss   | 0.154    |
|    learning_rate   | 0.0003   |
|    n_updates       | 607299   |
---------------------------------
=== Iterazione IRL 434 ===
Loss reward (iter 434): 4.936785697937012
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 781      |
|    critic_loss     | 960      |
|    ent_coef        | 0.346    |
|    ent_coef_loss   | -0.25    |
|    learning_rate   | 0.0003   |
|    n_updates       | 607899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 795      |
|    critic_loss     | 758      |
|    ent_coef        | 0.346    |
|    ent_coef_loss   | 0.111    |
|    learning_rate   | 0.0003   |
|    n_updates       | 608299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 771      |
|    critic_loss     | 628      |
|    ent_coef        | 0.346    |
|    ent_coef_loss   | 0.129    |
|    learning_rate   | 0.0003   |
|    n_updates       | 608699   |
---------------------------------
=== Iterazione IRL 435 ===
Loss reward (iter 435): 4.840999126434326
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 785      |
|    critic_loss     | 893      |
|    ent_coef        | 0.337    |
|    ent_coef_loss   | -0.014   |
|    learning_rate   | 0.0003   |
|    n_updates       | 609299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 780      |
|    critic_loss     | 719      |
|    ent_coef        | 0.338    |
|    ent_coef_loss   | 0.209    |
|    learning_rate   | 0.0003   |
|    n_updates       | 609699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 785      |
|    critic_loss     | 657      |
|    ent_coef        | 0.339    |
|    ent_coef_loss   | -0.135   |
|    learning_rate   | 0.0003   |
|    n_updates       | 610099   |
---------------------------------
=== Iterazione IRL 436 ===
Loss reward (iter 436): 4.4384894371032715
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 788      |
|    critic_loss     | 850      |
|    ent_coef        | 0.332    |
|    ent_coef_loss   | 0.263    |
|    learning_rate   | 0.0003   |
|    n_updates       | 610699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 808      |
|    critic_loss     | 1.02e+03 |
|    ent_coef        | 0.333    |
|    ent_coef_loss   | 0.116    |
|    learning_rate   | 0.0003   |
|    n_updates       | 611099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 794      |
|    critic_loss     | 986      |
|    ent_coef        | 0.333    |
|    ent_coef_loss   | -0.0481  |
|    learning_rate   | 0.0003   |
|    n_updates       | 611499   |
---------------------------------
=== Iterazione IRL 437 ===
Loss reward (iter 437): 4.821271896362305
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 809      |
|    critic_loss     | 937      |
|    ent_coef        | 0.351    |
|    ent_coef_loss   | 0.109    |
|    learning_rate   | 0.0003   |
|    n_updates       | 612099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 789      |
|    critic_loss     | 733      |
|    ent_coef        | 0.35     |
|    ent_coef_loss   | -0.0333  |
|    learning_rate   | 0.0003   |
|    n_updates       | 612499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 790      |
|    critic_loss     | 785      |
|    ent_coef        | 0.365    |
|    ent_coef_loss   | 0.0527   |
|    learning_rate   | 0.0003   |
|    n_updates       | 612899   |
---------------------------------
=== Iterazione IRL 438 ===
Loss reward (iter 438): 5.031207084655762
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 780      |
|    critic_loss     | 666      |
|    ent_coef        | 0.361    |
|    ent_coef_loss   | -0.103   |
|    learning_rate   | 0.0003   |
|    n_updates       | 613499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 816      |
|    critic_loss     | 924      |
|    ent_coef        | 0.356    |
|    ent_coef_loss   | -0.278   |
|    learning_rate   | 0.0003   |
|    n_updates       | 613899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 800      |
|    critic_loss     | 815      |
|    ent_coef        | 0.352    |
|    ent_coef_loss   | 0.115    |
|    learning_rate   | 0.0003   |
|    n_updates       | 614299   |
---------------------------------
=== Iterazione IRL 439 ===
Loss reward (iter 439): 4.839484214782715
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 769      |
|    critic_loss     | 790      |
|    ent_coef        | 0.358    |
|    ent_coef_loss   | -0.131   |
|    learning_rate   | 0.0003   |
|    n_updates       | 614899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 775      |
|    critic_loss     | 815      |
|    ent_coef        | 0.347    |
|    ent_coef_loss   | 0.0728   |
|    learning_rate   | 0.0003   |
|    n_updates       | 615299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 773      |
|    critic_loss     | 813      |
|    ent_coef        | 0.348    |
|    ent_coef_loss   | -0.0728  |
|    learning_rate   | 0.0003   |
|    n_updates       | 615699   |
---------------------------------
=== Iterazione IRL 440 ===
Loss reward (iter 440): 5.413485050201416
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 790      |
|    critic_loss     | 1.08e+03 |
|    ent_coef        | 0.346    |
|    ent_coef_loss   | -0.169   |
|    learning_rate   | 0.0003   |
|    n_updates       | 616299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 777      |
|    critic_loss     | 756      |
|    ent_coef        | 0.346    |
|    ent_coef_loss   | -0.194   |
|    learning_rate   | 0.0003   |
|    n_updates       | 616699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 790      |
|    critic_loss     | 749      |
|    ent_coef        | 0.337    |
|    ent_coef_loss   | 0.054    |
|    learning_rate   | 0.0003   |
|    n_updates       | 617099   |
---------------------------------
=== Iterazione IRL 441 ===
Loss reward (iter 441): 4.729928016662598
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 783      |
|    critic_loss     | 784      |
|    ent_coef        | 0.349    |
|    ent_coef_loss   | -0.142   |
|    learning_rate   | 0.0003   |
|    n_updates       | 617699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 782      |
|    critic_loss     | 656      |
|    ent_coef        | 0.33     |
|    ent_coef_loss   | -0.248   |
|    learning_rate   | 0.0003   |
|    n_updates       | 618099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 792      |
|    critic_loss     | 728      |
|    ent_coef        | 0.338    |
|    ent_coef_loss   | 0.404    |
|    learning_rate   | 0.0003   |
|    n_updates       | 618499   |
---------------------------------
=== Iterazione IRL 442 ===
Loss reward (iter 442): 4.811570644378662
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 798      |
|    critic_loss     | 818      |
|    ent_coef        | 0.339    |
|    ent_coef_loss   | 0.328    |
|    learning_rate   | 0.0003   |
|    n_updates       | 619099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 776      |
|    critic_loss     | 892      |
|    ent_coef        | 0.34     |
|    ent_coef_loss   | -0.133   |
|    learning_rate   | 0.0003   |
|    n_updates       | 619499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 779      |
|    critic_loss     | 805      |
|    ent_coef        | 0.337    |
|    ent_coef_loss   | 0.221    |
|    learning_rate   | 0.0003   |
|    n_updates       | 619899   |
---------------------------------
=== Iterazione IRL 443 ===
Loss reward (iter 443): 4.1386847496032715
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 799      |
|    critic_loss     | 945      |
|    ent_coef        | 0.343    |
|    ent_coef_loss   | -0.036   |
|    learning_rate   | 0.0003   |
|    n_updates       | 620499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 791      |
|    critic_loss     | 818      |
|    ent_coef        | 0.344    |
|    ent_coef_loss   | -0.0814  |
|    learning_rate   | 0.0003   |
|    n_updates       | 620899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 805      |
|    critic_loss     | 928      |
|    ent_coef        | 0.349    |
|    ent_coef_loss   | 0.0788   |
|    learning_rate   | 0.0003   |
|    n_updates       | 621299   |
---------------------------------
=== Iterazione IRL 444 ===
Loss reward (iter 444): 4.846936225891113
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 793      |
|    critic_loss     | 762      |
|    ent_coef        | 0.364    |
|    ent_coef_loss   | 0.0946   |
|    learning_rate   | 0.0003   |
|    n_updates       | 621899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 806      |
|    critic_loss     | 1.13e+03 |
|    ent_coef        | 0.372    |
|    ent_coef_loss   | 0.0819   |
|    learning_rate   | 0.0003   |
|    n_updates       | 622299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 808      |
|    critic_loss     | 875      |
|    ent_coef        | 0.371    |
|    ent_coef_loss   | 0.29     |
|    learning_rate   | 0.0003   |
|    n_updates       | 622699   |
---------------------------------
=== Iterazione IRL 445 ===
Loss reward (iter 445): 4.869843006134033
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 791      |
|    critic_loss     | 861      |
|    ent_coef        | 0.365    |
|    ent_coef_loss   | -0.107   |
|    learning_rate   | 0.0003   |
|    n_updates       | 623299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 786      |
|    critic_loss     | 974      |
|    ent_coef        | 0.351    |
|    ent_coef_loss   | -0.142   |
|    learning_rate   | 0.0003   |
|    n_updates       | 623699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 788      |
|    critic_loss     | 867      |
|    ent_coef        | 0.348    |
|    ent_coef_loss   | 0.0741   |
|    learning_rate   | 0.0003   |
|    n_updates       | 624099   |
---------------------------------
=== Iterazione IRL 446 ===
Loss reward (iter 446): 4.755341053009033
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 805      |
|    critic_loss     | 924      |
|    ent_coef        | 0.359    |
|    ent_coef_loss   | 0.00976  |
|    learning_rate   | 0.0003   |
|    n_updates       | 624699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 797      |
|    critic_loss     | 739      |
|    ent_coef        | 0.351    |
|    ent_coef_loss   | 0.0456   |
|    learning_rate   | 0.0003   |
|    n_updates       | 625099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 790      |
|    critic_loss     | 779      |
|    ent_coef        | 0.358    |
|    ent_coef_loss   | 0.0663   |
|    learning_rate   | 0.0003   |
|    n_updates       | 625499   |
---------------------------------
=== Iterazione IRL 447 ===
Loss reward (iter 447): 6.012331485748291
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 789      |
|    critic_loss     | 961      |
|    ent_coef        | 0.354    |
|    ent_coef_loss   | -0.0776  |
|    learning_rate   | 0.0003   |
|    n_updates       | 626099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 803      |
|    critic_loss     | 841      |
|    ent_coef        | 0.354    |
|    ent_coef_loss   | 0.269    |
|    learning_rate   | 0.0003   |
|    n_updates       | 626499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 791      |
|    critic_loss     | 685      |
|    ent_coef        | 0.347    |
|    ent_coef_loss   | 0.0409   |
|    learning_rate   | 0.0003   |
|    n_updates       | 626899   |
---------------------------------
=== Iterazione IRL 448 ===
Loss reward (iter 448): 5.602334499359131
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 785      |
|    critic_loss     | 838      |
|    ent_coef        | 0.342    |
|    ent_coef_loss   | -0.227   |
|    learning_rate   | 0.0003   |
|    n_updates       | 627499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 795      |
|    critic_loss     | 850      |
|    ent_coef        | 0.345    |
|    ent_coef_loss   | 0.00218  |
|    learning_rate   | 0.0003   |
|    n_updates       | 627899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 820      |
|    critic_loss     | 1.14e+03 |
|    ent_coef        | 0.35     |
|    ent_coef_loss   | 0.338    |
|    learning_rate   | 0.0003   |
|    n_updates       | 628299   |
---------------------------------
=== Iterazione IRL 449 ===
Loss reward (iter 449): 4.758821964263916
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 796      |
|    critic_loss     | 921      |
|    ent_coef        | 0.353    |
|    ent_coef_loss   | 0.0867   |
|    learning_rate   | 0.0003   |
|    n_updates       | 628899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 795      |
|    critic_loss     | 793      |
|    ent_coef        | 0.356    |
|    ent_coef_loss   | 0.0525   |
|    learning_rate   | 0.0003   |
|    n_updates       | 629299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 797      |
|    critic_loss     | 738      |
|    ent_coef        | 0.355    |
|    ent_coef_loss   | 0.103    |
|    learning_rate   | 0.0003   |
|    n_updates       | 629699   |
---------------------------------
=== Iterazione IRL 450 ===
Loss reward (iter 450): 5.127220630645752
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 798      |
|    critic_loss     | 1.2e+03  |
|    ent_coef        | 0.362    |
|    ent_coef_loss   | 0.0169   |
|    learning_rate   | 0.0003   |
|    n_updates       | 630299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 792      |
|    critic_loss     | 721      |
|    ent_coef        | 0.357    |
|    ent_coef_loss   | 0.229    |
|    learning_rate   | 0.0003   |
|    n_updates       | 630699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 795      |
|    critic_loss     | 816      |
|    ent_coef        | 0.354    |
|    ent_coef_loss   | 0.42     |
|    learning_rate   | 0.0003   |
|    n_updates       | 631099   |
---------------------------------
=== Iterazione IRL 451 ===
Loss reward (iter 451): 4.834797382354736
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 805      |
|    critic_loss     | 1.04e+03 |
|    ent_coef        | 0.348    |
|    ent_coef_loss   | -0.0502  |
|    learning_rate   | 0.0003   |
|    n_updates       | 631699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 812      |
|    critic_loss     | 1.05e+03 |
|    ent_coef        | 0.348    |
|    ent_coef_loss   | -0.177   |
|    learning_rate   | 0.0003   |
|    n_updates       | 632099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 805      |
|    critic_loss     | 846      |
|    ent_coef        | 0.342    |
|    ent_coef_loss   | 0.146    |
|    learning_rate   | 0.0003   |
|    n_updates       | 632499   |
---------------------------------
=== Iterazione IRL 452 ===
Loss reward (iter 452): 5.366549015045166
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 795      |
|    critic_loss     | 1.05e+03 |
|    ent_coef        | 0.363    |
|    ent_coef_loss   | -0.255   |
|    learning_rate   | 0.0003   |
|    n_updates       | 633099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 791      |
|    critic_loss     | 842      |
|    ent_coef        | 0.362    |
|    ent_coef_loss   | -0.0458  |
|    learning_rate   | 0.0003   |
|    n_updates       | 633499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 801      |
|    critic_loss     | 915      |
|    ent_coef        | 0.362    |
|    ent_coef_loss   | -0.107   |
|    learning_rate   | 0.0003   |
|    n_updates       | 633899   |
---------------------------------
=== Iterazione IRL 453 ===
Loss reward (iter 453): 4.502020835876465
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 799      |
|    critic_loss     | 870      |
|    ent_coef        | 0.347    |
|    ent_coef_loss   | 0.0352   |
|    learning_rate   | 0.0003   |
|    n_updates       | 634499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 818      |
|    critic_loss     | 870      |
|    ent_coef        | 0.346    |
|    ent_coef_loss   | 0.086    |
|    learning_rate   | 0.0003   |
|    n_updates       | 634899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 801      |
|    critic_loss     | 922      |
|    ent_coef        | 0.342    |
|    ent_coef_loss   | -0.296   |
|    learning_rate   | 0.0003   |
|    n_updates       | 635299   |
---------------------------------
=== Iterazione IRL 454 ===
Loss reward (iter 454): 4.984935760498047
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 832      |
|    critic_loss     | 1.49e+03 |
|    ent_coef        | 0.335    |
|    ent_coef_loss   | 0.235    |
|    learning_rate   | 0.0003   |
|    n_updates       | 635899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 821      |
|    critic_loss     | 831      |
|    ent_coef        | 0.339    |
|    ent_coef_loss   | -0.101   |
|    learning_rate   | 0.0003   |
|    n_updates       | 636299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 804      |
|    critic_loss     | 899      |
|    ent_coef        | 0.352    |
|    ent_coef_loss   | -0.145   |
|    learning_rate   | 0.0003   |
|    n_updates       | 636699   |
---------------------------------
=== Iterazione IRL 455 ===
Loss reward (iter 455): 5.775917053222656
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 819      |
|    critic_loss     | 1e+03    |
|    ent_coef        | 0.353    |
|    ent_coef_loss   | 0.274    |
|    learning_rate   | 0.0003   |
|    n_updates       | 637299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 791      |
|    critic_loss     | 881      |
|    ent_coef        | 0.353    |
|    ent_coef_loss   | -0.176   |
|    learning_rate   | 0.0003   |
|    n_updates       | 637699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 816      |
|    critic_loss     | 984      |
|    ent_coef        | 0.364    |
|    ent_coef_loss   | 0.584    |
|    learning_rate   | 0.0003   |
|    n_updates       | 638099   |
---------------------------------
=== Iterazione IRL 456 ===
Loss reward (iter 456): 4.825201988220215
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 828      |
|    critic_loss     | 1.2e+03  |
|    ent_coef        | 0.358    |
|    ent_coef_loss   | -0.26    |
|    learning_rate   | 0.0003   |
|    n_updates       | 638699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 825      |
|    critic_loss     | 1.25e+03 |
|    ent_coef        | 0.365    |
|    ent_coef_loss   | -0.00433 |
|    learning_rate   | 0.0003   |
|    n_updates       | 639099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 798      |
|    critic_loss     | 767      |
|    ent_coef        | 0.357    |
|    ent_coef_loss   | 0.00602  |
|    learning_rate   | 0.0003   |
|    n_updates       | 639499   |
---------------------------------
=== Iterazione IRL 457 ===
Loss reward (iter 457): 4.694108009338379
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 823      |
|    critic_loss     | 889      |
|    ent_coef        | 0.349    |
|    ent_coef_loss   | 0.0254   |
|    learning_rate   | 0.0003   |
|    n_updates       | 640099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 811      |
|    critic_loss     | 796      |
|    ent_coef        | 0.356    |
|    ent_coef_loss   | 0.262    |
|    learning_rate   | 0.0003   |
|    n_updates       | 640499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 798      |
|    critic_loss     | 867      |
|    ent_coef        | 0.358    |
|    ent_coef_loss   | -0.038   |
|    learning_rate   | 0.0003   |
|    n_updates       | 640899   |
---------------------------------
=== Iterazione IRL 458 ===
Loss reward (iter 458): 5.1334333419799805
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 815      |
|    critic_loss     | 1.13e+03 |
|    ent_coef        | 0.356    |
|    ent_coef_loss   | 0.0742   |
|    learning_rate   | 0.0003   |
|    n_updates       | 641499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 799      |
|    critic_loss     | 860      |
|    ent_coef        | 0.346    |
|    ent_coef_loss   | -0.13    |
|    learning_rate   | 0.0003   |
|    n_updates       | 641899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 796      |
|    critic_loss     | 588      |
|    ent_coef        | 0.339    |
|    ent_coef_loss   | 0.0323   |
|    learning_rate   | 0.0003   |
|    n_updates       | 642299   |
---------------------------------
=== Iterazione IRL 459 ===
Loss reward (iter 459): 4.883578300476074
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 830      |
|    critic_loss     | 988      |
|    ent_coef        | 0.336    |
|    ent_coef_loss   | 0.00865  |
|    learning_rate   | 0.0003   |
|    n_updates       | 642899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 821      |
|    critic_loss     | 922      |
|    ent_coef        | 0.346    |
|    ent_coef_loss   | 0.0774   |
|    learning_rate   | 0.0003   |
|    n_updates       | 643299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 817      |
|    critic_loss     | 873      |
|    ent_coef        | 0.345    |
|    ent_coef_loss   | 0.141    |
|    learning_rate   | 0.0003   |
|    n_updates       | 643699   |
---------------------------------
=== Iterazione IRL 460 ===
Loss reward (iter 460): 4.4962615966796875
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 822      |
|    critic_loss     | 1.09e+03 |
|    ent_coef        | 0.355    |
|    ent_coef_loss   | 0.183    |
|    learning_rate   | 0.0003   |
|    n_updates       | 644299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 813      |
|    critic_loss     | 857      |
|    ent_coef        | 0.357    |
|    ent_coef_loss   | 0.22     |
|    learning_rate   | 0.0003   |
|    n_updates       | 644699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 810      |
|    critic_loss     | 755      |
|    ent_coef        | 0.348    |
|    ent_coef_loss   | 0.169    |
|    learning_rate   | 0.0003   |
|    n_updates       | 645099   |
---------------------------------
=== Iterazione IRL 461 ===
Loss reward (iter 461): 4.482456684112549
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 825      |
|    critic_loss     | 902      |
|    ent_coef        | 0.349    |
|    ent_coef_loss   | 0.23     |
|    learning_rate   | 0.0003   |
|    n_updates       | 645699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 827      |
|    critic_loss     | 1.08e+03 |
|    ent_coef        | 0.354    |
|    ent_coef_loss   | 0.283    |
|    learning_rate   | 0.0003   |
|    n_updates       | 646099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 809      |
|    critic_loss     | 954      |
|    ent_coef        | 0.35     |
|    ent_coef_loss   | -0.235   |
|    learning_rate   | 0.0003   |
|    n_updates       | 646499   |
---------------------------------
=== Iterazione IRL 462 ===
Loss reward (iter 462): 5.044315814971924
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 818      |
|    critic_loss     | 1.03e+03 |
|    ent_coef        | 0.332    |
|    ent_coef_loss   | -0.166   |
|    learning_rate   | 0.0003   |
|    n_updates       | 647099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 822      |
|    critic_loss     | 867      |
|    ent_coef        | 0.332    |
|    ent_coef_loss   | -0.093   |
|    learning_rate   | 0.0003   |
|    n_updates       | 647499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 833      |
|    critic_loss     | 1.05e+03 |
|    ent_coef        | 0.337    |
|    ent_coef_loss   | 0.25     |
|    learning_rate   | 0.0003   |
|    n_updates       | 647899   |
---------------------------------
=== Iterazione IRL 463 ===
Loss reward (iter 463): 4.4605536460876465
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 817      |
|    critic_loss     | 1.12e+03 |
|    ent_coef        | 0.342    |
|    ent_coef_loss   | -0.225   |
|    learning_rate   | 0.0003   |
|    n_updates       | 648499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 812      |
|    critic_loss     | 902      |
|    ent_coef        | 0.344    |
|    ent_coef_loss   | 0.583    |
|    learning_rate   | 0.0003   |
|    n_updates       | 648899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 823      |
|    critic_loss     | 1.38e+03 |
|    ent_coef        | 0.336    |
|    ent_coef_loss   | 0.0664   |
|    learning_rate   | 0.0003   |
|    n_updates       | 649299   |
---------------------------------
=== Iterazione IRL 464 ===
Loss reward (iter 464): 4.810097694396973
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 814      |
|    critic_loss     | 940      |
|    ent_coef        | 0.337    |
|    ent_coef_loss   | 0.125    |
|    learning_rate   | 0.0003   |
|    n_updates       | 649899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 825      |
|    critic_loss     | 987      |
|    ent_coef        | 0.355    |
|    ent_coef_loss   | -0.0696  |
|    learning_rate   | 0.0003   |
|    n_updates       | 650299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 817      |
|    critic_loss     | 812      |
|    ent_coef        | 0.357    |
|    ent_coef_loss   | 0.0984   |
|    learning_rate   | 0.0003   |
|    n_updates       | 650699   |
---------------------------------
=== Iterazione IRL 465 ===
Loss reward (iter 465): 5.483197212219238
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 816      |
|    critic_loss     | 794      |
|    ent_coef        | 0.366    |
|    ent_coef_loss   | 0.346    |
|    learning_rate   | 0.0003   |
|    n_updates       | 651299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 836      |
|    critic_loss     | 1.09e+03 |
|    ent_coef        | 0.361    |
|    ent_coef_loss   | 0.333    |
|    learning_rate   | 0.0003   |
|    n_updates       | 651699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 824      |
|    critic_loss     | 1.11e+03 |
|    ent_coef        | 0.354    |
|    ent_coef_loss   | -0.181   |
|    learning_rate   | 0.0003   |
|    n_updates       | 652099   |
---------------------------------
=== Iterazione IRL 466 ===
Loss reward (iter 466): 5.310974597930908
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 841      |
|    critic_loss     | 1.1e+03  |
|    ent_coef        | 0.338    |
|    ent_coef_loss   | 0.225    |
|    learning_rate   | 0.0003   |
|    n_updates       | 652699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 827      |
|    critic_loss     | 1.03e+03 |
|    ent_coef        | 0.338    |
|    ent_coef_loss   | -0.423   |
|    learning_rate   | 0.0003   |
|    n_updates       | 653099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 806      |
|    critic_loss     | 990      |
|    ent_coef        | 0.349    |
|    ent_coef_loss   | 0.292    |
|    learning_rate   | 0.0003   |
|    n_updates       | 653499   |
---------------------------------
=== Iterazione IRL 467 ===
Loss reward (iter 467): 5.04649019241333
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 855      |
|    critic_loss     | 1.47e+03 |
|    ent_coef        | 0.362    |
|    ent_coef_loss   | 0.0346   |
|    learning_rate   | 0.0003   |
|    n_updates       | 654099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 838      |
|    critic_loss     | 911      |
|    ent_coef        | 0.363    |
|    ent_coef_loss   | 0.376    |
|    learning_rate   | 0.0003   |
|    n_updates       | 654499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 816      |
|    critic_loss     | 861      |
|    ent_coef        | 0.356    |
|    ent_coef_loss   | -0.472   |
|    learning_rate   | 0.0003   |
|    n_updates       | 654899   |
---------------------------------
=== Iterazione IRL 468 ===
Loss reward (iter 468): 5.244778633117676
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 841      |
|    critic_loss     | 1.19e+03 |
|    ent_coef        | 0.352    |
|    ent_coef_loss   | 0.521    |
|    learning_rate   | 0.0003   |
|    n_updates       | 655499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 805      |
|    critic_loss     | 1.06e+03 |
|    ent_coef        | 0.351    |
|    ent_coef_loss   | 0.189    |
|    learning_rate   | 0.0003   |
|    n_updates       | 655899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 833      |
|    critic_loss     | 962      |
|    ent_coef        | 0.35     |
|    ent_coef_loss   | 0.087    |
|    learning_rate   | 0.0003   |
|    n_updates       | 656299   |
---------------------------------
=== Iterazione IRL 469 ===
Loss reward (iter 469): 5.395388603210449
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 811      |
|    critic_loss     | 863      |
|    ent_coef        | 0.367    |
|    ent_coef_loss   | -0.281   |
|    learning_rate   | 0.0003   |
|    n_updates       | 656899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 802      |
|    critic_loss     | 927      |
|    ent_coef        | 0.363    |
|    ent_coef_loss   | -0.357   |
|    learning_rate   | 0.0003   |
|    n_updates       | 657299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 819      |
|    critic_loss     | 770      |
|    ent_coef        | 0.355    |
|    ent_coef_loss   | 0.106    |
|    learning_rate   | 0.0003   |
|    n_updates       | 657699   |
---------------------------------
=== Iterazione IRL 470 ===
Loss reward (iter 470): 4.744376182556152
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 829      |
|    critic_loss     | 1.06e+03 |
|    ent_coef        | 0.362    |
|    ent_coef_loss   | 0.00901  |
|    learning_rate   | 0.0003   |
|    n_updates       | 658299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 812      |
|    critic_loss     | 871      |
|    ent_coef        | 0.364    |
|    ent_coef_loss   | 0.0896   |
|    learning_rate   | 0.0003   |
|    n_updates       | 658699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 829      |
|    critic_loss     | 948      |
|    ent_coef        | 0.361    |
|    ent_coef_loss   | 0.354    |
|    learning_rate   | 0.0003   |
|    n_updates       | 659099   |
---------------------------------
=== Iterazione IRL 471 ===
Loss reward (iter 471): 4.798788070678711
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 829      |
|    critic_loss     | 1.04e+03 |
|    ent_coef        | 0.36     |
|    ent_coef_loss   | -0.0335  |
|    learning_rate   | 0.0003   |
|    n_updates       | 659699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 819      |
|    critic_loss     | 811      |
|    ent_coef        | 0.369    |
|    ent_coef_loss   | -0.391   |
|    learning_rate   | 0.0003   |
|    n_updates       | 660099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 815      |
|    critic_loss     | 929      |
|    ent_coef        | 0.363    |
|    ent_coef_loss   | 0.414    |
|    learning_rate   | 0.0003   |
|    n_updates       | 660499   |
---------------------------------
=== Iterazione IRL 472 ===
Loss reward (iter 472): 4.576448440551758
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 833      |
|    critic_loss     | 978      |
|    ent_coef        | 0.356    |
|    ent_coef_loss   | 0.263    |
|    learning_rate   | 0.0003   |
|    n_updates       | 661099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 824      |
|    critic_loss     | 907      |
|    ent_coef        | 0.363    |
|    ent_coef_loss   | 0.102    |
|    learning_rate   | 0.0003   |
|    n_updates       | 661499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 836      |
|    critic_loss     | 989      |
|    ent_coef        | 0.354    |
|    ent_coef_loss   | -0.305   |
|    learning_rate   | 0.0003   |
|    n_updates       | 661899   |
---------------------------------
=== Iterazione IRL 473 ===
Loss reward (iter 473): 4.815524578094482
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 818      |
|    critic_loss     | 745      |
|    ent_coef        | 0.358    |
|    ent_coef_loss   | -0.252   |
|    learning_rate   | 0.0003   |
|    n_updates       | 662499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 836      |
|    critic_loss     | 921      |
|    ent_coef        | 0.353    |
|    ent_coef_loss   | 0.206    |
|    learning_rate   | 0.0003   |
|    n_updates       | 662899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 820      |
|    critic_loss     | 698      |
|    ent_coef        | 0.351    |
|    ent_coef_loss   | 0.127    |
|    learning_rate   | 0.0003   |
|    n_updates       | 663299   |
---------------------------------
=== Iterazione IRL 474 ===
Loss reward (iter 474): 4.911158561706543
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 856      |
|    critic_loss     | 1.08e+03 |
|    ent_coef        | 0.35     |
|    ent_coef_loss   | 0.258    |
|    learning_rate   | 0.0003   |
|    n_updates       | 663899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 818      |
|    critic_loss     | 744      |
|    ent_coef        | 0.346    |
|    ent_coef_loss   | 0.0562   |
|    learning_rate   | 0.0003   |
|    n_updates       | 664299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 852      |
|    critic_loss     | 1.23e+03 |
|    ent_coef        | 0.353    |
|    ent_coef_loss   | 0.182    |
|    learning_rate   | 0.0003   |
|    n_updates       | 664699   |
---------------------------------
=== Iterazione IRL 475 ===
Loss reward (iter 475): 4.297410488128662
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 831      |
|    critic_loss     | 879      |
|    ent_coef        | 0.353    |
|    ent_coef_loss   | -0.373   |
|    learning_rate   | 0.0003   |
|    n_updates       | 665299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 826      |
|    critic_loss     | 1.08e+03 |
|    ent_coef        | 0.351    |
|    ent_coef_loss   | -0.185   |
|    learning_rate   | 0.0003   |
|    n_updates       | 665699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 828      |
|    critic_loss     | 1.01e+03 |
|    ent_coef        | 0.345    |
|    ent_coef_loss   | 0.12     |
|    learning_rate   | 0.0003   |
|    n_updates       | 666099   |
---------------------------------
=== Iterazione IRL 476 ===
Loss reward (iter 476): 4.173835754394531
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 850      |
|    critic_loss     | 935      |
|    ent_coef        | 0.351    |
|    ent_coef_loss   | -0.183   |
|    learning_rate   | 0.0003   |
|    n_updates       | 666699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 815      |
|    critic_loss     | 960      |
|    ent_coef        | 0.348    |
|    ent_coef_loss   | -0.331   |
|    learning_rate   | 0.0003   |
|    n_updates       | 667099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 842      |
|    critic_loss     | 993      |
|    ent_coef        | 0.363    |
|    ent_coef_loss   | 0.165    |
|    learning_rate   | 0.0003   |
|    n_updates       | 667499   |
---------------------------------
=== Iterazione IRL 477 ===
Loss reward (iter 477): 4.410120964050293
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 817      |
|    critic_loss     | 930      |
|    ent_coef        | 0.35     |
|    ent_coef_loss   | -0.187   |
|    learning_rate   | 0.0003   |
|    n_updates       | 668099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 835      |
|    critic_loss     | 873      |
|    ent_coef        | 0.358    |
|    ent_coef_loss   | 0.0622   |
|    learning_rate   | 0.0003   |
|    n_updates       | 668499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 844      |
|    critic_loss     | 1.11e+03 |
|    ent_coef        | 0.349    |
|    ent_coef_loss   | 0.00312  |
|    learning_rate   | 0.0003   |
|    n_updates       | 668899   |
---------------------------------
=== Iterazione IRL 478 ===
Loss reward (iter 478): 4.429437637329102
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 835      |
|    critic_loss     | 809      |
|    ent_coef        | 0.352    |
|    ent_coef_loss   | 0.125    |
|    learning_rate   | 0.0003   |
|    n_updates       | 669499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 829      |
|    critic_loss     | 1.04e+03 |
|    ent_coef        | 0.369    |
|    ent_coef_loss   | -0.0198  |
|    learning_rate   | 0.0003   |
|    n_updates       | 669899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 840      |
|    critic_loss     | 880      |
|    ent_coef        | 0.373    |
|    ent_coef_loss   | 0.274    |
|    learning_rate   | 0.0003   |
|    n_updates       | 670299   |
---------------------------------
=== Iterazione IRL 479 ===
Loss reward (iter 479): 4.322778224945068
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 823      |
|    critic_loss     | 978      |
|    ent_coef        | 0.365    |
|    ent_coef_loss   | 0.216    |
|    learning_rate   | 0.0003   |
|    n_updates       | 670899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 826      |
|    critic_loss     | 1.11e+03 |
|    ent_coef        | 0.367    |
|    ent_coef_loss   | 0.0537   |
|    learning_rate   | 0.0003   |
|    n_updates       | 671299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 851      |
|    critic_loss     | 1e+03    |
|    ent_coef        | 0.368    |
|    ent_coef_loss   | 0.269    |
|    learning_rate   | 0.0003   |
|    n_updates       | 671699   |
---------------------------------
=== Iterazione IRL 480 ===
Loss reward (iter 480): 5.149406433105469
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 816      |
|    critic_loss     | 912      |
|    ent_coef        | 0.372    |
|    ent_coef_loss   | -0.0641  |
|    learning_rate   | 0.0003   |
|    n_updates       | 672299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 839      |
|    critic_loss     | 999      |
|    ent_coef        | 0.364    |
|    ent_coef_loss   | 0.0683   |
|    learning_rate   | 0.0003   |
|    n_updates       | 672699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 855      |
|    critic_loss     | 1.01e+03 |
|    ent_coef        | 0.362    |
|    ent_coef_loss   | 0.015    |
|    learning_rate   | 0.0003   |
|    n_updates       | 673099   |
---------------------------------
=== Iterazione IRL 481 ===
Loss reward (iter 481): 5.038995265960693
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 829      |
|    critic_loss     | 838      |
|    ent_coef        | 0.36     |
|    ent_coef_loss   | -0.11    |
|    learning_rate   | 0.0003   |
|    n_updates       | 673699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 840      |
|    critic_loss     | 1.1e+03  |
|    ent_coef        | 0.352    |
|    ent_coef_loss   | -0.263   |
|    learning_rate   | 0.0003   |
|    n_updates       | 674099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 828      |
|    critic_loss     | 651      |
|    ent_coef        | 0.35     |
|    ent_coef_loss   | -0.0696  |
|    learning_rate   | 0.0003   |
|    n_updates       | 674499   |
---------------------------------
=== Iterazione IRL 482 ===
Loss reward (iter 482): 5.021286487579346
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 828      |
|    critic_loss     | 760      |
|    ent_coef        | 0.35     |
|    ent_coef_loss   | 0.193    |
|    learning_rate   | 0.0003   |
|    n_updates       | 675099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 831      |
|    critic_loss     | 789      |
|    ent_coef        | 0.352    |
|    ent_coef_loss   | 0.172    |
|    learning_rate   | 0.0003   |
|    n_updates       | 675499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 827      |
|    critic_loss     | 850      |
|    ent_coef        | 0.357    |
|    ent_coef_loss   | 0.0683   |
|    learning_rate   | 0.0003   |
|    n_updates       | 675899   |
---------------------------------
=== Iterazione IRL 483 ===
Loss reward (iter 483): 4.73632287979126
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 819      |
|    critic_loss     | 667      |
|    ent_coef        | 0.359    |
|    ent_coef_loss   | -0.258   |
|    learning_rate   | 0.0003   |
|    n_updates       | 676499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 824      |
|    critic_loss     | 1.05e+03 |
|    ent_coef        | 0.342    |
|    ent_coef_loss   | 0.202    |
|    learning_rate   | 0.0003   |
|    n_updates       | 676899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 818      |
|    critic_loss     | 795      |
|    ent_coef        | 0.353    |
|    ent_coef_loss   | -0.204   |
|    learning_rate   | 0.0003   |
|    n_updates       | 677299   |
---------------------------------
=== Iterazione IRL 484 ===
Loss reward (iter 484): 4.910387992858887
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 825      |
|    critic_loss     | 767      |
|    ent_coef        | 0.356    |
|    ent_coef_loss   | 0.272    |
|    learning_rate   | 0.0003   |
|    n_updates       | 677899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 848      |
|    critic_loss     | 819      |
|    ent_coef        | 0.346    |
|    ent_coef_loss   | -0.158   |
|    learning_rate   | 0.0003   |
|    n_updates       | 678299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 822      |
|    critic_loss     | 1.16e+03 |
|    ent_coef        | 0.349    |
|    ent_coef_loss   | -0.196   |
|    learning_rate   | 0.0003   |
|    n_updates       | 678699   |
---------------------------------
=== Iterazione IRL 485 ===
Loss reward (iter 485): 5.628553867340088
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 845      |
|    critic_loss     | 884      |
|    ent_coef        | 0.341    |
|    ent_coef_loss   | -0.106   |
|    learning_rate   | 0.0003   |
|    n_updates       | 679299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 836      |
|    critic_loss     | 888      |
|    ent_coef        | 0.333    |
|    ent_coef_loss   | 0.067    |
|    learning_rate   | 0.0003   |
|    n_updates       | 679699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 824      |
|    critic_loss     | 930      |
|    ent_coef        | 0.342    |
|    ent_coef_loss   | -0.102   |
|    learning_rate   | 0.0003   |
|    n_updates       | 680099   |
---------------------------------
=== Iterazione IRL 486 ===
Loss reward (iter 486): 5.130894660949707
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 818      |
|    critic_loss     | 843      |
|    ent_coef        | 0.347    |
|    ent_coef_loss   | -0.0331  |
|    learning_rate   | 0.0003   |
|    n_updates       | 680699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 838      |
|    critic_loss     | 950      |
|    ent_coef        | 0.356    |
|    ent_coef_loss   | 0.0778   |
|    learning_rate   | 0.0003   |
|    n_updates       | 681099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 863      |
|    critic_loss     | 1.35e+03 |
|    ent_coef        | 0.361    |
|    ent_coef_loss   | -0.291   |
|    learning_rate   | 0.0003   |
|    n_updates       | 681499   |
---------------------------------
=== Iterazione IRL 487 ===
Loss reward (iter 487): 4.86492919921875
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 825      |
|    critic_loss     | 880      |
|    ent_coef        | 0.362    |
|    ent_coef_loss   | -0.169   |
|    learning_rate   | 0.0003   |
|    n_updates       | 682099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 825      |
|    critic_loss     | 709      |
|    ent_coef        | 0.353    |
|    ent_coef_loss   | -0.0839  |
|    learning_rate   | 0.0003   |
|    n_updates       | 682499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 846      |
|    critic_loss     | 1.14e+03 |
|    ent_coef        | 0.345    |
|    ent_coef_loss   | 0.0777   |
|    learning_rate   | 0.0003   |
|    n_updates       | 682899   |
---------------------------------
=== Iterazione IRL 488 ===
Loss reward (iter 488): 4.919328689575195
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 847      |
|    critic_loss     | 1.59e+03 |
|    ent_coef        | 0.353    |
|    ent_coef_loss   | -0.0187  |
|    learning_rate   | 0.0003   |
|    n_updates       | 683499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 842      |
|    critic_loss     | 1.35e+03 |
|    ent_coef        | 0.351    |
|    ent_coef_loss   | 0.00618  |
|    learning_rate   | 0.0003   |
|    n_updates       | 683899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 845      |
|    critic_loss     | 777      |
|    ent_coef        | 0.35     |
|    ent_coef_loss   | -0.18    |
|    learning_rate   | 0.0003   |
|    n_updates       | 684299   |
---------------------------------
=== Iterazione IRL 489 ===
Loss reward (iter 489): 4.5204854011535645
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 836      |
|    critic_loss     | 1.1e+03  |
|    ent_coef        | 0.355    |
|    ent_coef_loss   | 0.017    |
|    learning_rate   | 0.0003   |
|    n_updates       | 684899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 820      |
|    critic_loss     | 816      |
|    ent_coef        | 0.348    |
|    ent_coef_loss   | 0.48     |
|    learning_rate   | 0.0003   |
|    n_updates       | 685299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 839      |
|    critic_loss     | 1.18e+03 |
|    ent_coef        | 0.349    |
|    ent_coef_loss   | 0.292    |
|    learning_rate   | 0.0003   |
|    n_updates       | 685699   |
---------------------------------
=== Iterazione IRL 490 ===
Loss reward (iter 490): 4.654352188110352
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 852      |
|    critic_loss     | 1.11e+03 |
|    ent_coef        | 0.36     |
|    ent_coef_loss   | 0.116    |
|    learning_rate   | 0.0003   |
|    n_updates       | 686299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 833      |
|    critic_loss     | 1.14e+03 |
|    ent_coef        | 0.355    |
|    ent_coef_loss   | 0.0491   |
|    learning_rate   | 0.0003   |
|    n_updates       | 686699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 835      |
|    critic_loss     | 899      |
|    ent_coef        | 0.358    |
|    ent_coef_loss   | -0.158   |
|    learning_rate   | 0.0003   |
|    n_updates       | 687099   |
---------------------------------
=== Iterazione IRL 491 ===
Loss reward (iter 491): 5.562655448913574
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 835      |
|    critic_loss     | 1.03e+03 |
|    ent_coef        | 0.373    |
|    ent_coef_loss   | 0.0764   |
|    learning_rate   | 0.0003   |
|    n_updates       | 687699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 825      |
|    critic_loss     | 802      |
|    ent_coef        | 0.371    |
|    ent_coef_loss   | -0.236   |
|    learning_rate   | 0.0003   |
|    n_updates       | 688099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 825      |
|    critic_loss     | 775      |
|    ent_coef        | 0.367    |
|    ent_coef_loss   | -0.21    |
|    learning_rate   | 0.0003   |
|    n_updates       | 688499   |
---------------------------------
=== Iterazione IRL 492 ===
Loss reward (iter 492): 4.974164962768555
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 841      |
|    critic_loss     | 892      |
|    ent_coef        | 0.362    |
|    ent_coef_loss   | -0.109   |
|    learning_rate   | 0.0003   |
|    n_updates       | 689099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 829      |
|    critic_loss     | 762      |
|    ent_coef        | 0.368    |
|    ent_coef_loss   | 0.0342   |
|    learning_rate   | 0.0003   |
|    n_updates       | 689499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 847      |
|    critic_loss     | 923      |
|    ent_coef        | 0.363    |
|    ent_coef_loss   | -0.371   |
|    learning_rate   | 0.0003   |
|    n_updates       | 689899   |
---------------------------------
=== Iterazione IRL 493 ===
Loss reward (iter 493): 4.8944292068481445
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 832      |
|    critic_loss     | 778      |
|    ent_coef        | 0.354    |
|    ent_coef_loss   | 0.192    |
|    learning_rate   | 0.0003   |
|    n_updates       | 690499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 828      |
|    critic_loss     | 686      |
|    ent_coef        | 0.361    |
|    ent_coef_loss   | 0.0331   |
|    learning_rate   | 0.0003   |
|    n_updates       | 690899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 825      |
|    critic_loss     | 849      |
|    ent_coef        | 0.363    |
|    ent_coef_loss   | -0.0839  |
|    learning_rate   | 0.0003   |
|    n_updates       | 691299   |
---------------------------------
=== Iterazione IRL 494 ===
Loss reward (iter 494): 4.8851213455200195
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 838      |
|    critic_loss     | 1.01e+03 |
|    ent_coef        | 0.368    |
|    ent_coef_loss   | 0.117    |
|    learning_rate   | 0.0003   |
|    n_updates       | 691899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 846      |
|    critic_loss     | 808      |
|    ent_coef        | 0.37     |
|    ent_coef_loss   | 0.539    |
|    learning_rate   | 0.0003   |
|    n_updates       | 692299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 830      |
|    critic_loss     | 983      |
|    ent_coef        | 0.363    |
|    ent_coef_loss   | -0.0744  |
|    learning_rate   | 0.0003   |
|    n_updates       | 692699   |
---------------------------------
=== Iterazione IRL 495 ===
Loss reward (iter 495): 4.586437702178955
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 838      |
|    critic_loss     | 870      |
|    ent_coef        | 0.359    |
|    ent_coef_loss   | 0.00127  |
|    learning_rate   | 0.0003   |
|    n_updates       | 693299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 827      |
|    critic_loss     | 941      |
|    ent_coef        | 0.352    |
|    ent_coef_loss   | -0.41    |
|    learning_rate   | 0.0003   |
|    n_updates       | 693699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 837      |
|    critic_loss     | 738      |
|    ent_coef        | 0.353    |
|    ent_coef_loss   | -0.00605 |
|    learning_rate   | 0.0003   |
|    n_updates       | 694099   |
---------------------------------
=== Iterazione IRL 496 ===
Loss reward (iter 496): 4.984126091003418
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 834      |
|    critic_loss     | 895      |
|    ent_coef        | 0.366    |
|    ent_coef_loss   | -0.216   |
|    learning_rate   | 0.0003   |
|    n_updates       | 694699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 826      |
|    critic_loss     | 713      |
|    ent_coef        | 0.369    |
|    ent_coef_loss   | 0.413    |
|    learning_rate   | 0.0003   |
|    n_updates       | 695099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 845      |
|    critic_loss     | 872      |
|    ent_coef        | 0.373    |
|    ent_coef_loss   | -0.119   |
|    learning_rate   | 0.0003   |
|    n_updates       | 695499   |
---------------------------------
=== Iterazione IRL 497 ===
Loss reward (iter 497): 4.743150234222412
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 844      |
|    critic_loss     | 876      |
|    ent_coef        | 0.366    |
|    ent_coef_loss   | -0.0174  |
|    learning_rate   | 0.0003   |
|    n_updates       | 696099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 839      |
|    critic_loss     | 1.17e+03 |
|    ent_coef        | 0.364    |
|    ent_coef_loss   | -0.0122  |
|    learning_rate   | 0.0003   |
|    n_updates       | 696499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 807      |
|    critic_loss     | 638      |
|    ent_coef        | 0.372    |
|    ent_coef_loss   | -0.178   |
|    learning_rate   | 0.0003   |
|    n_updates       | 696899   |
---------------------------------
=== Iterazione IRL 498 ===
Loss reward (iter 498): 4.986664295196533
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 847      |
|    critic_loss     | 1.06e+03 |
|    ent_coef        | 0.365    |
|    ent_coef_loss   | 0.0247   |
|    learning_rate   | 0.0003   |
|    n_updates       | 697499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 855      |
|    critic_loss     | 1.07e+03 |
|    ent_coef        | 0.36     |
|    ent_coef_loss   | 0.18     |
|    learning_rate   | 0.0003   |
|    n_updates       | 697899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 837      |
|    critic_loss     | 899      |
|    ent_coef        | 0.353    |
|    ent_coef_loss   | -0.0201  |
|    learning_rate   | 0.0003   |
|    n_updates       | 698299   |
---------------------------------
=== Iterazione IRL 499 ===
Loss reward (iter 499): 5.222100734710693
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 838      |
|    critic_loss     | 741      |
|    ent_coef        | 0.344    |
|    ent_coef_loss   | -0.433   |
|    learning_rate   | 0.0003   |
|    n_updates       | 698899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 827      |
|    critic_loss     | 870      |
|    ent_coef        | 0.346    |
|    ent_coef_loss   | -0.0496  |
|    learning_rate   | 0.0003   |
|    n_updates       | 699299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 837      |
|    critic_loss     | 860      |
|    ent_coef        | 0.355    |
|    ent_coef_loss   | 0.0225   |
|    learning_rate   | 0.0003   |
|    n_updates       | 699699   |
---------------------------------
=== Iterazione IRL 500 ===
Loss reward (iter 500): 4.944834232330322
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 838      |
|    critic_loss     | 707      |
|    ent_coef        | 0.363    |
|    ent_coef_loss   | 0.256    |
|    learning_rate   | 0.0003   |
|    n_updates       | 700299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 827      |
|    critic_loss     | 983      |
|    ent_coef        | 0.362    |
|    ent_coef_loss   | -0.00979 |
|    learning_rate   | 0.0003   |
|    n_updates       | 700699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 841      |
|    critic_loss     | 1.03e+03 |
|    ent_coef        | 0.361    |
|    ent_coef_loss   | 0.0398   |
|    learning_rate   | 0.0003   |
|    n_updates       | 701099   |
---------------------------------
=== Iterazione IRL 501 ===
Loss reward (iter 501): 5.518212795257568
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 839      |
|    critic_loss     | 960      |
|    ent_coef        | 0.359    |
|    ent_coef_loss   | -0.155   |
|    learning_rate   | 0.0003   |
|    n_updates       | 701699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 843      |
|    critic_loss     | 700      |
|    ent_coef        | 0.358    |
|    ent_coef_loss   | 0.0506   |
|    learning_rate   | 0.0003   |
|    n_updates       | 702099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 825      |
|    critic_loss     | 833      |
|    ent_coef        | 0.355    |
|    ent_coef_loss   | 0.116    |
|    learning_rate   | 0.0003   |
|    n_updates       | 702499   |
---------------------------------
=== Iterazione IRL 502 ===
Loss reward (iter 502): 5.02007532119751
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 857      |
|    critic_loss     | 832      |
|    ent_coef        | 0.353    |
|    ent_coef_loss   | -0.0507  |
|    learning_rate   | 0.0003   |
|    n_updates       | 703099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 829      |
|    critic_loss     | 606      |
|    ent_coef        | 0.347    |
|    ent_coef_loss   | -0.0357  |
|    learning_rate   | 0.0003   |
|    n_updates       | 703499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 822      |
|    critic_loss     | 779      |
|    ent_coef        | 0.339    |
|    ent_coef_loss   | -0.0108  |
|    learning_rate   | 0.0003   |
|    n_updates       | 703899   |
---------------------------------
=== Iterazione IRL 503 ===
Loss reward (iter 503): 5.042755126953125
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 835      |
|    critic_loss     | 779      |
|    ent_coef        | 0.355    |
|    ent_coef_loss   | 0.27     |
|    learning_rate   | 0.0003   |
|    n_updates       | 704499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 824      |
|    critic_loss     | 797      |
|    ent_coef        | 0.362    |
|    ent_coef_loss   | -0.0979  |
|    learning_rate   | 0.0003   |
|    n_updates       | 704899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 818      |
|    critic_loss     | 803      |
|    ent_coef        | 0.359    |
|    ent_coef_loss   | 0.174    |
|    learning_rate   | 0.0003   |
|    n_updates       | 705299   |
---------------------------------
=== Iterazione IRL 504 ===
Loss reward (iter 504): 5.155407905578613
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 854      |
|    critic_loss     | 698      |
|    ent_coef        | 0.349    |
|    ent_coef_loss   | 0.127    |
|    learning_rate   | 0.0003   |
|    n_updates       | 705899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 836      |
|    critic_loss     | 818      |
|    ent_coef        | 0.349    |
|    ent_coef_loss   | -0.0297  |
|    learning_rate   | 0.0003   |
|    n_updates       | 706299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 855      |
|    critic_loss     | 1.11e+03 |
|    ent_coef        | 0.344    |
|    ent_coef_loss   | 0.186    |
|    learning_rate   | 0.0003   |
|    n_updates       | 706699   |
---------------------------------
=== Iterazione IRL 505 ===
Loss reward (iter 505): 5.1085205078125
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 852      |
|    critic_loss     | 971      |
|    ent_coef        | 0.357    |
|    ent_coef_loss   | 0.144    |
|    learning_rate   | 0.0003   |
|    n_updates       | 707299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 856      |
|    critic_loss     | 938      |
|    ent_coef        | 0.357    |
|    ent_coef_loss   | -0.136   |
|    learning_rate   | 0.0003   |
|    n_updates       | 707699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 823      |
|    critic_loss     | 534      |
|    ent_coef        | 0.354    |
|    ent_coef_loss   | 0.0358   |
|    learning_rate   | 0.0003   |
|    n_updates       | 708099   |
---------------------------------
=== Iterazione IRL 506 ===
Loss reward (iter 506): 4.963002681732178
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 829      |
|    critic_loss     | 770      |
|    ent_coef        | 0.358    |
|    ent_coef_loss   | 0.104    |
|    learning_rate   | 0.0003   |
|    n_updates       | 708699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 841      |
|    critic_loss     | 634      |
|    ent_coef        | 0.352    |
|    ent_coef_loss   | -0.303   |
|    learning_rate   | 0.0003   |
|    n_updates       | 709099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 833      |
|    critic_loss     | 933      |
|    ent_coef        | 0.348    |
|    ent_coef_loss   | -0.0041  |
|    learning_rate   | 0.0003   |
|    n_updates       | 709499   |
---------------------------------
=== Iterazione IRL 507 ===
Loss reward (iter 507): 4.971953868865967
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 837      |
|    critic_loss     | 878      |
|    ent_coef        | 0.346    |
|    ent_coef_loss   | -0.135   |
|    learning_rate   | 0.0003   |
|    n_updates       | 710099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 826      |
|    critic_loss     | 765      |
|    ent_coef        | 0.351    |
|    ent_coef_loss   | 0.393    |
|    learning_rate   | 0.0003   |
|    n_updates       | 710499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 129      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 841      |
|    critic_loss     | 751      |
|    ent_coef        | 0.353    |
|    ent_coef_loss   | -0.0749  |
|    learning_rate   | 0.0003   |
|    n_updates       | 710899   |
---------------------------------
=== Iterazione IRL 508 ===
Loss reward (iter 508): 4.708338260650635
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 844      |
|    critic_loss     | 732      |
|    ent_coef        | 0.363    |
|    ent_coef_loss   | -0.0575  |
|    learning_rate   | 0.0003   |
|    n_updates       | 711499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 861      |
|    critic_loss     | 1.28e+03 |
|    ent_coef        | 0.358    |
|    ent_coef_loss   | 0.222    |
|    learning_rate   | 0.0003   |
|    n_updates       | 711899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 840      |
|    critic_loss     | 715      |
|    ent_coef        | 0.355    |
|    ent_coef_loss   | -0.291   |
|    learning_rate   | 0.0003   |
|    n_updates       | 712299   |
---------------------------------
=== Iterazione IRL 509 ===
Loss reward (iter 509): 5.297831058502197
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 155      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 826      |
|    critic_loss     | 812      |
|    ent_coef        | 0.363    |
|    ent_coef_loss   | -0.253   |
|    learning_rate   | 0.0003   |
|    n_updates       | 712899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 135      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 825      |
|    critic_loss     | 726      |
|    ent_coef        | 0.365    |
|    ent_coef_loss   | 0.331    |
|    learning_rate   | 0.0003   |
|    n_updates       | 713299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 130      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | 830      |
|    critic_loss     | 971      |
|    ent_coef        | 0.367    |
|    ent_coef_loss   | -0.116   |
|    learning_rate   | 0.0003   |
|    n_updates       | 713699   |
---------------------------------
