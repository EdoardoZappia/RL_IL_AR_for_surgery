Inizio training IRL
Using device: cuda
Using cuda device
=== Iterazione IRL 0 ===
Loss reward (iter 0): 6.828519821166992
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 245      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.62    |
|    critic_loss     | 0.016    |
|    learning_rate   | 0.001    |
|    n_updates       | 299      |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 218      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.1     |
|    critic_loss     | 0.0317   |
|    learning_rate   | 0.001    |
|    n_updates       | 699      |
---------------------------------
=== Iterazione IRL 1 ===
Loss reward (iter 1): 6.916604518890381
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 252      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.19    |
|    critic_loss     | 0.986    |
|    learning_rate   | 0.001    |
|    n_updates       | 1199     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 221      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.67    |
|    critic_loss     | 1.41     |
|    learning_rate   | 0.001    |
|    n_updates       | 1599     |
---------------------------------
=== Iterazione IRL 2 ===
Loss reward (iter 2): 4.95564079284668
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 252      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.8     |
|    critic_loss     | 2.27     |
|    learning_rate   | 0.001    |
|    n_updates       | 2099     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 221      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.3     |
|    critic_loss     | 3.48     |
|    learning_rate   | 0.001    |
|    n_updates       | 2499     |
---------------------------------
=== Iterazione IRL 3 ===
Loss reward (iter 3): 3.0483763217926025
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 252      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.85    |
|    critic_loss     | 5.38     |
|    learning_rate   | 0.001    |
|    n_updates       | 2999     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 221      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.66    |
|    critic_loss     | 5.25     |
|    learning_rate   | 0.001    |
|    n_updates       | 3399     |
---------------------------------
=== Iterazione IRL 4 ===
Loss reward (iter 4): 0.32061299681663513
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 252      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -1.64    |
|    critic_loss     | 8.49     |
|    learning_rate   | 0.001    |
|    n_updates       | 3899     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 221      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 0.114    |
|    critic_loss     | 11.4     |
|    learning_rate   | 0.001    |
|    n_updates       | 4299     |
---------------------------------
=== Iterazione IRL 5 ===
Loss reward (iter 5): -3.1363837718963623
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 252      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 4.1      |
|    critic_loss     | 19.1     |
|    learning_rate   | 0.001    |
|    n_updates       | 4799     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 221      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 7.49     |
|    critic_loss     | 30.5     |
|    learning_rate   | 0.001    |
|    n_updates       | 5199     |
---------------------------------
=== Iterazione IRL 6 ===
Loss reward (iter 6): -8.175508499145508
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 252      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 11.6     |
|    critic_loss     | 37       |
|    learning_rate   | 0.001    |
|    n_updates       | 5699     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 221      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 16.6     |
|    critic_loss     | 52       |
|    learning_rate   | 0.001    |
|    n_updates       | 6099     |
---------------------------------
=== Iterazione IRL 7 ===
Loss reward (iter 7): -14.772353172302246
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 252      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 23.6     |
|    critic_loss     | 78.3     |
|    learning_rate   | 0.001    |
|    n_updates       | 6599     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 221      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 28.8     |
|    critic_loss     | 81.8     |
|    learning_rate   | 0.001    |
|    n_updates       | 6999     |
---------------------------------
=== Iterazione IRL 8 ===
Loss reward (iter 8): -22.877622604370117
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 251      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 41.2     |
|    critic_loss     | 126      |
|    learning_rate   | 0.001    |
|    n_updates       | 7499     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 220      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 50       |
|    critic_loss     | 154      |
|    learning_rate   | 0.001    |
|    n_updates       | 7899     |
---------------------------------
=== Iterazione IRL 9 ===
Loss reward (iter 9): -32.50221633911133
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 252      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 65.5     |
|    critic_loss     | 207      |
|    learning_rate   | 0.001    |
|    n_updates       | 8399     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 220      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 69.7     |
|    critic_loss     | 273      |
|    learning_rate   | 0.001    |
|    n_updates       | 8799     |
---------------------------------
=== Iterazione IRL 10 ===
Loss reward (iter 10): -45.267967224121094
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 260      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 91.1     |
|    critic_loss     | 367      |
|    learning_rate   | 0.001    |
|    n_updates       | 9299     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 222      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 109      |
|    critic_loss     | 430      |
|    learning_rate   | 0.001    |
|    n_updates       | 9699     |
---------------------------------
=== Iterazione IRL 11 ===
Loss reward (iter 11): -59.35390853881836
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 250      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 124      |
|    critic_loss     | 603      |
|    learning_rate   | 0.001    |
|    n_updates       | 10199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 218      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 139      |
|    critic_loss     | 755      |
|    learning_rate   | 0.001    |
|    n_updates       | 10599    |
---------------------------------
=== Iterazione IRL 12 ===
Loss reward (iter 12): -77.11366271972656
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 248      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 164      |
|    critic_loss     | 757      |
|    learning_rate   | 0.001    |
|    n_updates       | 11099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 217      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 187      |
|    critic_loss     | 1.05e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 11499    |
---------------------------------
=== Iterazione IRL 13 ===
Loss reward (iter 13): -98.40188598632812
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 249      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 222      |
|    critic_loss     | 1.15e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 11999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 223      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 244      |
|    critic_loss     | 1.5e+03  |
|    learning_rate   | 0.001    |
|    n_updates       | 12399    |
---------------------------------
=== Iterazione IRL 14 ===
Loss reward (iter 14): -124.29922485351562
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 263      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 270      |
|    critic_loss     | 1.73e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 12899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 305      |
|    critic_loss     | 2.21e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 13299    |
---------------------------------
=== Iterazione IRL 15 ===
Loss reward (iter 15): -151.04039001464844
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 263      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 356      |
|    critic_loss     | 2.58e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 13799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 384      |
|    critic_loss     | 3.51e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 14199    |
---------------------------------
=== Iterazione IRL 16 ===
Loss reward (iter 16): -186.86141967773438
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 416      |
|    critic_loss     | 4.09e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 14699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 460      |
|    critic_loss     | 4.95e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 15099    |
---------------------------------
=== Iterazione IRL 17 ===
Loss reward (iter 17): -223.87295532226562
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 264      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 524      |
|    critic_loss     | 6.22e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 15599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 569      |
|    critic_loss     | 6.28e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 15999    |
---------------------------------
=== Iterazione IRL 18 ===
Loss reward (iter 18): -266.5504455566406
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 263      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 633      |
|    critic_loss     | 7.14e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 16499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 679      |
|    critic_loss     | 7.23e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 16899    |
---------------------------------
=== Iterazione IRL 19 ===
Loss reward (iter 19): -313.7633972167969
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 263      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 766      |
|    critic_loss     | 9.96e+03 |
|    learning_rate   | 0.001    |
|    n_updates       | 17399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 836      |
|    critic_loss     | 1.2e+04  |
|    learning_rate   | 0.001    |
|    n_updates       | 17799    |
---------------------------------
=== Iterazione IRL 20 ===
Loss reward (iter 20): -368.3013916015625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 263      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 936      |
|    critic_loss     | 1.31e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 18299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 992      |
|    critic_loss     | 1.31e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 18699    |
---------------------------------
=== Iterazione IRL 21 ===
Loss reward (iter 21): -422.6715087890625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 263      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.07e+03 |
|    critic_loss     | 1.93e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 19199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.16e+03 |
|    critic_loss     | 2.02e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 19599    |
---------------------------------
=== Iterazione IRL 22 ===
Loss reward (iter 22): -495.9808654785156
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 263      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.27e+03 |
|    critic_loss     | 2.46e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 20099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | 1.36e+03 |
|    critic_loss     | 2.8e+04  |
|    learning_rate   | 0.001    |
|    n_updates       | 20499    |
---------------------------------
=== Iterazione IRL 23 ===
Loss reward (iter 23): -575.0787353515625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 248      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | 1.5e+03  |
|    critic_loss     | 3.19e+04 |
|    learning_rate   | 0.001    |
|    n_updates       | 20999    |
---------------------------------
