Inizio training IRL
Using device: cuda
Using cuda device
=== Iterazione IRL 0 ===
Loss reward (iter 0): 7.387985706329346
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 248      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -0.507   |
|    critic_loss     | 0.000668 |
|    learning_rate   | 0.001    |
|    n_updates       | 299      |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 226      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -0.911   |
|    critic_loss     | 0.00157  |
|    learning_rate   | 0.001    |
|    n_updates       | 699      |
---------------------------------
=== Iterazione IRL 1 ===
Loss reward (iter 1): 7.293301582336426
=== Iterazione IRL 2 ===
Loss reward (iter 2): 7.2881646156311035
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -1.3     |
|    critic_loss     | 0.00406  |
|    learning_rate   | 0.001    |
|    n_updates       | 1199     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -1.62    |
|    critic_loss     | 0.00483  |
|    learning_rate   | 0.001    |
|    n_updates       | 1599     |
---------------------------------
=== Iterazione IRL 3 ===
Loss reward (iter 3): 7.321118354797363
=== Iterazione IRL 4 ===
Loss reward (iter 4): 7.330606460571289
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2       |
|    critic_loss     | 0.00733  |
|    learning_rate   | 0.001    |
|    n_updates       | 2099     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.32    |
|    critic_loss     | 0.0101   |
|    learning_rate   | 0.001    |
|    n_updates       | 2499     |
---------------------------------
=== Iterazione IRL 5 ===
Loss reward (iter 5): 7.322754383087158
=== Iterazione IRL 6 ===
Loss reward (iter 6): 7.3428874015808105
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.65    |
|    critic_loss     | 0.0111   |
|    learning_rate   | 0.001    |
|    n_updates       | 2999     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.89    |
|    critic_loss     | 0.0135   |
|    learning_rate   | 0.001    |
|    n_updates       | 3399     |
---------------------------------
=== Iterazione IRL 7 ===
Loss reward (iter 7): 7.306488990783691
=== Iterazione IRL 8 ===
Loss reward (iter 8): 7.331013202667236
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.3     |
|    critic_loss     | 0.0156   |
|    learning_rate   | 0.001    |
|    n_updates       | 3899     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.53    |
|    critic_loss     | 0.0221   |
|    learning_rate   | 0.001    |
|    n_updates       | 4299     |
---------------------------------
=== Iterazione IRL 9 ===
Loss reward (iter 9): 7.315803050994873
=== Iterazione IRL 10 ===
Loss reward (iter 10): 7.318241596221924
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.89    |
|    critic_loss     | 0.0197   |
|    learning_rate   | 0.001    |
|    n_updates       | 4799     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.22    |
|    critic_loss     | 0.0369   |
|    learning_rate   | 0.001    |
|    n_updates       | 5199     |
---------------------------------
=== Iterazione IRL 11 ===
Loss reward (iter 11): 7.259909629821777
=== Iterazione IRL 12 ===
Loss reward (iter 12): 7.286928653717041
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4.46    |
|    critic_loss     | 0.025    |
|    learning_rate   | 0.001    |
|    n_updates       | 5699     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.66    |
|    critic_loss     | 0.0276   |
|    learning_rate   | 0.001    |
|    n_updates       | 6099     |
---------------------------------
=== Iterazione IRL 13 ===
Loss reward (iter 13): 7.284389495849609
=== Iterazione IRL 14 ===
Loss reward (iter 14): 7.289835453033447
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.03    |
|    critic_loss     | 0.0359   |
|    learning_rate   | 0.001    |
|    n_updates       | 6599     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.27    |
|    critic_loss     | 0.034    |
|    learning_rate   | 0.001    |
|    n_updates       | 6999     |
---------------------------------
=== Iterazione IRL 15 ===
Loss reward (iter 15): 7.279880046844482
=== Iterazione IRL 16 ===
Loss reward (iter 16): 7.291471481323242
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -5.58    |
|    critic_loss     | 0.0322   |
|    learning_rate   | 0.001    |
|    n_updates       | 7499     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -5.83    |
|    critic_loss     | 0.034    |
|    learning_rate   | 0.001    |
|    n_updates       | 7899     |
---------------------------------
=== Iterazione IRL 17 ===
Loss reward (iter 17): 7.286341667175293
=== Iterazione IRL 18 ===
Loss reward (iter 18): 7.272878646850586
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 235      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.08    |
|    critic_loss     | 0.0421   |
|    learning_rate   | 0.001    |
|    n_updates       | 8399     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 204      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.32    |
|    critic_loss     | 0.043    |
|    learning_rate   | 0.001    |
|    n_updates       | 8799     |
---------------------------------
=== Iterazione IRL 19 ===
Loss reward (iter 19): 7.282450199127197
=== Iterazione IRL 20 ===
Loss reward (iter 20): 7.275038719177246
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 230      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -6.61    |
|    critic_loss     | 0.0482   |
|    learning_rate   | 0.001    |
|    n_updates       | 9299     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 202      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.8     |
|    critic_loss     | 0.0429   |
|    learning_rate   | 0.001    |
|    n_updates       | 9699     |
---------------------------------
=== Iterazione IRL 21 ===
Loss reward (iter 21): 7.286057472229004
=== Iterazione IRL 22 ===
Loss reward (iter 22): 7.29895544052124
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 230      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.04    |
|    critic_loss     | 0.0509   |
|    learning_rate   | 0.001    |
|    n_updates       | 10199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.18    |
|    critic_loss     | 0.0524   |
|    learning_rate   | 0.001    |
|    n_updates       | 10599    |
---------------------------------
=== Iterazione IRL 23 ===
Loss reward (iter 23): 7.288340091705322
=== Iterazione IRL 24 ===
Loss reward (iter 24): 7.287357330322266
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 249      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.44    |
|    critic_loss     | 0.0577   |
|    learning_rate   | 0.001    |
|    n_updates       | 11099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 219      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -7.65    |
|    critic_loss     | 0.0571   |
|    learning_rate   | 0.001    |
|    n_updates       | 11499    |
---------------------------------
=== Iterazione IRL 25 ===
Loss reward (iter 25): 7.286966323852539
=== Iterazione IRL 26 ===
Loss reward (iter 26): 7.2636919021606445
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 249      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -7.85    |
|    critic_loss     | 0.0676   |
|    learning_rate   | 0.001    |
|    n_updates       | 11999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 218      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.04    |
|    critic_loss     | 0.0746   |
|    learning_rate   | 0.001    |
|    n_updates       | 12399    |
---------------------------------
=== Iterazione IRL 27 ===
Loss reward (iter 27): 7.274492263793945
=== Iterazione IRL 28 ===
Loss reward (iter 28): 7.2712883949279785
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 249      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.34    |
|    critic_loss     | 0.0725   |
|    learning_rate   | 0.001    |
|    n_updates       | 12899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 218      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.43    |
|    critic_loss     | 0.0722   |
|    learning_rate   | 0.001    |
|    n_updates       | 13299    |
---------------------------------
=== Iterazione IRL 29 ===
Loss reward (iter 29): 7.276342391967773
=== Iterazione IRL 30 ===
Loss reward (iter 30): 7.257093906402588
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 249      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.66    |
|    critic_loss     | 0.0703   |
|    learning_rate   | 0.001    |
|    n_updates       | 13799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 218      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -8.7     |
|    critic_loss     | 0.0737   |
|    learning_rate   | 0.001    |
|    n_updates       | 14199    |
---------------------------------
=== Iterazione IRL 31 ===
Loss reward (iter 31): 7.274365425109863
=== Iterazione IRL 32 ===
Loss reward (iter 32): 7.270235061645508
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 249      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -8.97    |
|    critic_loss     | 0.0811   |
|    learning_rate   | 0.001    |
|    n_updates       | 14699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 218      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -9.09    |
|    critic_loss     | 0.107    |
|    learning_rate   | 0.001    |
|    n_updates       | 15099    |
---------------------------------
=== Iterazione IRL 33 ===
Loss reward (iter 33): 7.241958141326904
=== Iterazione IRL 34 ===
Loss reward (iter 34): 7.283831596374512
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 247      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -9.29    |
|    critic_loss     | 0.077    |
|    learning_rate   | 0.001    |
|    n_updates       | 15599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 216      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -9.44    |
|    critic_loss     | 0.0844   |
|    learning_rate   | 0.001    |
|    n_updates       | 15999    |
---------------------------------
=== Iterazione IRL 35 ===
Loss reward (iter 35): 7.259091854095459
=== Iterazione IRL 36 ===
Loss reward (iter 36): 7.254910469055176
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 246      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -9.47    |
|    critic_loss     | 0.0817   |
|    learning_rate   | 0.001    |
|    n_updates       | 16499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 215      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -9.62    |
|    critic_loss     | 0.0901   |
|    learning_rate   | 0.001    |
|    n_updates       | 16899    |
---------------------------------
=== Iterazione IRL 37 ===
Loss reward (iter 37): 7.259459972381592
=== Iterazione IRL 38 ===
Loss reward (iter 38): 7.261971473693848
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 246      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -9.89    |
|    critic_loss     | 0.0899   |
|    learning_rate   | 0.001    |
|    n_updates       | 17399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 216      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -9.95    |
|    critic_loss     | 0.0927   |
|    learning_rate   | 0.001    |
|    n_updates       | 17799    |
---------------------------------
=== Iterazione IRL 39 ===
Loss reward (iter 39): 7.258239269256592
=== Iterazione IRL 40 ===
Loss reward (iter 40): 7.272027492523193
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 246      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10      |
|    critic_loss     | 0.0905   |
|    learning_rate   | 0.001    |
|    n_updates       | 18299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 216      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.2    |
|    critic_loss     | 0.123    |
|    learning_rate   | 0.001    |
|    n_updates       | 18699    |
---------------------------------
=== Iterazione IRL 41 ===
Loss reward (iter 41): 7.257831573486328
=== Iterazione IRL 42 ===
Loss reward (iter 42): 7.277796268463135
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 246      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.2    |
|    critic_loss     | 0.0773   |
|    learning_rate   | 0.001    |
|    n_updates       | 19199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 216      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.3    |
|    critic_loss     | 0.0881   |
|    learning_rate   | 0.001    |
|    n_updates       | 19599    |
---------------------------------
=== Iterazione IRL 43 ===
Loss reward (iter 43): 7.2461066246032715
=== Iterazione IRL 44 ===
Loss reward (iter 44): 7.263895034790039
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 246      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.5    |
|    critic_loss     | 0.125    |
|    learning_rate   | 0.001    |
|    n_updates       | 20099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 215      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.5    |
|    critic_loss     | 0.0931   |
|    learning_rate   | 0.001    |
|    n_updates       | 20499    |
---------------------------------
=== Iterazione IRL 45 ===
Loss reward (iter 45): 7.248729705810547
=== Iterazione IRL 46 ===
Loss reward (iter 46): 7.234395503997803
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 246      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.7    |
|    critic_loss     | 0.134    |
|    learning_rate   | 0.001    |
|    n_updates       | 20999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 216      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.7    |
|    critic_loss     | 0.151    |
|    learning_rate   | 0.001    |
|    n_updates       | 21399    |
---------------------------------
=== Iterazione IRL 47 ===
Loss reward (iter 47): 7.246395111083984
=== Iterazione IRL 48 ===
Loss reward (iter 48): 7.256618022918701
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 249      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.8    |
|    critic_loss     | 0.105    |
|    learning_rate   | 0.001    |
|    n_updates       | 21899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 219      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11      |
|    critic_loss     | 0.106    |
|    learning_rate   | 0.001    |
|    n_updates       | 22299    |
---------------------------------
=== Iterazione IRL 49 ===
Loss reward (iter 49): 7.2553839683532715
=== Iterazione IRL 50 ===
Loss reward (iter 50): 7.258419513702393
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 250      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11      |
|    critic_loss     | 0.119    |
|    learning_rate   | 0.001    |
|    n_updates       | 22799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 219      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.3    |
|    critic_loss     | 0.14     |
|    learning_rate   | 0.001    |
|    n_updates       | 23199    |
---------------------------------
=== Iterazione IRL 51 ===
Loss reward (iter 51): 7.1922383308410645
=== Iterazione IRL 52 ===
Loss reward (iter 52): 7.2457122802734375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.1    |
|    critic_loss     | 0.104    |
|    learning_rate   | 0.001    |
|    n_updates       | 23699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.3    |
|    critic_loss     | 0.127    |
|    learning_rate   | 0.001    |
|    n_updates       | 24099    |
---------------------------------
=== Iterazione IRL 53 ===
Loss reward (iter 53): 7.230559825897217
=== Iterazione IRL 54 ===
Loss reward (iter 54): 7.252654552459717
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.2    |
|    critic_loss     | 0.135    |
|    learning_rate   | 0.001    |
|    n_updates       | 24599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.3    |
|    critic_loss     | 0.165    |
|    learning_rate   | 0.001    |
|    n_updates       | 24999    |
---------------------------------
=== Iterazione IRL 55 ===
Loss reward (iter 55): 7.238362789154053
=== Iterazione IRL 56 ===
Loss reward (iter 56): 7.230147361755371
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.4    |
|    critic_loss     | 0.135    |
|    learning_rate   | 0.001    |
|    n_updates       | 25499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.6    |
|    critic_loss     | 0.141    |
|    learning_rate   | 0.001    |
|    n_updates       | 25899    |
---------------------------------
=== Iterazione IRL 57 ===
Loss reward (iter 57): 7.235313892364502
=== Iterazione IRL 58 ===
Loss reward (iter 58): 7.223021507263184
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.5    |
|    critic_loss     | 0.122    |
|    learning_rate   | 0.001    |
|    n_updates       | 26399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.5    |
|    critic_loss     | 0.137    |
|    learning_rate   | 0.001    |
|    n_updates       | 26799    |
---------------------------------
=== Iterazione IRL 59 ===
Loss reward (iter 59): 7.2490763664245605
=== Iterazione IRL 60 ===
Loss reward (iter 60): 7.208989143371582
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.8    |
|    critic_loss     | 0.128    |
|    learning_rate   | 0.001    |
|    n_updates       | 27299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 232      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.7    |
|    critic_loss     | 0.11     |
|    learning_rate   | 0.001    |
|    n_updates       | 27699    |
---------------------------------
=== Iterazione IRL 61 ===
Loss reward (iter 61): 7.240684986114502
=== Iterazione IRL 62 ===
Loss reward (iter 62): 7.225744247436523
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.7    |
|    critic_loss     | 0.11     |
|    learning_rate   | 0.001    |
|    n_updates       | 28199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.8    |
|    critic_loss     | 0.12     |
|    learning_rate   | 0.001    |
|    n_updates       | 28599    |
---------------------------------
=== Iterazione IRL 63 ===
Loss reward (iter 63): 7.232870578765869
=== Iterazione IRL 64 ===
Loss reward (iter 64): 7.235225200653076
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.7    |
|    critic_loss     | 0.124    |
|    learning_rate   | 0.001    |
|    n_updates       | 29099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12      |
|    critic_loss     | 0.128    |
|    learning_rate   | 0.001    |
|    n_updates       | 29499    |
---------------------------------
=== Iterazione IRL 65 ===
Loss reward (iter 65): 7.202007293701172
=== Iterazione IRL 66 ===
Loss reward (iter 66): 7.242672920227051
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12      |
|    critic_loss     | 0.126    |
|    learning_rate   | 0.001    |
|    n_updates       | 29999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.9    |
|    critic_loss     | 0.108    |
|    learning_rate   | 0.001    |
|    n_updates       | 30399    |
---------------------------------
=== Iterazione IRL 67 ===
Loss reward (iter 67): 7.226532936096191
=== Iterazione IRL 68 ===
Loss reward (iter 68): 7.2063422203063965
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.1    |
|    critic_loss     | 0.188    |
|    learning_rate   | 0.001    |
|    n_updates       | 30899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.3    |
|    critic_loss     | 0.122    |
|    learning_rate   | 0.001    |
|    n_updates       | 31299    |
---------------------------------
=== Iterazione IRL 69 ===
Loss reward (iter 69): 7.210161209106445
=== Iterazione IRL 70 ===
Loss reward (iter 70): 7.210455417633057
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 262      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.1    |
|    critic_loss     | 0.0748   |
|    learning_rate   | 0.001    |
|    n_updates       | 31799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 230      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.2    |
|    critic_loss     | 0.0941   |
|    learning_rate   | 0.001    |
|    n_updates       | 32199    |
---------------------------------
=== Iterazione IRL 71 ===
Loss reward (iter 71): 7.222463607788086
=== Iterazione IRL 72 ===
Loss reward (iter 72): 7.231088638305664
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 262      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.3    |
|    critic_loss     | 0.101    |
|    learning_rate   | 0.001    |
|    n_updates       | 32699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 230      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.3    |
|    critic_loss     | 0.0917   |
|    learning_rate   | 0.001    |
|    n_updates       | 33099    |
---------------------------------
=== Iterazione IRL 73 ===
Loss reward (iter 73): 7.232902526855469
=== Iterazione IRL 74 ===
Loss reward (iter 74): 7.252490997314453
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 261      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.4    |
|    critic_loss     | 0.0671   |
|    learning_rate   | 0.001    |
|    n_updates       | 33599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 230      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.6    |
|    critic_loss     | 0.0921   |
|    learning_rate   | 0.001    |
|    n_updates       | 33999    |
---------------------------------
=== Iterazione IRL 75 ===
Loss reward (iter 75): 7.223604202270508
=== Iterazione IRL 76 ===
Loss reward (iter 76): 7.219980239868164
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 261      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.7    |
|    critic_loss     | 0.0756   |
|    learning_rate   | 0.001    |
|    n_updates       | 34499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 229      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.7    |
|    critic_loss     | 0.0514   |
|    learning_rate   | 0.001    |
|    n_updates       | 34899    |
---------------------------------
=== Iterazione IRL 77 ===
Loss reward (iter 77): 7.2164177894592285
=== Iterazione IRL 78 ===
Loss reward (iter 78): 7.234983921051025
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 261      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.8    |
|    critic_loss     | 0.0515   |
|    learning_rate   | 0.001    |
|    n_updates       | 35399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 229      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.9    |
|    critic_loss     | 0.0525   |
|    learning_rate   | 0.001    |
|    n_updates       | 35799    |
---------------------------------
=== Iterazione IRL 79 ===
Loss reward (iter 79): 7.228965759277344
=== Iterazione IRL 80 ===
Loss reward (iter 80): 7.234569549560547
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 261      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.9    |
|    critic_loss     | 0.032    |
|    learning_rate   | 0.001    |
|    n_updates       | 36299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 229      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.1    |
|    critic_loss     | 0.0379   |
|    learning_rate   | 0.001    |
|    n_updates       | 36699    |
---------------------------------
=== Iterazione IRL 81 ===
Loss reward (iter 81): 7.233537673950195
=== Iterazione IRL 82 ===
Loss reward (iter 82): 7.229840278625488
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 261      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.2    |
|    critic_loss     | 0.0359   |
|    learning_rate   | 0.001    |
|    n_updates       | 37199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 229      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.2    |
|    critic_loss     | 0.0406   |
|    learning_rate   | 0.001    |
|    n_updates       | 37599    |
---------------------------------
=== Iterazione IRL 83 ===
Loss reward (iter 83): 7.200348854064941
=== Iterazione IRL 84 ===
Loss reward (iter 84): 7.200753688812256
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 261      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.3    |
|    critic_loss     | 0.0395   |
|    learning_rate   | 0.001    |
|    n_updates       | 38099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 229      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.4    |
|    critic_loss     | 0.0463   |
|    learning_rate   | 0.001    |
|    n_updates       | 38499    |
---------------------------------
=== Iterazione IRL 85 ===
Loss reward (iter 85): 7.220664024353027
=== Iterazione IRL 86 ===
Loss reward (iter 86): 7.202192783355713
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 261      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.5    |
|    critic_loss     | 0.0518   |
|    learning_rate   | 0.001    |
|    n_updates       | 38999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 230      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.6    |
|    critic_loss     | 0.0616   |
|    learning_rate   | 0.001    |
|    n_updates       | 39399    |
---------------------------------
=== Iterazione IRL 87 ===
Loss reward (iter 87): 7.215882301330566
=== Iterazione IRL 88 ===
Loss reward (iter 88): 7.210367202758789
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 262      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.7    |
|    critic_loss     | 0.0525   |
|    learning_rate   | 0.001    |
|    n_updates       | 39899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 230      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.8    |
|    critic_loss     | 0.0518   |
|    learning_rate   | 0.001    |
|    n_updates       | 40299    |
---------------------------------
=== Iterazione IRL 89 ===
Loss reward (iter 89): 7.205278396606445
=== Iterazione IRL 90 ===
Loss reward (iter 90): 7.207370758056641
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 262      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.9    |
|    critic_loss     | 0.0448   |
|    learning_rate   | 0.001    |
|    n_updates       | 40799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 230      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.9    |
|    critic_loss     | 0.0486   |
|    learning_rate   | 0.001    |
|    n_updates       | 41199    |
---------------------------------
=== Iterazione IRL 91 ===
Loss reward (iter 91): 7.203708171844482
=== Iterazione IRL 92 ===
Loss reward (iter 92): 7.213056564331055
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 262      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14      |
|    critic_loss     | 0.0473   |
|    learning_rate   | 0.001    |
|    n_updates       | 41699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 230      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.2    |
|    critic_loss     | 0.0454   |
|    learning_rate   | 0.001    |
|    n_updates       | 42099    |
---------------------------------
=== Iterazione IRL 93 ===
Loss reward (iter 93): 7.219264507293701
=== Iterazione IRL 94 ===
Loss reward (iter 94): 7.206552028656006
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 261      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.2    |
|    critic_loss     | 0.046    |
|    learning_rate   | 0.001    |
|    n_updates       | 42599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 229      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.1    |
|    critic_loss     | 0.0624   |
|    learning_rate   | 0.001    |
|    n_updates       | 42999    |
---------------------------------
=== Iterazione IRL 95 ===
Loss reward (iter 95): 7.1928839683532715
=== Iterazione IRL 96 ===
Loss reward (iter 96): 7.1733198165893555
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 261      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.2    |
|    critic_loss     | 0.0559   |
|    learning_rate   | 0.001    |
|    n_updates       | 43499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 230      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.4    |
|    critic_loss     | 0.05     |
|    learning_rate   | 0.001    |
|    n_updates       | 43899    |
---------------------------------
=== Iterazione IRL 97 ===
Loss reward (iter 97): 7.202465057373047
=== Iterazione IRL 98 ===
Loss reward (iter 98): 7.157378196716309
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 261      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.3    |
|    critic_loss     | 0.0638   |
|    learning_rate   | 0.001    |
|    n_updates       | 44399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 229      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.3    |
|    critic_loss     | 0.0597   |
|    learning_rate   | 0.001    |
|    n_updates       | 44799    |
---------------------------------
=== Iterazione IRL 99 ===
Loss reward (iter 99): 7.199392795562744
=== Iterazione IRL 100 ===
Loss reward (iter 100): 7.184959411621094
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 261      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.5    |
|    critic_loss     | 0.0688   |
|    learning_rate   | 0.001    |
|    n_updates       | 45299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 230      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.5    |
|    critic_loss     | 0.0444   |
|    learning_rate   | 0.001    |
|    n_updates       | 45699    |
---------------------------------
=== Iterazione IRL 101 ===
Loss reward (iter 101): 7.203860282897949
=== Iterazione IRL 102 ===
Loss reward (iter 102): 7.182459354400635
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 261      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.7    |
|    critic_loss     | 0.0591   |
|    learning_rate   | 0.001    |
|    n_updates       | 46199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 230      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.8    |
|    critic_loss     | 0.0642   |
|    learning_rate   | 0.001    |
|    n_updates       | 46599    |
---------------------------------
=== Iterazione IRL 103 ===
Loss reward (iter 103): 7.183241844177246
=== Iterazione IRL 104 ===
Loss reward (iter 104): 7.178107738494873
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 261      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.5    |
|    critic_loss     | 0.0938   |
|    learning_rate   | 0.001    |
|    n_updates       | 47099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 229      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.7    |
|    critic_loss     | 0.0744   |
|    learning_rate   | 0.001    |
|    n_updates       | 47499    |
---------------------------------
=== Iterazione IRL 105 ===
Loss reward (iter 105): 7.205513000488281
=== Iterazione IRL 106 ===
Loss reward (iter 106): 7.184082984924316
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 261      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.8    |
|    critic_loss     | 0.0805   |
|    learning_rate   | 0.001    |
|    n_updates       | 47999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 230      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.9    |
|    critic_loss     | 0.079    |
|    learning_rate   | 0.001    |
|    n_updates       | 48399    |
---------------------------------
=== Iterazione IRL 107 ===
Loss reward (iter 107): 7.21154260635376
=== Iterazione IRL 108 ===
Loss reward (iter 108): 7.183442115783691
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 262      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.9    |
|    critic_loss     | 0.0781   |
|    learning_rate   | 0.001    |
|    n_updates       | 48899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 230      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.7    |
|    critic_loss     | 0.0795   |
|    learning_rate   | 0.001    |
|    n_updates       | 49299    |
---------------------------------
=== Iterazione IRL 109 ===
Loss reward (iter 109): 7.185892581939697
=== Iterazione IRL 110 ===
Loss reward (iter 110): 7.198848724365234
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 262      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -15      |
|    critic_loss     | 0.0847   |
|    learning_rate   | 0.001    |
|    n_updates       | 49799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 230      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.9    |
|    critic_loss     | 0.0944   |
|    learning_rate   | 0.001    |
|    n_updates       | 50199    |
---------------------------------
=== Iterazione IRL 111 ===
Loss reward (iter 111): 7.169360637664795
=== Iterazione IRL 112 ===
Loss reward (iter 112): 7.153070449829102
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 262      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -15.1    |
|    critic_loss     | 0.105    |
|    learning_rate   | 0.001    |
|    n_updates       | 50699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 230      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.8    |
|    critic_loss     | 0.0875   |
|    learning_rate   | 0.001    |
|    n_updates       | 51099    |
---------------------------------
=== Iterazione IRL 113 ===
Loss reward (iter 113): 7.185207366943359
=== Iterazione IRL 114 ===
Loss reward (iter 114): 7.2176079750061035
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 261      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -15.1    |
|    critic_loss     | 0.0786   |
|    learning_rate   | 0.001    |
|    n_updates       | 51599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 229      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.9    |
|    critic_loss     | 0.0909   |
|    learning_rate   | 0.001    |
|    n_updates       | 51999    |
---------------------------------
=== Iterazione IRL 115 ===
Loss reward (iter 115): 7.1505866050720215
=== Iterazione IRL 116 ===
Loss reward (iter 116): 7.192473411560059
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 258      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.9    |
|    critic_loss     | 0.0865   |
|    learning_rate   | 0.001    |
|    n_updates       | 52499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 220      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.8    |
|    critic_loss     | 0.081    |
|    learning_rate   | 0.001    |
|    n_updates       | 52899    |
---------------------------------
=== Iterazione IRL 117 ===
Loss reward (iter 117): 7.19149923324585
=== Iterazione IRL 118 ===
Loss reward (iter 118): 7.1792778968811035
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 261      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -15.1    |
|    critic_loss     | 0.0898   |
|    learning_rate   | 0.001    |
|    n_updates       | 53399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 230      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -15.1    |
|    critic_loss     | 0.0973   |
|    learning_rate   | 0.001    |
|    n_updates       | 53799    |
---------------------------------
=== Iterazione IRL 119 ===
Loss reward (iter 119): 7.184704303741455
=== Iterazione IRL 120 ===
Loss reward (iter 120): 7.17141056060791
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 249      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -15      |
|    critic_loss     | 0.107    |
|    learning_rate   | 0.001    |
|    n_updates       | 54299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 217      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -15      |
|    critic_loss     | 0.0912   |
|    learning_rate   | 0.001    |
|    n_updates       | 54699    |
---------------------------------
=== Iterazione IRL 121 ===
Loss reward (iter 121): 7.160163879394531
=== Iterazione IRL 122 ===
Loss reward (iter 122): 7.217641830444336
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 246      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -15.1    |
|    critic_loss     | 0.107    |
|    learning_rate   | 0.001    |
|    n_updates       | 55199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 216      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -15      |
|    critic_loss     | 0.0986   |
|    learning_rate   | 0.001    |
|    n_updates       | 55599    |
---------------------------------
=== Iterazione IRL 123 ===
Loss reward (iter 123): 7.176513671875
=== Iterazione IRL 124 ===
Loss reward (iter 124): 7.173236846923828
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 246      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -15.1    |
|    critic_loss     | 0.105    |
|    learning_rate   | 0.001    |
|    n_updates       | 56099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 216      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.7    |
|    critic_loss     | 0.0985   |
|    learning_rate   | 0.001    |
|    n_updates       | 56499    |
---------------------------------
=== Iterazione IRL 125 ===
Loss reward (iter 125): 7.204797267913818
=== Iterazione IRL 126 ===
Loss reward (iter 126): 7.213576316833496
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 246      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.9    |
|    critic_loss     | 0.125    |
|    learning_rate   | 0.001    |
|    n_updates       | 56999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 216      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -15      |
|    critic_loss     | 0.134    |
|    learning_rate   | 0.001    |
|    n_updates       | 57399    |
---------------------------------
=== Iterazione IRL 127 ===
Loss reward (iter 127): 7.21077823638916
=== Iterazione IRL 128 ===
Loss reward (iter 128): 7.178423881530762
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 247      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -15      |
|    critic_loss     | 0.0993   |
|    learning_rate   | 0.001    |
|    n_updates       | 57899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 216      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -15.1    |
|    critic_loss     | 0.12     |
|    learning_rate   | 0.001    |
|    n_updates       | 58299    |
---------------------------------
=== Iterazione IRL 129 ===
Loss reward (iter 129): 7.189455509185791
=== Iterazione IRL 130 ===
Loss reward (iter 130): 7.203812122344971
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 247      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -15.1    |
|    critic_loss     | 0.115    |
|    learning_rate   | 0.001    |
|    n_updates       | 58799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 216      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.9    |
|    critic_loss     | 0.123    |
|    learning_rate   | 0.001    |
|    n_updates       | 59199    |
---------------------------------
=== Iterazione IRL 131 ===
Loss reward (iter 131): 7.200717926025391
=== Iterazione IRL 132 ===
Loss reward (iter 132): 7.179646968841553
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 247      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.9    |
|    critic_loss     | 0.132    |
|    learning_rate   | 0.001    |
|    n_updates       | 59699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 216      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.9    |
|    critic_loss     | 0.201    |
|    learning_rate   | 0.001    |
|    n_updates       | 60099    |
---------------------------------
=== Iterazione IRL 133 ===
Loss reward (iter 133): 7.174118995666504
=== Iterazione IRL 134 ===
Loss reward (iter 134): 7.164637565612793
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 247      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.9    |
|    critic_loss     | 0.147    |
|    learning_rate   | 0.001    |
|    n_updates       | 60599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 216      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.8    |
|    critic_loss     | 0.112    |
|    learning_rate   | 0.001    |
|    n_updates       | 60999    |
---------------------------------
=== Iterazione IRL 135 ===
Loss reward (iter 135): 7.169378280639648
=== Iterazione IRL 136 ===
Loss reward (iter 136): 7.182823181152344
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 247      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.9    |
|    critic_loss     | 0.104    |
|    learning_rate   | 0.001    |
|    n_updates       | 61499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 216      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.8    |
|    critic_loss     | 0.165    |
|    learning_rate   | 0.001    |
|    n_updates       | 61899    |
---------------------------------
=== Iterazione IRL 137 ===
Loss reward (iter 137): 7.179357528686523
=== Iterazione IRL 138 ===
Loss reward (iter 138): 7.166327953338623
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 247      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.8    |
|    critic_loss     | 0.14     |
|    learning_rate   | 0.001    |
|    n_updates       | 62399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 216      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.7    |
|    critic_loss     | 0.0977   |
|    learning_rate   | 0.001    |
|    n_updates       | 62799    |
---------------------------------
=== Iterazione IRL 139 ===
Loss reward (iter 139): 7.162633419036865
=== Iterazione IRL 140 ===
Loss reward (iter 140): 7.169960021972656
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 262      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.9    |
|    critic_loss     | 0.12     |
|    learning_rate   | 0.001    |
|    n_updates       | 63299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 222      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.8    |
|    critic_loss     | 0.139    |
|    learning_rate   | 0.001    |
|    n_updates       | 63699    |
---------------------------------
=== Iterazione IRL 141 ===
Loss reward (iter 141): 7.153507232666016
=== Iterazione IRL 142 ===
Loss reward (iter 142): 7.148063659667969
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 246      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.6    |
|    critic_loss     | 0.12     |
|    learning_rate   | 0.001    |
|    n_updates       | 64199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 216      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.7    |
|    critic_loss     | 0.161    |
|    learning_rate   | 0.001    |
|    n_updates       | 64599    |
---------------------------------
=== Iterazione IRL 143 ===
Loss reward (iter 143): 7.164107799530029
=== Iterazione IRL 144 ===
Loss reward (iter 144): 7.1762471199035645
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 247      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.7    |
|    critic_loss     | 0.147    |
|    learning_rate   | 0.001    |
|    n_updates       | 65099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 216      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.4    |
|    critic_loss     | 0.123    |
|    learning_rate   | 0.001    |
|    n_updates       | 65499    |
---------------------------------
=== Iterazione IRL 145 ===
Loss reward (iter 145): 7.1572394371032715
=== Iterazione IRL 146 ===
Loss reward (iter 146): 7.157925128936768
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 247      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.6    |
|    critic_loss     | 0.125    |
|    learning_rate   | 0.001    |
|    n_updates       | 65999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 216      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.3    |
|    critic_loss     | 0.13     |
|    learning_rate   | 0.001    |
|    n_updates       | 66399    |
---------------------------------
=== Iterazione IRL 147 ===
Loss reward (iter 147): 7.192816734313965
=== Iterazione IRL 148 ===
Loss reward (iter 148): 7.179236888885498
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 247      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.6    |
|    critic_loss     | 0.162    |
|    learning_rate   | 0.001    |
|    n_updates       | 66899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 216      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.4    |
|    critic_loss     | 0.136    |
|    learning_rate   | 0.001    |
|    n_updates       | 67299    |
---------------------------------
=== Iterazione IRL 149 ===
Loss reward (iter 149): 7.174374103546143
=== Iterazione IRL 150 ===
Loss reward (iter 150): 7.145671367645264
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 246      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.5    |
|    critic_loss     | 0.161    |
|    learning_rate   | 0.001    |
|    n_updates       | 67799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 216      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.6    |
|    critic_loss     | 0.173    |
|    learning_rate   | 0.001    |
|    n_updates       | 68199    |
---------------------------------
=== Iterazione IRL 151 ===
Loss reward (iter 151): 7.178836345672607
=== Iterazione IRL 152 ===
Loss reward (iter 152): 7.174055099487305
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 246      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.4    |
|    critic_loss     | 0.121    |
|    learning_rate   | 0.001    |
|    n_updates       | 68699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 216      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.2    |
|    critic_loss     | 0.151    |
|    learning_rate   | 0.001    |
|    n_updates       | 69099    |
---------------------------------
=== Iterazione IRL 153 ===
Loss reward (iter 153): 7.136130332946777
=== Iterazione IRL 154 ===
Loss reward (iter 154): 7.153936862945557
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 247      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.2    |
|    critic_loss     | 0.162    |
|    learning_rate   | 0.001    |
|    n_updates       | 69599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 216      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.2    |
|    critic_loss     | 0.158    |
|    learning_rate   | 0.001    |
|    n_updates       | 69999    |
---------------------------------
=== Iterazione IRL 155 ===
Loss reward (iter 155): 7.131887912750244
=== Iterazione IRL 156 ===
Loss reward (iter 156): 7.154903411865234
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 247      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.3    |
|    critic_loss     | 0.115    |
|    learning_rate   | 0.001    |
|    n_updates       | 70499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 216      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.2    |
|    critic_loss     | 0.131    |
|    learning_rate   | 0.001    |
|    n_updates       | 70899    |
---------------------------------
=== Iterazione IRL 157 ===
Loss reward (iter 157): 7.170378684997559
=== Iterazione IRL 158 ===
Loss reward (iter 158): 7.148031234741211
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 246      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.1    |
|    critic_loss     | 0.105    |
|    learning_rate   | 0.001    |
|    n_updates       | 71399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 216      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.1    |
|    critic_loss     | 0.142    |
|    learning_rate   | 0.001    |
|    n_updates       | 71799    |
---------------------------------
=== Iterazione IRL 159 ===
Loss reward (iter 159): 7.169571399688721
=== Iterazione IRL 160 ===
Loss reward (iter 160): 7.157170295715332
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 246      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.2    |
|    critic_loss     | 0.123    |
|    learning_rate   | 0.001    |
|    n_updates       | 72299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 216      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.1    |
|    critic_loss     | 0.134    |
|    learning_rate   | 0.001    |
|    n_updates       | 72699    |
---------------------------------
=== Iterazione IRL 161 ===
Loss reward (iter 161): 7.122159957885742
=== Iterazione IRL 162 ===
Loss reward (iter 162): 7.150578498840332
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 247      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.3    |
|    critic_loss     | 0.138    |
|    learning_rate   | 0.001    |
|    n_updates       | 73199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 216      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14      |
|    critic_loss     | 0.121    |
|    learning_rate   | 0.001    |
|    n_updates       | 73599    |
---------------------------------
=== Iterazione IRL 163 ===
Loss reward (iter 163): 7.1579670906066895
=== Iterazione IRL 164 ===
Loss reward (iter 164): 7.146742820739746
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 246      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.2    |
|    critic_loss     | 0.141    |
|    learning_rate   | 0.001    |
|    n_updates       | 74099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 215      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.1    |
|    critic_loss     | 0.112    |
|    learning_rate   | 0.001    |
|    n_updates       | 74499    |
---------------------------------
=== Iterazione IRL 165 ===
Loss reward (iter 165): 7.161883354187012
=== Iterazione IRL 166 ===
Loss reward (iter 166): 7.128591060638428
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 246      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.2    |
|    critic_loss     | 0.119    |
|    learning_rate   | 0.001    |
|    n_updates       | 74999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 216      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.2    |
|    critic_loss     | 0.14     |
|    learning_rate   | 0.001    |
|    n_updates       | 75399    |
---------------------------------
=== Iterazione IRL 167 ===
Loss reward (iter 167): 7.140343189239502
=== Iterazione IRL 168 ===
Loss reward (iter 168): 7.144705772399902
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 247      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.1    |
|    critic_loss     | 0.115    |
|    learning_rate   | 0.001    |
|    n_updates       | 75899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 216      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.1    |
|    critic_loss     | 0.121    |
|    learning_rate   | 0.001    |
|    n_updates       | 76299    |
---------------------------------
=== Iterazione IRL 169 ===
Loss reward (iter 169): 7.169395923614502
=== Iterazione IRL 170 ===
Loss reward (iter 170): 7.161302089691162
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 247      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.2    |
|    critic_loss     | 0.106    |
|    learning_rate   | 0.001    |
|    n_updates       | 76799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 216      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.2    |
|    critic_loss     | 0.0896   |
|    learning_rate   | 0.001    |
|    n_updates       | 77199    |
---------------------------------
=== Iterazione IRL 171 ===
Loss reward (iter 171): 7.137495040893555
=== Iterazione IRL 172 ===
Loss reward (iter 172): 7.136362075805664
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 247      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.2    |
|    critic_loss     | 0.111    |
|    learning_rate   | 0.001    |
|    n_updates       | 77699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 216      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.2    |
|    critic_loss     | 0.123    |
|    learning_rate   | 0.001    |
|    n_updates       | 78099    |
---------------------------------
=== Iterazione IRL 173 ===
Loss reward (iter 173): 7.131570816040039
=== Iterazione IRL 174 ===
Loss reward (iter 174): 7.126710414886475
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 247      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.2    |
|    critic_loss     | 0.104    |
|    learning_rate   | 0.001    |
|    n_updates       | 78599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 216      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.2    |
|    critic_loss     | 0.141    |
|    learning_rate   | 0.001    |
|    n_updates       | 78999    |
---------------------------------
=== Iterazione IRL 175 ===
Loss reward (iter 175): 7.1254096031188965
=== Iterazione IRL 176 ===
Loss reward (iter 176): 7.116565227508545
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 247      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.1    |
|    critic_loss     | 0.124    |
|    learning_rate   | 0.001    |
|    n_updates       | 79499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 216      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.1    |
|    critic_loss     | 0.0899   |
|    learning_rate   | 0.001    |
|    n_updates       | 79899    |
---------------------------------
=== Iterazione IRL 177 ===
Loss reward (iter 177): 7.155432224273682
=== Iterazione IRL 178 ===
Loss reward (iter 178): 7.146326065063477
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 247      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.1    |
|    critic_loss     | 0.104    |
|    learning_rate   | 0.001    |
|    n_updates       | 80399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 217      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.1    |
|    critic_loss     | 0.139    |
|    learning_rate   | 0.001    |
|    n_updates       | 80799    |
---------------------------------
=== Iterazione IRL 179 ===
Loss reward (iter 179): 7.130432605743408
=== Iterazione IRL 180 ===
Loss reward (iter 180): 7.1396284103393555
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 247      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.1    |
|    critic_loss     | 0.155    |
|    learning_rate   | 0.001    |
|    n_updates       | 81299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 216      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14      |
|    critic_loss     | 0.113    |
|    learning_rate   | 0.001    |
|    n_updates       | 81699    |
---------------------------------
=== Iterazione IRL 181 ===
Loss reward (iter 181): 7.121835231781006
=== Iterazione IRL 182 ===
Loss reward (iter 182): 7.120701313018799
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 247      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.3    |
|    critic_loss     | 0.103    |
|    learning_rate   | 0.001    |
|    n_updates       | 82199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 216      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.1    |
|    critic_loss     | 0.0941   |
|    learning_rate   | 0.001    |
|    n_updates       | 82599    |
---------------------------------
=== Iterazione IRL 183 ===
Loss reward (iter 183): 7.130050182342529
=== Iterazione IRL 184 ===
Loss reward (iter 184): 7.143208026885986
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 248      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.2    |
|    critic_loss     | 0.112    |
|    learning_rate   | 0.001    |
|    n_updates       | 83099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 223      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.1    |
|    critic_loss     | 0.105    |
|    learning_rate   | 0.001    |
|    n_updates       | 83499    |
---------------------------------
=== Iterazione IRL 185 ===
Loss reward (iter 185): 7.125658988952637
=== Iterazione IRL 186 ===
Loss reward (iter 186): 7.145235538482666
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 262      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14      |
|    critic_loss     | 0.114    |
|    learning_rate   | 0.001    |
|    n_updates       | 83999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 230      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.9    |
|    critic_loss     | 0.11     |
|    learning_rate   | 0.001    |
|    n_updates       | 84399    |
---------------------------------
=== Iterazione IRL 187 ===
Loss reward (iter 187): 7.099831581115723
=== Iterazione IRL 188 ===
Loss reward (iter 188): 7.142592906951904
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 263      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.2    |
|    critic_loss     | 0.14     |
|    learning_rate   | 0.001    |
|    n_updates       | 84899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 231      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.9    |
|    critic_loss     | 0.109    |
|    learning_rate   | 0.001    |
|    n_updates       | 85299    |
---------------------------------
=== Iterazione IRL 189 ===
Loss reward (iter 189): 7.130445957183838
=== Iterazione IRL 190 ===
Loss reward (iter 190): 7.101593494415283
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.9    |
|    critic_loss     | 0.146    |
|    learning_rate   | 0.001    |
|    n_updates       | 85799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14      |
|    critic_loss     | 0.125    |
|    learning_rate   | 0.001    |
|    n_updates       | 86199    |
---------------------------------
=== Iterazione IRL 191 ===
Loss reward (iter 191): 7.1075663566589355
=== Iterazione IRL 192 ===
Loss reward (iter 192): 7.104484558105469
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.8    |
|    critic_loss     | 0.118    |
|    learning_rate   | 0.001    |
|    n_updates       | 86699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 234      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.9    |
|    critic_loss     | 0.152    |
|    learning_rate   | 0.001    |
|    n_updates       | 87099    |
---------------------------------
=== Iterazione IRL 193 ===
Loss reward (iter 193): 7.127711296081543
=== Iterazione IRL 194 ===
Loss reward (iter 194): 7.093623638153076
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 267      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14      |
|    critic_loss     | 0.167    |
|    learning_rate   | 0.001    |
|    n_updates       | 87599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 234      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.8    |
|    critic_loss     | 0.137    |
|    learning_rate   | 0.001    |
|    n_updates       | 87999    |
---------------------------------
=== Iterazione IRL 195 ===
Loss reward (iter 195): 7.131788730621338
=== Iterazione IRL 196 ===
Loss reward (iter 196): 7.107944488525391
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 267      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.8    |
|    critic_loss     | 0.114    |
|    learning_rate   | 0.001    |
|    n_updates       | 88499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 234      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.8    |
|    critic_loss     | 0.133    |
|    learning_rate   | 0.001    |
|    n_updates       | 88899    |
---------------------------------
=== Iterazione IRL 197 ===
Loss reward (iter 197): 7.1163129806518555
=== Iterazione IRL 198 ===
Loss reward (iter 198): 7.122003078460693
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 267      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.6    |
|    critic_loss     | 0.124    |
|    learning_rate   | 0.001    |
|    n_updates       | 89399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 234      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.9    |
|    critic_loss     | 0.124    |
|    learning_rate   | 0.001    |
|    n_updates       | 89799    |
---------------------------------
=== Iterazione IRL 199 ===
Loss reward (iter 199): 7.117132186889648
=== Iterazione IRL 200 ===
Loss reward (iter 200): 7.126422882080078
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 267      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.8    |
|    critic_loss     | 0.143    |
|    learning_rate   | 0.001    |
|    n_updates       | 90299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 234      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.7    |
|    critic_loss     | 0.164    |
|    learning_rate   | 0.001    |
|    n_updates       | 90699    |
---------------------------------
=== Iterazione IRL 201 ===
Loss reward (iter 201): 7.107607364654541
=== Iterazione IRL 202 ===
Loss reward (iter 202): 7.135234832763672
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 267      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.6    |
|    critic_loss     | 0.115    |
|    learning_rate   | 0.001    |
|    n_updates       | 91199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 234      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.7    |
|    critic_loss     | 0.137    |
|    learning_rate   | 0.001    |
|    n_updates       | 91599    |
---------------------------------
=== Iterazione IRL 203 ===
Loss reward (iter 203): 7.084517955780029
=== Iterazione IRL 204 ===
Loss reward (iter 204): 7.10793399810791
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.5    |
|    critic_loss     | 0.128    |
|    learning_rate   | 0.001    |
|    n_updates       | 92099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.8    |
|    critic_loss     | 0.123    |
|    learning_rate   | 0.001    |
|    n_updates       | 92499    |
---------------------------------
=== Iterazione IRL 205 ===
Loss reward (iter 205): 7.092074871063232
=== Iterazione IRL 206 ===
Loss reward (iter 206): 7.127350807189941
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.7    |
|    critic_loss     | 0.131    |
|    learning_rate   | 0.001    |
|    n_updates       | 92999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.5    |
|    critic_loss     | 0.144    |
|    learning_rate   | 0.001    |
|    n_updates       | 93399    |
---------------------------------
=== Iterazione IRL 207 ===
Loss reward (iter 207): 7.109518051147461
=== Iterazione IRL 208 ===
Loss reward (iter 208): 7.124721050262451
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 267      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.3    |
|    critic_loss     | 0.16     |
|    learning_rate   | 0.001    |
|    n_updates       | 93899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.4    |
|    critic_loss     | 0.133    |
|    learning_rate   | 0.001    |
|    n_updates       | 94299    |
---------------------------------
=== Iterazione IRL 209 ===
Loss reward (iter 209): 7.126198768615723
=== Iterazione IRL 210 ===
Loss reward (iter 210): 7.09418249130249
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 267      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.3    |
|    critic_loss     | 0.154    |
|    learning_rate   | 0.001    |
|    n_updates       | 94799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 234      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.4    |
|    critic_loss     | 0.125    |
|    learning_rate   | 0.001    |
|    n_updates       | 95199    |
---------------------------------
=== Iterazione IRL 211 ===
Loss reward (iter 211): 7.104203224182129
=== Iterazione IRL 212 ===
Loss reward (iter 212): 7.0986328125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.3    |
|    critic_loss     | 0.124    |
|    learning_rate   | 0.001    |
|    n_updates       | 95699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.1    |
|    critic_loss     | 0.123    |
|    learning_rate   | 0.001    |
|    n_updates       | 96099    |
---------------------------------
=== Iterazione IRL 213 ===
Loss reward (iter 213): 7.106970310211182
=== Iterazione IRL 214 ===
Loss reward (iter 214): 7.096844673156738
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.2    |
|    critic_loss     | 0.162    |
|    learning_rate   | 0.001    |
|    n_updates       | 96599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.1    |
|    critic_loss     | 0.144    |
|    learning_rate   | 0.001    |
|    n_updates       | 96999    |
---------------------------------
=== Iterazione IRL 215 ===
Loss reward (iter 215): 7.113626956939697
=== Iterazione IRL 216 ===
Loss reward (iter 216): 7.106910228729248
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13      |
|    critic_loss     | 0.139    |
|    learning_rate   | 0.001    |
|    n_updates       | 97499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.1    |
|    critic_loss     | 0.167    |
|    learning_rate   | 0.001    |
|    n_updates       | 97899    |
---------------------------------
=== Iterazione IRL 217 ===
Loss reward (iter 217): 7.118561744689941
=== Iterazione IRL 218 ===
Loss reward (iter 218): 7.3155837059021
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.2    |
|    critic_loss     | 0.156    |
|    learning_rate   | 0.001    |
|    n_updates       | 98399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.9    |
|    critic_loss     | 0.143    |
|    learning_rate   | 0.001    |
|    n_updates       | 98799    |
---------------------------------
=== Iterazione IRL 219 ===
Loss reward (iter 219): 7.089667320251465
=== Iterazione IRL 220 ===
Loss reward (iter 220): 7.164990425109863
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.8    |
|    critic_loss     | 0.155    |
|    learning_rate   | 0.001    |
|    n_updates       | 99299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 225      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.8    |
|    critic_loss     | 0.153    |
|    learning_rate   | 0.001    |
|    n_updates       | 99699    |
---------------------------------
=== Iterazione IRL 221 ===
Loss reward (iter 221): 7.161779403686523
=== Iterazione IRL 222 ===
Loss reward (iter 222): 7.187472343444824
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 232      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.8    |
|    critic_loss     | 0.145    |
|    learning_rate   | 0.001    |
|    n_updates       | 100199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 204      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.7    |
|    critic_loss     | 0.105    |
|    learning_rate   | 0.001    |
|    n_updates       | 100599   |
---------------------------------
=== Iterazione IRL 223 ===
Loss reward (iter 223): 7.182039260864258
=== Iterazione IRL 224 ===
Loss reward (iter 224): 7.180614948272705
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 232      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.8    |
|    critic_loss     | 0.116    |
|    learning_rate   | 0.001    |
|    n_updates       | 101099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 204      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.4    |
|    critic_loss     | 0.13     |
|    learning_rate   | 0.001    |
|    n_updates       | 101499   |
---------------------------------
=== Iterazione IRL 225 ===
Loss reward (iter 225): 7.203329563140869
=== Iterazione IRL 226 ===
Loss reward (iter 226): 7.185739517211914
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 232      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.9    |
|    critic_loss     | 0.14     |
|    learning_rate   | 0.001    |
|    n_updates       | 101999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 204      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.9    |
|    critic_loss     | 0.14     |
|    learning_rate   | 0.001    |
|    n_updates       | 102399   |
---------------------------------
=== Iterazione IRL 227 ===
Loss reward (iter 227): 7.175693988800049
=== Iterazione IRL 228 ===
Loss reward (iter 228): 7.16325569152832
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 232      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.6    |
|    critic_loss     | 0.119    |
|    learning_rate   | 0.001    |
|    n_updates       | 102899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 204      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.4    |
|    critic_loss     | 0.152    |
|    learning_rate   | 0.001    |
|    n_updates       | 103299   |
---------------------------------
=== Iterazione IRL 229 ===
Loss reward (iter 229): 7.163774013519287
=== Iterazione IRL 230 ===
Loss reward (iter 230): 7.139955520629883
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 232      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.6    |
|    critic_loss     | 0.099    |
|    learning_rate   | 0.001    |
|    n_updates       | 103799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 208      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.4    |
|    critic_loss     | 0.127    |
|    learning_rate   | 0.001    |
|    n_updates       | 104199   |
---------------------------------
=== Iterazione IRL 231 ===
Loss reward (iter 231): 7.1337080001831055
=== Iterazione IRL 232 ===
Loss reward (iter 232): 7.14546012878418
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 235      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.6    |
|    critic_loss     | 0.0907   |
|    learning_rate   | 0.001    |
|    n_updates       | 104699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 205      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.5    |
|    critic_loss     | 0.0712   |
|    learning_rate   | 0.001    |
|    n_updates       | 105099   |
---------------------------------
=== Iterazione IRL 233 ===
Loss reward (iter 233): 7.122954368591309
=== Iterazione IRL 234 ===
Loss reward (iter 234): 7.12451696395874
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 232      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.4    |
|    critic_loss     | 0.0919   |
|    learning_rate   | 0.001    |
|    n_updates       | 105599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 204      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.6    |
|    critic_loss     | 0.0805   |
|    learning_rate   | 0.001    |
|    n_updates       | 105999   |
---------------------------------
=== Iterazione IRL 235 ===
Loss reward (iter 235): 7.110559463500977
=== Iterazione IRL 236 ===
Loss reward (iter 236): 7.117451190948486
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 233      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.6    |
|    critic_loss     | 0.0811   |
|    learning_rate   | 0.001    |
|    n_updates       | 106499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 204      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.6    |
|    critic_loss     | 0.097    |
|    learning_rate   | 0.001    |
|    n_updates       | 106899   |
---------------------------------
=== Iterazione IRL 237 ===
Loss reward (iter 237): 7.134442329406738
=== Iterazione IRL 238 ===
Loss reward (iter 238): 7.120840549468994
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 233      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.5    |
|    critic_loss     | 0.0793   |
|    learning_rate   | 0.001    |
|    n_updates       | 107399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 204      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.5    |
|    critic_loss     | 0.095    |
|    learning_rate   | 0.001    |
|    n_updates       | 107799   |
---------------------------------
=== Iterazione IRL 239 ===
Loss reward (iter 239): 7.142083644866943
=== Iterazione IRL 240 ===
Loss reward (iter 240): 7.136938571929932
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 233      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.7    |
|    critic_loss     | 0.0914   |
|    learning_rate   | 0.001    |
|    n_updates       | 108299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 205      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.5    |
|    critic_loss     | 0.083    |
|    learning_rate   | 0.001    |
|    n_updates       | 108699   |
---------------------------------
=== Iterazione IRL 241 ===
Loss reward (iter 241): 7.144375801086426
=== Iterazione IRL 242 ===
Loss reward (iter 242): 7.125522613525391
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 233      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.6    |
|    critic_loss     | 0.0916   |
|    learning_rate   | 0.001    |
|    n_updates       | 109199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 205      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.6    |
|    critic_loss     | 0.0731   |
|    learning_rate   | 0.001    |
|    n_updates       | 109599   |
---------------------------------
=== Iterazione IRL 243 ===
Loss reward (iter 243): 7.11828088760376
=== Iterazione IRL 244 ===
Loss reward (iter 244): 7.115285396575928
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 233      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.6    |
|    critic_loss     | 0.0747   |
|    learning_rate   | 0.001    |
|    n_updates       | 110099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 205      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.6    |
|    critic_loss     | 0.0726   |
|    learning_rate   | 0.001    |
|    n_updates       | 110499   |
---------------------------------
=== Iterazione IRL 245 ===
Loss reward (iter 245): 7.1159162521362305
=== Iterazione IRL 246 ===
Loss reward (iter 246): 7.110563278198242
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 233      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.5    |
|    critic_loss     | 0.108    |
|    learning_rate   | 0.001    |
|    n_updates       | 110999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 205      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.6    |
|    critic_loss     | 0.0983   |
|    learning_rate   | 0.001    |
|    n_updates       | 111399   |
---------------------------------
=== Iterazione IRL 247 ===
Loss reward (iter 247): 7.124948978424072
=== Iterazione IRL 248 ===
Loss reward (iter 248): 7.089992046356201
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 233      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.5    |
|    critic_loss     | 0.0805   |
|    learning_rate   | 0.001    |
|    n_updates       | 111899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 205      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.7    |
|    critic_loss     | 0.0746   |
|    learning_rate   | 0.001    |
|    n_updates       | 112299   |
---------------------------------
=== Iterazione IRL 249 ===
Loss reward (iter 249): 7.131396293640137
=== Iterazione IRL 250 ===
Loss reward (iter 250): 7.0981764793396
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 233      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.6    |
|    critic_loss     | 0.0806   |
|    learning_rate   | 0.001    |
|    n_updates       | 112799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 205      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.7    |
|    critic_loss     | 0.0757   |
|    learning_rate   | 0.001    |
|    n_updates       | 113199   |
---------------------------------
=== Iterazione IRL 251 ===
Loss reward (iter 251): 7.14516544342041
=== Iterazione IRL 252 ===
Loss reward (iter 252): 7.1081342697143555
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 233      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.5    |
|    critic_loss     | 0.112    |
|    learning_rate   | 0.001    |
|    n_updates       | 113699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 205      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.6    |
|    critic_loss     | 0.0902   |
|    learning_rate   | 0.001    |
|    n_updates       | 114099   |
---------------------------------
=== Iterazione IRL 253 ===
Loss reward (iter 253): 7.122194290161133
=== Iterazione IRL 254 ===
Loss reward (iter 254): 7.085254669189453
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.7    |
|    critic_loss     | 0.0964   |
|    learning_rate   | 0.001    |
|    n_updates       | 114599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.3    |
|    critic_loss     | 0.0719   |
|    learning_rate   | 0.001    |
|    n_updates       | 114999   |
---------------------------------
=== Iterazione IRL 255 ===
Loss reward (iter 255): 7.1216020584106445
=== Iterazione IRL 256 ===
Loss reward (iter 256): 7.117808818817139
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.7    |
|    critic_loss     | 0.094    |
|    learning_rate   | 0.001    |
|    n_updates       | 115499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.5    |
|    critic_loss     | 0.0884   |
|    learning_rate   | 0.001    |
|    n_updates       | 115899   |
---------------------------------
=== Iterazione IRL 257 ===
Loss reward (iter 257): 7.092447757720947
=== Iterazione IRL 258 ===
Loss reward (iter 258): 7.115917205810547
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.8    |
|    critic_loss     | 0.114    |
|    learning_rate   | 0.001    |
|    n_updates       | 116399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 234      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.5    |
|    critic_loss     | 0.0817   |
|    learning_rate   | 0.001    |
|    n_updates       | 116799   |
---------------------------------
=== Iterazione IRL 259 ===
Loss reward (iter 259): 7.088956356048584
=== Iterazione IRL 260 ===
Loss reward (iter 260): 7.131490230560303
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.6    |
|    critic_loss     | 0.122    |
|    learning_rate   | 0.001    |
|    n_updates       | 117299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.5    |
|    critic_loss     | 0.0957   |
|    learning_rate   | 0.001    |
|    n_updates       | 117699   |
---------------------------------
=== Iterazione IRL 261 ===
Loss reward (iter 261): 7.091723918914795
=== Iterazione IRL 262 ===
Loss reward (iter 262): 7.1248674392700195
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.3    |
|    critic_loss     | 0.0968   |
|    learning_rate   | 0.001    |
|    n_updates       | 118199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.5    |
|    critic_loss     | 0.111    |
|    learning_rate   | 0.001    |
|    n_updates       | 118599   |
---------------------------------
=== Iterazione IRL 263 ===
Loss reward (iter 263): 7.088081359863281
=== Iterazione IRL 264 ===
Loss reward (iter 264): 7.0744781494140625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.4    |
|    critic_loss     | 0.105    |
|    learning_rate   | 0.001    |
|    n_updates       | 119099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.6    |
|    critic_loss     | 0.134    |
|    learning_rate   | 0.001    |
|    n_updates       | 119499   |
---------------------------------
=== Iterazione IRL 265 ===
Loss reward (iter 265): 7.116787433624268
=== Iterazione IRL 266 ===
Loss reward (iter 266): 7.06633996963501
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.4    |
|    critic_loss     | 0.0865   |
|    learning_rate   | 0.001    |
|    n_updates       | 119999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.3    |
|    critic_loss     | 0.102    |
|    learning_rate   | 0.001    |
|    n_updates       | 120399   |
---------------------------------
=== Iterazione IRL 267 ===
Loss reward (iter 267): 7.045020580291748
=== Iterazione IRL 268 ===
Loss reward (iter 268): 7.044932842254639
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 267      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.1    |
|    critic_loss     | 0.108    |
|    learning_rate   | 0.001    |
|    n_updates       | 120899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 234      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.3    |
|    critic_loss     | 0.103    |
|    learning_rate   | 0.001    |
|    n_updates       | 121299   |
---------------------------------
=== Iterazione IRL 269 ===
Loss reward (iter 269): 7.0931878089904785
=== Iterazione IRL 270 ===
Loss reward (iter 270): 7.125508785247803
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.2    |
|    critic_loss     | 0.0993   |
|    learning_rate   | 0.001    |
|    n_updates       | 121799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.2    |
|    critic_loss     | 0.104    |
|    learning_rate   | 0.001    |
|    n_updates       | 122199   |
---------------------------------
=== Iterazione IRL 271 ===
Loss reward (iter 271): 7.101561069488525
=== Iterazione IRL 272 ===
Loss reward (iter 272): 7.027801036834717
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.2    |
|    critic_loss     | 0.0867   |
|    learning_rate   | 0.001    |
|    n_updates       | 122699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.2    |
|    critic_loss     | 0.11     |
|    learning_rate   | 0.001    |
|    n_updates       | 123099   |
---------------------------------
=== Iterazione IRL 273 ===
Loss reward (iter 273): 7.097421646118164
=== Iterazione IRL 274 ===
Loss reward (iter 274): 7.098034381866455
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.2    |
|    critic_loss     | 0.103    |
|    learning_rate   | 0.001    |
|    n_updates       | 123599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.2    |
|    critic_loss     | 0.125    |
|    learning_rate   | 0.001    |
|    n_updates       | 123999   |
---------------------------------
=== Iterazione IRL 275 ===
Loss reward (iter 275): 7.091822624206543
=== Iterazione IRL 276 ===
Loss reward (iter 276): 7.073456764221191
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.9    |
|    critic_loss     | 0.0993   |
|    learning_rate   | 0.001    |
|    n_updates       | 124499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.1    |
|    critic_loss     | 0.112    |
|    learning_rate   | 0.001    |
|    n_updates       | 124899   |
---------------------------------
=== Iterazione IRL 277 ===
Loss reward (iter 277): 7.050971031188965
=== Iterazione IRL 278 ===
Loss reward (iter 278): 7.079371929168701
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.9    |
|    critic_loss     | 0.0951   |
|    learning_rate   | 0.001    |
|    n_updates       | 125399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12      |
|    critic_loss     | 0.106    |
|    learning_rate   | 0.001    |
|    n_updates       | 125799   |
---------------------------------
=== Iterazione IRL 279 ===
Loss reward (iter 279): 7.093132019042969
=== Iterazione IRL 280 ===
Loss reward (iter 280): 7.080839157104492
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12      |
|    critic_loss     | 0.138    |
|    learning_rate   | 0.001    |
|    n_updates       | 126299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.9    |
|    critic_loss     | 0.0874   |
|    learning_rate   | 0.001    |
|    n_updates       | 126699   |
---------------------------------
=== Iterazione IRL 281 ===
Loss reward (iter 281): 7.087612152099609
=== Iterazione IRL 282 ===
Loss reward (iter 282): 7.0765910148620605
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.9    |
|    critic_loss     | 0.102    |
|    learning_rate   | 0.001    |
|    n_updates       | 127199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.8    |
|    critic_loss     | 0.0925   |
|    learning_rate   | 0.001    |
|    n_updates       | 127599   |
---------------------------------
=== Iterazione IRL 283 ===
Loss reward (iter 283): 7.094257354736328
=== Iterazione IRL 284 ===
Loss reward (iter 284): 7.08286714553833
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.9    |
|    critic_loss     | 0.0839   |
|    learning_rate   | 0.001    |
|    n_updates       | 128099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12      |
|    critic_loss     | 0.0783   |
|    learning_rate   | 0.001    |
|    n_updates       | 128499   |
---------------------------------
=== Iterazione IRL 285 ===
Loss reward (iter 285): 7.048961639404297
=== Iterazione IRL 286 ===
Loss reward (iter 286): 7.060156345367432
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.9    |
|    critic_loss     | 0.0938   |
|    learning_rate   | 0.001    |
|    n_updates       | 128999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.8    |
|    critic_loss     | 0.0842   |
|    learning_rate   | 0.001    |
|    n_updates       | 129399   |
---------------------------------
=== Iterazione IRL 287 ===
Loss reward (iter 287): 7.031551361083984
=== Iterazione IRL 288 ===
Loss reward (iter 288): 7.079058647155762
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.8    |
|    critic_loss     | 0.0723   |
|    learning_rate   | 0.001    |
|    n_updates       | 129899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.9    |
|    critic_loss     | 0.104    |
|    learning_rate   | 0.001    |
|    n_updates       | 130299   |
---------------------------------
=== Iterazione IRL 289 ===
Loss reward (iter 289): 7.110970497131348
=== Iterazione IRL 290 ===
Loss reward (iter 290): 7.142467021942139
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.7    |
|    critic_loss     | 0.0965   |
|    learning_rate   | 0.001    |
|    n_updates       | 130799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.7    |
|    critic_loss     | 0.094    |
|    learning_rate   | 0.001    |
|    n_updates       | 131199   |
---------------------------------
=== Iterazione IRL 291 ===
Loss reward (iter 291): 7.0475969314575195
=== Iterazione IRL 292 ===
Loss reward (iter 292): 7.070565700531006
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 233      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.6    |
|    critic_loss     | 0.0744   |
|    learning_rate   | 0.001    |
|    n_updates       | 131699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 204      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.7    |
|    critic_loss     | 0.0712   |
|    learning_rate   | 0.001    |
|    n_updates       | 132099   |
---------------------------------
=== Iterazione IRL 293 ===
Loss reward (iter 293): 7.069674968719482
=== Iterazione IRL 294 ===
Loss reward (iter 294): 7.088367938995361
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 233      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.5    |
|    critic_loss     | 0.0725   |
|    learning_rate   | 0.001    |
|    n_updates       | 132599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 205      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.6    |
|    critic_loss     | 0.0775   |
|    learning_rate   | 0.001    |
|    n_updates       | 132999   |
---------------------------------
=== Iterazione IRL 295 ===
Loss reward (iter 295): 7.154879570007324
=== Iterazione IRL 296 ===
Loss reward (iter 296): 7.062426567077637
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 233      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.6    |
|    critic_loss     | 0.0814   |
|    learning_rate   | 0.001    |
|    n_updates       | 133499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 205      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.7    |
|    critic_loss     | 0.0625   |
|    learning_rate   | 0.001    |
|    n_updates       | 133899   |
---------------------------------
=== Iterazione IRL 297 ===
Loss reward (iter 297): 7.061045169830322
=== Iterazione IRL 298 ===
Loss reward (iter 298): 7.054059028625488
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 233      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.5    |
|    critic_loss     | 0.0794   |
|    learning_rate   | 0.001    |
|    n_updates       | 134399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 205      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.8    |
|    critic_loss     | 0.0547   |
|    learning_rate   | 0.001    |
|    n_updates       | 134799   |
---------------------------------
=== Iterazione IRL 299 ===
Loss reward (iter 299): 7.086572647094727
=== Iterazione IRL 300 ===
Loss reward (iter 300): 7.076927661895752
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 233      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.6    |
|    critic_loss     | 0.0512   |
|    learning_rate   | 0.001    |
|    n_updates       | 135299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 205      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.7    |
|    critic_loss     | 0.0544   |
|    learning_rate   | 0.001    |
|    n_updates       | 135699   |
---------------------------------
=== Iterazione IRL 301 ===
Loss reward (iter 301): 7.081427574157715
=== Iterazione IRL 302 ===
Loss reward (iter 302): 7.0540595054626465
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 257      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.9    |
|    critic_loss     | 0.0489   |
|    learning_rate   | 0.001    |
|    n_updates       | 136199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 227      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.8    |
|    critic_loss     | 0.036    |
|    learning_rate   | 0.001    |
|    n_updates       | 136599   |
---------------------------------
=== Iterazione IRL 303 ===
Loss reward (iter 303): 7.078191757202148
=== Iterazione IRL 304 ===
Loss reward (iter 304): 7.088452339172363
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 233      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.8    |
|    critic_loss     | 0.0442   |
|    learning_rate   | 0.001    |
|    n_updates       | 137099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 205      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.7    |
|    critic_loss     | 0.067    |
|    learning_rate   | 0.001    |
|    n_updates       | 137499   |
---------------------------------
=== Iterazione IRL 305 ===
Loss reward (iter 305): 7.059925079345703
=== Iterazione IRL 306 ===
Loss reward (iter 306): 7.079063415527344
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 233      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.7    |
|    critic_loss     | 0.0361   |
|    learning_rate   | 0.001    |
|    n_updates       | 137999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 205      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12      |
|    critic_loss     | 0.0421   |
|    learning_rate   | 0.001    |
|    n_updates       | 138399   |
---------------------------------
=== Iterazione IRL 307 ===
Loss reward (iter 307): 7.066717147827148
=== Iterazione IRL 308 ===
Loss reward (iter 308): 7.0751423835754395
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12      |
|    critic_loss     | 0.0249   |
|    learning_rate   | 0.001    |
|    n_updates       | 138899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12      |
|    critic_loss     | 0.0311   |
|    learning_rate   | 0.001    |
|    n_updates       | 139299   |
---------------------------------
=== Iterazione IRL 309 ===
Loss reward (iter 309): 7.06226110458374
=== Iterazione IRL 310 ===
Loss reward (iter 310): 7.079715251922607
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.2    |
|    critic_loss     | 0.0283   |
|    learning_rate   | 0.001    |
|    n_updates       | 139799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.1    |
|    critic_loss     | 0.0314   |
|    learning_rate   | 0.001    |
|    n_updates       | 140199   |
---------------------------------
=== Iterazione IRL 311 ===
Loss reward (iter 311): 7.094300270080566
=== Iterazione IRL 312 ===
Loss reward (iter 312): 7.0522918701171875
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.2    |
|    critic_loss     | 0.0269   |
|    learning_rate   | 0.001    |
|    n_updates       | 140699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 234      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.2    |
|    critic_loss     | 0.0288   |
|    learning_rate   | 0.001    |
|    n_updates       | 141099   |
---------------------------------
=== Iterazione IRL 313 ===
Loss reward (iter 313): 7.065780162811279
=== Iterazione IRL 314 ===
Loss reward (iter 314): 7.049342155456543
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.3    |
|    critic_loss     | 0.0219   |
|    learning_rate   | 0.001    |
|    n_updates       | 141599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.4    |
|    critic_loss     | 0.0232   |
|    learning_rate   | 0.001    |
|    n_updates       | 141999   |
---------------------------------
=== Iterazione IRL 315 ===
Loss reward (iter 315): 7.051673889160156
=== Iterazione IRL 316 ===
Loss reward (iter 316): 7.016117095947266
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 252      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.4    |
|    critic_loss     | 0.0187   |
|    learning_rate   | 0.001    |
|    n_updates       | 142499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 212      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.5    |
|    critic_loss     | 0.0281   |
|    learning_rate   | 0.001    |
|    n_updates       | 142899   |
---------------------------------
=== Iterazione IRL 317 ===
Loss reward (iter 317): 7.058536529541016
=== Iterazione IRL 318 ===
Loss reward (iter 318): 7.0524163246154785
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 233      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.7    |
|    critic_loss     | 0.0255   |
|    learning_rate   | 0.001    |
|    n_updates       | 143399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 205      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.7    |
|    critic_loss     | 0.0202   |
|    learning_rate   | 0.001    |
|    n_updates       | 143799   |
---------------------------------
=== Iterazione IRL 319 ===
Loss reward (iter 319): 7.038459777832031
=== Iterazione IRL 320 ===
Loss reward (iter 320): 7.0406599044799805
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 233      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.7    |
|    critic_loss     | 0.0218   |
|    learning_rate   | 0.001    |
|    n_updates       | 144299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 205      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.7    |
|    critic_loss     | 0.0252   |
|    learning_rate   | 0.001    |
|    n_updates       | 144699   |
---------------------------------
=== Iterazione IRL 321 ===
Loss reward (iter 321): 7.143523216247559
=== Iterazione IRL 322 ===
Loss reward (iter 322): 7.091820240020752
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 234      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.9    |
|    critic_loss     | 0.0197   |
|    learning_rate   | 0.001    |
|    n_updates       | 145199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 205      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.8    |
|    critic_loss     | 0.0246   |
|    learning_rate   | 0.001    |
|    n_updates       | 145599   |
---------------------------------
=== Iterazione IRL 323 ===
Loss reward (iter 323): 7.1033196449279785
=== Iterazione IRL 324 ===
Loss reward (iter 324): 7.246404647827148
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13      |
|    critic_loss     | 0.0252   |
|    learning_rate   | 0.001    |
|    n_updates       | 146099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.9    |
|    critic_loss     | 0.0224   |
|    learning_rate   | 0.001    |
|    n_updates       | 146499   |
---------------------------------
=== Iterazione IRL 325 ===
Loss reward (iter 325): 7.185819149017334
=== Iterazione IRL 326 ===
Loss reward (iter 326): 7.174621105194092
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.9    |
|    critic_loss     | 0.0218   |
|    learning_rate   | 0.001    |
|    n_updates       | 146999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.1    |
|    critic_loss     | 0.0191   |
|    learning_rate   | 0.001    |
|    n_updates       | 147399   |
---------------------------------
=== Iterazione IRL 327 ===
Loss reward (iter 327): 7.239933490753174
=== Iterazione IRL 328 ===
Loss reward (iter 328): 7.213194370269775
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.2    |
|    critic_loss     | 0.023    |
|    learning_rate   | 0.001    |
|    n_updates       | 147899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.2    |
|    critic_loss     | 0.0266   |
|    learning_rate   | 0.001    |
|    n_updates       | 148299   |
---------------------------------
=== Iterazione IRL 329 ===
Loss reward (iter 329): 7.236523151397705
=== Iterazione IRL 330 ===
Loss reward (iter 330): 7.242537021636963
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.2    |
|    critic_loss     | 0.0197   |
|    learning_rate   | 0.001    |
|    n_updates       | 148799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.2    |
|    critic_loss     | 0.0329   |
|    learning_rate   | 0.001    |
|    n_updates       | 149199   |
---------------------------------
=== Iterazione IRL 331 ===
Loss reward (iter 331): 7.187367916107178
=== Iterazione IRL 332 ===
Loss reward (iter 332): 7.195313453674316
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.3    |
|    critic_loss     | 0.0255   |
|    learning_rate   | 0.001    |
|    n_updates       | 149699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.3    |
|    critic_loss     | 0.0321   |
|    learning_rate   | 0.001    |
|    n_updates       | 150099   |
---------------------------------
=== Iterazione IRL 333 ===
Loss reward (iter 333): 7.18807315826416
=== Iterazione IRL 334 ===
Loss reward (iter 334): 7.165493011474609
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.4    |
|    critic_loss     | 0.0324   |
|    learning_rate   | 0.001    |
|    n_updates       | 150599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.3    |
|    critic_loss     | 0.0229   |
|    learning_rate   | 0.001    |
|    n_updates       | 150999   |
---------------------------------
=== Iterazione IRL 335 ===
Loss reward (iter 335): 7.162924289703369
=== Iterazione IRL 336 ===
Loss reward (iter 336): 7.164809226989746
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.5    |
|    critic_loss     | 0.0256   |
|    learning_rate   | 0.001    |
|    n_updates       | 151499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.5    |
|    critic_loss     | 0.0276   |
|    learning_rate   | 0.001    |
|    n_updates       | 151899   |
---------------------------------
=== Iterazione IRL 337 ===
Loss reward (iter 337): 7.14299201965332
=== Iterazione IRL 338 ===
Loss reward (iter 338): 7.164454460144043
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.6    |
|    critic_loss     | 0.0336   |
|    learning_rate   | 0.001    |
|    n_updates       | 152399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.5    |
|    critic_loss     | 0.0278   |
|    learning_rate   | 0.001    |
|    n_updates       | 152799   |
---------------------------------
=== Iterazione IRL 339 ===
Loss reward (iter 339): 7.149301528930664
=== Iterazione IRL 340 ===
Loss reward (iter 340): 7.145632266998291
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.7    |
|    critic_loss     | 0.0216   |
|    learning_rate   | 0.001    |
|    n_updates       | 153299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.8    |
|    critic_loss     | 0.0222   |
|    learning_rate   | 0.001    |
|    n_updates       | 153699   |
---------------------------------
=== Iterazione IRL 341 ===
Loss reward (iter 341): 7.136584758758545
=== Iterazione IRL 342 ===
Loss reward (iter 342): 7.140532970428467
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.8    |
|    critic_loss     | 0.0192   |
|    learning_rate   | 0.001    |
|    n_updates       | 154199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.9    |
|    critic_loss     | 0.0273   |
|    learning_rate   | 0.001    |
|    n_updates       | 154599   |
---------------------------------
=== Iterazione IRL 343 ===
Loss reward (iter 343): 7.130283832550049
=== Iterazione IRL 344 ===
Loss reward (iter 344): 7.127448081970215
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.7    |
|    critic_loss     | 0.021    |
|    learning_rate   | 0.001    |
|    n_updates       | 155099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.9    |
|    critic_loss     | 0.022    |
|    learning_rate   | 0.001    |
|    n_updates       | 155499   |
---------------------------------
=== Iterazione IRL 345 ===
Loss reward (iter 345): 7.130788326263428
=== Iterazione IRL 346 ===
Loss reward (iter 346): 7.112512588500977
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.2    |
|    critic_loss     | 0.0241   |
|    learning_rate   | 0.001    |
|    n_updates       | 155999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14      |
|    critic_loss     | 0.0224   |
|    learning_rate   | 0.001    |
|    n_updates       | 156399   |
---------------------------------
=== Iterazione IRL 347 ===
Loss reward (iter 347): 7.123190402984619
=== Iterazione IRL 348 ===
Loss reward (iter 348): 7.106846809387207
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 267      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.1    |
|    critic_loss     | 0.0213   |
|    learning_rate   | 0.001    |
|    n_updates       | 156899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 234      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.9    |
|    critic_loss     | 0.0305   |
|    learning_rate   | 0.001    |
|    n_updates       | 157299   |
---------------------------------
=== Iterazione IRL 349 ===
Loss reward (iter 349): 7.100412845611572
=== Iterazione IRL 350 ===
Loss reward (iter 350): 7.091386795043945
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14      |
|    critic_loss     | 0.0237   |
|    learning_rate   | 0.001    |
|    n_updates       | 157799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.1    |
|    critic_loss     | 0.0383   |
|    learning_rate   | 0.001    |
|    n_updates       | 158199   |
---------------------------------
=== Iterazione IRL 351 ===
Loss reward (iter 351): 7.100157737731934
=== Iterazione IRL 352 ===
Loss reward (iter 352): 7.075141906738281
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.2    |
|    critic_loss     | 0.0354   |
|    learning_rate   | 0.001    |
|    n_updates       | 158699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.3    |
|    critic_loss     | 0.0469   |
|    learning_rate   | 0.001    |
|    n_updates       | 159099   |
---------------------------------
=== Iterazione IRL 353 ===
Loss reward (iter 353): 7.090441703796387
=== Iterazione IRL 354 ===
Loss reward (iter 354): 7.100539207458496
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.2    |
|    critic_loss     | 0.0392   |
|    learning_rate   | 0.001    |
|    n_updates       | 159599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14      |
|    critic_loss     | 0.0393   |
|    learning_rate   | 0.001    |
|    n_updates       | 159999   |
---------------------------------
=== Iterazione IRL 355 ===
Loss reward (iter 355): 7.05999231338501
=== Iterazione IRL 356 ===
Loss reward (iter 356): 7.078155517578125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.1    |
|    critic_loss     | 0.037    |
|    learning_rate   | 0.001    |
|    n_updates       | 160499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14      |
|    critic_loss     | 0.0261   |
|    learning_rate   | 0.001    |
|    n_updates       | 160899   |
---------------------------------
=== Iterazione IRL 357 ===
Loss reward (iter 357): 7.089409828186035
=== Iterazione IRL 358 ===
Loss reward (iter 358): 7.081208229064941
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14      |
|    critic_loss     | 0.0262   |
|    learning_rate   | 0.001    |
|    n_updates       | 161399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.1    |
|    critic_loss     | 0.0206   |
|    learning_rate   | 0.001    |
|    n_updates       | 161799   |
---------------------------------
=== Iterazione IRL 359 ===
Loss reward (iter 359): 7.082979679107666
=== Iterazione IRL 360 ===
Loss reward (iter 360): 7.070141792297363
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.3    |
|    critic_loss     | 0.0363   |
|    learning_rate   | 0.001    |
|    n_updates       | 162299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.3    |
|    critic_loss     | 0.0531   |
|    learning_rate   | 0.001    |
|    n_updates       | 162699   |
---------------------------------
=== Iterazione IRL 361 ===
Loss reward (iter 361): 7.082902431488037
=== Iterazione IRL 362 ===
Loss reward (iter 362): 7.071499824523926
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.2    |
|    critic_loss     | 0.0471   |
|    learning_rate   | 0.001    |
|    n_updates       | 163199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14      |
|    critic_loss     | 0.0582   |
|    learning_rate   | 0.001    |
|    n_updates       | 163599   |
---------------------------------
=== Iterazione IRL 363 ===
Loss reward (iter 363): 7.049345016479492
=== Iterazione IRL 364 ===
Loss reward (iter 364): 7.065296173095703
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.1    |
|    critic_loss     | 0.039    |
|    learning_rate   | 0.001    |
|    n_updates       | 164099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.1    |
|    critic_loss     | 0.0358   |
|    learning_rate   | 0.001    |
|    n_updates       | 164499   |
---------------------------------
=== Iterazione IRL 365 ===
Loss reward (iter 365): 7.075307846069336
=== Iterazione IRL 366 ===
Loss reward (iter 366): 7.041067600250244
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.9    |
|    critic_loss     | 0.0378   |
|    learning_rate   | 0.001    |
|    n_updates       | 164999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.2    |
|    critic_loss     | 0.0383   |
|    learning_rate   | 0.001    |
|    n_updates       | 165399   |
---------------------------------
=== Iterazione IRL 367 ===
Loss reward (iter 367): 7.062758922576904
=== Iterazione IRL 368 ===
Loss reward (iter 368): 7.066425800323486
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14      |
|    critic_loss     | 0.0343   |
|    learning_rate   | 0.001    |
|    n_updates       | 165899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.1    |
|    critic_loss     | 0.0332   |
|    learning_rate   | 0.001    |
|    n_updates       | 166299   |
---------------------------------
=== Iterazione IRL 369 ===
Loss reward (iter 369): 7.081750869750977
=== Iterazione IRL 370 ===
Loss reward (iter 370): 7.037818908691406
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.3    |
|    critic_loss     | 0.0537   |
|    learning_rate   | 0.001    |
|    n_updates       | 166799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.1    |
|    critic_loss     | 0.0422   |
|    learning_rate   | 0.001    |
|    n_updates       | 167199   |
---------------------------------
=== Iterazione IRL 371 ===
Loss reward (iter 371): 7.059091567993164
=== Iterazione IRL 372 ===
Loss reward (iter 372): 7.069719314575195
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.1    |
|    critic_loss     | 0.0368   |
|    learning_rate   | 0.001    |
|    n_updates       | 167699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14      |
|    critic_loss     | 0.029    |
|    learning_rate   | 0.001    |
|    n_updates       | 168099   |
---------------------------------
=== Iterazione IRL 373 ===
Loss reward (iter 373): 7.066827297210693
=== Iterazione IRL 374 ===
Loss reward (iter 374): 7.043509006500244
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14      |
|    critic_loss     | 0.0358   |
|    learning_rate   | 0.001    |
|    n_updates       | 168599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 234      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14      |
|    critic_loss     | 0.038    |
|    learning_rate   | 0.001    |
|    n_updates       | 168999   |
---------------------------------
=== Iterazione IRL 375 ===
Loss reward (iter 375): 7.079249858856201
=== Iterazione IRL 376 ===
Loss reward (iter 376): 7.085609436035156
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 267      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.1    |
|    critic_loss     | 0.0435   |
|    learning_rate   | 0.001    |
|    n_updates       | 169499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 234      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.9    |
|    critic_loss     | 0.0432   |
|    learning_rate   | 0.001    |
|    n_updates       | 169899   |
---------------------------------
=== Iterazione IRL 377 ===
Loss reward (iter 377): 7.063631057739258
=== Iterazione IRL 378 ===
Loss reward (iter 378): 7.050182819366455
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 267      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.7    |
|    critic_loss     | 0.0351   |
|    learning_rate   | 0.001    |
|    n_updates       | 170399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 234      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.9    |
|    critic_loss     | 0.0447   |
|    learning_rate   | 0.001    |
|    n_updates       | 170799   |
---------------------------------
=== Iterazione IRL 379 ===
Loss reward (iter 379): 7.044464588165283
=== Iterazione IRL 380 ===
Loss reward (iter 380): 7.032423973083496
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14      |
|    critic_loss     | 0.0408   |
|    learning_rate   | 0.001    |
|    n_updates       | 171299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 234      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.8    |
|    critic_loss     | 0.0428   |
|    learning_rate   | 0.001    |
|    n_updates       | 171699   |
---------------------------------
=== Iterazione IRL 381 ===
Loss reward (iter 381): 7.036517143249512
=== Iterazione IRL 382 ===
Loss reward (iter 382): 7.050695419311523
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 267      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.7    |
|    critic_loss     | 0.0408   |
|    learning_rate   | 0.001    |
|    n_updates       | 172199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 234      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.9    |
|    critic_loss     | 0.0346   |
|    learning_rate   | 0.001    |
|    n_updates       | 172599   |
---------------------------------
=== Iterazione IRL 383 ===
Loss reward (iter 383): 7.031696796417236
=== Iterazione IRL 384 ===
Loss reward (iter 384): 7.044826507568359
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.9    |
|    critic_loss     | 0.0322   |
|    learning_rate   | 0.001    |
|    n_updates       | 173099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 234      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.9    |
|    critic_loss     | 0.0406   |
|    learning_rate   | 0.001    |
|    n_updates       | 173499   |
---------------------------------
=== Iterazione IRL 385 ===
Loss reward (iter 385): 7.081789493560791
=== Iterazione IRL 386 ===
Loss reward (iter 386): 7.014901638031006
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 267      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.5    |
|    critic_loss     | 0.0406   |
|    learning_rate   | 0.001    |
|    n_updates       | 173999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 234      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.9    |
|    critic_loss     | 0.0351   |
|    learning_rate   | 0.001    |
|    n_updates       | 174399   |
---------------------------------
=== Iterazione IRL 387 ===
Loss reward (iter 387): 7.060640811920166
=== Iterazione IRL 388 ===
Loss reward (iter 388): 7.069438934326172
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.7    |
|    critic_loss     | 0.0352   |
|    learning_rate   | 0.001    |
|    n_updates       | 174899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 234      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.5    |
|    critic_loss     | 0.0349   |
|    learning_rate   | 0.001    |
|    n_updates       | 175299   |
---------------------------------
=== Iterazione IRL 389 ===
Loss reward (iter 389): 7.062463760375977
=== Iterazione IRL 390 ===
Loss reward (iter 390): 7.08824348449707
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 267      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.5    |
|    critic_loss     | 0.0379   |
|    learning_rate   | 0.001    |
|    n_updates       | 175799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 234      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.5    |
|    critic_loss     | 0.0358   |
|    learning_rate   | 0.001    |
|    n_updates       | 176199   |
---------------------------------
=== Iterazione IRL 391 ===
Loss reward (iter 391): 7.084475040435791
=== Iterazione IRL 392 ===
Loss reward (iter 392): 7.064603805541992
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 267      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.6    |
|    critic_loss     | 0.0321   |
|    learning_rate   | 0.001    |
|    n_updates       | 176699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.6    |
|    critic_loss     | 0.0382   |
|    learning_rate   | 0.001    |
|    n_updates       | 177099   |
---------------------------------
=== Iterazione IRL 393 ===
Loss reward (iter 393): 7.049054145812988
=== Iterazione IRL 394 ===
Loss reward (iter 394): 7.070007801055908
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.8    |
|    critic_loss     | 0.039    |
|    learning_rate   | 0.001    |
|    n_updates       | 177599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.7    |
|    critic_loss     | 0.0356   |
|    learning_rate   | 0.001    |
|    n_updates       | 177999   |
---------------------------------
=== Iterazione IRL 395 ===
Loss reward (iter 395): 7.046604633331299
=== Iterazione IRL 396 ===
Loss reward (iter 396): 7.032923221588135
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.5    |
|    critic_loss     | 0.0363   |
|    learning_rate   | 0.001    |
|    n_updates       | 178499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.4    |
|    critic_loss     | 0.0247   |
|    learning_rate   | 0.001    |
|    n_updates       | 178899   |
---------------------------------
=== Iterazione IRL 397 ===
Loss reward (iter 397): 7.0214619636535645
=== Iterazione IRL 398 ===
Loss reward (iter 398): 6.984861373901367
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.7    |
|    critic_loss     | 0.0399   |
|    learning_rate   | 0.001    |
|    n_updates       | 179399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.8    |
|    critic_loss     | 0.0347   |
|    learning_rate   | 0.001    |
|    n_updates       | 179799   |
---------------------------------
=== Iterazione IRL 399 ===
Loss reward (iter 399): 7.020984172821045
=== Iterazione IRL 400 ===
Loss reward (iter 400): 7.027451992034912
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.7    |
|    critic_loss     | 0.0357   |
|    learning_rate   | 0.001    |
|    n_updates       | 180299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.6    |
|    critic_loss     | 0.0415   |
|    learning_rate   | 0.001    |
|    n_updates       | 180699   |
---------------------------------
=== Iterazione IRL 401 ===
Loss reward (iter 401): 7.032434940338135
=== Iterazione IRL 402 ===
Loss reward (iter 402): 7.0353803634643555
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.4    |
|    critic_loss     | 0.036    |
|    learning_rate   | 0.001    |
|    n_updates       | 181199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 234      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.6    |
|    critic_loss     | 0.0324   |
|    learning_rate   | 0.001    |
|    n_updates       | 181599   |
---------------------------------
=== Iterazione IRL 403 ===
Loss reward (iter 403): 7.027975559234619
=== Iterazione IRL 404 ===
Loss reward (iter 404): 7.073466777801514
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.2    |
|    critic_loss     | 0.0333   |
|    learning_rate   | 0.001    |
|    n_updates       | 182099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 234      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.5    |
|    critic_loss     | 0.0342   |
|    learning_rate   | 0.001    |
|    n_updates       | 182499   |
---------------------------------
=== Iterazione IRL 405 ===
Loss reward (iter 405): 7.010421276092529
=== Iterazione IRL 406 ===
Loss reward (iter 406): 7.058570384979248
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.7    |
|    critic_loss     | 0.04     |
|    learning_rate   | 0.001    |
|    n_updates       | 182999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 233      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.4    |
|    critic_loss     | 0.0396   |
|    learning_rate   | 0.001    |
|    n_updates       | 183399   |
---------------------------------
=== Iterazione IRL 407 ===
Loss reward (iter 407): 7.026782035827637
=== Iterazione IRL 408 ===
Loss reward (iter 408): 7.050815105438232
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.3    |
|    critic_loss     | 0.0447   |
|    learning_rate   | 0.001    |
|    n_updates       | 183899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 234      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.1    |
|    critic_loss     | 0.0353   |
|    learning_rate   | 0.001    |
|    n_updates       | 184299   |
---------------------------------
=== Iterazione IRL 409 ===
Loss reward (iter 409): 7.093993663787842
=== Iterazione IRL 410 ===
Loss reward (iter 410): 7.07252311706543
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.3    |
|    critic_loss     | 0.0475   |
|    learning_rate   | 0.001    |
|    n_updates       | 184799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 234      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.4    |
|    critic_loss     | 0.0404   |
|    learning_rate   | 0.001    |
|    n_updates       | 185199   |
---------------------------------
=== Iterazione IRL 411 ===
Loss reward (iter 411): 7.069055080413818
=== Iterazione IRL 412 ===
Loss reward (iter 412): 7.05817985534668
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.1    |
|    critic_loss     | 0.0472   |
|    learning_rate   | 0.001    |
|    n_updates       | 185699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 234      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.9    |
|    critic_loss     | 0.0448   |
|    learning_rate   | 0.001    |
|    n_updates       | 186099   |
---------------------------------
=== Iterazione IRL 413 ===
Loss reward (iter 413): 7.056739807128906
=== Iterazione IRL 414 ===
Loss reward (iter 414): 7.055039405822754
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 267      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.4    |
|    critic_loss     | 0.0519   |
|    learning_rate   | 0.001    |
|    n_updates       | 186599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 234      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.7    |
|    critic_loss     | 0.0466   |
|    learning_rate   | 0.001    |
|    n_updates       | 186999   |
---------------------------------
=== Iterazione IRL 415 ===
Loss reward (iter 415): 7.078453063964844
=== Iterazione IRL 416 ===
Loss reward (iter 416): 7.066075325012207
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 267      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.2    |
|    critic_loss     | 0.0587   |
|    learning_rate   | 0.001    |
|    n_updates       | 187499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 234      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13      |
|    critic_loss     | 0.0652   |
|    learning_rate   | 0.001    |
|    n_updates       | 187899   |
---------------------------------
=== Iterazione IRL 417 ===
Loss reward (iter 417): 7.027751922607422
=== Iterazione IRL 418 ===
Loss reward (iter 418): 7.01499080657959
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.9    |
|    critic_loss     | 0.0663   |
|    learning_rate   | 0.001    |
|    n_updates       | 188399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 234      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.8    |
|    critic_loss     | 0.0621   |
|    learning_rate   | 0.001    |
|    n_updates       | 188799   |
---------------------------------
=== Iterazione IRL 419 ===
Loss reward (iter 419): 7.063477039337158
=== Iterazione IRL 420 ===
Loss reward (iter 420): 7.068511962890625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 266      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.8    |
|    critic_loss     | 0.0692   |
|    learning_rate   | 0.001    |
|    n_updates       | 189299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 234      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.9    |
|    critic_loss     | 0.0843   |
|    learning_rate   | 0.001    |
|    n_updates       | 189699   |
---------------------------------
=== Iterazione IRL 421 ===
Loss reward (iter 421): 7.034081935882568
=== Iterazione IRL 422 ===
Loss reward (iter 422): 7.022682189941406
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.7    |
|    critic_loss     | 0.0821   |
|    learning_rate   | 0.001    |
|    n_updates       | 190199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.8    |
|    critic_loss     | 0.0792   |
|    learning_rate   | 0.001    |
|    n_updates       | 190599   |
---------------------------------
=== Iterazione IRL 423 ===
Loss reward (iter 423): 7.019270896911621
=== Iterazione IRL 424 ===
Loss reward (iter 424): 7.0395612716674805
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.1    |
|    critic_loss     | 0.0746   |
|    learning_rate   | 0.001    |
|    n_updates       | 191099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.7    |
|    critic_loss     | 0.0625   |
|    learning_rate   | 0.001    |
|    n_updates       | 191499   |
---------------------------------
=== Iterazione IRL 425 ===
Loss reward (iter 425): 7.0442795753479
=== Iterazione IRL 426 ===
Loss reward (iter 426): 7.027961730957031
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.6    |
|    critic_loss     | 0.0811   |
|    learning_rate   | 0.001    |
|    n_updates       | 191999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.7    |
|    critic_loss     | 0.0702   |
|    learning_rate   | 0.001    |
|    n_updates       | 192399   |
---------------------------------
=== Iterazione IRL 427 ===
Loss reward (iter 427): 6.977232933044434
=== Iterazione IRL 428 ===
Loss reward (iter 428): 7.020652770996094
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.3    |
|    critic_loss     | 0.105    |
|    learning_rate   | 0.001    |
|    n_updates       | 192899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.5    |
|    critic_loss     | 0.0812   |
|    learning_rate   | 0.001    |
|    n_updates       | 193299   |
---------------------------------
=== Iterazione IRL 429 ===
Loss reward (iter 429): 7.007381439208984
=== Iterazione IRL 430 ===
Loss reward (iter 430): 7.034031391143799
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.4    |
|    critic_loss     | 0.0898   |
|    learning_rate   | 0.001    |
|    n_updates       | 193799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.3    |
|    critic_loss     | 0.102    |
|    learning_rate   | 0.001    |
|    n_updates       | 194199   |
---------------------------------
=== Iterazione IRL 431 ===
Loss reward (iter 431): 7.008347511291504
=== Iterazione IRL 432 ===
Loss reward (iter 432): 7.020062446594238
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.4    |
|    critic_loss     | 0.0669   |
|    learning_rate   | 0.001    |
|    n_updates       | 194699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.2    |
|    critic_loss     | 0.105    |
|    learning_rate   | 0.001    |
|    n_updates       | 195099   |
---------------------------------
=== Iterazione IRL 433 ===
Loss reward (iter 433): 7.020816326141357
=== Iterazione IRL 434 ===
Loss reward (iter 434): 7.038835048675537
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.2    |
|    critic_loss     | 0.0658   |
|    learning_rate   | 0.001    |
|    n_updates       | 195599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.2    |
|    critic_loss     | 0.0943   |
|    learning_rate   | 0.001    |
|    n_updates       | 195999   |
---------------------------------
=== Iterazione IRL 435 ===
Loss reward (iter 435): 7.027979850769043
=== Iterazione IRL 436 ===
Loss reward (iter 436): 7.034255504608154
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12      |
|    critic_loss     | 0.114    |
|    learning_rate   | 0.001    |
|    n_updates       | 196499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.9    |
|    critic_loss     | 0.101    |
|    learning_rate   | 0.001    |
|    n_updates       | 196899   |
---------------------------------
=== Iterazione IRL 437 ===
Loss reward (iter 437): 6.988831520080566
=== Iterazione IRL 438 ===
Loss reward (iter 438): 7.0041704177856445
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.8    |
|    critic_loss     | 0.0919   |
|    learning_rate   | 0.001    |
|    n_updates       | 197399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.9    |
|    critic_loss     | 0.0848   |
|    learning_rate   | 0.001    |
|    n_updates       | 197799   |
---------------------------------
=== Iterazione IRL 439 ===
Loss reward (iter 439): 7.008456707000732
=== Iterazione IRL 440 ===
Loss reward (iter 440): 7.009154319763184
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.7    |
|    critic_loss     | 0.097    |
|    learning_rate   | 0.001    |
|    n_updates       | 198299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.8    |
|    critic_loss     | 0.0756   |
|    learning_rate   | 0.001    |
|    n_updates       | 198699   |
---------------------------------
=== Iterazione IRL 441 ===
Loss reward (iter 441): 7.033946514129639
=== Iterazione IRL 442 ===
Loss reward (iter 442): 6.997382164001465
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.6    |
|    critic_loss     | 0.0792   |
|    learning_rate   | 0.001    |
|    n_updates       | 199199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.4    |
|    critic_loss     | 0.098    |
|    learning_rate   | 0.001    |
|    n_updates       | 199599   |
---------------------------------
=== Iterazione IRL 443 ===
Loss reward (iter 443): 7.005305767059326
=== Iterazione IRL 444 ===
Loss reward (iter 444): 7.00167989730835
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.6    |
|    critic_loss     | 0.0928   |
|    learning_rate   | 0.001    |
|    n_updates       | 200099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.6    |
|    critic_loss     | 0.088    |
|    learning_rate   | 0.001    |
|    n_updates       | 200499   |
---------------------------------
=== Iterazione IRL 445 ===
Loss reward (iter 445): 7.0355048179626465
=== Iterazione IRL 446 ===
Loss reward (iter 446): 7.038861274719238
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.4    |
|    critic_loss     | 0.0984   |
|    learning_rate   | 0.001    |
|    n_updates       | 200999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.6    |
|    critic_loss     | 0.0943   |
|    learning_rate   | 0.001    |
|    n_updates       | 201399   |
---------------------------------
=== Iterazione IRL 447 ===
Loss reward (iter 447): 7.017305850982666
=== Iterazione IRL 448 ===
Loss reward (iter 448): 7.013484954833984
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.1    |
|    critic_loss     | 0.0857   |
|    learning_rate   | 0.001    |
|    n_updates       | 201899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.4    |
|    critic_loss     | 0.0741   |
|    learning_rate   | 0.001    |
|    n_updates       | 202299   |
---------------------------------
=== Iterazione IRL 449 ===
Loss reward (iter 449): 7.006787300109863
=== Iterazione IRL 450 ===
Loss reward (iter 450): 7.02389669418335
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.5    |
|    critic_loss     | 0.104    |
|    learning_rate   | 0.001    |
|    n_updates       | 202799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.2    |
|    critic_loss     | 0.0934   |
|    learning_rate   | 0.001    |
|    n_updates       | 203199   |
---------------------------------
=== Iterazione IRL 451 ===
Loss reward (iter 451): 7.019802570343018
=== Iterazione IRL 452 ===
Loss reward (iter 452): 7.044513702392578
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.1    |
|    critic_loss     | 0.0913   |
|    learning_rate   | 0.001    |
|    n_updates       | 203699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.1    |
|    critic_loss     | 0.105    |
|    learning_rate   | 0.001    |
|    n_updates       | 204099   |
---------------------------------
=== Iterazione IRL 453 ===
Loss reward (iter 453): 7.008433818817139
=== Iterazione IRL 454 ===
Loss reward (iter 454): 7.010756015777588
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11      |
|    critic_loss     | 0.0918   |
|    learning_rate   | 0.001    |
|    n_updates       | 204599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.9    |
|    critic_loss     | 0.0913   |
|    learning_rate   | 0.001    |
|    n_updates       | 204999   |
---------------------------------
=== Iterazione IRL 455 ===
Loss reward (iter 455): 7.031277656555176
=== Iterazione IRL 456 ===
Loss reward (iter 456): 6.99799108505249
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.9    |
|    critic_loss     | 0.0808   |
|    learning_rate   | 0.001    |
|    n_updates       | 205499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.9    |
|    critic_loss     | 0.103    |
|    learning_rate   | 0.001    |
|    n_updates       | 205899   |
---------------------------------
=== Iterazione IRL 457 ===
Loss reward (iter 457): 7.016943454742432
=== Iterazione IRL 458 ===
Loss reward (iter 458): 6.999924659729004
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11      |
|    critic_loss     | 0.11     |
|    learning_rate   | 0.001    |
|    n_updates       | 206399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.7    |
|    critic_loss     | 0.107    |
|    learning_rate   | 0.001    |
|    n_updates       | 206799   |
---------------------------------
=== Iterazione IRL 459 ===
Loss reward (iter 459): 7.021512985229492
=== Iterazione IRL 460 ===
Loss reward (iter 460): 6.993307113647461
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.6    |
|    critic_loss     | 0.0757   |
|    learning_rate   | 0.001    |
|    n_updates       | 207299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.9    |
|    critic_loss     | 0.0899   |
|    learning_rate   | 0.001    |
|    n_updates       | 207699   |
---------------------------------
=== Iterazione IRL 461 ===
Loss reward (iter 461): 7.001572608947754
=== Iterazione IRL 462 ===
Loss reward (iter 462): 6.995071887969971
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.9    |
|    critic_loss     | 0.0945   |
|    learning_rate   | 0.001    |
|    n_updates       | 208199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.7    |
|    critic_loss     | 0.12     |
|    learning_rate   | 0.001    |
|    n_updates       | 208599   |
---------------------------------
=== Iterazione IRL 463 ===
Loss reward (iter 463): 6.982906341552734
=== Iterazione IRL 464 ===
Loss reward (iter 464): 7.014005184173584
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.9    |
|    critic_loss     | 0.112    |
|    learning_rate   | 0.001    |
|    n_updates       | 209099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.5    |
|    critic_loss     | 0.0813   |
|    learning_rate   | 0.001    |
|    n_updates       | 209499   |
---------------------------------
=== Iterazione IRL 465 ===
Loss reward (iter 465): 7.006852626800537
=== Iterazione IRL 466 ===
Loss reward (iter 466): 7.000726222991943
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.6    |
|    critic_loss     | 0.0889   |
|    learning_rate   | 0.001    |
|    n_updates       | 209999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.6    |
|    critic_loss     | 0.093    |
|    learning_rate   | 0.001    |
|    n_updates       | 210399   |
---------------------------------
=== Iterazione IRL 467 ===
Loss reward (iter 467): 6.967665195465088
=== Iterazione IRL 468 ===
Loss reward (iter 468): 6.9961347579956055
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.5    |
|    critic_loss     | 0.0889   |
|    learning_rate   | 0.001    |
|    n_updates       | 210899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.8    |
|    critic_loss     | 0.0862   |
|    learning_rate   | 0.001    |
|    n_updates       | 211299   |
---------------------------------
=== Iterazione IRL 469 ===
Loss reward (iter 469): 6.99863862991333
=== Iterazione IRL 470 ===
Loss reward (iter 470): 6.995882511138916
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.5    |
|    critic_loss     | 0.0947   |
|    learning_rate   | 0.001    |
|    n_updates       | 211799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.4    |
|    critic_loss     | 0.0764   |
|    learning_rate   | 0.001    |
|    n_updates       | 212199   |
---------------------------------
=== Iterazione IRL 471 ===
Loss reward (iter 471): 6.995938777923584
=== Iterazione IRL 472 ===
Loss reward (iter 472): 7.0165510177612305
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.5    |
|    critic_loss     | 0.0725   |
|    learning_rate   | 0.001    |
|    n_updates       | 212699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.2    |
|    critic_loss     | 0.0488   |
|    learning_rate   | 0.001    |
|    n_updates       | 213099   |
---------------------------------
=== Iterazione IRL 473 ===
Loss reward (iter 473): 7.008728981018066
=== Iterazione IRL 474 ===
Loss reward (iter 474): 6.967135429382324
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.5    |
|    critic_loss     | 0.0753   |
|    learning_rate   | 0.001    |
|    n_updates       | 213599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.5    |
|    critic_loss     | 0.0806   |
|    learning_rate   | 0.001    |
|    n_updates       | 213999   |
---------------------------------
=== Iterazione IRL 475 ===
Loss reward (iter 475): 6.994858741760254
=== Iterazione IRL 476 ===
Loss reward (iter 476): 7.007855415344238
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.3    |
|    critic_loss     | 0.0864   |
|    learning_rate   | 0.001    |
|    n_updates       | 214499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.4    |
|    critic_loss     | 0.0823   |
|    learning_rate   | 0.001    |
|    n_updates       | 214899   |
---------------------------------
=== Iterazione IRL 477 ===
Loss reward (iter 477): 7.019022464752197
=== Iterazione IRL 478 ===
Loss reward (iter 478): 7.0101704597473145
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.4    |
|    critic_loss     | 0.0866   |
|    learning_rate   | 0.001    |
|    n_updates       | 215399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.5    |
|    critic_loss     | 0.0859   |
|    learning_rate   | 0.001    |
|    n_updates       | 215799   |
---------------------------------
=== Iterazione IRL 479 ===
Loss reward (iter 479): 7.022112846374512
=== Iterazione IRL 480 ===
Loss reward (iter 480): 7.011094570159912
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.2    |
|    critic_loss     | 0.0799   |
|    learning_rate   | 0.001    |
|    n_updates       | 216299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.5    |
|    critic_loss     | 0.0845   |
|    learning_rate   | 0.001    |
|    n_updates       | 216699   |
---------------------------------
=== Iterazione IRL 481 ===
Loss reward (iter 481): 7.001795768737793
=== Iterazione IRL 482 ===
Loss reward (iter 482): 7.0082197189331055
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.3    |
|    critic_loss     | 0.09     |
|    learning_rate   | 0.001    |
|    n_updates       | 217199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.4    |
|    critic_loss     | 0.0761   |
|    learning_rate   | 0.001    |
|    n_updates       | 217599   |
---------------------------------
=== Iterazione IRL 483 ===
Loss reward (iter 483): 6.992998123168945
=== Iterazione IRL 484 ===
Loss reward (iter 484): 7.106686592102051
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.4    |
|    critic_loss     | 0.089    |
|    learning_rate   | 0.001    |
|    n_updates       | 218099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.3    |
|    critic_loss     | 0.0796   |
|    learning_rate   | 0.001    |
|    n_updates       | 218499   |
---------------------------------
=== Iterazione IRL 485 ===
Loss reward (iter 485): 7.193144798278809
=== Iterazione IRL 486 ===
Loss reward (iter 486): 7.12147855758667
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.4    |
|    critic_loss     | 0.0805   |
|    learning_rate   | 0.001    |
|    n_updates       | 218999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.5    |
|    critic_loss     | 0.0778   |
|    learning_rate   | 0.001    |
|    n_updates       | 219399   |
---------------------------------
=== Iterazione IRL 487 ===
Loss reward (iter 487): 7.1265974044799805
=== Iterazione IRL 488 ===
Loss reward (iter 488): 7.15809440612793
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.4    |
|    critic_loss     | 0.0855   |
|    learning_rate   | 0.001    |
|    n_updates       | 219899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.4    |
|    critic_loss     | 0.0799   |
|    learning_rate   | 0.001    |
|    n_updates       | 220299   |
---------------------------------
=== Iterazione IRL 489 ===
Loss reward (iter 489): 7.166141033172607
=== Iterazione IRL 490 ===
Loss reward (iter 490): 7.147331237792969
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.5    |
|    critic_loss     | 0.0895   |
|    learning_rate   | 0.001    |
|    n_updates       | 220799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.5    |
|    critic_loss     | 0.0719   |
|    learning_rate   | 0.001    |
|    n_updates       | 221199   |
---------------------------------
=== Iterazione IRL 491 ===
Loss reward (iter 491): 7.1539692878723145
=== Iterazione IRL 492 ===
Loss reward (iter 492): 7.175487995147705
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.5    |
|    critic_loss     | 0.0889   |
|    learning_rate   | 0.001    |
|    n_updates       | 221699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.5    |
|    critic_loss     | 0.0533   |
|    learning_rate   | 0.001    |
|    n_updates       | 222099   |
---------------------------------
=== Iterazione IRL 493 ===
Loss reward (iter 493): 7.178829669952393
=== Iterazione IRL 494 ===
Loss reward (iter 494): 7.155125141143799
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.4    |
|    critic_loss     | 0.0566   |
|    learning_rate   | 0.001    |
|    n_updates       | 222599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.4    |
|    critic_loss     | 0.0593   |
|    learning_rate   | 0.001    |
|    n_updates       | 222999   |
---------------------------------
=== Iterazione IRL 495 ===
Loss reward (iter 495): 7.207468032836914
=== Iterazione IRL 496 ===
Loss reward (iter 496): 7.18605899810791
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.5    |
|    critic_loss     | 0.0742   |
|    learning_rate   | 0.001    |
|    n_updates       | 223499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.6    |
|    critic_loss     | 0.0686   |
|    learning_rate   | 0.001    |
|    n_updates       | 223899   |
---------------------------------
=== Iterazione IRL 497 ===
Loss reward (iter 497): 7.207729339599609
=== Iterazione IRL 498 ===
Loss reward (iter 498): 7.149932861328125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.8    |
|    critic_loss     | 0.0894   |
|    learning_rate   | 0.001    |
|    n_updates       | 224399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.5    |
|    critic_loss     | 0.0527   |
|    learning_rate   | 0.001    |
|    n_updates       | 224799   |
---------------------------------
=== Iterazione IRL 499 ===
Loss reward (iter 499): 7.182608604431152
=== Iterazione IRL 500 ===
Loss reward (iter 500): 7.165529251098633
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.7    |
|    critic_loss     | 0.0534   |
|    learning_rate   | 0.001    |
|    n_updates       | 225299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.5    |
|    critic_loss     | 0.061    |
|    learning_rate   | 0.001    |
|    n_updates       | 225699   |
---------------------------------
=== Iterazione IRL 501 ===
Loss reward (iter 501): 7.159670352935791
=== Iterazione IRL 502 ===
Loss reward (iter 502): 7.146678924560547
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.7    |
|    critic_loss     | 0.0733   |
|    learning_rate   | 0.001    |
|    n_updates       | 226199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 240      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.5    |
|    critic_loss     | 0.0736   |
|    learning_rate   | 0.001    |
|    n_updates       | 226599   |
---------------------------------
=== Iterazione IRL 503 ===
Loss reward (iter 503): 7.151986122131348
=== Iterazione IRL 504 ===
Loss reward (iter 504): 7.146596431732178
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.6    |
|    critic_loss     | 0.0711   |
|    learning_rate   | 0.001    |
|    n_updates       | 227099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.6    |
|    critic_loss     | 0.0672   |
|    learning_rate   | 0.001    |
|    n_updates       | 227499   |
---------------------------------
=== Iterazione IRL 505 ===
Loss reward (iter 505): 7.166507720947266
=== Iterazione IRL 506 ===
Loss reward (iter 506): 7.151498794555664
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.5    |
|    critic_loss     | 0.0647   |
|    learning_rate   | 0.001    |
|    n_updates       | 227999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.6    |
|    critic_loss     | 0.0696   |
|    learning_rate   | 0.001    |
|    n_updates       | 228399   |
---------------------------------
=== Iterazione IRL 507 ===
Loss reward (iter 507): 7.161825180053711
=== Iterazione IRL 508 ===
Loss reward (iter 508): 7.140129089355469
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.7    |
|    critic_loss     | 0.0655   |
|    learning_rate   | 0.001    |
|    n_updates       | 228899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.9    |
|    critic_loss     | 0.0534   |
|    learning_rate   | 0.001    |
|    n_updates       | 229299   |
---------------------------------
=== Iterazione IRL 509 ===
Loss reward (iter 509): 7.1387529373168945
=== Iterazione IRL 510 ===
Loss reward (iter 510): 7.144150733947754
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.7    |
|    critic_loss     | 0.0621   |
|    learning_rate   | 0.001    |
|    n_updates       | 229799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.7    |
|    critic_loss     | 0.0548   |
|    learning_rate   | 0.001    |
|    n_updates       | 230199   |
---------------------------------
=== Iterazione IRL 511 ===
Loss reward (iter 511): 7.141282081604004
=== Iterazione IRL 512 ===
Loss reward (iter 512): 7.159689426422119
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11      |
|    critic_loss     | 0.0575   |
|    learning_rate   | 0.001    |
|    n_updates       | 230699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.9    |
|    critic_loss     | 0.0688   |
|    learning_rate   | 0.001    |
|    n_updates       | 231099   |
---------------------------------
=== Iterazione IRL 513 ===
Loss reward (iter 513): 7.134876251220703
=== Iterazione IRL 514 ===
Loss reward (iter 514): 7.125371932983398
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.9    |
|    critic_loss     | 0.0439   |
|    learning_rate   | 0.001    |
|    n_updates       | 231599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11      |
|    critic_loss     | 0.0554   |
|    learning_rate   | 0.001    |
|    n_updates       | 231999   |
---------------------------------
=== Iterazione IRL 515 ===
Loss reward (iter 515): 7.114617347717285
=== Iterazione IRL 516 ===
Loss reward (iter 516): 7.130450248718262
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11      |
|    critic_loss     | 0.0447   |
|    learning_rate   | 0.001    |
|    n_updates       | 232499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.7    |
|    critic_loss     | 0.0514   |
|    learning_rate   | 0.001    |
|    n_updates       | 232899   |
---------------------------------
=== Iterazione IRL 517 ===
Loss reward (iter 517): 7.11967658996582
=== Iterazione IRL 518 ===
Loss reward (iter 518): 7.10331392288208
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.9    |
|    critic_loss     | 0.0539   |
|    learning_rate   | 0.001    |
|    n_updates       | 233399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.9    |
|    critic_loss     | 0.0393   |
|    learning_rate   | 0.001    |
|    n_updates       | 233799   |
---------------------------------
=== Iterazione IRL 519 ===
Loss reward (iter 519): 7.111185073852539
=== Iterazione IRL 520 ===
Loss reward (iter 520): 7.1283111572265625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11      |
|    critic_loss     | 0.0445   |
|    learning_rate   | 0.001    |
|    n_updates       | 234299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.2    |
|    critic_loss     | 0.0649   |
|    learning_rate   | 0.001    |
|    n_updates       | 234699   |
---------------------------------
=== Iterazione IRL 521 ===
Loss reward (iter 521): 7.104615688323975
=== Iterazione IRL 522 ===
Loss reward (iter 522): 7.117062091827393
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11      |
|    critic_loss     | 0.0511   |
|    learning_rate   | 0.001    |
|    n_updates       | 235199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.3    |
|    critic_loss     | 0.0425   |
|    learning_rate   | 0.001    |
|    n_updates       | 235599   |
---------------------------------
=== Iterazione IRL 523 ===
Loss reward (iter 523): 7.0935587882995605
=== Iterazione IRL 524 ===
Loss reward (iter 524): 7.108153343200684
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11      |
|    critic_loss     | 0.037    |
|    learning_rate   | 0.001    |
|    n_updates       | 236099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.2    |
|    critic_loss     | 0.0519   |
|    learning_rate   | 0.001    |
|    n_updates       | 236499   |
---------------------------------
=== Iterazione IRL 525 ===
Loss reward (iter 525): 7.1011552810668945
=== Iterazione IRL 526 ===
Loss reward (iter 526): 7.097841739654541
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.2    |
|    critic_loss     | 0.0449   |
|    learning_rate   | 0.001    |
|    n_updates       | 236999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.1    |
|    critic_loss     | 0.0386   |
|    learning_rate   | 0.001    |
|    n_updates       | 237399   |
---------------------------------
=== Iterazione IRL 527 ===
Loss reward (iter 527): 7.092377185821533
=== Iterazione IRL 528 ===
Loss reward (iter 528): 7.086923122406006
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.2    |
|    critic_loss     | 0.0545   |
|    learning_rate   | 0.001    |
|    n_updates       | 237899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.2    |
|    critic_loss     | 0.0472   |
|    learning_rate   | 0.001    |
|    n_updates       | 238299   |
---------------------------------
=== Iterazione IRL 529 ===
Loss reward (iter 529): 7.086493492126465
=== Iterazione IRL 530 ===
Loss reward (iter 530): 7.090775966644287
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.3    |
|    critic_loss     | 0.0518   |
|    learning_rate   | 0.001    |
|    n_updates       | 238799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.4    |
|    critic_loss     | 0.0374   |
|    learning_rate   | 0.001    |
|    n_updates       | 239199   |
---------------------------------
=== Iterazione IRL 531 ===
Loss reward (iter 531): 7.092655658721924
=== Iterazione IRL 532 ===
Loss reward (iter 532): 7.0779523849487305
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.4    |
|    critic_loss     | 0.038    |
|    learning_rate   | 0.001    |
|    n_updates       | 239699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.3    |
|    critic_loss     | 0.0417   |
|    learning_rate   | 0.001    |
|    n_updates       | 240099   |
---------------------------------
=== Iterazione IRL 533 ===
Loss reward (iter 533): 7.068671703338623
=== Iterazione IRL 534 ===
Loss reward (iter 534): 7.063143253326416
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.4    |
|    critic_loss     | 0.0501   |
|    learning_rate   | 0.001    |
|    n_updates       | 240599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.5    |
|    critic_loss     | 0.0477   |
|    learning_rate   | 0.001    |
|    n_updates       | 240999   |
---------------------------------
=== Iterazione IRL 535 ===
Loss reward (iter 535): 7.070596694946289
=== Iterazione IRL 536 ===
Loss reward (iter 536): 7.081977367401123
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.5    |
|    critic_loss     | 0.0382   |
|    learning_rate   | 0.001    |
|    n_updates       | 241499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.4    |
|    critic_loss     | 0.037    |
|    learning_rate   | 0.001    |
|    n_updates       | 241899   |
---------------------------------
=== Iterazione IRL 537 ===
Loss reward (iter 537): 7.054475784301758
=== Iterazione IRL 538 ===
Loss reward (iter 538): 7.058152675628662
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.5    |
|    critic_loss     | 0.0413   |
|    learning_rate   | 0.001    |
|    n_updates       | 242399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.6    |
|    critic_loss     | 0.0361   |
|    learning_rate   | 0.001    |
|    n_updates       | 242799   |
---------------------------------
=== Iterazione IRL 539 ===
Loss reward (iter 539): 7.050347805023193
=== Iterazione IRL 540 ===
Loss reward (iter 540): 7.053216934204102
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.6    |
|    critic_loss     | 0.0359   |
|    learning_rate   | 0.001    |
|    n_updates       | 243299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.6    |
|    critic_loss     | 0.0388   |
|    learning_rate   | 0.001    |
|    n_updates       | 243699   |
---------------------------------
=== Iterazione IRL 541 ===
Loss reward (iter 541): 7.030159950256348
=== Iterazione IRL 542 ===
Loss reward (iter 542): 7.045760631561279
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.8    |
|    critic_loss     | 0.0362   |
|    learning_rate   | 0.001    |
|    n_updates       | 244199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.7    |
|    critic_loss     | 0.0449   |
|    learning_rate   | 0.001    |
|    n_updates       | 244599   |
---------------------------------
=== Iterazione IRL 543 ===
Loss reward (iter 543): 7.035645484924316
=== Iterazione IRL 544 ===
Loss reward (iter 544): 7.026583671569824
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.7    |
|    critic_loss     | 0.0383   |
|    learning_rate   | 0.001    |
|    n_updates       | 245099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.7    |
|    critic_loss     | 0.037    |
|    learning_rate   | 0.001    |
|    n_updates       | 245499   |
---------------------------------
=== Iterazione IRL 545 ===
Loss reward (iter 545): 7.049195766448975
=== Iterazione IRL 546 ===
Loss reward (iter 546): 7.059958457946777
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.8    |
|    critic_loss     | 0.0323   |
|    learning_rate   | 0.001    |
|    n_updates       | 245999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.9    |
|    critic_loss     | 0.0327   |
|    learning_rate   | 0.001    |
|    n_updates       | 246399   |
---------------------------------
=== Iterazione IRL 547 ===
Loss reward (iter 547): 7.046005725860596
=== Iterazione IRL 548 ===
Loss reward (iter 548): 7.0268683433532715
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.8    |
|    critic_loss     | 0.0412   |
|    learning_rate   | 0.001    |
|    n_updates       | 246899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.8    |
|    critic_loss     | 0.0364   |
|    learning_rate   | 0.001    |
|    n_updates       | 247299   |
---------------------------------
=== Iterazione IRL 549 ===
Loss reward (iter 549): 7.033443927764893
=== Iterazione IRL 550 ===
Loss reward (iter 550): 7.045001983642578
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.8    |
|    critic_loss     | 0.0449   |
|    learning_rate   | 0.001    |
|    n_updates       | 247799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.7    |
|    critic_loss     | 0.0485   |
|    learning_rate   | 0.001    |
|    n_updates       | 248199   |
---------------------------------
=== Iterazione IRL 551 ===
Loss reward (iter 551): 7.057714939117432
=== Iterazione IRL 552 ===
Loss reward (iter 552): 7.004456043243408
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.9    |
|    critic_loss     | 0.0329   |
|    learning_rate   | 0.001    |
|    n_updates       | 248699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.9    |
|    critic_loss     | 0.0383   |
|    learning_rate   | 0.001    |
|    n_updates       | 249099   |
---------------------------------
=== Iterazione IRL 553 ===
Loss reward (iter 553): 7.038186073303223
=== Iterazione IRL 554 ===
Loss reward (iter 554): 7.025580406188965
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.8    |
|    critic_loss     | 0.0454   |
|    learning_rate   | 0.001    |
|    n_updates       | 249599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12      |
|    critic_loss     | 0.0432   |
|    learning_rate   | 0.001    |
|    n_updates       | 249999   |
---------------------------------
=== Iterazione IRL 555 ===
Loss reward (iter 555): 7.063581466674805
=== Iterazione IRL 556 ===
Loss reward (iter 556): 7.04440450668335
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.9    |
|    critic_loss     | 0.0584   |
|    learning_rate   | 0.001    |
|    n_updates       | 250499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.1    |
|    critic_loss     | 0.0494   |
|    learning_rate   | 0.001    |
|    n_updates       | 250899   |
---------------------------------
=== Iterazione IRL 557 ===
Loss reward (iter 557): 7.073424339294434
=== Iterazione IRL 558 ===
Loss reward (iter 558): 7.046515464782715
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.2    |
|    critic_loss     | 0.0443   |
|    learning_rate   | 0.001    |
|    n_updates       | 251399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.9    |
|    critic_loss     | 0.0467   |
|    learning_rate   | 0.001    |
|    n_updates       | 251799   |
---------------------------------
=== Iterazione IRL 559 ===
Loss reward (iter 559): 7.035927772521973
=== Iterazione IRL 560 ===
Loss reward (iter 560): 7.059697151184082
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.1    |
|    critic_loss     | 0.0523   |
|    learning_rate   | 0.001    |
|    n_updates       | 252299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.1    |
|    critic_loss     | 0.0433   |
|    learning_rate   | 0.001    |
|    n_updates       | 252699   |
---------------------------------
=== Iterazione IRL 561 ===
Loss reward (iter 561): 7.036165714263916
=== Iterazione IRL 562 ===
Loss reward (iter 562): 7.04385232925415
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.9    |
|    critic_loss     | 0.0438   |
|    learning_rate   | 0.001    |
|    n_updates       | 253199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.1    |
|    critic_loss     | 0.0528   |
|    learning_rate   | 0.001    |
|    n_updates       | 253599   |
---------------------------------
=== Iterazione IRL 563 ===
Loss reward (iter 563): 7.040167331695557
=== Iterazione IRL 564 ===
Loss reward (iter 564): 7.003286838531494
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.1    |
|    critic_loss     | 0.0429   |
|    learning_rate   | 0.001    |
|    n_updates       | 254099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12      |
|    critic_loss     | 0.0515   |
|    learning_rate   | 0.001    |
|    n_updates       | 254499   |
---------------------------------
=== Iterazione IRL 565 ===
Loss reward (iter 565): 7.032524585723877
=== Iterazione IRL 566 ===
Loss reward (iter 566): 7.048575401306152
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.2    |
|    critic_loss     | 0.0445   |
|    learning_rate   | 0.001    |
|    n_updates       | 254999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.3    |
|    critic_loss     | 0.0443   |
|    learning_rate   | 0.001    |
|    n_updates       | 255399   |
---------------------------------
=== Iterazione IRL 567 ===
Loss reward (iter 567): 7.030940532684326
=== Iterazione IRL 568 ===
Loss reward (iter 568): 7.02564811706543
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.4    |
|    critic_loss     | 0.0351   |
|    learning_rate   | 0.001    |
|    n_updates       | 255899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.2    |
|    critic_loss     | 0.0454   |
|    learning_rate   | 0.001    |
|    n_updates       | 256299   |
---------------------------------
=== Iterazione IRL 569 ===
Loss reward (iter 569): 7.043196201324463
=== Iterazione IRL 570 ===
Loss reward (iter 570): 7.018543243408203
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.2    |
|    critic_loss     | 0.0397   |
|    learning_rate   | 0.001    |
|    n_updates       | 256799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.2    |
|    critic_loss     | 0.0377   |
|    learning_rate   | 0.001    |
|    n_updates       | 257199   |
---------------------------------
=== Iterazione IRL 571 ===
Loss reward (iter 571): 6.979557514190674
=== Iterazione IRL 572 ===
Loss reward (iter 572): 7.040257930755615
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.2    |
|    critic_loss     | 0.0486   |
|    learning_rate   | 0.001    |
|    n_updates       | 257699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.4    |
|    critic_loss     | 0.0364   |
|    learning_rate   | 0.001    |
|    n_updates       | 258099   |
---------------------------------
=== Iterazione IRL 573 ===
Loss reward (iter 573): 7.025991439819336
=== Iterazione IRL 574 ===
Loss reward (iter 574): 7.010959148406982
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.3    |
|    critic_loss     | 0.0509   |
|    learning_rate   | 0.001    |
|    n_updates       | 258599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.5    |
|    critic_loss     | 0.0511   |
|    learning_rate   | 0.001    |
|    n_updates       | 258999   |
---------------------------------
=== Iterazione IRL 575 ===
Loss reward (iter 575): 7.016070365905762
=== Iterazione IRL 576 ===
Loss reward (iter 576): 7.023153781890869
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.2    |
|    critic_loss     | 0.0517   |
|    learning_rate   | 0.001    |
|    n_updates       | 259499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.4    |
|    critic_loss     | 0.0379   |
|    learning_rate   | 0.001    |
|    n_updates       | 259899   |
---------------------------------
=== Iterazione IRL 577 ===
Loss reward (iter 577): 7.009747505187988
=== Iterazione IRL 578 ===
Loss reward (iter 578): 7.023440361022949
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.2    |
|    critic_loss     | 0.0342   |
|    learning_rate   | 0.001    |
|    n_updates       | 260399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.5    |
|    critic_loss     | 0.0575   |
|    learning_rate   | 0.001    |
|    n_updates       | 260799   |
---------------------------------
=== Iterazione IRL 579 ===
Loss reward (iter 579): 7.0376410484313965
=== Iterazione IRL 580 ===
Loss reward (iter 580): 7.034497261047363
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.3    |
|    critic_loss     | 0.0498   |
|    learning_rate   | 0.001    |
|    n_updates       | 261299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.3    |
|    critic_loss     | 0.045    |
|    learning_rate   | 0.001    |
|    n_updates       | 261699   |
---------------------------------
=== Iterazione IRL 581 ===
Loss reward (iter 581): 7.0092291831970215
=== Iterazione IRL 582 ===
Loss reward (iter 582): 7.029502868652344
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.6    |
|    critic_loss     | 0.0381   |
|    learning_rate   | 0.001    |
|    n_updates       | 262199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.5    |
|    critic_loss     | 0.0452   |
|    learning_rate   | 0.001    |
|    n_updates       | 262599   |
---------------------------------
=== Iterazione IRL 583 ===
Loss reward (iter 583): 7.004949569702148
=== Iterazione IRL 584 ===
Loss reward (iter 584): 7.008366584777832
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.4    |
|    critic_loss     | 0.0384   |
|    learning_rate   | 0.001    |
|    n_updates       | 263099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.3    |
|    critic_loss     | 0.0504   |
|    learning_rate   | 0.001    |
|    n_updates       | 263499   |
---------------------------------
=== Iterazione IRL 585 ===
Loss reward (iter 585): 7.006563186645508
=== Iterazione IRL 586 ===
Loss reward (iter 586): 6.9860053062438965
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.5    |
|    critic_loss     | 0.0468   |
|    learning_rate   | 0.001    |
|    n_updates       | 263999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.4    |
|    critic_loss     | 0.0523   |
|    learning_rate   | 0.001    |
|    n_updates       | 264399   |
---------------------------------
=== Iterazione IRL 587 ===
Loss reward (iter 587): 6.998932838439941
=== Iterazione IRL 588 ===
Loss reward (iter 588): 7.01861047744751
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.4    |
|    critic_loss     | 0.0566   |
|    learning_rate   | 0.001    |
|    n_updates       | 264899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.3    |
|    critic_loss     | 0.0567   |
|    learning_rate   | 0.001    |
|    n_updates       | 265299   |
---------------------------------
=== Iterazione IRL 589 ===
Loss reward (iter 589): 7.003175258636475
=== Iterazione IRL 590 ===
Loss reward (iter 590): 6.9779372215271
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.5    |
|    critic_loss     | 0.047    |
|    learning_rate   | 0.001    |
|    n_updates       | 265799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.4    |
|    critic_loss     | 0.057    |
|    learning_rate   | 0.001    |
|    n_updates       | 266199   |
---------------------------------
=== Iterazione IRL 591 ===
Loss reward (iter 591): 6.995726585388184
=== Iterazione IRL 592 ===
Loss reward (iter 592): 7.0033278465271
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.2    |
|    critic_loss     | 0.0477   |
|    learning_rate   | 0.001    |
|    n_updates       | 266699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.4    |
|    critic_loss     | 0.069    |
|    learning_rate   | 0.001    |
|    n_updates       | 267099   |
---------------------------------
=== Iterazione IRL 593 ===
Loss reward (iter 593): 7.008294582366943
=== Iterazione IRL 594 ===
Loss reward (iter 594): 6.980644702911377
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.3    |
|    critic_loss     | 0.0664   |
|    learning_rate   | 0.001    |
|    n_updates       | 267599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.3    |
|    critic_loss     | 0.083    |
|    learning_rate   | 0.001    |
|    n_updates       | 267999   |
---------------------------------
=== Iterazione IRL 595 ===
Loss reward (iter 595): 7.005945682525635
=== Iterazione IRL 596 ===
Loss reward (iter 596): 7.012396812438965
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.3    |
|    critic_loss     | 0.0492   |
|    learning_rate   | 0.001    |
|    n_updates       | 268499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.2    |
|    critic_loss     | 0.0674   |
|    learning_rate   | 0.001    |
|    n_updates       | 268899   |
---------------------------------
=== Iterazione IRL 597 ===
Loss reward (iter 597): 7.027817726135254
=== Iterazione IRL 598 ===
Loss reward (iter 598): 6.9985246658325195
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.3    |
|    critic_loss     | 0.0714   |
|    learning_rate   | 0.001    |
|    n_updates       | 269399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.3    |
|    critic_loss     | 0.0716   |
|    learning_rate   | 0.001    |
|    n_updates       | 269799   |
---------------------------------
=== Iterazione IRL 599 ===
Loss reward (iter 599): 6.997622013092041
=== Iterazione IRL 600 ===
Loss reward (iter 600): 7.0040788650512695
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.2    |
|    critic_loss     | 0.0642   |
|    learning_rate   | 0.001    |
|    n_updates       | 270299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12      |
|    critic_loss     | 0.0916   |
|    learning_rate   | 0.001    |
|    n_updates       | 270699   |
---------------------------------
=== Iterazione IRL 601 ===
Loss reward (iter 601): 7.019136905670166
=== Iterazione IRL 602 ===
Loss reward (iter 602): 6.994793891906738
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.3    |
|    critic_loss     | 0.0756   |
|    learning_rate   | 0.001    |
|    n_updates       | 271199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12      |
|    critic_loss     | 0.0599   |
|    learning_rate   | 0.001    |
|    n_updates       | 271599   |
---------------------------------
=== Iterazione IRL 603 ===
Loss reward (iter 603): 6.996642112731934
=== Iterazione IRL 604 ===
Loss reward (iter 604): 6.988453388214111
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.2    |
|    critic_loss     | 0.0612   |
|    learning_rate   | 0.001    |
|    n_updates       | 272099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12      |
|    critic_loss     | 0.067    |
|    learning_rate   | 0.001    |
|    n_updates       | 272499   |
---------------------------------
=== Iterazione IRL 605 ===
Loss reward (iter 605): 7.012065410614014
=== Iterazione IRL 606 ===
Loss reward (iter 606): 7.006918907165527
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.2    |
|    critic_loss     | 0.0733   |
|    learning_rate   | 0.001    |
|    n_updates       | 272999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.9    |
|    critic_loss     | 0.0757   |
|    learning_rate   | 0.001    |
|    n_updates       | 273399   |
---------------------------------
=== Iterazione IRL 607 ===
Loss reward (iter 607): 6.996014595031738
=== Iterazione IRL 608 ===
Loss reward (iter 608): 6.981800556182861
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12      |
|    critic_loss     | 0.0789   |
|    learning_rate   | 0.001    |
|    n_updates       | 273899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12      |
|    critic_loss     | 0.0756   |
|    learning_rate   | 0.001    |
|    n_updates       | 274299   |
---------------------------------
=== Iterazione IRL 609 ===
Loss reward (iter 609): 6.973599433898926
=== Iterazione IRL 610 ===
Loss reward (iter 610): 7.038815021514893
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.8    |
|    critic_loss     | 0.0695   |
|    learning_rate   | 0.001    |
|    n_updates       | 274799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.8    |
|    critic_loss     | 0.0668   |
|    learning_rate   | 0.001    |
|    n_updates       | 275199   |
---------------------------------
=== Iterazione IRL 611 ===
Loss reward (iter 611): 7.002868175506592
=== Iterazione IRL 612 ===
Loss reward (iter 612): 7.009239673614502
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12      |
|    critic_loss     | 0.0805   |
|    learning_rate   | 0.001    |
|    n_updates       | 275699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12      |
|    critic_loss     | 0.0782   |
|    learning_rate   | 0.001    |
|    n_updates       | 276099   |
---------------------------------
=== Iterazione IRL 613 ===
Loss reward (iter 613): 7.021285057067871
=== Iterazione IRL 614 ===
Loss reward (iter 614): 6.997315406799316
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.9    |
|    critic_loss     | 0.0804   |
|    learning_rate   | 0.001    |
|    n_updates       | 276599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.8    |
|    critic_loss     | 0.094    |
|    learning_rate   | 0.001    |
|    n_updates       | 276999   |
---------------------------------
=== Iterazione IRL 615 ===
Loss reward (iter 615): 6.991191864013672
=== Iterazione IRL 616 ===
Loss reward (iter 616): 7.001499652862549
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.8    |
|    critic_loss     | 0.0961   |
|    learning_rate   | 0.001    |
|    n_updates       | 277499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.8    |
|    critic_loss     | 0.064    |
|    learning_rate   | 0.001    |
|    n_updates       | 277899   |
---------------------------------
=== Iterazione IRL 617 ===
Loss reward (iter 617): 6.97674560546875
=== Iterazione IRL 618 ===
Loss reward (iter 618): 7.021903038024902
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.8    |
|    critic_loss     | 0.0924   |
|    learning_rate   | 0.001    |
|    n_updates       | 278399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.7    |
|    critic_loss     | 0.1      |
|    learning_rate   | 0.001    |
|    n_updates       | 278799   |
---------------------------------
=== Iterazione IRL 619 ===
Loss reward (iter 619): 6.981709003448486
=== Iterazione IRL 620 ===
Loss reward (iter 620): 6.980506896972656
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.6    |
|    critic_loss     | 0.0917   |
|    learning_rate   | 0.001    |
|    n_updates       | 279299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.4    |
|    critic_loss     | 0.0669   |
|    learning_rate   | 0.001    |
|    n_updates       | 279699   |
---------------------------------
=== Iterazione IRL 621 ===
Loss reward (iter 621): 6.968191623687744
=== Iterazione IRL 622 ===
Loss reward (iter 622): 6.956118106842041
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.4    |
|    critic_loss     | 0.106    |
|    learning_rate   | 0.001    |
|    n_updates       | 280199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.6    |
|    critic_loss     | 0.0886   |
|    learning_rate   | 0.001    |
|    n_updates       | 280599   |
---------------------------------
=== Iterazione IRL 623 ===
Loss reward (iter 623): 7.014655113220215
=== Iterazione IRL 624 ===
Loss reward (iter 624): 7.001017093658447
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.7    |
|    critic_loss     | 0.0992   |
|    learning_rate   | 0.001    |
|    n_updates       | 281099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.5    |
|    critic_loss     | 0.0793   |
|    learning_rate   | 0.001    |
|    n_updates       | 281499   |
---------------------------------
=== Iterazione IRL 625 ===
Loss reward (iter 625): 6.985578536987305
=== Iterazione IRL 626 ===
Loss reward (iter 626): 6.974494457244873
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.4    |
|    critic_loss     | 0.0772   |
|    learning_rate   | 0.001    |
|    n_updates       | 281999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.5    |
|    critic_loss     | 0.0742   |
|    learning_rate   | 0.001    |
|    n_updates       | 282399   |
---------------------------------
=== Iterazione IRL 627 ===
Loss reward (iter 627): 6.978653907775879
=== Iterazione IRL 628 ===
Loss reward (iter 628): 6.977724075317383
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.4    |
|    critic_loss     | 0.0741   |
|    learning_rate   | 0.001    |
|    n_updates       | 282899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.2    |
|    critic_loss     | 0.0867   |
|    learning_rate   | 0.001    |
|    n_updates       | 283299   |
---------------------------------
=== Iterazione IRL 629 ===
Loss reward (iter 629): 6.961059093475342
=== Iterazione IRL 630 ===
Loss reward (iter 630): 6.971612453460693
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.3    |
|    critic_loss     | 0.079    |
|    learning_rate   | 0.001    |
|    n_updates       | 283799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.2    |
|    critic_loss     | 0.0628   |
|    learning_rate   | 0.001    |
|    n_updates       | 284199   |
---------------------------------
=== Iterazione IRL 631 ===
Loss reward (iter 631): 6.966119766235352
=== Iterazione IRL 632 ===
Loss reward (iter 632): 6.975358963012695
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.1    |
|    critic_loss     | 0.0712   |
|    learning_rate   | 0.001    |
|    n_updates       | 284699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.2    |
|    critic_loss     | 0.0614   |
|    learning_rate   | 0.001    |
|    n_updates       | 285099   |
---------------------------------
=== Iterazione IRL 633 ===
Loss reward (iter 633): 7.009192943572998
=== Iterazione IRL 634 ===
Loss reward (iter 634): 6.982004165649414
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.1    |
|    critic_loss     | 0.0697   |
|    learning_rate   | 0.001    |
|    n_updates       | 285599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.9    |
|    critic_loss     | 0.0892   |
|    learning_rate   | 0.001    |
|    n_updates       | 285999   |
---------------------------------
=== Iterazione IRL 635 ===
Loss reward (iter 635): 6.986738681793213
=== Iterazione IRL 636 ===
Loss reward (iter 636): 6.994512557983398
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.8    |
|    critic_loss     | 0.0688   |
|    learning_rate   | 0.001    |
|    n_updates       | 286499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.4    |
|    critic_loss     | 0.0607   |
|    learning_rate   | 0.001    |
|    n_updates       | 286899   |
---------------------------------
=== Iterazione IRL 637 ===
Loss reward (iter 637): 6.998348712921143
=== Iterazione IRL 638 ===
Loss reward (iter 638): 6.977942943572998
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.1    |
|    critic_loss     | 0.074    |
|    learning_rate   | 0.001    |
|    n_updates       | 287399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11      |
|    critic_loss     | 0.067    |
|    learning_rate   | 0.001    |
|    n_updates       | 287799   |
---------------------------------
=== Iterazione IRL 639 ===
Loss reward (iter 639): 6.9408721923828125
=== Iterazione IRL 640 ===
Loss reward (iter 640): 6.986955642700195
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11      |
|    critic_loss     | 0.0638   |
|    learning_rate   | 0.001    |
|    n_updates       | 288299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.1    |
|    critic_loss     | 0.0501   |
|    learning_rate   | 0.001    |
|    n_updates       | 288699   |
---------------------------------
=== Iterazione IRL 641 ===
Loss reward (iter 641): 6.983053684234619
=== Iterazione IRL 642 ===
Loss reward (iter 642): 6.9621734619140625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.8    |
|    critic_loss     | 0.0644   |
|    learning_rate   | 0.001    |
|    n_updates       | 289199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.2    |
|    critic_loss     | 0.0551   |
|    learning_rate   | 0.001    |
|    n_updates       | 289599   |
---------------------------------
=== Iterazione IRL 643 ===
Loss reward (iter 643): 6.941765785217285
=== Iterazione IRL 644 ===
Loss reward (iter 644): 6.947844505310059
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11      |
|    critic_loss     | 0.0587   |
|    learning_rate   | 0.001    |
|    n_updates       | 290099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.3    |
|    critic_loss     | 0.0389   |
|    learning_rate   | 0.001    |
|    n_updates       | 290499   |
---------------------------------
=== Iterazione IRL 645 ===
Loss reward (iter 645): 6.929478168487549
=== Iterazione IRL 646 ===
Loss reward (iter 646): 6.9666337966918945
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.8    |
|    critic_loss     | 0.0556   |
|    learning_rate   | 0.001    |
|    n_updates       | 290999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.1    |
|    critic_loss     | 0.0475   |
|    learning_rate   | 0.001    |
|    n_updates       | 291399   |
---------------------------------
=== Iterazione IRL 647 ===
Loss reward (iter 647): 6.9426655769348145
=== Iterazione IRL 648 ===
Loss reward (iter 648): 6.947300910949707
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.2    |
|    critic_loss     | 0.0623   |
|    learning_rate   | 0.001    |
|    n_updates       | 291899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.4    |
|    critic_loss     | 0.0567   |
|    learning_rate   | 0.001    |
|    n_updates       | 292299   |
---------------------------------
=== Iterazione IRL 649 ===
Loss reward (iter 649): 6.959873199462891
=== Iterazione IRL 650 ===
Loss reward (iter 650): 6.945170879364014
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.3    |
|    critic_loss     | 0.0402   |
|    learning_rate   | 0.001    |
|    n_updates       | 292799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11      |
|    critic_loss     | 0.0526   |
|    learning_rate   | 0.001    |
|    n_updates       | 293199   |
---------------------------------
=== Iterazione IRL 651 ===
Loss reward (iter 651): 6.968935489654541
=== Iterazione IRL 652 ===
Loss reward (iter 652): 6.998806953430176
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.2    |
|    critic_loss     | 0.0487   |
|    learning_rate   | 0.001    |
|    n_updates       | 293699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.2    |
|    critic_loss     | 0.0403   |
|    learning_rate   | 0.001    |
|    n_updates       | 294099   |
---------------------------------
=== Iterazione IRL 653 ===
Loss reward (iter 653): 6.974810600280762
=== Iterazione IRL 654 ===
Loss reward (iter 654): 6.946566581726074
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11      |
|    critic_loss     | 0.048    |
|    learning_rate   | 0.001    |
|    n_updates       | 294599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.2    |
|    critic_loss     | 0.0467   |
|    learning_rate   | 0.001    |
|    n_updates       | 294999   |
---------------------------------
=== Iterazione IRL 655 ===
Loss reward (iter 655): 6.923944473266602
=== Iterazione IRL 656 ===
Loss reward (iter 656): 6.995298862457275
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11      |
|    critic_loss     | 0.0324   |
|    learning_rate   | 0.001    |
|    n_updates       | 295499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.1    |
|    critic_loss     | 0.0451   |
|    learning_rate   | 0.001    |
|    n_updates       | 295899   |
---------------------------------
=== Iterazione IRL 657 ===
Loss reward (iter 657): 6.957308292388916
=== Iterazione IRL 658 ===
Loss reward (iter 658): 6.96224308013916
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.3    |
|    critic_loss     | 0.0522   |
|    learning_rate   | 0.001    |
|    n_updates       | 296399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.1    |
|    critic_loss     | 0.0365   |
|    learning_rate   | 0.001    |
|    n_updates       | 296799   |
---------------------------------
=== Iterazione IRL 659 ===
Loss reward (iter 659): 6.9590253829956055
=== Iterazione IRL 660 ===
Loss reward (iter 660): 6.9520697593688965
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.3    |
|    critic_loss     | 0.0343   |
|    learning_rate   | 0.001    |
|    n_updates       | 297299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.4    |
|    critic_loss     | 0.0309   |
|    learning_rate   | 0.001    |
|    n_updates       | 297699   |
---------------------------------
=== Iterazione IRL 661 ===
Loss reward (iter 661): 6.981802463531494
=== Iterazione IRL 662 ===
Loss reward (iter 662): 6.960378170013428
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.2    |
|    critic_loss     | 0.0261   |
|    learning_rate   | 0.001    |
|    n_updates       | 298199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.6    |
|    critic_loss     | 0.0363   |
|    learning_rate   | 0.001    |
|    n_updates       | 298599   |
---------------------------------
=== Iterazione IRL 663 ===
Loss reward (iter 663): 6.919078826904297
=== Iterazione IRL 664 ===
Loss reward (iter 664): 6.930147171020508
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.3    |
|    critic_loss     | 0.0384   |
|    learning_rate   | 0.001    |
|    n_updates       | 299099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.7    |
|    critic_loss     | 0.0256   |
|    learning_rate   | 0.001    |
|    n_updates       | 299499   |
---------------------------------
=== Iterazione IRL 665 ===
Loss reward (iter 665): 6.932888031005859
=== Iterazione IRL 666 ===
Loss reward (iter 666): 6.926032066345215
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.6    |
|    critic_loss     | 0.0272   |
|    learning_rate   | 0.001    |
|    n_updates       | 299999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.3    |
|    critic_loss     | 0.0402   |
|    learning_rate   | 0.001    |
|    n_updates       | 300399   |
---------------------------------
=== Iterazione IRL 667 ===
Loss reward (iter 667): 6.967144966125488
=== Iterazione IRL 668 ===
Loss reward (iter 668): 6.939675807952881
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.3    |
|    critic_loss     | 0.0268   |
|    learning_rate   | 0.001    |
|    n_updates       | 300899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.3    |
|    critic_loss     | 0.0283   |
|    learning_rate   | 0.001    |
|    n_updates       | 301299   |
---------------------------------
=== Iterazione IRL 669 ===
Loss reward (iter 669): 6.941372871398926
=== Iterazione IRL 670 ===
Loss reward (iter 670): 6.941522121429443
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.5    |
|    critic_loss     | 0.0331   |
|    learning_rate   | 0.001    |
|    n_updates       | 301799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.1    |
|    critic_loss     | 0.0296   |
|    learning_rate   | 0.001    |
|    n_updates       | 302199   |
---------------------------------
=== Iterazione IRL 671 ===
Loss reward (iter 671): 6.949185371398926
=== Iterazione IRL 672 ===
Loss reward (iter 672): 6.953932285308838
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.5    |
|    critic_loss     | 0.0334   |
|    learning_rate   | 0.001    |
|    n_updates       | 302699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.4    |
|    critic_loss     | 0.0287   |
|    learning_rate   | 0.001    |
|    n_updates       | 303099   |
---------------------------------
=== Iterazione IRL 673 ===
Loss reward (iter 673): 6.958381175994873
=== Iterazione IRL 674 ===
Loss reward (iter 674): 6.943550109863281
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.6    |
|    critic_loss     | 0.0287   |
|    learning_rate   | 0.001    |
|    n_updates       | 303599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.7    |
|    critic_loss     | 0.0307   |
|    learning_rate   | 0.001    |
|    n_updates       | 303999   |
---------------------------------
=== Iterazione IRL 675 ===
Loss reward (iter 675): 6.9228515625
=== Iterazione IRL 676 ===
Loss reward (iter 676): 7.008352279663086
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12      |
|    critic_loss     | 0.0261   |
|    learning_rate   | 0.001    |
|    n_updates       | 304499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.6    |
|    critic_loss     | 0.0224   |
|    learning_rate   | 0.001    |
|    n_updates       | 304899   |
---------------------------------
=== Iterazione IRL 677 ===
Loss reward (iter 677): 6.933618068695068
=== Iterazione IRL 678 ===
Loss reward (iter 678): 7.000049591064453
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.6    |
|    critic_loss     | 0.0324   |
|    learning_rate   | 0.001    |
|    n_updates       | 305399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.8    |
|    critic_loss     | 0.0239   |
|    learning_rate   | 0.001    |
|    n_updates       | 305799   |
---------------------------------
=== Iterazione IRL 679 ===
Loss reward (iter 679): 6.995811939239502
=== Iterazione IRL 680 ===
Loss reward (iter 680): 7.019670486450195
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.6    |
|    critic_loss     | 0.0286   |
|    learning_rate   | 0.001    |
|    n_updates       | 306299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.8    |
|    critic_loss     | 0.0319   |
|    learning_rate   | 0.001    |
|    n_updates       | 306699   |
---------------------------------
=== Iterazione IRL 681 ===
Loss reward (iter 681): 7.01577615737915
=== Iterazione IRL 682 ===
Loss reward (iter 682): 7.040108680725098
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.9    |
|    critic_loss     | 0.0397   |
|    learning_rate   | 0.001    |
|    n_updates       | 307199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.8    |
|    critic_loss     | 0.0311   |
|    learning_rate   | 0.001    |
|    n_updates       | 307599   |
---------------------------------
=== Iterazione IRL 683 ===
Loss reward (iter 683): 7.008307933807373
=== Iterazione IRL 684 ===
Loss reward (iter 684): 7.0059638023376465
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.9    |
|    critic_loss     | 0.029    |
|    learning_rate   | 0.001    |
|    n_updates       | 308099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.9    |
|    critic_loss     | 0.033    |
|    learning_rate   | 0.001    |
|    n_updates       | 308499   |
---------------------------------
=== Iterazione IRL 685 ===
Loss reward (iter 685): 7.007173538208008
=== Iterazione IRL 686 ===
Loss reward (iter 686): 6.970848083496094
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12      |
|    critic_loss     | 0.0327   |
|    learning_rate   | 0.001    |
|    n_updates       | 308999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12      |
|    critic_loss     | 0.0407   |
|    learning_rate   | 0.001    |
|    n_updates       | 309399   |
---------------------------------
=== Iterazione IRL 687 ===
Loss reward (iter 687): 6.983643054962158
=== Iterazione IRL 688 ===
Loss reward (iter 688): 6.981994152069092
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12      |
|    critic_loss     | 0.0278   |
|    learning_rate   | 0.001    |
|    n_updates       | 309899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12      |
|    critic_loss     | 0.0419   |
|    learning_rate   | 0.001    |
|    n_updates       | 310299   |
---------------------------------
=== Iterazione IRL 689 ===
Loss reward (iter 689): 6.974008083343506
=== Iterazione IRL 690 ===
Loss reward (iter 690): 6.956747055053711
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.9    |
|    critic_loss     | 0.0428   |
|    learning_rate   | 0.001    |
|    n_updates       | 310799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12      |
|    critic_loss     | 0.0307   |
|    learning_rate   | 0.001    |
|    n_updates       | 311199   |
---------------------------------
=== Iterazione IRL 691 ===
Loss reward (iter 691): 6.926849365234375
=== Iterazione IRL 692 ===
Loss reward (iter 692): 6.9305524826049805
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.1    |
|    critic_loss     | 0.0382   |
|    learning_rate   | 0.001    |
|    n_updates       | 311699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.9    |
|    critic_loss     | 0.0407   |
|    learning_rate   | 0.001    |
|    n_updates       | 312099   |
---------------------------------
=== Iterazione IRL 693 ===
Loss reward (iter 693): 6.968375205993652
=== Iterazione IRL 694 ===
Loss reward (iter 694): 6.931119918823242
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12      |
|    critic_loss     | 0.04     |
|    learning_rate   | 0.001    |
|    n_updates       | 312599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.9    |
|    critic_loss     | 0.041    |
|    learning_rate   | 0.001    |
|    n_updates       | 312999   |
---------------------------------
=== Iterazione IRL 695 ===
Loss reward (iter 695): 6.917038917541504
=== Iterazione IRL 696 ===
Loss reward (iter 696): 6.946749210357666
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.6    |
|    critic_loss     | 0.05     |
|    learning_rate   | 0.001    |
|    n_updates       | 313499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.8    |
|    critic_loss     | 0.0422   |
|    learning_rate   | 0.001    |
|    n_updates       | 313899   |
---------------------------------
=== Iterazione IRL 697 ===
Loss reward (iter 697): 6.948637962341309
=== Iterazione IRL 698 ===
Loss reward (iter 698): 6.944880962371826
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.9    |
|    critic_loss     | 0.0363   |
|    learning_rate   | 0.001    |
|    n_updates       | 314399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12      |
|    critic_loss     | 0.029    |
|    learning_rate   | 0.001    |
|    n_updates       | 314799   |
---------------------------------
=== Iterazione IRL 699 ===
Loss reward (iter 699): 6.940024375915527
=== Iterazione IRL 700 ===
Loss reward (iter 700): 6.944786071777344
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.1    |
|    critic_loss     | 0.034    |
|    learning_rate   | 0.001    |
|    n_updates       | 315299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.6    |
|    critic_loss     | 0.0377   |
|    learning_rate   | 0.001    |
|    n_updates       | 315699   |
---------------------------------
=== Iterazione IRL 701 ===
Loss reward (iter 701): 6.938664436340332
=== Iterazione IRL 702 ===
Loss reward (iter 702): 6.961649417877197
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.9    |
|    critic_loss     | 0.0411   |
|    learning_rate   | 0.001    |
|    n_updates       | 316199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.9    |
|    critic_loss     | 0.0289   |
|    learning_rate   | 0.001    |
|    n_updates       | 316599   |
---------------------------------
=== Iterazione IRL 703 ===
Loss reward (iter 703): 6.952759742736816
=== Iterazione IRL 704 ===
Loss reward (iter 704): 6.953235626220703
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.9    |
|    critic_loss     | 0.0477   |
|    learning_rate   | 0.001    |
|    n_updates       | 317099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.9    |
|    critic_loss     | 0.0345   |
|    learning_rate   | 0.001    |
|    n_updates       | 317499   |
---------------------------------
=== Iterazione IRL 705 ===
Loss reward (iter 705): 7.000094413757324
=== Iterazione IRL 706 ===
Loss reward (iter 706): 6.95708703994751
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.8    |
|    critic_loss     | 0.0417   |
|    learning_rate   | 0.001    |
|    n_updates       | 317999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.9    |
|    critic_loss     | 0.0488   |
|    learning_rate   | 0.001    |
|    n_updates       | 318399   |
---------------------------------
=== Iterazione IRL 707 ===
Loss reward (iter 707): 6.964840412139893
=== Iterazione IRL 708 ===
Loss reward (iter 708): 6.955741882324219
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.6    |
|    critic_loss     | 0.0289   |
|    learning_rate   | 0.001    |
|    n_updates       | 318899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.7    |
|    critic_loss     | 0.0339   |
|    learning_rate   | 0.001    |
|    n_updates       | 319299   |
---------------------------------
=== Iterazione IRL 709 ===
Loss reward (iter 709): 6.987863540649414
=== Iterazione IRL 710 ===
Loss reward (iter 710): 6.959280014038086
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.1    |
|    critic_loss     | 0.0325   |
|    learning_rate   | 0.001    |
|    n_updates       | 319799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.7    |
|    critic_loss     | 0.0401   |
|    learning_rate   | 0.001    |
|    n_updates       | 320199   |
---------------------------------
=== Iterazione IRL 711 ===
Loss reward (iter 711): 6.972382545471191
=== Iterazione IRL 712 ===
Loss reward (iter 712): 6.975882053375244
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12      |
|    critic_loss     | 0.0421   |
|    learning_rate   | 0.001    |
|    n_updates       | 320699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.7    |
|    critic_loss     | 0.0364   |
|    learning_rate   | 0.001    |
|    n_updates       | 321099   |
---------------------------------
=== Iterazione IRL 713 ===
Loss reward (iter 713): 6.972304344177246
=== Iterazione IRL 714 ===
Loss reward (iter 714): 6.993821144104004
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.1    |
|    critic_loss     | 0.0372   |
|    learning_rate   | 0.001    |
|    n_updates       | 321599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.9    |
|    critic_loss     | 0.0358   |
|    learning_rate   | 0.001    |
|    n_updates       | 321999   |
---------------------------------
=== Iterazione IRL 715 ===
Loss reward (iter 715): 6.942863941192627
=== Iterazione IRL 716 ===
Loss reward (iter 716): 6.955863952636719
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.7    |
|    critic_loss     | 0.0359   |
|    learning_rate   | 0.001    |
|    n_updates       | 322499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.8    |
|    critic_loss     | 0.0472   |
|    learning_rate   | 0.001    |
|    n_updates       | 322899   |
---------------------------------
=== Iterazione IRL 717 ===
Loss reward (iter 717): 6.974889755249023
=== Iterazione IRL 718 ===
Loss reward (iter 718): 6.964836120605469
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.1    |
|    critic_loss     | 0.0345   |
|    learning_rate   | 0.001    |
|    n_updates       | 323399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 235      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.8    |
|    critic_loss     | 0.0437   |
|    learning_rate   | 0.001    |
|    n_updates       | 323799   |
---------------------------------
=== Iterazione IRL 719 ===
Loss reward (iter 719): 6.945267677307129
=== Iterazione IRL 720 ===
Loss reward (iter 720): 6.953197479248047
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.4    |
|    critic_loss     | 0.0389   |
|    learning_rate   | 0.001    |
|    n_updates       | 324299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.9    |
|    critic_loss     | 0.042    |
|    learning_rate   | 0.001    |
|    n_updates       | 324699   |
---------------------------------
=== Iterazione IRL 721 ===
Loss reward (iter 721): 6.961743354797363
=== Iterazione IRL 722 ===
Loss reward (iter 722): 6.946682453155518
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.6    |
|    critic_loss     | 0.0476   |
|    learning_rate   | 0.001    |
|    n_updates       | 325199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.7    |
|    critic_loss     | 0.0397   |
|    learning_rate   | 0.001    |
|    n_updates       | 325599   |
---------------------------------
=== Iterazione IRL 723 ===
Loss reward (iter 723): 6.915108680725098
=== Iterazione IRL 724 ===
Loss reward (iter 724): 6.963628768920898
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.6    |
|    critic_loss     | 0.0464   |
|    learning_rate   | 0.001    |
|    n_updates       | 326099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.3    |
|    critic_loss     | 0.0517   |
|    learning_rate   | 0.001    |
|    n_updates       | 326499   |
---------------------------------
=== Iterazione IRL 725 ===
Loss reward (iter 725): 6.923113822937012
=== Iterazione IRL 726 ===
Loss reward (iter 726): 6.963383674621582
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.4    |
|    critic_loss     | 0.0459   |
|    learning_rate   | 0.001    |
|    n_updates       | 326999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.8    |
|    critic_loss     | 0.046    |
|    learning_rate   | 0.001    |
|    n_updates       | 327399   |
---------------------------------
=== Iterazione IRL 727 ===
Loss reward (iter 727): 6.985208511352539
=== Iterazione IRL 728 ===
Loss reward (iter 728): 6.973217964172363
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.6    |
|    critic_loss     | 0.0536   |
|    learning_rate   | 0.001    |
|    n_updates       | 327899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.5    |
|    critic_loss     | 0.054    |
|    learning_rate   | 0.001    |
|    n_updates       | 328299   |
---------------------------------
=== Iterazione IRL 729 ===
Loss reward (iter 729): 6.977855682373047
=== Iterazione IRL 730 ===
Loss reward (iter 730): 6.958255767822266
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.3    |
|    critic_loss     | 0.0415   |
|    learning_rate   | 0.001    |
|    n_updates       | 328799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.5    |
|    critic_loss     | 0.0538   |
|    learning_rate   | 0.001    |
|    n_updates       | 329199   |
---------------------------------
=== Iterazione IRL 731 ===
Loss reward (iter 731): 6.965447425842285
=== Iterazione IRL 732 ===
Loss reward (iter 732): 6.957768440246582
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.2    |
|    critic_loss     | 0.0566   |
|    learning_rate   | 0.001    |
|    n_updates       | 329699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.5    |
|    critic_loss     | 0.0528   |
|    learning_rate   | 0.001    |
|    n_updates       | 330099   |
---------------------------------
=== Iterazione IRL 733 ===
Loss reward (iter 733): 6.950075149536133
=== Iterazione IRL 734 ===
Loss reward (iter 734): 6.93456506729126
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.7    |
|    critic_loss     | 0.0558   |
|    learning_rate   | 0.001    |
|    n_updates       | 330599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.1    |
|    critic_loss     | 0.051    |
|    learning_rate   | 0.001    |
|    n_updates       | 330999   |
---------------------------------
=== Iterazione IRL 735 ===
Loss reward (iter 735): 6.939231872558594
=== Iterazione IRL 736 ===
Loss reward (iter 736): 6.935888290405273
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.1    |
|    critic_loss     | 0.0506   |
|    learning_rate   | 0.001    |
|    n_updates       | 331499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.1    |
|    critic_loss     | 0.065    |
|    learning_rate   | 0.001    |
|    n_updates       | 331899   |
---------------------------------
=== Iterazione IRL 737 ===
Loss reward (iter 737): 6.970043659210205
=== Iterazione IRL 738 ===
Loss reward (iter 738): 6.954197406768799
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.8    |
|    critic_loss     | 0.046    |
|    learning_rate   | 0.001    |
|    n_updates       | 332399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.9    |
|    critic_loss     | 0.0546   |
|    learning_rate   | 0.001    |
|    n_updates       | 332799   |
---------------------------------
=== Iterazione IRL 739 ===
Loss reward (iter 739): 6.927056312561035
=== Iterazione IRL 740 ===
Loss reward (iter 740): 6.91293478012085
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.9    |
|    critic_loss     | 0.0721   |
|    learning_rate   | 0.001    |
|    n_updates       | 333299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11      |
|    critic_loss     | 0.0579   |
|    learning_rate   | 0.001    |
|    n_updates       | 333699   |
---------------------------------
=== Iterazione IRL 741 ===
Loss reward (iter 741): 6.890651702880859
=== Iterazione IRL 742 ===
Loss reward (iter 742): 6.9210028648376465
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.9    |
|    critic_loss     | 0.0586   |
|    learning_rate   | 0.001    |
|    n_updates       | 334199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.7    |
|    critic_loss     | 0.0606   |
|    learning_rate   | 0.001    |
|    n_updates       | 334599   |
---------------------------------
=== Iterazione IRL 743 ===
Loss reward (iter 743): 6.956822395324707
=== Iterazione IRL 744 ===
Loss reward (iter 744): 6.933913707733154
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.9    |
|    critic_loss     | 0.0582   |
|    learning_rate   | 0.001    |
|    n_updates       | 335099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.8    |
|    critic_loss     | 0.0668   |
|    learning_rate   | 0.001    |
|    n_updates       | 335499   |
---------------------------------
=== Iterazione IRL 745 ===
Loss reward (iter 745): 6.938348770141602
=== Iterazione IRL 746 ===
Loss reward (iter 746): 6.959063529968262
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.9    |
|    critic_loss     | 0.0594   |
|    learning_rate   | 0.001    |
|    n_updates       | 335999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.5    |
|    critic_loss     | 0.074    |
|    learning_rate   | 0.001    |
|    n_updates       | 336399   |
---------------------------------
=== Iterazione IRL 747 ===
Loss reward (iter 747): 6.911591053009033
=== Iterazione IRL 748 ===
Loss reward (iter 748): 6.921891212463379
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.5    |
|    critic_loss     | 0.0509   |
|    learning_rate   | 0.001    |
|    n_updates       | 336899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.6    |
|    critic_loss     | 0.0588   |
|    learning_rate   | 0.001    |
|    n_updates       | 337299   |
---------------------------------
=== Iterazione IRL 749 ===
Loss reward (iter 749): 6.912312030792236
=== Iterazione IRL 750 ===
Loss reward (iter 750): 6.999799728393555
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.8    |
|    critic_loss     | 0.0749   |
|    learning_rate   | 0.001    |
|    n_updates       | 337799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.6    |
|    critic_loss     | 0.0971   |
|    learning_rate   | 0.001    |
|    n_updates       | 338199   |
---------------------------------
=== Iterazione IRL 751 ===
Loss reward (iter 751): 6.972919464111328
=== Iterazione IRL 752 ===
Loss reward (iter 752): 6.927525043487549
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.5    |
|    critic_loss     | 0.0725   |
|    learning_rate   | 0.001    |
|    n_updates       | 338699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.7    |
|    critic_loss     | 0.0649   |
|    learning_rate   | 0.001    |
|    n_updates       | 339099   |
---------------------------------
=== Iterazione IRL 753 ===
Loss reward (iter 753): 6.926700115203857
=== Iterazione IRL 754 ===
Loss reward (iter 754): 6.916866302490234
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.7    |
|    critic_loss     | 0.0825   |
|    learning_rate   | 0.001    |
|    n_updates       | 339599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.4    |
|    critic_loss     | 0.0854   |
|    learning_rate   | 0.001    |
|    n_updates       | 339999   |
---------------------------------
=== Iterazione IRL 755 ===
Loss reward (iter 755): 6.937358856201172
=== Iterazione IRL 756 ===
Loss reward (iter 756): 6.933093547821045
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.2    |
|    critic_loss     | 0.0645   |
|    learning_rate   | 0.001    |
|    n_updates       | 340499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.5    |
|    critic_loss     | 0.0751   |
|    learning_rate   | 0.001    |
|    n_updates       | 340899   |
---------------------------------
=== Iterazione IRL 757 ===
Loss reward (iter 757): 6.958203315734863
=== Iterazione IRL 758 ===
Loss reward (iter 758): 6.936435699462891
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.5    |
|    critic_loss     | 0.0756   |
|    learning_rate   | 0.001    |
|    n_updates       | 341399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.5    |
|    critic_loss     | 0.0549   |
|    learning_rate   | 0.001    |
|    n_updates       | 341799   |
---------------------------------
=== Iterazione IRL 759 ===
Loss reward (iter 759): 6.949793815612793
=== Iterazione IRL 760 ===
Loss reward (iter 760): 6.976510524749756
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.3    |
|    critic_loss     | 0.0635   |
|    learning_rate   | 0.001    |
|    n_updates       | 342299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.6    |
|    critic_loss     | 0.0618   |
|    learning_rate   | 0.001    |
|    n_updates       | 342699   |
---------------------------------
=== Iterazione IRL 761 ===
Loss reward (iter 761): 6.954137802124023
=== Iterazione IRL 762 ===
Loss reward (iter 762): 6.901518821716309
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.3    |
|    critic_loss     | 0.0486   |
|    learning_rate   | 0.001    |
|    n_updates       | 343199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.4    |
|    critic_loss     | 0.0453   |
|    learning_rate   | 0.001    |
|    n_updates       | 343599   |
---------------------------------
=== Iterazione IRL 763 ===
Loss reward (iter 763): 6.940273761749268
=== Iterazione IRL 764 ===
Loss reward (iter 764): 6.941452503204346
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.3    |
|    critic_loss     | 0.0629   |
|    learning_rate   | 0.001    |
|    n_updates       | 344099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.3    |
|    critic_loss     | 0.0752   |
|    learning_rate   | 0.001    |
|    n_updates       | 344499   |
---------------------------------
=== Iterazione IRL 765 ===
Loss reward (iter 765): 6.947106838226318
=== Iterazione IRL 766 ===
Loss reward (iter 766): 6.950652122497559
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.1    |
|    critic_loss     | 0.0688   |
|    learning_rate   | 0.001    |
|    n_updates       | 344999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.1    |
|    critic_loss     | 0.0744   |
|    learning_rate   | 0.001    |
|    n_updates       | 345399   |
---------------------------------
=== Iterazione IRL 767 ===
Loss reward (iter 767): 6.927387714385986
=== Iterazione IRL 768 ===
Loss reward (iter 768): 6.918715476989746
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.2    |
|    critic_loss     | 0.0679   |
|    learning_rate   | 0.001    |
|    n_updates       | 345899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.3    |
|    critic_loss     | 0.0802   |
|    learning_rate   | 0.001    |
|    n_updates       | 346299   |
---------------------------------
=== Iterazione IRL 769 ===
Loss reward (iter 769): 6.954472541809082
=== Iterazione IRL 770 ===
Loss reward (iter 770): 6.960037708282471
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.5    |
|    critic_loss     | 0.0662   |
|    learning_rate   | 0.001    |
|    n_updates       | 346799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.2    |
|    critic_loss     | 0.0705   |
|    learning_rate   | 0.001    |
|    n_updates       | 347199   |
---------------------------------
=== Iterazione IRL 771 ===
Loss reward (iter 771): 6.934887409210205
=== Iterazione IRL 772 ===
Loss reward (iter 772): 6.90728759765625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.3    |
|    critic_loss     | 0.0625   |
|    learning_rate   | 0.001    |
|    n_updates       | 347699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.1    |
|    critic_loss     | 0.0595   |
|    learning_rate   | 0.001    |
|    n_updates       | 348099   |
---------------------------------
=== Iterazione IRL 773 ===
Loss reward (iter 773): 6.940129280090332
=== Iterazione IRL 774 ===
Loss reward (iter 774): 6.946173667907715
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.3    |
|    critic_loss     | 0.0622   |
|    learning_rate   | 0.001    |
|    n_updates       | 348599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.4    |
|    critic_loss     | 0.0554   |
|    learning_rate   | 0.001    |
|    n_updates       | 348999   |
---------------------------------
=== Iterazione IRL 775 ===
Loss reward (iter 775): 6.909132957458496
=== Iterazione IRL 776 ===
Loss reward (iter 776): 6.946050643920898
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.4    |
|    critic_loss     | 0.0415   |
|    learning_rate   | 0.001    |
|    n_updates       | 349499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.4    |
|    critic_loss     | 0.0374   |
|    learning_rate   | 0.001    |
|    n_updates       | 349899   |
---------------------------------
=== Iterazione IRL 777 ===
Loss reward (iter 777): 6.88992977142334
=== Iterazione IRL 778 ===
Loss reward (iter 778): 6.915953159332275
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.4    |
|    critic_loss     | 0.0538   |
|    learning_rate   | 0.001    |
|    n_updates       | 350399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.3    |
|    critic_loss     | 0.0637   |
|    learning_rate   | 0.001    |
|    n_updates       | 350799   |
---------------------------------
=== Iterazione IRL 779 ===
Loss reward (iter 779): 6.949460029602051
=== Iterazione IRL 780 ===
Loss reward (iter 780): 6.93964958190918
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.5    |
|    critic_loss     | 0.0604   |
|    learning_rate   | 0.001    |
|    n_updates       | 351299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.4    |
|    critic_loss     | 0.0544   |
|    learning_rate   | 0.001    |
|    n_updates       | 351699   |
---------------------------------
=== Iterazione IRL 781 ===
Loss reward (iter 781): 6.991474151611328
=== Iterazione IRL 782 ===
Loss reward (iter 782): 6.997379779815674
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.3    |
|    critic_loss     | 0.0542   |
|    learning_rate   | 0.001    |
|    n_updates       | 352199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.3    |
|    critic_loss     | 0.0386   |
|    learning_rate   | 0.001    |
|    n_updates       | 352599   |
---------------------------------
=== Iterazione IRL 783 ===
Loss reward (iter 783): 7.03290319442749
=== Iterazione IRL 784 ===
Loss reward (iter 784): 7.007697105407715
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.6    |
|    critic_loss     | 0.0474   |
|    learning_rate   | 0.001    |
|    n_updates       | 353099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.5    |
|    critic_loss     | 0.0467   |
|    learning_rate   | 0.001    |
|    n_updates       | 353499   |
---------------------------------
=== Iterazione IRL 785 ===
Loss reward (iter 785): 7.038782119750977
=== Iterazione IRL 786 ===
Loss reward (iter 786): 7.101956844329834
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.4    |
|    critic_loss     | 0.0363   |
|    learning_rate   | 0.001    |
|    n_updates       | 353999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.4    |
|    critic_loss     | 0.0425   |
|    learning_rate   | 0.001    |
|    n_updates       | 354399   |
---------------------------------
=== Iterazione IRL 787 ===
Loss reward (iter 787): 7.079474925994873
=== Iterazione IRL 788 ===
Loss reward (iter 788): 7.178928852081299
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.6    |
|    critic_loss     | 0.0414   |
|    learning_rate   | 0.001    |
|    n_updates       | 354899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.7    |
|    critic_loss     | 0.0334   |
|    learning_rate   | 0.001    |
|    n_updates       | 355299   |
---------------------------------
=== Iterazione IRL 789 ===
Loss reward (iter 789): 7.0068535804748535
=== Iterazione IRL 790 ===
Loss reward (iter 790): 6.970781326293945
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.9    |
|    critic_loss     | 0.0365   |
|    learning_rate   | 0.001    |
|    n_updates       | 355799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.7    |
|    critic_loss     | 0.03     |
|    learning_rate   | 0.001    |
|    n_updates       | 356199   |
---------------------------------
=== Iterazione IRL 791 ===
Loss reward (iter 791): 6.982987880706787
=== Iterazione IRL 792 ===
Loss reward (iter 792): 6.95805549621582
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.7    |
|    critic_loss     | 0.0338   |
|    learning_rate   | 0.001    |
|    n_updates       | 356699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11      |
|    critic_loss     | 0.0324   |
|    learning_rate   | 0.001    |
|    n_updates       | 357099   |
---------------------------------
=== Iterazione IRL 793 ===
Loss reward (iter 793): 6.968655109405518
=== Iterazione IRL 794 ===
Loss reward (iter 794): 6.968133926391602
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.9    |
|    critic_loss     | 0.027    |
|    learning_rate   | 0.001    |
|    n_updates       | 357599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11      |
|    critic_loss     | 0.0282   |
|    learning_rate   | 0.001    |
|    n_updates       | 357999   |
---------------------------------
=== Iterazione IRL 795 ===
Loss reward (iter 795): 6.933307647705078
=== Iterazione IRL 796 ===
Loss reward (iter 796): 6.9449310302734375
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.1    |
|    critic_loss     | 0.0167   |
|    learning_rate   | 0.001    |
|    n_updates       | 358499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11      |
|    critic_loss     | 0.0262   |
|    learning_rate   | 0.001    |
|    n_updates       | 358899   |
---------------------------------
=== Iterazione IRL 797 ===
Loss reward (iter 797): 6.964529037475586
=== Iterazione IRL 798 ===
Loss reward (iter 798): 6.950884819030762
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11      |
|    critic_loss     | 0.0156   |
|    learning_rate   | 0.001    |
|    n_updates       | 359399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.2    |
|    critic_loss     | 0.0207   |
|    learning_rate   | 0.001    |
|    n_updates       | 359799   |
---------------------------------
=== Iterazione IRL 799 ===
Loss reward (iter 799): 6.971314430236816
=== Iterazione IRL 800 ===
Loss reward (iter 800): 6.977972030639648
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.3    |
|    critic_loss     | 0.0201   |
|    learning_rate   | 0.001    |
|    n_updates       | 360299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.4    |
|    critic_loss     | 0.0188   |
|    learning_rate   | 0.001    |
|    n_updates       | 360699   |
---------------------------------
=== Iterazione IRL 801 ===
Loss reward (iter 801): 6.959630012512207
=== Iterazione IRL 802 ===
Loss reward (iter 802): 6.936183929443359
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.4    |
|    critic_loss     | 0.0181   |
|    learning_rate   | 0.001    |
|    n_updates       | 361199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.6    |
|    critic_loss     | 0.0169   |
|    learning_rate   | 0.001    |
|    n_updates       | 361599   |
---------------------------------
=== Iterazione IRL 803 ===
Loss reward (iter 803): 6.972569465637207
=== Iterazione IRL 804 ===
Loss reward (iter 804): 6.968092441558838
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.6    |
|    critic_loss     | 0.0187   |
|    learning_rate   | 0.001    |
|    n_updates       | 362099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.6    |
|    critic_loss     | 0.0169   |
|    learning_rate   | 0.001    |
|    n_updates       | 362499   |
---------------------------------
=== Iterazione IRL 805 ===
Loss reward (iter 805): 6.974941253662109
=== Iterazione IRL 806 ===
Loss reward (iter 806): 6.989333152770996
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.8    |
|    critic_loss     | 0.0176   |
|    learning_rate   | 0.001    |
|    n_updates       | 362999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.9    |
|    critic_loss     | 0.0209   |
|    learning_rate   | 0.001    |
|    n_updates       | 363399   |
---------------------------------
=== Iterazione IRL 807 ===
Loss reward (iter 807): 6.970641136169434
=== Iterazione IRL 808 ===
Loss reward (iter 808): 6.9365339279174805
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.1    |
|    critic_loss     | 0.0147   |
|    learning_rate   | 0.001    |
|    n_updates       | 363899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12      |
|    critic_loss     | 0.0181   |
|    learning_rate   | 0.001    |
|    n_updates       | 364299   |
---------------------------------
=== Iterazione IRL 809 ===
Loss reward (iter 809): 6.9397382736206055
=== Iterazione IRL 810 ===
Loss reward (iter 810): 6.993682384490967
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.3    |
|    critic_loss     | 0.0178   |
|    learning_rate   | 0.001    |
|    n_updates       | 364799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.1    |
|    critic_loss     | 0.0155   |
|    learning_rate   | 0.001    |
|    n_updates       | 365199   |
---------------------------------
=== Iterazione IRL 811 ===
Loss reward (iter 811): 6.964099884033203
=== Iterazione IRL 812 ===
Loss reward (iter 812): 6.943706512451172
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.3    |
|    critic_loss     | 0.0151   |
|    learning_rate   | 0.001    |
|    n_updates       | 365699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.6    |
|    critic_loss     | 0.0157   |
|    learning_rate   | 0.001    |
|    n_updates       | 366099   |
---------------------------------
=== Iterazione IRL 813 ===
Loss reward (iter 813): 6.933135986328125
=== Iterazione IRL 814 ===
Loss reward (iter 814): 6.963671684265137
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.4    |
|    critic_loss     | 0.0245   |
|    learning_rate   | 0.001    |
|    n_updates       | 366599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 229      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.5    |
|    critic_loss     | 0.0177   |
|    learning_rate   | 0.001    |
|    n_updates       | 366999   |
---------------------------------
=== Iterazione IRL 815 ===
Loss reward (iter 815): 6.944175720214844
=== Iterazione IRL 816 ===
Loss reward (iter 816): 6.9683051109313965
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.5    |
|    critic_loss     | 0.0151   |
|    learning_rate   | 0.001    |
|    n_updates       | 367499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.6    |
|    critic_loss     | 0.0148   |
|    learning_rate   | 0.001    |
|    n_updates       | 367899   |
---------------------------------
=== Iterazione IRL 817 ===
Loss reward (iter 817): 6.930295467376709
=== Iterazione IRL 818 ===
Loss reward (iter 818): 6.914334297180176
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.7    |
|    critic_loss     | 0.014    |
|    learning_rate   | 0.001    |
|    n_updates       | 368399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.9    |
|    critic_loss     | 0.0114   |
|    learning_rate   | 0.001    |
|    n_updates       | 368799   |
---------------------------------
=== Iterazione IRL 819 ===
Loss reward (iter 819): 6.958609104156494
=== Iterazione IRL 820 ===
Loss reward (iter 820): 6.9429240226745605
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.9    |
|    critic_loss     | 0.0114   |
|    learning_rate   | 0.001    |
|    n_updates       | 369299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.1    |
|    critic_loss     | 0.0111   |
|    learning_rate   | 0.001    |
|    n_updates       | 369699   |
---------------------------------
=== Iterazione IRL 821 ===
Loss reward (iter 821): 6.92758321762085
=== Iterazione IRL 822 ===
Loss reward (iter 822): 6.946563243865967
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.9    |
|    critic_loss     | 0.0127   |
|    learning_rate   | 0.001    |
|    n_updates       | 370199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.9    |
|    critic_loss     | 0.0123   |
|    learning_rate   | 0.001    |
|    n_updates       | 370599   |
---------------------------------
=== Iterazione IRL 823 ===
Loss reward (iter 823): 6.90011739730835
=== Iterazione IRL 824 ===
Loss reward (iter 824): 6.943897247314453
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.2    |
|    critic_loss     | 0.0105   |
|    learning_rate   | 0.001    |
|    n_updates       | 371099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.2    |
|    critic_loss     | 0.0102   |
|    learning_rate   | 0.001    |
|    n_updates       | 371499   |
---------------------------------
=== Iterazione IRL 825 ===
Loss reward (iter 825): 6.895598411560059
=== Iterazione IRL 826 ===
Loss reward (iter 826): 6.946974277496338
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.2    |
|    critic_loss     | 0.0129   |
|    learning_rate   | 0.001    |
|    n_updates       | 371999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.3    |
|    critic_loss     | 0.0122   |
|    learning_rate   | 0.001    |
|    n_updates       | 372399   |
---------------------------------
=== Iterazione IRL 827 ===
Loss reward (iter 827): 6.9246506690979
=== Iterazione IRL 828 ===
Loss reward (iter 828): 6.896425724029541
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.7    |
|    critic_loss     | 0.0144   |
|    learning_rate   | 0.001    |
|    n_updates       | 372899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.5    |
|    critic_loss     | 0.0138   |
|    learning_rate   | 0.001    |
|    n_updates       | 373299   |
---------------------------------
=== Iterazione IRL 829 ===
Loss reward (iter 829): 6.898101329803467
=== Iterazione IRL 830 ===
Loss reward (iter 830): 6.926208972930908
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.5    |
|    critic_loss     | 0.0139   |
|    learning_rate   | 0.001    |
|    n_updates       | 373799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.6    |
|    critic_loss     | 0.0114   |
|    learning_rate   | 0.001    |
|    n_updates       | 374199   |
---------------------------------
=== Iterazione IRL 831 ===
Loss reward (iter 831): 6.891660213470459
=== Iterazione IRL 832 ===
Loss reward (iter 832): 6.9070329666137695
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.7    |
|    critic_loss     | 0.0154   |
|    learning_rate   | 0.001    |
|    n_updates       | 374699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.5    |
|    critic_loss     | 0.0159   |
|    learning_rate   | 0.001    |
|    n_updates       | 375099   |
---------------------------------
=== Iterazione IRL 833 ===
Loss reward (iter 833): 6.923151016235352
=== Iterazione IRL 834 ===
Loss reward (iter 834): 6.9003801345825195
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.6    |
|    critic_loss     | 0.0118   |
|    learning_rate   | 0.001    |
|    n_updates       | 375599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.8    |
|    critic_loss     | 0.0159   |
|    learning_rate   | 0.001    |
|    n_updates       | 375999   |
---------------------------------
=== Iterazione IRL 835 ===
Loss reward (iter 835): 6.897758483886719
=== Iterazione IRL 836 ===
Loss reward (iter 836): 6.932044506072998
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.9    |
|    critic_loss     | 0.0139   |
|    learning_rate   | 0.001    |
|    n_updates       | 376499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.8    |
|    critic_loss     | 0.0101   |
|    learning_rate   | 0.001    |
|    n_updates       | 376899   |
---------------------------------
=== Iterazione IRL 837 ===
Loss reward (iter 837): 6.893260955810547
=== Iterazione IRL 838 ===
Loss reward (iter 838): 6.936389923095703
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.9    |
|    critic_loss     | 0.0139   |
|    learning_rate   | 0.001    |
|    n_updates       | 377399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14      |
|    critic_loss     | 0.0193   |
|    learning_rate   | 0.001    |
|    n_updates       | 377799   |
---------------------------------
=== Iterazione IRL 839 ===
Loss reward (iter 839): 6.931308746337891
=== Iterazione IRL 840 ===
Loss reward (iter 840): 6.939265251159668
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14      |
|    critic_loss     | 0.0167   |
|    learning_rate   | 0.001    |
|    n_updates       | 378299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 213      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.2    |
|    critic_loss     | 0.0163   |
|    learning_rate   | 0.001    |
|    n_updates       | 378699   |
---------------------------------
=== Iterazione IRL 841 ===
Loss reward (iter 841): 6.930283546447754
=== Iterazione IRL 842 ===
Loss reward (iter 842): 6.917992115020752
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.1    |
|    critic_loss     | 0.015    |
|    learning_rate   | 0.001    |
|    n_updates       | 379199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.1    |
|    critic_loss     | 0.0133   |
|    learning_rate   | 0.001    |
|    n_updates       | 379599   |
---------------------------------
=== Iterazione IRL 843 ===
Loss reward (iter 843): 6.922521591186523
=== Iterazione IRL 844 ===
Loss reward (iter 844): 6.936883926391602
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.3    |
|    critic_loss     | 0.0121   |
|    learning_rate   | 0.001    |
|    n_updates       | 380099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.2    |
|    critic_loss     | 0.0131   |
|    learning_rate   | 0.001    |
|    n_updates       | 380499   |
---------------------------------
=== Iterazione IRL 845 ===
Loss reward (iter 845): 6.88978910446167
=== Iterazione IRL 846 ===
Loss reward (iter 846): 6.930824279785156
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.4    |
|    critic_loss     | 0.0183   |
|    learning_rate   | 0.001    |
|    n_updates       | 380999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.3    |
|    critic_loss     | 0.0167   |
|    learning_rate   | 0.001    |
|    n_updates       | 381399   |
---------------------------------
=== Iterazione IRL 847 ===
Loss reward (iter 847): 6.917240619659424
=== Iterazione IRL 848 ===
Loss reward (iter 848): 6.881775379180908
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 273      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.5    |
|    critic_loss     | 0.0202   |
|    learning_rate   | 0.001    |
|    n_updates       | 381899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.4    |
|    critic_loss     | 0.0188   |
|    learning_rate   | 0.001    |
|    n_updates       | 382299   |
---------------------------------
=== Iterazione IRL 849 ===
Loss reward (iter 849): 6.8812456130981445
=== Iterazione IRL 850 ===
Loss reward (iter 850): 6.882284641265869
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.5    |
|    critic_loss     | 0.0176   |
|    learning_rate   | 0.001    |
|    n_updates       | 382799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.4    |
|    critic_loss     | 0.0147   |
|    learning_rate   | 0.001    |
|    n_updates       | 383199   |
---------------------------------
=== Iterazione IRL 851 ===
Loss reward (iter 851): 6.912945747375488
=== Iterazione IRL 852 ===
Loss reward (iter 852): 6.913840293884277
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.4    |
|    critic_loss     | 0.0159   |
|    learning_rate   | 0.001    |
|    n_updates       | 383699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.4    |
|    critic_loss     | 0.0198   |
|    learning_rate   | 0.001    |
|    n_updates       | 384099   |
---------------------------------
=== Iterazione IRL 853 ===
Loss reward (iter 853): 6.918518543243408
=== Iterazione IRL 854 ===
Loss reward (iter 854): 6.979473114013672
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.6    |
|    critic_loss     | 0.0209   |
|    learning_rate   | 0.001    |
|    n_updates       | 384599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.6    |
|    critic_loss     | 0.0119   |
|    learning_rate   | 0.001    |
|    n_updates       | 384999   |
---------------------------------
=== Iterazione IRL 855 ===
Loss reward (iter 855): 6.903617858886719
=== Iterazione IRL 856 ===
Loss reward (iter 856): 6.935884475708008
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.6    |
|    critic_loss     | 0.0194   |
|    learning_rate   | 0.001    |
|    n_updates       | 385499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.7    |
|    critic_loss     | 0.0184   |
|    learning_rate   | 0.001    |
|    n_updates       | 385899   |
---------------------------------
=== Iterazione IRL 857 ===
Loss reward (iter 857): 6.961424350738525
=== Iterazione IRL 858 ===
Loss reward (iter 858): 6.96038293838501
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.6    |
|    critic_loss     | 0.0147   |
|    learning_rate   | 0.001    |
|    n_updates       | 386399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.6    |
|    critic_loss     | 0.0136   |
|    learning_rate   | 0.001    |
|    n_updates       | 386799   |
---------------------------------
=== Iterazione IRL 859 ===
Loss reward (iter 859): 6.9547038078308105
=== Iterazione IRL 860 ===
Loss reward (iter 860): 6.900701999664307
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.6    |
|    critic_loss     | 0.0153   |
|    learning_rate   | 0.001    |
|    n_updates       | 387299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.6    |
|    critic_loss     | 0.0168   |
|    learning_rate   | 0.001    |
|    n_updates       | 387699   |
---------------------------------
=== Iterazione IRL 861 ===
Loss reward (iter 861): 6.925634384155273
=== Iterazione IRL 862 ===
Loss reward (iter 862): 6.909695148468018
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.8    |
|    critic_loss     | 0.0174   |
|    learning_rate   | 0.001    |
|    n_updates       | 388199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.8    |
|    critic_loss     | 0.0175   |
|    learning_rate   | 0.001    |
|    n_updates       | 388599   |
---------------------------------
=== Iterazione IRL 863 ===
Loss reward (iter 863): 6.91967248916626
=== Iterazione IRL 864 ===
Loss reward (iter 864): 6.933478832244873
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.8    |
|    critic_loss     | 0.0194   |
|    learning_rate   | 0.001    |
|    n_updates       | 389099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 239      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.7    |
|    critic_loss     | 0.0213   |
|    learning_rate   | 0.001    |
|    n_updates       | 389499   |
---------------------------------
=== Iterazione IRL 865 ===
Loss reward (iter 865): 6.9274702072143555
=== Iterazione IRL 866 ===
Loss reward (iter 866): 6.942440986633301
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 272      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.6    |
|    critic_loss     | 0.0306   |
|    learning_rate   | 0.001    |
|    n_updates       | 389999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 227      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.6    |
|    critic_loss     | 0.0454   |
|    learning_rate   | 0.001    |
|    n_updates       | 390399   |
---------------------------------
=== Iterazione IRL 867 ===
Loss reward (iter 867): 6.92191743850708
=== Iterazione IRL 868 ===
Loss reward (iter 868): 6.915200233459473
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.7    |
|    critic_loss     | 0.0308   |
|    learning_rate   | 0.001    |
|    n_updates       | 390899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 208      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.5    |
|    critic_loss     | 0.0311   |
|    learning_rate   | 0.001    |
|    n_updates       | 391299   |
---------------------------------
=== Iterazione IRL 869 ===
Loss reward (iter 869): 6.904062271118164
=== Iterazione IRL 870 ===
Loss reward (iter 870): 6.877249717712402
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 242      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.4    |
|    critic_loss     | 0.0363   |
|    learning_rate   | 0.001    |
|    n_updates       | 391799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 226      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.5    |
|    critic_loss     | 0.0306   |
|    learning_rate   | 0.001    |
|    n_updates       | 392199   |
---------------------------------
=== Iterazione IRL 871 ===
Loss reward (iter 871): 6.916448593139648
=== Iterazione IRL 872 ===
Loss reward (iter 872): 6.920799732208252
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.4    |
|    critic_loss     | 0.0339   |
|    learning_rate   | 0.001    |
|    n_updates       | 392699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.3    |
|    critic_loss     | 0.0284   |
|    learning_rate   | 0.001    |
|    n_updates       | 393099   |
---------------------------------
=== Iterazione IRL 873 ===
Loss reward (iter 873): 6.952213764190674
=== Iterazione IRL 874 ===
Loss reward (iter 874): 6.923739433288574
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.4    |
|    critic_loss     | 0.0305   |
|    learning_rate   | 0.001    |
|    n_updates       | 393599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.3    |
|    critic_loss     | 0.0301   |
|    learning_rate   | 0.001    |
|    n_updates       | 393999   |
---------------------------------
=== Iterazione IRL 875 ===
Loss reward (iter 875): 6.9405622482299805
=== Iterazione IRL 876 ===
Loss reward (iter 876): 6.964827060699463
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.3    |
|    critic_loss     | 0.0223   |
|    learning_rate   | 0.001    |
|    n_updates       | 394499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.5    |
|    critic_loss     | 0.022    |
|    learning_rate   | 0.001    |
|    n_updates       | 394899   |
---------------------------------
=== Iterazione IRL 877 ===
Loss reward (iter 877): 6.9411115646362305
=== Iterazione IRL 878 ===
Loss reward (iter 878): 6.890766620635986
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.3    |
|    critic_loss     | 0.0245   |
|    learning_rate   | 0.001    |
|    n_updates       | 395399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.2    |
|    critic_loss     | 0.0196   |
|    learning_rate   | 0.001    |
|    n_updates       | 395799   |
---------------------------------
=== Iterazione IRL 879 ===
Loss reward (iter 879): 6.917303562164307
=== Iterazione IRL 880 ===
Loss reward (iter 880): 6.931512832641602
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.3    |
|    critic_loss     | 0.0182   |
|    learning_rate   | 0.001    |
|    n_updates       | 396299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.2    |
|    critic_loss     | 0.0185   |
|    learning_rate   | 0.001    |
|    n_updates       | 396699   |
---------------------------------
=== Iterazione IRL 881 ===
Loss reward (iter 881): 6.893270015716553
=== Iterazione IRL 882 ===
Loss reward (iter 882): 6.94570255279541
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.4    |
|    critic_loss     | 0.0215   |
|    learning_rate   | 0.001    |
|    n_updates       | 397199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.1    |
|    critic_loss     | 0.022    |
|    learning_rate   | 0.001    |
|    n_updates       | 397599   |
---------------------------------
=== Iterazione IRL 883 ===
Loss reward (iter 883): 6.932089328765869
=== Iterazione IRL 884 ===
Loss reward (iter 884): 6.910269737243652
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.1    |
|    critic_loss     | 0.0226   |
|    learning_rate   | 0.001    |
|    n_updates       | 398099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.2    |
|    critic_loss     | 0.0252   |
|    learning_rate   | 0.001    |
|    n_updates       | 398499   |
---------------------------------
=== Iterazione IRL 885 ===
Loss reward (iter 885): 6.895459175109863
=== Iterazione IRL 886 ===
Loss reward (iter 886): 6.893340587615967
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.3    |
|    critic_loss     | 0.0324   |
|    learning_rate   | 0.001    |
|    n_updates       | 398999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.1    |
|    critic_loss     | 0.0322   |
|    learning_rate   | 0.001    |
|    n_updates       | 399399   |
---------------------------------
=== Iterazione IRL 887 ===
Loss reward (iter 887): 6.916823387145996
=== Iterazione IRL 888 ===
Loss reward (iter 888): 6.893634796142578
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14.2    |
|    critic_loss     | 0.0374   |
|    learning_rate   | 0.001    |
|    n_updates       | 399899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14      |
|    critic_loss     | 0.0356   |
|    learning_rate   | 0.001    |
|    n_updates       | 400299   |
---------------------------------
=== Iterazione IRL 889 ===
Loss reward (iter 889): 6.908102512359619
=== Iterazione IRL 890 ===
Loss reward (iter 890): 6.930140495300293
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -14      |
|    critic_loss     | 0.033    |
|    learning_rate   | 0.001    |
|    n_updates       | 400799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -14.2    |
|    critic_loss     | 0.0394   |
|    learning_rate   | 0.001    |
|    n_updates       | 401199   |
---------------------------------
=== Iterazione IRL 891 ===
Loss reward (iter 891): 6.880237579345703
=== Iterazione IRL 892 ===
Loss reward (iter 892): 6.938056945800781
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.8    |
|    critic_loss     | 0.0322   |
|    learning_rate   | 0.001    |
|    n_updates       | 401699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 225      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.6    |
|    critic_loss     | 0.0278   |
|    learning_rate   | 0.001    |
|    n_updates       | 402099   |
---------------------------------
=== Iterazione IRL 893 ===
Loss reward (iter 893): 6.894729137420654
=== Iterazione IRL 894 ===
Loss reward (iter 894): 6.899785995483398
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.7    |
|    critic_loss     | 0.0397   |
|    learning_rate   | 0.001    |
|    n_updates       | 402599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.6    |
|    critic_loss     | 0.0372   |
|    learning_rate   | 0.001    |
|    n_updates       | 402999   |
---------------------------------
=== Iterazione IRL 895 ===
Loss reward (iter 895): 6.905287265777588
=== Iterazione IRL 896 ===
Loss reward (iter 896): 6.898332118988037
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.3    |
|    critic_loss     | 0.0389   |
|    learning_rate   | 0.001    |
|    n_updates       | 403499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.5    |
|    critic_loss     | 0.0441   |
|    learning_rate   | 0.001    |
|    n_updates       | 403899   |
---------------------------------
=== Iterazione IRL 897 ===
Loss reward (iter 897): 6.912871360778809
=== Iterazione IRL 898 ===
Loss reward (iter 898): 6.905641555786133
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.4    |
|    critic_loss     | 0.0353   |
|    learning_rate   | 0.001    |
|    n_updates       | 404399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.4    |
|    critic_loss     | 0.047    |
|    learning_rate   | 0.001    |
|    n_updates       | 404799   |
---------------------------------
=== Iterazione IRL 899 ===
Loss reward (iter 899): 6.9213786125183105
=== Iterazione IRL 900 ===
Loss reward (iter 900): 6.876452922821045
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.1    |
|    critic_loss     | 0.049    |
|    learning_rate   | 0.001    |
|    n_updates       | 405299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.3    |
|    critic_loss     | 0.0503   |
|    learning_rate   | 0.001    |
|    n_updates       | 405699   |
---------------------------------
=== Iterazione IRL 901 ===
Loss reward (iter 901): 6.908422470092773
=== Iterazione IRL 902 ===
Loss reward (iter 902): 6.871521949768066
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.9    |
|    critic_loss     | 0.04     |
|    learning_rate   | 0.001    |
|    n_updates       | 406199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.1    |
|    critic_loss     | 0.0528   |
|    learning_rate   | 0.001    |
|    n_updates       | 406599   |
---------------------------------
=== Iterazione IRL 903 ===
Loss reward (iter 903): 6.905426979064941
=== Iterazione IRL 904 ===
Loss reward (iter 904): 6.8802642822265625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.1    |
|    critic_loss     | 0.0515   |
|    learning_rate   | 0.001    |
|    n_updates       | 407099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.9    |
|    critic_loss     | 0.043    |
|    learning_rate   | 0.001    |
|    n_updates       | 407499   |
---------------------------------
=== Iterazione IRL 905 ===
Loss reward (iter 905): 6.8885111808776855
=== Iterazione IRL 906 ===
Loss reward (iter 906): 6.924203872680664
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -13.1    |
|    critic_loss     | 0.0544   |
|    learning_rate   | 0.001    |
|    n_updates       | 407999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -13.1    |
|    critic_loss     | 0.0545   |
|    learning_rate   | 0.001    |
|    n_updates       | 408399   |
---------------------------------
=== Iterazione IRL 907 ===
Loss reward (iter 907): 6.9427008628845215
=== Iterazione IRL 908 ===
Loss reward (iter 908): 6.925611972808838
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.5    |
|    critic_loss     | 0.0427   |
|    learning_rate   | 0.001    |
|    n_updates       | 408899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 208      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.5    |
|    critic_loss     | 0.0485   |
|    learning_rate   | 0.001    |
|    n_updates       | 409299   |
---------------------------------
=== Iterazione IRL 909 ===
Loss reward (iter 909): 6.880397796630859
=== Iterazione IRL 910 ===
Loss reward (iter 910): 6.901188850402832
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.3    |
|    critic_loss     | 0.0532   |
|    learning_rate   | 0.001    |
|    n_updates       | 409799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 208      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.2    |
|    critic_loss     | 0.0575   |
|    learning_rate   | 0.001    |
|    n_updates       | 410199   |
---------------------------------
=== Iterazione IRL 911 ===
Loss reward (iter 911): 6.858875274658203
=== Iterazione IRL 912 ===
Loss reward (iter 912): 6.933140754699707
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.4    |
|    critic_loss     | 0.0495   |
|    learning_rate   | 0.001    |
|    n_updates       | 410699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.3    |
|    critic_loss     | 0.0462   |
|    learning_rate   | 0.001    |
|    n_updates       | 411099   |
---------------------------------
=== Iterazione IRL 913 ===
Loss reward (iter 913): 6.9014129638671875
=== Iterazione IRL 914 ===
Loss reward (iter 914): 6.909286975860596
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.4    |
|    critic_loss     | 0.0499   |
|    learning_rate   | 0.001    |
|    n_updates       | 411599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12      |
|    critic_loss     | 0.0425   |
|    learning_rate   | 0.001    |
|    n_updates       | 411999   |
---------------------------------
=== Iterazione IRL 915 ===
Loss reward (iter 915): 6.893244743347168
=== Iterazione IRL 916 ===
Loss reward (iter 916): 6.8889031410217285
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -12.1    |
|    critic_loss     | 0.0437   |
|    learning_rate   | 0.001    |
|    n_updates       | 412499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -12.1    |
|    critic_loss     | 0.0569   |
|    learning_rate   | 0.001    |
|    n_updates       | 412899   |
---------------------------------
=== Iterazione IRL 917 ===
Loss reward (iter 917): 6.906548023223877
=== Iterazione IRL 918 ===
Loss reward (iter 918): 6.872951507568359
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.9    |
|    critic_loss     | 0.0515   |
|    learning_rate   | 0.001    |
|    n_updates       | 413399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.8    |
|    critic_loss     | 0.045    |
|    learning_rate   | 0.001    |
|    n_updates       | 413799   |
---------------------------------
=== Iterazione IRL 919 ===
Loss reward (iter 919): 6.8911333084106445
=== Iterazione IRL 920 ===
Loss reward (iter 920): 6.909195899963379
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.8    |
|    critic_loss     | 0.0508   |
|    learning_rate   | 0.001    |
|    n_updates       | 414299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.6    |
|    critic_loss     | 0.0499   |
|    learning_rate   | 0.001    |
|    n_updates       | 414699   |
---------------------------------
=== Iterazione IRL 921 ===
Loss reward (iter 921): 6.878272533416748
=== Iterazione IRL 922 ===
Loss reward (iter 922): 6.88621711730957
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.3    |
|    critic_loss     | 0.0437   |
|    learning_rate   | 0.001    |
|    n_updates       | 415199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.5    |
|    critic_loss     | 0.05     |
|    learning_rate   | 0.001    |
|    n_updates       | 415599   |
---------------------------------
=== Iterazione IRL 923 ===
Loss reward (iter 923): 6.8940534591674805
=== Iterazione IRL 924 ===
Loss reward (iter 924): 6.915501117706299
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.2    |
|    critic_loss     | 0.0659   |
|    learning_rate   | 0.001    |
|    n_updates       | 416099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.8    |
|    critic_loss     | 0.051    |
|    learning_rate   | 0.001    |
|    n_updates       | 416499   |
---------------------------------
=== Iterazione IRL 925 ===
Loss reward (iter 925): 6.9194254875183105
=== Iterazione IRL 926 ===
Loss reward (iter 926): 6.945528507232666
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.6    |
|    critic_loss     | 0.0594   |
|    learning_rate   | 0.001    |
|    n_updates       | 416999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11.3    |
|    critic_loss     | 0.0469   |
|    learning_rate   | 0.001    |
|    n_updates       | 417399   |
---------------------------------
=== Iterazione IRL 927 ===
Loss reward (iter 927): 7.094522476196289
=== Iterazione IRL 928 ===
Loss reward (iter 928): 6.968044757843018
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.1    |
|    critic_loss     | 0.0443   |
|    learning_rate   | 0.001    |
|    n_updates       | 417899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11      |
|    critic_loss     | 0.0387   |
|    learning_rate   | 0.001    |
|    n_updates       | 418299   |
---------------------------------
=== Iterazione IRL 929 ===
Loss reward (iter 929): 7.129644870758057
=== Iterazione IRL 930 ===
Loss reward (iter 930): 6.9350481033325195
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -11.5    |
|    critic_loss     | 0.0413   |
|    learning_rate   | 0.001    |
|    n_updates       | 418799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -11      |
|    critic_loss     | 0.0465   |
|    learning_rate   | 0.001    |
|    n_updates       | 419199   |
---------------------------------
=== Iterazione IRL 931 ===
Loss reward (iter 931): 6.9618988037109375
=== Iterazione IRL 932 ===
Loss reward (iter 932): 6.962677001953125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.8    |
|    critic_loss     | 0.0386   |
|    learning_rate   | 0.001    |
|    n_updates       | 419699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.9    |
|    critic_loss     | 0.0265   |
|    learning_rate   | 0.001    |
|    n_updates       | 420099   |
---------------------------------
=== Iterazione IRL 933 ===
Loss reward (iter 933): 6.983206272125244
=== Iterazione IRL 934 ===
Loss reward (iter 934): 6.982117652893066
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.8    |
|    critic_loss     | 0.0387   |
|    learning_rate   | 0.001    |
|    n_updates       | 420599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.9    |
|    critic_loss     | 0.0347   |
|    learning_rate   | 0.001    |
|    n_updates       | 420999   |
---------------------------------
=== Iterazione IRL 935 ===
Loss reward (iter 935): 7.081184387207031
=== Iterazione IRL 936 ===
Loss reward (iter 936): 6.9999589920043945
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.7    |
|    critic_loss     | 0.0328   |
|    learning_rate   | 0.001    |
|    n_updates       | 421499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.9    |
|    critic_loss     | 0.0317   |
|    learning_rate   | 0.001    |
|    n_updates       | 421899   |
---------------------------------
=== Iterazione IRL 937 ===
Loss reward (iter 937): 6.984652519226074
=== Iterazione IRL 938 ===
Loss reward (iter 938): 7.037747383117676
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.9    |
|    critic_loss     | 0.0283   |
|    learning_rate   | 0.001    |
|    n_updates       | 422399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.7    |
|    critic_loss     | 0.0291   |
|    learning_rate   | 0.001    |
|    n_updates       | 422799   |
---------------------------------
=== Iterazione IRL 939 ===
Loss reward (iter 939): 6.988054275512695
=== Iterazione IRL 940 ===
Loss reward (iter 940): 6.978995323181152
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.6    |
|    critic_loss     | 0.0386   |
|    learning_rate   | 0.001    |
|    n_updates       | 423299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.6    |
|    critic_loss     | 0.0303   |
|    learning_rate   | 0.001    |
|    n_updates       | 423699   |
---------------------------------
=== Iterazione IRL 941 ===
Loss reward (iter 941): 6.970429420471191
=== Iterazione IRL 942 ===
Loss reward (iter 942): 6.970224380493164
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.7    |
|    critic_loss     | 0.0387   |
|    learning_rate   | 0.001    |
|    n_updates       | 424199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.2    |
|    critic_loss     | 0.0456   |
|    learning_rate   | 0.001    |
|    n_updates       | 424599   |
---------------------------------
=== Iterazione IRL 943 ===
Loss reward (iter 943): 6.973519325256348
=== Iterazione IRL 944 ===
Loss reward (iter 944): 6.963040828704834
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.4    |
|    critic_loss     | 0.0406   |
|    learning_rate   | 0.001    |
|    n_updates       | 425099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 217      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.4    |
|    critic_loss     | 0.032    |
|    learning_rate   | 0.001    |
|    n_updates       | 425499   |
---------------------------------
=== Iterazione IRL 945 ===
Loss reward (iter 945): 7.044482231140137
=== Iterazione IRL 946 ===
Loss reward (iter 946): 6.9311418533325195
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.4    |
|    critic_loss     | 0.035    |
|    learning_rate   | 0.001    |
|    n_updates       | 425999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.5    |
|    critic_loss     | 0.0426   |
|    learning_rate   | 0.001    |
|    n_updates       | 426399   |
---------------------------------
=== Iterazione IRL 947 ===
Loss reward (iter 947): 6.962281227111816
=== Iterazione IRL 948 ===
Loss reward (iter 948): 6.959784507751465
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.2    |
|    critic_loss     | 0.0453   |
|    learning_rate   | 0.001    |
|    n_updates       | 426899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -9.98    |
|    critic_loss     | 0.0368   |
|    learning_rate   | 0.001    |
|    n_updates       | 427299   |
---------------------------------
=== Iterazione IRL 949 ===
Loss reward (iter 949): 6.924948692321777
=== Iterazione IRL 950 ===
Loss reward (iter 950): 6.92988395690918
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.1    |
|    critic_loss     | 0.0348   |
|    learning_rate   | 0.001    |
|    n_updates       | 427799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 238      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.4    |
|    critic_loss     | 0.0377   |
|    learning_rate   | 0.001    |
|    n_updates       | 428199   |
---------------------------------
=== Iterazione IRL 951 ===
Loss reward (iter 951): 6.946879863739014
=== Iterazione IRL 952 ===
Loss reward (iter 952): 6.971726417541504
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 253      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.5    |
|    critic_loss     | 0.04     |
|    learning_rate   | 0.001    |
|    n_updates       | 428699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 214      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.3    |
|    critic_loss     | 0.0458   |
|    learning_rate   | 0.001    |
|    n_updates       | 429099   |
---------------------------------
=== Iterazione IRL 953 ===
Loss reward (iter 953): 6.939627170562744
=== Iterazione IRL 954 ===
Loss reward (iter 954): 6.94486141204834
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.3    |
|    critic_loss     | 0.0362   |
|    learning_rate   | 0.001    |
|    n_updates       | 429599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.3    |
|    critic_loss     | 0.0352   |
|    learning_rate   | 0.001    |
|    n_updates       | 429999   |
---------------------------------
=== Iterazione IRL 955 ===
Loss reward (iter 955): 6.9674530029296875
=== Iterazione IRL 956 ===
Loss reward (iter 956): 6.940008640289307
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.1    |
|    critic_loss     | 0.0393   |
|    learning_rate   | 0.001    |
|    n_updates       | 430499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.3    |
|    critic_loss     | 0.0337   |
|    learning_rate   | 0.001    |
|    n_updates       | 430899   |
---------------------------------
=== Iterazione IRL 957 ===
Loss reward (iter 957): 6.958677768707275
=== Iterazione IRL 958 ===
Loss reward (iter 958): 6.961764335632324
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -9.89    |
|    critic_loss     | 0.0323   |
|    learning_rate   | 0.001    |
|    n_updates       | 431399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 208      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -9.92    |
|    critic_loss     | 0.0324   |
|    learning_rate   | 0.001    |
|    n_updates       | 431799   |
---------------------------------
=== Iterazione IRL 959 ===
Loss reward (iter 959): 6.937965393066406
=== Iterazione IRL 960 ===
Loss reward (iter 960): 6.918704986572266
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.3    |
|    critic_loss     | 0.0478   |
|    learning_rate   | 0.001    |
|    n_updates       | 432299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10      |
|    critic_loss     | 0.05     |
|    learning_rate   | 0.001    |
|    n_updates       | 432699   |
---------------------------------
=== Iterazione IRL 961 ===
Loss reward (iter 961): 6.946963310241699
=== Iterazione IRL 962 ===
Loss reward (iter 962): 6.934601783752441
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.1    |
|    critic_loss     | 0.0412   |
|    learning_rate   | 0.001    |
|    n_updates       | 433199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.6    |
|    critic_loss     | 0.0326   |
|    learning_rate   | 0.001    |
|    n_updates       | 433599   |
---------------------------------
=== Iterazione IRL 963 ===
Loss reward (iter 963): 6.953792095184326
=== Iterazione IRL 964 ===
Loss reward (iter 964): 6.927977561950684
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.4    |
|    critic_loss     | 0.0303   |
|    learning_rate   | 0.001    |
|    n_updates       | 434099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10      |
|    critic_loss     | 0.0339   |
|    learning_rate   | 0.001    |
|    n_updates       | 434499   |
---------------------------------
=== Iterazione IRL 965 ===
Loss reward (iter 965): 6.93640661239624
=== Iterazione IRL 966 ===
Loss reward (iter 966): 6.9460601806640625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -9.91    |
|    critic_loss     | 0.0319   |
|    learning_rate   | 0.001    |
|    n_updates       | 434999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -9.96    |
|    critic_loss     | 0.0315   |
|    learning_rate   | 0.001    |
|    n_updates       | 435399   |
---------------------------------
=== Iterazione IRL 967 ===
Loss reward (iter 967): 6.938239574432373
=== Iterazione IRL 968 ===
Loss reward (iter 968): 6.933160781860352
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.1    |
|    critic_loss     | 0.0295   |
|    learning_rate   | 0.001    |
|    n_updates       | 435899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -9.79    |
|    critic_loss     | 0.0277   |
|    learning_rate   | 0.001    |
|    n_updates       | 436299   |
---------------------------------
=== Iterazione IRL 969 ===
Loss reward (iter 969): 6.949978351593018
=== Iterazione IRL 970 ===
Loss reward (iter 970): 6.920369625091553
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -9.96    |
|    critic_loss     | 0.0453   |
|    learning_rate   | 0.001    |
|    n_updates       | 436799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 207      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.2    |
|    critic_loss     | 0.0278   |
|    learning_rate   | 0.001    |
|    n_updates       | 437199   |
---------------------------------
=== Iterazione IRL 971 ===
Loss reward (iter 971): 6.915225028991699
=== Iterazione IRL 972 ===
Loss reward (iter 972): 6.933502197265625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 236      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.1    |
|    critic_loss     | 0.0471   |
|    learning_rate   | 0.001    |
|    n_updates       | 437699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 214      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -9.91    |
|    critic_loss     | 0.0427   |
|    learning_rate   | 0.001    |
|    n_updates       | 438099   |
---------------------------------
=== Iterazione IRL 973 ===
Loss reward (iter 973): 6.942716598510742
=== Iterazione IRL 974 ===
Loss reward (iter 974): 6.909210205078125
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 271      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.2    |
|    critic_loss     | 0.0522   |
|    learning_rate   | 0.001    |
|    n_updates       | 438599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.2    |
|    critic_loss     | 0.0348   |
|    learning_rate   | 0.001    |
|    n_updates       | 438999   |
---------------------------------
=== Iterazione IRL 975 ===
Loss reward (iter 975): 6.932059288024902
=== Iterazione IRL 976 ===
Loss reward (iter 976): 6.9104156494140625
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.1    |
|    critic_loss     | 0.0314   |
|    learning_rate   | 0.001    |
|    n_updates       | 439499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.1    |
|    critic_loss     | 0.0534   |
|    learning_rate   | 0.001    |
|    n_updates       | 439899   |
---------------------------------
=== Iterazione IRL 977 ===
Loss reward (iter 977): 6.922163009643555
=== Iterazione IRL 978 ===
Loss reward (iter 978): 6.909524440765381
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 270      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -9.9     |
|    critic_loss     | 0.037    |
|    learning_rate   | 0.001    |
|    n_updates       | 440399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 237      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.1    |
|    critic_loss     | 0.0339   |
|    learning_rate   | 0.001    |
|    n_updates       | 440799   |
---------------------------------
=== Iterazione IRL 979 ===
Loss reward (iter 979): 6.921681880950928
=== Iterazione IRL 980 ===
Loss reward (iter 980): 6.9055280685424805
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -9.9     |
|    critic_loss     | 0.0403   |
|    learning_rate   | 0.001    |
|    n_updates       | 441299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.1    |
|    critic_loss     | 0.0487   |
|    learning_rate   | 0.001    |
|    n_updates       | 441699   |
---------------------------------
=== Iterazione IRL 981 ===
Loss reward (iter 981): 6.874725341796875
=== Iterazione IRL 982 ===
Loss reward (iter 982): 6.903885841369629
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.2    |
|    critic_loss     | 0.0441   |
|    learning_rate   | 0.001    |
|    n_updates       | 442199   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -9.9     |
|    critic_loss     | 0.0463   |
|    learning_rate   | 0.001    |
|    n_updates       | 442599   |
---------------------------------
=== Iterazione IRL 983 ===
Loss reward (iter 983): 6.933884143829346
=== Iterazione IRL 984 ===
Loss reward (iter 984): 6.905836582183838
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10      |
|    critic_loss     | 0.0442   |
|    learning_rate   | 0.001    |
|    n_updates       | 443099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -9.98    |
|    critic_loss     | 0.0365   |
|    learning_rate   | 0.001    |
|    n_updates       | 443499   |
---------------------------------
=== Iterazione IRL 985 ===
Loss reward (iter 985): 6.887212753295898
=== Iterazione IRL 986 ===
Loss reward (iter 986): 7.08912467956543
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 265      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -9.68    |
|    critic_loss     | 0.0367   |
|    learning_rate   | 0.001    |
|    n_updates       | 443999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 234      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.1    |
|    critic_loss     | 0.0402   |
|    learning_rate   | 0.001    |
|    n_updates       | 444399   |
---------------------------------
=== Iterazione IRL 987 ===
Loss reward (iter 987): 6.932088851928711
=== Iterazione IRL 988 ===
Loss reward (iter 988): 6.887397289276123
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 268      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.1    |
|    critic_loss     | 0.0436   |
|    learning_rate   | 0.001    |
|    n_updates       | 444899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -9.93    |
|    critic_loss     | 0.0464   |
|    learning_rate   | 0.001    |
|    n_updates       | 445299   |
---------------------------------
=== Iterazione IRL 989 ===
Loss reward (iter 989): 6.911821365356445
=== Iterazione IRL 990 ===
Loss reward (iter 990): 6.917260646820068
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10      |
|    critic_loss     | 0.0518   |
|    learning_rate   | 0.001    |
|    n_updates       | 445799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -9.91    |
|    critic_loss     | 0.0414   |
|    learning_rate   | 0.001    |
|    n_updates       | 446199   |
---------------------------------
=== Iterazione IRL 991 ===
Loss reward (iter 991): 6.920172691345215
=== Iterazione IRL 992 ===
Loss reward (iter 992): 6.930891990661621
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.1    |
|    critic_loss     | 0.0445   |
|    learning_rate   | 0.001    |
|    n_updates       | 446699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -9.73    |
|    critic_loss     | 0.0396   |
|    learning_rate   | 0.001    |
|    n_updates       | 447099   |
---------------------------------
=== Iterazione IRL 993 ===
Loss reward (iter 993): 6.871304512023926
=== Iterazione IRL 994 ===
Loss reward (iter 994): 6.891534805297852
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.1    |
|    critic_loss     | 0.0441   |
|    learning_rate   | 0.001    |
|    n_updates       | 447599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -9.84    |
|    critic_loss     | 0.0377   |
|    learning_rate   | 0.001    |
|    n_updates       | 447999   |
---------------------------------
=== Iterazione IRL 995 ===
Loss reward (iter 995): 6.901627540588379
=== Iterazione IRL 996 ===
Loss reward (iter 996): 6.908145904541016
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -10.1    |
|    critic_loss     | 0.0471   |
|    learning_rate   | 0.001    |
|    n_updates       | 448499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -9.96    |
|    critic_loss     | 0.0407   |
|    learning_rate   | 0.001    |
|    n_updates       | 448899   |
---------------------------------
=== Iterazione IRL 997 ===
Loss reward (iter 997): 6.906322479248047
=== Iterazione IRL 998 ===
Loss reward (iter 998): 6.852041244506836
>>> Aggiorno la policy con TD3
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 269      |
|    time_elapsed    | 1        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -9.98    |
|    critic_loss     | 0.0384   |
|    learning_rate   | 0.001    |
|    n_updates       | 449399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 236      |
|    time_elapsed    | 3        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -10.2    |
|    critic_loss     | 0.0339   |
|    learning_rate   | 0.001    |
|    n_updates       | 449799   |
---------------------------------
=== Iterazione IRL 999 ===
Loss reward (iter 999): 6.868139743804932
Modello TD3 salvato.
