Inizio training IRL
Using device: cuda
Using cuda device
=== Iterazione IRL 0 ===
Loss reward (iter 0): 6.82220983505249
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 146      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -4       |
|    critic_loss     | 0.0758   |
|    ent_coef        | 0.916    |
|    ent_coef_loss   | -0.134   |
|    learning_rate   | 0.0003   |
|    n_updates       | 299      |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 129      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -6.58    |
|    critic_loss     | 0.0416   |
|    ent_coef        | 0.815    |
|    ent_coef_loss   | -0.32    |
|    learning_rate   | 0.0003   |
|    n_updates       | 699      |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 124      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -9.05    |
|    critic_loss     | 0.059    |
|    ent_coef        | 0.724    |
|    ent_coef_loss   | -0.48    |
|    learning_rate   | 0.0003   |
|    n_updates       | 1099     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 16       |
|    fps             | 121      |
|    time_elapsed    | 13       |
|    total_timesteps | 1600     |
| train/             |          |
|    actor_loss      | -11.6    |
|    critic_loss     | 0.0685   |
|    ent_coef        | 0.645    |
|    ent_coef_loss   | -0.657   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1499     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 20       |
|    fps             | 120      |
|    time_elapsed    | 16       |
|    total_timesteps | 2000     |
| train/             |          |
|    actor_loss      | -13.8    |
|    critic_loss     | 0.0701   |
|    ent_coef        | 0.575    |
|    ent_coef_loss   | -0.755   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1899     |
---------------------------------
=== Iterazione IRL 1 ===
Loss reward (iter 1): 6.5140767097473145
=== Iterazione IRL 2 ===
Loss reward (iter 2): 5.226656436920166
=== Iterazione IRL 3 ===
Loss reward (iter 3): 3.7178781032562256
=== Iterazione IRL 4 ===
Loss reward (iter 4): 1.5208492279052734
=== Iterazione IRL 5 ===
Loss reward (iter 5): -1.8175209760665894
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -20.7    |
|    critic_loss     | 7.87     |
|    ent_coef        | 0.554    |
|    ent_coef_loss   | 0.0453   |
|    learning_rate   | 0.0003   |
|    n_updates       | 2199     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 130      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -37.8    |
|    critic_loss     | 5.62     |
|    ent_coef        | 0.574    |
|    ent_coef_loss   | 0.38     |
|    learning_rate   | 0.0003   |
|    n_updates       | 2599     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -61.4    |
|    critic_loss     | 6.1      |
|    ent_coef        | 0.632    |
|    ent_coef_loss   | 0.41     |
|    learning_rate   | 0.0003   |
|    n_updates       | 2999     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 16       |
|    fps             | 122      |
|    time_elapsed    | 13       |
|    total_timesteps | 1600     |
| train/             |          |
|    actor_loss      | -91.5    |
|    critic_loss     | 5.87     |
|    ent_coef        | 0.712    |
|    ent_coef_loss   | 0.342    |
|    learning_rate   | 0.0003   |
|    n_updates       | 3399     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 20       |
|    fps             | 121      |
|    time_elapsed    | 16       |
|    total_timesteps | 2000     |
| train/             |          |
|    actor_loss      | -105     |
|    critic_loss     | 4.77     |
|    ent_coef        | 0.804    |
|    ent_coef_loss   | 0.22     |
|    learning_rate   | 0.0003   |
|    n_updates       | 3799     |
---------------------------------
=== Iterazione IRL 6 ===
Loss reward (iter 6): 19.312379837036133
=== Iterazione IRL 7 ===
Loss reward (iter 7): 13.468721389770508
=== Iterazione IRL 8 ===
Loss reward (iter 8): 9.5060453414917
=== Iterazione IRL 9 ===
Loss reward (iter 9): 6.899318695068359
=== Iterazione IRL 10 ===
Loss reward (iter 10): 4.784428596496582
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -129     |
|    critic_loss     | 27.1     |
|    ent_coef        | 0.88     |
|    ent_coef_loss   | 0.112    |
|    learning_rate   | 0.0003   |
|    n_updates       | 4099     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 130      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -148     |
|    critic_loss     | 39       |
|    ent_coef        | 0.993    |
|    ent_coef_loss   | 0.00702  |
|    learning_rate   | 0.0003   |
|    n_updates       | 4499     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -165     |
|    critic_loss     | 43.9     |
|    ent_coef        | 1.11     |
|    ent_coef_loss   | -0.121   |
|    learning_rate   | 0.0003   |
|    n_updates       | 4899     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 16       |
|    fps             | 122      |
|    time_elapsed    | 13       |
|    total_timesteps | 1600     |
| train/             |          |
|    actor_loss      | -176     |
|    critic_loss     | 53.2     |
|    ent_coef        | 1.23     |
|    ent_coef_loss   | -0.137   |
|    learning_rate   | 0.0003   |
|    n_updates       | 5299     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 20       |
|    fps             | 121      |
|    time_elapsed    | 16       |
|    total_timesteps | 2000     |
| train/             |          |
|    actor_loss      | -189     |
|    critic_loss     | 55.3     |
|    ent_coef        | 1.34     |
|    ent_coef_loss   | -0.125   |
|    learning_rate   | 0.0003   |
|    n_updates       | 5699     |
---------------------------------
=== Iterazione IRL 11 ===
Loss reward (iter 11): 3.054076910018921
=== Iterazione IRL 12 ===
Loss reward (iter 12): 1.3646684885025024
=== Iterazione IRL 13 ===
Loss reward (iter 13): -0.6949608325958252
=== Iterazione IRL 14 ===
Loss reward (iter 14): -3.2397501468658447
=== Iterazione IRL 15 ===
Loss reward (iter 15): -6.3923821449279785
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -207     |
|    critic_loss     | 51.5     |
|    ent_coef        | 1.39     |
|    ent_coef_loss   | -0.177   |
|    learning_rate   | 0.0003   |
|    n_updates       | 5999     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 130      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -234     |
|    critic_loss     | 60.6     |
|    ent_coef        | 1.5      |
|    ent_coef_loss   | -0.123   |
|    learning_rate   | 0.0003   |
|    n_updates       | 6399     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -259     |
|    critic_loss     | 56.8     |
|    ent_coef        | 1.6      |
|    ent_coef_loss   | -0.0854  |
|    learning_rate   | 0.0003   |
|    n_updates       | 6799     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 16       |
|    fps             | 122      |
|    time_elapsed    | 13       |
|    total_timesteps | 1600     |
| train/             |          |
|    actor_loss      | -276     |
|    critic_loss     | 47.5     |
|    ent_coef        | 1.66     |
|    ent_coef_loss   | -0.029   |
|    learning_rate   | 0.0003   |
|    n_updates       | 7199     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 20       |
|    fps             | 121      |
|    time_elapsed    | 16       |
|    total_timesteps | 2000     |
| train/             |          |
|    actor_loss      | -280     |
|    critic_loss     | 47.8     |
|    ent_coef        | 1.69     |
|    ent_coef_loss   | 0.0134   |
|    learning_rate   | 0.0003   |
|    n_updates       | 7599     |
---------------------------------
=== Iterazione IRL 16 ===
Loss reward (iter 16): 8.115060806274414
=== Iterazione IRL 17 ===
Loss reward (iter 17): 7.259902000427246
=== Iterazione IRL 18 ===
Loss reward (iter 18): 7.175599098205566
=== Iterazione IRL 19 ===
Loss reward (iter 19): 7.167959690093994
=== Iterazione IRL 20 ===
Loss reward (iter 20): 6.841315746307373
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -299     |
|    critic_loss     | 49.2     |
|    ent_coef        | 1.68     |
|    ent_coef_loss   | 0.0579   |
|    learning_rate   | 0.0003   |
|    n_updates       | 7899     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 130      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -308     |
|    critic_loss     | 51.4     |
|    ent_coef        | 1.62     |
|    ent_coef_loss   | 0.124    |
|    learning_rate   | 0.0003   |
|    n_updates       | 8299     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -333     |
|    critic_loss     | 49.8     |
|    ent_coef        | 1.52     |
|    ent_coef_loss   | 0.0661   |
|    learning_rate   | 0.0003   |
|    n_updates       | 8699     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 16       |
|    fps             | 122      |
|    time_elapsed    | 13       |
|    total_timesteps | 1600     |
| train/             |          |
|    actor_loss      | -340     |
|    critic_loss     | 44.1     |
|    ent_coef        | 1.47     |
|    ent_coef_loss   | 0.0857   |
|    learning_rate   | 0.0003   |
|    n_updates       | 9099     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 20       |
|    fps             | 121      |
|    time_elapsed    | 16       |
|    total_timesteps | 2000     |
| train/             |          |
|    actor_loss      | -334     |
|    critic_loss     | 47.2     |
|    ent_coef        | 1.45     |
|    ent_coef_loss   | 0.0722   |
|    learning_rate   | 0.0003   |
|    n_updates       | 9499     |
---------------------------------
=== Iterazione IRL 21 ===
Loss reward (iter 21): 5.803808689117432
=== Iterazione IRL 22 ===
Loss reward (iter 22): 5.309302806854248
=== Iterazione IRL 23 ===
Loss reward (iter 23): 4.264158725738525
=== Iterazione IRL 24 ===
Loss reward (iter 24): 3.5305023193359375
=== Iterazione IRL 25 ===
Loss reward (iter 25): 1.7983158826828003
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -357     |
|    critic_loss     | 41.9     |
|    ent_coef        | 1.46     |
|    ent_coef_loss   | -0.0589  |
|    learning_rate   | 0.0003   |
|    n_updates       | 9799     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 130      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -367     |
|    critic_loss     | 47.2     |
|    ent_coef        | 1.47     |
|    ent_coef_loss   | -0.0238  |
|    learning_rate   | 0.0003   |
|    n_updates       | 10199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -380     |
|    critic_loss     | 45.2     |
|    ent_coef        | 1.48     |
|    ent_coef_loss   | 0.0849   |
|    learning_rate   | 0.0003   |
|    n_updates       | 10599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 16       |
|    fps             | 122      |
|    time_elapsed    | 13       |
|    total_timesteps | 1600     |
| train/             |          |
|    actor_loss      | -389     |
|    critic_loss     | 45.5     |
|    ent_coef        | 1.49     |
|    ent_coef_loss   | -0.0198  |
|    learning_rate   | 0.0003   |
|    n_updates       | 10999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 20       |
|    fps             | 121      |
|    time_elapsed    | 16       |
|    total_timesteps | 2000     |
| train/             |          |
|    actor_loss      | -391     |
|    critic_loss     | 34.4     |
|    ent_coef        | 1.47     |
|    ent_coef_loss   | -0.0575  |
|    learning_rate   | 0.0003   |
|    n_updates       | 11399    |
---------------------------------
=== Iterazione IRL 26 ===
Loss reward (iter 26): -5.6593098640441895
=== Iterazione IRL 27 ===
Loss reward (iter 27): -9.608046531677246
=== Iterazione IRL 28 ===
Loss reward (iter 28): -15.650788307189941
=== Iterazione IRL 29 ===
Loss reward (iter 29): -23.363880157470703
=== Iterazione IRL 30 ===
Loss reward (iter 30): -30.123640060424805
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -406     |
|    critic_loss     | 64.3     |
|    ent_coef        | 1.4      |
|    ent_coef_loss   | 0.0704   |
|    learning_rate   | 0.0003   |
|    n_updates       | 11699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 130      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -402     |
|    critic_loss     | 63.4     |
|    ent_coef        | 1.25     |
|    ent_coef_loss   | 0.0266   |
|    learning_rate   | 0.0003   |
|    n_updates       | 12099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -426     |
|    critic_loss     | 79.5     |
|    ent_coef        | 1.14     |
|    ent_coef_loss   | 0.0278   |
|    learning_rate   | 0.0003   |
|    n_updates       | 12499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 16       |
|    fps             | 122      |
|    time_elapsed    | 13       |
|    total_timesteps | 1600     |
| train/             |          |
|    actor_loss      | -438     |
|    critic_loss     | 120      |
|    ent_coef        | 1.13     |
|    ent_coef_loss   | 0.0061   |
|    learning_rate   | 0.0003   |
|    n_updates       | 12899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 20       |
|    fps             | 121      |
|    time_elapsed    | 16       |
|    total_timesteps | 2000     |
| train/             |          |
|    actor_loss      | -425     |
|    critic_loss     | 114      |
|    ent_coef        | 1.13     |
|    ent_coef_loss   | -0.00547 |
|    learning_rate   | 0.0003   |
|    n_updates       | 13299    |
---------------------------------
=== Iterazione IRL 31 ===
Loss reward (iter 31): 18.598709106445312
=== Iterazione IRL 32 ===
Loss reward (iter 32): 7.418176651000977
=== Iterazione IRL 33 ===
Loss reward (iter 33): 7.230225086212158
=== Iterazione IRL 34 ===
Loss reward (iter 34): 7.319379806518555
=== Iterazione IRL 35 ===
Loss reward (iter 35): 5.168328285217285
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -407     |
|    critic_loss     | 99       |
|    ent_coef        | 1.11     |
|    ent_coef_loss   | 0.0279   |
|    learning_rate   | 0.0003   |
|    n_updates       | 13599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 130      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -465     |
|    critic_loss     | 116      |
|    ent_coef        | 1.13     |
|    ent_coef_loss   | -0.0189  |
|    learning_rate   | 0.0003   |
|    n_updates       | 13999    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -474     |
|    critic_loss     | 133      |
|    ent_coef        | 1.28     |
|    ent_coef_loss   | -0.0276  |
|    learning_rate   | 0.0003   |
|    n_updates       | 14399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 16       |
|    fps             | 122      |
|    time_elapsed    | 13       |
|    total_timesteps | 1600     |
| train/             |          |
|    actor_loss      | -512     |
|    critic_loss     | 123      |
|    ent_coef        | 1.42     |
|    ent_coef_loss   | -0.0551  |
|    learning_rate   | 0.0003   |
|    n_updates       | 14799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 20       |
|    fps             | 121      |
|    time_elapsed    | 16       |
|    total_timesteps | 2000     |
| train/             |          |
|    actor_loss      | -524     |
|    critic_loss     | 133      |
|    ent_coef        | 1.57     |
|    ent_coef_loss   | -0.0624  |
|    learning_rate   | 0.0003   |
|    n_updates       | 15199    |
---------------------------------
=== Iterazione IRL 36 ===
Loss reward (iter 36): 12.4401273727417
=== Iterazione IRL 37 ===
Loss reward (iter 37): 12.20052433013916
=== Iterazione IRL 38 ===
Loss reward (iter 38): 11.581557273864746
=== Iterazione IRL 39 ===
Loss reward (iter 39): 10.71997356414795
=== Iterazione IRL 40 ===
Loss reward (iter 40): 10.348021507263184
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -545     |
|    critic_loss     | 133      |
|    ent_coef        | 1.68     |
|    ent_coef_loss   | -0.035   |
|    learning_rate   | 0.0003   |
|    n_updates       | 15499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 130      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -599     |
|    critic_loss     | 150      |
|    ent_coef        | 1.83     |
|    ent_coef_loss   | -0.03    |
|    learning_rate   | 0.0003   |
|    n_updates       | 15899    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -598     |
|    critic_loss     | 143      |
|    ent_coef        | 1.96     |
|    ent_coef_loss   | -0.12    |
|    learning_rate   | 0.0003   |
|    n_updates       | 16299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 16       |
|    fps             | 122      |
|    time_elapsed    | 13       |
|    total_timesteps | 1600     |
| train/             |          |
|    actor_loss      | -602     |
|    critic_loss     | 141      |
|    ent_coef        | 2.06     |
|    ent_coef_loss   | -0.0696  |
|    learning_rate   | 0.0003   |
|    n_updates       | 16699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 20       |
|    fps             | 121      |
|    time_elapsed    | 16       |
|    total_timesteps | 2000     |
| train/             |          |
|    actor_loss      | -660     |
|    critic_loss     | 116      |
|    ent_coef        | 2.14     |
|    ent_coef_loss   | -0.0928  |
|    learning_rate   | 0.0003   |
|    n_updates       | 17099    |
---------------------------------
=== Iterazione IRL 41 ===
Loss reward (iter 41): 10.472188949584961
=== Iterazione IRL 42 ===
Loss reward (iter 42): 9.988082885742188
=== Iterazione IRL 43 ===
Loss reward (iter 43): 9.3501615524292
=== Iterazione IRL 44 ===
Loss reward (iter 44): 9.0165433883667
=== Iterazione IRL 45 ===
Loss reward (iter 45): 8.99083137512207
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 150      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -667     |
|    critic_loss     | 123      |
|    ent_coef        | 2.19     |
|    ent_coef_loss   | -0.054   |
|    learning_rate   | 0.0003   |
|    n_updates       | 17399    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 130      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -706     |
|    critic_loss     | 130      |
|    ent_coef        | 2.27     |
|    ent_coef_loss   | -0.0876  |
|    learning_rate   | 0.0003   |
|    n_updates       | 17799    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 125      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -726     |
|    critic_loss     | 123      |
|    ent_coef        | 2.31     |
|    ent_coef_loss   | -0.0748  |
|    learning_rate   | 0.0003   |
|    n_updates       | 18199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 16       |
|    fps             | 122      |
|    time_elapsed    | 13       |
|    total_timesteps | 1600     |
| train/             |          |
|    actor_loss      | -751     |
|    critic_loss     | 117      |
|    ent_coef        | 2.39     |
|    ent_coef_loss   | -0.0927  |
|    learning_rate   | 0.0003   |
|    n_updates       | 18599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 20       |
|    fps             | 121      |
|    time_elapsed    | 16       |
|    total_timesteps | 2000     |
| train/             |          |
|    actor_loss      | -776     |
|    critic_loss     | 120      |
|    ent_coef        | 2.44     |
|    ent_coef_loss   | -0.0219  |
|    learning_rate   | 0.0003   |
|    n_updates       | 18999    |
---------------------------------
=== Iterazione IRL 46 ===
Loss reward (iter 46): 8.493188858032227
=== Iterazione IRL 47 ===
Loss reward (iter 47): 8.061601638793945
=== Iterazione IRL 48 ===
Loss reward (iter 48): 7.589735984802246
=== Iterazione IRL 49 ===
Loss reward (iter 49): 7.754415512084961
=== Iterazione IRL 50 ===
Loss reward (iter 50): 7.45194149017334
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -796     |
|    critic_loss     | 109      |
|    ent_coef        | 2.47     |
|    ent_coef_loss   | 0.00469  |
|    learning_rate   | 0.0003   |
|    n_updates       | 19299    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -824     |
|    critic_loss     | 101      |
|    ent_coef        | 2.52     |
|    ent_coef_loss   | 0.0611   |
|    learning_rate   | 0.0003   |
|    n_updates       | 19699    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -854     |
|    critic_loss     | 135      |
|    ent_coef        | 2.58     |
|    ent_coef_loss   | -0.0889  |
|    learning_rate   | 0.0003   |
|    n_updates       | 20099    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 16       |
|    fps             | 123      |
|    time_elapsed    | 12       |
|    total_timesteps | 1600     |
| train/             |          |
|    actor_loss      | -889     |
|    critic_loss     | 133      |
|    ent_coef        | 2.58     |
|    ent_coef_loss   | -0.0333  |
|    learning_rate   | 0.0003   |
|    n_updates       | 20499    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 20       |
|    fps             | 121      |
|    time_elapsed    | 16       |
|    total_timesteps | 2000     |
| train/             |          |
|    actor_loss      | -913     |
|    critic_loss     | 93.6     |
|    ent_coef        | 2.58     |
|    ent_coef_loss   | -0.0238  |
|    learning_rate   | 0.0003   |
|    n_updates       | 20899    |
---------------------------------
=== Iterazione IRL 51 ===
Loss reward (iter 51): 7.246230125427246
=== Iterazione IRL 52 ===
Loss reward (iter 52): 6.880249500274658
=== Iterazione IRL 53 ===
Loss reward (iter 53): 6.703802108764648
=== Iterazione IRL 54 ===
Loss reward (iter 54): 6.687568187713623
=== Iterazione IRL 55 ===
Loss reward (iter 55): 6.523094177246094
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -936     |
|    critic_loss     | 115      |
|    ent_coef        | 2.64     |
|    ent_coef_loss   | -0.0223  |
|    learning_rate   | 0.0003   |
|    n_updates       | 21199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -963     |
|    critic_loss     | 84.8     |
|    ent_coef        | 2.68     |
|    ent_coef_loss   | -0.0502  |
|    learning_rate   | 0.0003   |
|    n_updates       | 21599    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -984     |
|    critic_loss     | 125      |
|    ent_coef        | 2.72     |
|    ent_coef_loss   | 0.05     |
|    learning_rate   | 0.0003   |
|    n_updates       | 21999    |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 123       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.02e+03 |
|    critic_loss     | 101       |
|    ent_coef        | 2.75      |
|    ent_coef_loss   | 0.0895    |
|    learning_rate   | 0.0003    |
|    n_updates       | 22399     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 121       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.06e+03 |
|    critic_loss     | 96        |
|    ent_coef        | 2.71      |
|    ent_coef_loss   | -0.051    |
|    learning_rate   | 0.0003    |
|    n_updates       | 22799     |
----------------------------------
=== Iterazione IRL 56 ===
Loss reward (iter 56): 6.093326568603516
=== Iterazione IRL 57 ===
Loss reward (iter 57): 5.803735256195068
=== Iterazione IRL 58 ===
Loss reward (iter 58): 5.997696399688721
=== Iterazione IRL 59 ===
Loss reward (iter 59): 5.599252223968506
=== Iterazione IRL 60 ===
Loss reward (iter 60): 5.605158805847168
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.08e+03 |
|    critic_loss     | 107       |
|    ent_coef        | 2.69      |
|    ent_coef_loss   | -0.129    |
|    learning_rate   | 0.0003    |
|    n_updates       | 23099     |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -1.1e+03 |
|    critic_loss     | 121      |
|    ent_coef        | 2.7      |
|    ent_coef_loss   | 0.0117   |
|    learning_rate   | 0.0003   |
|    n_updates       | 23499    |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.14e+03 |
|    critic_loss     | 94.7      |
|    ent_coef        | 2.68      |
|    ent_coef_loss   | 0.0883    |
|    learning_rate   | 0.0003    |
|    n_updates       | 23899     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 123       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.15e+03 |
|    critic_loss     | 105       |
|    ent_coef        | 2.67      |
|    ent_coef_loss   | 0.156     |
|    learning_rate   | 0.0003    |
|    n_updates       | 24299     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 121       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.22e+03 |
|    critic_loss     | 131       |
|    ent_coef        | 2.63      |
|    ent_coef_loss   | 0.0206    |
|    learning_rate   | 0.0003    |
|    n_updates       | 24699     |
----------------------------------
=== Iterazione IRL 61 ===
Loss reward (iter 61): 5.508192539215088
=== Iterazione IRL 62 ===
Loss reward (iter 62): 5.420088768005371
=== Iterazione IRL 63 ===
Loss reward (iter 63): 5.239238262176514
=== Iterazione IRL 64 ===
Loss reward (iter 64): 4.715562343597412
=== Iterazione IRL 65 ===
Loss reward (iter 65): 4.969023704528809
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -1.2e+03 |
|    critic_loss     | 112      |
|    ent_coef        | 2.59     |
|    ent_coef_loss   | 0.00996  |
|    learning_rate   | 0.0003   |
|    n_updates       | 24999    |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.21e+03 |
|    critic_loss     | 106       |
|    ent_coef        | 2.54      |
|    ent_coef_loss   | 0.0278    |
|    learning_rate   | 0.0003    |
|    n_updates       | 25399     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.25e+03 |
|    critic_loss     | 79.5      |
|    ent_coef        | 2.51      |
|    ent_coef_loss   | -0.0227   |
|    learning_rate   | 0.0003    |
|    n_updates       | 25799     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 123       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.29e+03 |
|    critic_loss     | 88.1      |
|    ent_coef        | 2.49      |
|    ent_coef_loss   | 0.0393    |
|    learning_rate   | 0.0003    |
|    n_updates       | 26199     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.33e+03 |
|    critic_loss     | 101       |
|    ent_coef        | 2.47      |
|    ent_coef_loss   | 0.0304    |
|    learning_rate   | 0.0003    |
|    n_updates       | 26599     |
----------------------------------
=== Iterazione IRL 66 ===
Loss reward (iter 66): 4.863456726074219
=== Iterazione IRL 67 ===
Loss reward (iter 67): 4.155263423919678
=== Iterazione IRL 68 ===
Loss reward (iter 68): 3.782623291015625
=== Iterazione IRL 69 ===
Loss reward (iter 69): 4.072310924530029
=== Iterazione IRL 70 ===
Loss reward (iter 70): 3.4933855533599854
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.33e+03 |
|    critic_loss     | 114       |
|    ent_coef        | 2.44      |
|    ent_coef_loss   | -0.0403   |
|    learning_rate   | 0.0003    |
|    n_updates       | 26899     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.35e+03 |
|    critic_loss     | 95.1      |
|    ent_coef        | 2.43      |
|    ent_coef_loss   | 0.0815    |
|    learning_rate   | 0.0003    |
|    n_updates       | 27299     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.35e+03 |
|    critic_loss     | 88.9      |
|    ent_coef        | 2.37      |
|    ent_coef_loss   | -0.0267   |
|    learning_rate   | 0.0003    |
|    n_updates       | 27699     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 123       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.38e+03 |
|    critic_loss     | 127       |
|    ent_coef        | 2.34      |
|    ent_coef_loss   | 0.00212   |
|    learning_rate   | 0.0003    |
|    n_updates       | 28099     |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 20       |
|    fps             | 122      |
|    time_elapsed    | 16       |
|    total_timesteps | 2000     |
| train/             |          |
|    actor_loss      | -1.4e+03 |
|    critic_loss     | 113      |
|    ent_coef        | 2.3      |
|    ent_coef_loss   | 0.0586   |
|    learning_rate   | 0.0003   |
|    n_updates       | 28499    |
---------------------------------
=== Iterazione IRL 71 ===
Loss reward (iter 71): 3.917694330215454
=== Iterazione IRL 72 ===
Loss reward (iter 72): 2.8332083225250244
=== Iterazione IRL 73 ===
Loss reward (iter 73): 2.7455296516418457
=== Iterazione IRL 74 ===
Loss reward (iter 74): 1.8780840635299683
=== Iterazione IRL 75 ===
Loss reward (iter 75): 1.3433268070220947
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.41e+03 |
|    critic_loss     | 96.6      |
|    ent_coef        | 2.28      |
|    ent_coef_loss   | -0.0068   |
|    learning_rate   | 0.0003    |
|    n_updates       | 28799     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.45e+03 |
|    critic_loss     | 110       |
|    ent_coef        | 2.26      |
|    ent_coef_loss   | 0.0852    |
|    learning_rate   | 0.0003    |
|    n_updates       | 29199     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.44e+03 |
|    critic_loss     | 112       |
|    ent_coef        | 2.21      |
|    ent_coef_loss   | -0.014    |
|    learning_rate   | 0.0003    |
|    n_updates       | 29599     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 123       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.49e+03 |
|    critic_loss     | 114       |
|    ent_coef        | 2.19      |
|    ent_coef_loss   | -0.026    |
|    learning_rate   | 0.0003    |
|    n_updates       | 29999     |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 20       |
|    fps             | 122      |
|    time_elapsed    | 16       |
|    total_timesteps | 2000     |
| train/             |          |
|    actor_loss      | -1.5e+03 |
|    critic_loss     | 97.4     |
|    ent_coef        | 2.16     |
|    ent_coef_loss   | 0.00858  |
|    learning_rate   | 0.0003   |
|    n_updates       | 30399    |
---------------------------------
=== Iterazione IRL 76 ===
Loss reward (iter 76): 2.111179828643799
=== Iterazione IRL 77 ===
Loss reward (iter 77): 2.034109592437744
=== Iterazione IRL 78 ===
Loss reward (iter 78): 1.252005934715271
=== Iterazione IRL 79 ===
Loss reward (iter 79): 0.6093562841415405
=== Iterazione IRL 80 ===
Loss reward (iter 80): 0.7377012968063354
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.52e+03 |
|    critic_loss     | 117       |
|    ent_coef        | 2.11      |
|    ent_coef_loss   | 0.0943    |
|    learning_rate   | 0.0003    |
|    n_updates       | 30699     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.53e+03 |
|    critic_loss     | 86.8      |
|    ent_coef        | 2.05      |
|    ent_coef_loss   | -0.0526   |
|    learning_rate   | 0.0003    |
|    n_updates       | 31099     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.54e+03 |
|    critic_loss     | 87.6      |
|    ent_coef        | 2.01      |
|    ent_coef_loss   | -0.0798   |
|    learning_rate   | 0.0003    |
|    n_updates       | 31499     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 123       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.57e+03 |
|    critic_loss     | 117       |
|    ent_coef        | 2         |
|    ent_coef_loss   | 0.011     |
|    learning_rate   | 0.0003    |
|    n_updates       | 31899     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.58e+03 |
|    critic_loss     | 99.3      |
|    ent_coef        | 1.95      |
|    ent_coef_loss   | -0.00578  |
|    learning_rate   | 0.0003    |
|    n_updates       | 32299     |
----------------------------------
=== Iterazione IRL 81 ===
Loss reward (iter 81): -0.15209746360778809
=== Iterazione IRL 82 ===
Loss reward (iter 82): 0.0815536379814148
=== Iterazione IRL 83 ===
Loss reward (iter 83): 0.2608696222305298
=== Iterazione IRL 84 ===
Loss reward (iter 84): -1.2969437837600708
=== Iterazione IRL 85 ===
Loss reward (iter 85): -0.9728004932403564
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.59e+03 |
|    critic_loss     | 116       |
|    ent_coef        | 1.93      |
|    ent_coef_loss   | 0.0462    |
|    learning_rate   | 0.0003    |
|    n_updates       | 32599     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.61e+03 |
|    critic_loss     | 110       |
|    ent_coef        | 1.87      |
|    ent_coef_loss   | 0.0355    |
|    learning_rate   | 0.0003    |
|    n_updates       | 32999     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.62e+03 |
|    critic_loss     | 115       |
|    ent_coef        | 1.82      |
|    ent_coef_loss   | -0.0134   |
|    learning_rate   | 0.0003    |
|    n_updates       | 33399     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 123       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.64e+03 |
|    critic_loss     | 84.4      |
|    ent_coef        | 1.79      |
|    ent_coef_loss   | 0.00256   |
|    learning_rate   | 0.0003    |
|    n_updates       | 33799     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.66e+03 |
|    critic_loss     | 90.4      |
|    ent_coef        | 1.77      |
|    ent_coef_loss   | -0.00373  |
|    learning_rate   | 0.0003    |
|    n_updates       | 34199     |
----------------------------------
=== Iterazione IRL 86 ===
Loss reward (iter 86): -2.395296573638916
=== Iterazione IRL 87 ===
Loss reward (iter 87): -2.8481626510620117
=== Iterazione IRL 88 ===
Loss reward (iter 88): -3.9530675411224365
=== Iterazione IRL 89 ===
Loss reward (iter 89): -5.093029022216797
=== Iterazione IRL 90 ===
Loss reward (iter 90): -7.084876537322998
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.67e+03 |
|    critic_loss     | 96.7      |
|    ent_coef        | 1.74      |
|    ent_coef_loss   | 0.0346    |
|    learning_rate   | 0.0003    |
|    n_updates       | 34499     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.67e+03 |
|    critic_loss     | 100       |
|    ent_coef        | 1.71      |
|    ent_coef_loss   | 0.0467    |
|    learning_rate   | 0.0003    |
|    n_updates       | 34899     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.69e+03 |
|    critic_loss     | 110       |
|    ent_coef        | 1.7       |
|    ent_coef_loss   | 0.031     |
|    learning_rate   | 0.0003    |
|    n_updates       | 35299     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 123       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.69e+03 |
|    critic_loss     | 93        |
|    ent_coef        | 1.68      |
|    ent_coef_loss   | -0.00297  |
|    learning_rate   | 0.0003    |
|    n_updates       | 35699     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.72e+03 |
|    critic_loss     | 103       |
|    ent_coef        | 1.66      |
|    ent_coef_loss   | 0.0482    |
|    learning_rate   | 0.0003    |
|    n_updates       | 36099     |
----------------------------------
=== Iterazione IRL 91 ===
Loss reward (iter 91): -3.218852996826172
=== Iterazione IRL 92 ===
Loss reward (iter 92): -5.93096923828125
=== Iterazione IRL 93 ===
Loss reward (iter 93): -6.734304428100586
=== Iterazione IRL 94 ===
Loss reward (iter 94): -6.264328956604004
=== Iterazione IRL 95 ===
Loss reward (iter 95): -7.940638542175293
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.73e+03 |
|    critic_loss     | 101       |
|    ent_coef        | 1.64      |
|    ent_coef_loss   | 0.0512    |
|    learning_rate   | 0.0003    |
|    n_updates       | 36399     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.74e+03 |
|    critic_loss     | 101       |
|    ent_coef        | 1.6       |
|    ent_coef_loss   | 0.000854  |
|    learning_rate   | 0.0003    |
|    n_updates       | 36799     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.75e+03 |
|    critic_loss     | 105       |
|    ent_coef        | 1.6       |
|    ent_coef_loss   | -0.0285   |
|    learning_rate   | 0.0003    |
|    n_updates       | 37199     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 123       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.75e+03 |
|    critic_loss     | 102       |
|    ent_coef        | 1.58      |
|    ent_coef_loss   | -0.0681   |
|    learning_rate   | 0.0003    |
|    n_updates       | 37599     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.78e+03 |
|    critic_loss     | 89.8      |
|    ent_coef        | 1.56      |
|    ent_coef_loss   | 0.0272    |
|    learning_rate   | 0.0003    |
|    n_updates       | 37999     |
----------------------------------
=== Iterazione IRL 96 ===
Loss reward (iter 96): -8.921853065490723
=== Iterazione IRL 97 ===
Loss reward (iter 97): -9.3131103515625
=== Iterazione IRL 98 ===
Loss reward (iter 98): -11.947478294372559
=== Iterazione IRL 99 ===
Loss reward (iter 99): -11.060620307922363
=== Iterazione IRL 100 ===
Loss reward (iter 100): -14.819345474243164
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.79e+03 |
|    critic_loss     | 91.9      |
|    ent_coef        | 1.54      |
|    ent_coef_loss   | -0.0106   |
|    learning_rate   | 0.0003    |
|    n_updates       | 38299     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.79e+03 |
|    critic_loss     | 159       |
|    ent_coef        | 1.52      |
|    ent_coef_loss   | -0.0233   |
|    learning_rate   | 0.0003    |
|    n_updates       | 38699     |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -1.8e+03 |
|    critic_loss     | 131      |
|    ent_coef        | 1.51     |
|    ent_coef_loss   | -0.0285  |
|    learning_rate   | 0.0003   |
|    n_updates       | 39099    |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.82e+03 |
|    critic_loss     | 161       |
|    ent_coef        | 1.49      |
|    ent_coef_loss   | -0.0171   |
|    learning_rate   | 0.0003    |
|    n_updates       | 39499     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.82e+03 |
|    critic_loss     | 90.6      |
|    ent_coef        | 1.48      |
|    ent_coef_loss   | -0.0473   |
|    learning_rate   | 0.0003    |
|    n_updates       | 39899     |
----------------------------------
=== Iterazione IRL 101 ===
Loss reward (iter 101): -10.546651840209961
=== Iterazione IRL 102 ===
Loss reward (iter 102): -11.921883583068848
=== Iterazione IRL 103 ===
Loss reward (iter 103): -14.175993919372559
=== Iterazione IRL 104 ===
Loss reward (iter 104): -13.976495742797852
=== Iterazione IRL 105 ===
Loss reward (iter 105): -17.760202407836914
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.85e+03 |
|    critic_loss     | 116       |
|    ent_coef        | 1.49      |
|    ent_coef_loss   | 0.00511   |
|    learning_rate   | 0.0003    |
|    n_updates       | 40199     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.86e+03 |
|    critic_loss     | 163       |
|    ent_coef        | 1.45      |
|    ent_coef_loss   | -0.0193   |
|    learning_rate   | 0.0003    |
|    n_updates       | 40599     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.84e+03 |
|    critic_loss     | 120       |
|    ent_coef        | 1.44      |
|    ent_coef_loss   | 0.0194    |
|    learning_rate   | 0.0003    |
|    n_updates       | 40999     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.87e+03 |
|    critic_loss     | 155       |
|    ent_coef        | 1.4       |
|    ent_coef_loss   | -0.0251   |
|    learning_rate   | 0.0003    |
|    n_updates       | 41399     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.86e+03 |
|    critic_loss     | 138       |
|    ent_coef        | 1.38      |
|    ent_coef_loss   | 0.00222   |
|    learning_rate   | 0.0003    |
|    n_updates       | 41799     |
----------------------------------
=== Iterazione IRL 106 ===
Loss reward (iter 106): -6.514320373535156
=== Iterazione IRL 107 ===
Loss reward (iter 107): -6.552081108093262
=== Iterazione IRL 108 ===
Loss reward (iter 108): -10.402535438537598
=== Iterazione IRL 109 ===
Loss reward (iter 109): -13.143366813659668
=== Iterazione IRL 110 ===
Loss reward (iter 110): -11.347798347473145
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.88e+03 |
|    critic_loss     | 279       |
|    ent_coef        | 1.38      |
|    ent_coef_loss   | 0.0363    |
|    learning_rate   | 0.0003    |
|    n_updates       | 42099     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.88e+03 |
|    critic_loss     | 118       |
|    ent_coef        | 1.37      |
|    ent_coef_loss   | -0.0389   |
|    learning_rate   | 0.0003    |
|    n_updates       | 42499     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.89e+03 |
|    critic_loss     | 105       |
|    ent_coef        | 1.33      |
|    ent_coef_loss   | 0.000236  |
|    learning_rate   | 0.0003    |
|    n_updates       | 42899     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.87e+03 |
|    critic_loss     | 128       |
|    ent_coef        | 1.31      |
|    ent_coef_loss   | -0.0134   |
|    learning_rate   | 0.0003    |
|    n_updates       | 43299     |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 20       |
|    fps             | 122      |
|    time_elapsed    | 16       |
|    total_timesteps | 2000     |
| train/             |          |
|    actor_loss      | -1.9e+03 |
|    critic_loss     | 192      |
|    ent_coef        | 1.29     |
|    ent_coef_loss   | 0.0351   |
|    learning_rate   | 0.0003   |
|    n_updates       | 43699    |
---------------------------------
=== Iterazione IRL 111 ===
Loss reward (iter 111): -1.1730501651763916
=== Iterazione IRL 112 ===
Loss reward (iter 112): -4.0448479652404785
=== Iterazione IRL 113 ===
Loss reward (iter 113): -4.427303314208984
=== Iterazione IRL 114 ===
Loss reward (iter 114): -4.7810893058776855
=== Iterazione IRL 115 ===
Loss reward (iter 115): -5.826339244842529
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.89e+03 |
|    critic_loss     | 159       |
|    ent_coef        | 1.28      |
|    ent_coef_loss   | 0.0318    |
|    learning_rate   | 0.0003    |
|    n_updates       | 43999     |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -1.9e+03 |
|    critic_loss     | 158      |
|    ent_coef        | 1.24     |
|    ent_coef_loss   | -0.0111  |
|    learning_rate   | 0.0003   |
|    n_updates       | 44399    |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.91e+03 |
|    critic_loss     | 231       |
|    ent_coef        | 1.21      |
|    ent_coef_loss   | 0.041     |
|    learning_rate   | 0.0003    |
|    n_updates       | 44799     |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 16       |
|    fps             | 124      |
|    time_elapsed    | 12       |
|    total_timesteps | 1600     |
| train/             |          |
|    actor_loss      | -1.9e+03 |
|    critic_loss     | 129      |
|    ent_coef        | 1.19     |
|    ent_coef_loss   | 0.018    |
|    learning_rate   | 0.0003   |
|    n_updates       | 45199    |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 20       |
|    fps             | 122      |
|    time_elapsed    | 16       |
|    total_timesteps | 2000     |
| train/             |          |
|    actor_loss      | -1.9e+03 |
|    critic_loss     | 166      |
|    ent_coef        | 1.17     |
|    ent_coef_loss   | -0.00567 |
|    learning_rate   | 0.0003   |
|    n_updates       | 45599    |
---------------------------------
=== Iterazione IRL 116 ===
Loss reward (iter 116): -0.9411019682884216
=== Iterazione IRL 117 ===
Loss reward (iter 117): -1.3264274597167969
=== Iterazione IRL 118 ===
Loss reward (iter 118): -3.0343410968780518
=== Iterazione IRL 119 ===
Loss reward (iter 119): 2.558791160583496
=== Iterazione IRL 120 ===
Loss reward (iter 120): 3.9088497161865234
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.92e+03 |
|    critic_loss     | 264       |
|    ent_coef        | 1.16      |
|    ent_coef_loss   | -0.028    |
|    learning_rate   | 0.0003    |
|    n_updates       | 45899     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.92e+03 |
|    critic_loss     | 258       |
|    ent_coef        | 1.12      |
|    ent_coef_loss   | 0.0197    |
|    learning_rate   | 0.0003    |
|    n_updates       | 46299     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.93e+03 |
|    critic_loss     | 185       |
|    ent_coef        | 1.07      |
|    ent_coef_loss   | -0.00527  |
|    learning_rate   | 0.0003    |
|    n_updates       | 46699     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.94e+03 |
|    critic_loss     | 229       |
|    ent_coef        | 1.04      |
|    ent_coef_loss   | 0.0022    |
|    learning_rate   | 0.0003    |
|    n_updates       | 47099     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.94e+03 |
|    critic_loss     | 206       |
|    ent_coef        | 1.02      |
|    ent_coef_loss   | 0.00309   |
|    learning_rate   | 0.0003    |
|    n_updates       | 47499     |
----------------------------------
=== Iterazione IRL 121 ===
Loss reward (iter 121): 26.862403869628906
=== Iterazione IRL 122 ===
Loss reward (iter 122): 15.558059692382812
=== Iterazione IRL 123 ===
Loss reward (iter 123): 14.546512603759766
=== Iterazione IRL 124 ===
Loss reward (iter 124): 11.888834953308105
=== Iterazione IRL 125 ===
Loss reward (iter 125): 6.720151424407959
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.94e+03 |
|    critic_loss     | 206       |
|    ent_coef        | 1.02      |
|    ent_coef_loss   | 4.8e-06   |
|    learning_rate   | 0.0003    |
|    n_updates       | 47799     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.95e+03 |
|    critic_loss     | 251       |
|    ent_coef        | 1.03      |
|    ent_coef_loss   | 0.00495   |
|    learning_rate   | 0.0003    |
|    n_updates       | 48199     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.95e+03 |
|    critic_loss     | 217       |
|    ent_coef        | 1.05      |
|    ent_coef_loss   | 0.00221   |
|    learning_rate   | 0.0003    |
|    n_updates       | 48599     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.95e+03 |
|    critic_loss     | 234       |
|    ent_coef        | 1.08      |
|    ent_coef_loss   | 0.00617   |
|    learning_rate   | 0.0003    |
|    n_updates       | 48999     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.95e+03 |
|    critic_loss     | 324       |
|    ent_coef        | 1.11      |
|    ent_coef_loss   | -0.00201  |
|    learning_rate   | 0.0003    |
|    n_updates       | 49399     |
----------------------------------
=== Iterazione IRL 126 ===
Loss reward (iter 126): 6.305060386657715
=== Iterazione IRL 127 ===
Loss reward (iter 127): 7.739469528198242
=== Iterazione IRL 128 ===
Loss reward (iter 128): 7.926286697387695
=== Iterazione IRL 129 ===
Loss reward (iter 129): 6.529064655303955
=== Iterazione IRL 130 ===
Loss reward (iter 130): 6.915672779083252
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.97e+03 |
|    critic_loss     | 184       |
|    ent_coef        | 1.09      |
|    ent_coef_loss   | -0.00677  |
|    learning_rate   | 0.0003    |
|    n_updates       | 49699     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.97e+03 |
|    critic_loss     | 248       |
|    ent_coef        | 1.14      |
|    ent_coef_loss   | -0.00685  |
|    learning_rate   | 0.0003    |
|    n_updates       | 50099     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.97e+03 |
|    critic_loss     | 236       |
|    ent_coef        | 1.16      |
|    ent_coef_loss   | 0.00665   |
|    learning_rate   | 0.0003    |
|    n_updates       | 50499     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.96e+03 |
|    critic_loss     | 188       |
|    ent_coef        | 1.2       |
|    ent_coef_loss   | -0.0304   |
|    learning_rate   | 0.0003    |
|    n_updates       | 50899     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.98e+03 |
|    critic_loss     | 253       |
|    ent_coef        | 1.22      |
|    ent_coef_loss   | 0.00269   |
|    learning_rate   | 0.0003    |
|    n_updates       | 51299     |
----------------------------------
=== Iterazione IRL 131 ===
Loss reward (iter 131): 8.24301528930664
=== Iterazione IRL 132 ===
Loss reward (iter 132): 7.065745830535889
=== Iterazione IRL 133 ===
Loss reward (iter 133): 6.360001564025879
=== Iterazione IRL 134 ===
Loss reward (iter 134): 6.106504440307617
=== Iterazione IRL 135 ===
Loss reward (iter 135): 5.64550256729126
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.97e+03 |
|    critic_loss     | 246       |
|    ent_coef        | 1.23      |
|    ent_coef_loss   | 0.00376   |
|    learning_rate   | 0.0003    |
|    n_updates       | 51599     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.95e+03 |
|    critic_loss     | 335       |
|    ent_coef        | 1.21      |
|    ent_coef_loss   | 0.00885   |
|    learning_rate   | 0.0003    |
|    n_updates       | 51999     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.96e+03 |
|    critic_loss     | 212       |
|    ent_coef        | 1.21      |
|    ent_coef_loss   | -0.0208   |
|    learning_rate   | 0.0003    |
|    n_updates       | 52399     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 122       |
|    time_elapsed    | 13        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.97e+03 |
|    critic_loss     | 346       |
|    ent_coef        | 1.25      |
|    ent_coef_loss   | 0.0321    |
|    learning_rate   | 0.0003    |
|    n_updates       | 52799     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 117       |
|    time_elapsed    | 17        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.97e+03 |
|    critic_loss     | 328       |
|    ent_coef        | 1.26      |
|    ent_coef_loss   | -0.0254   |
|    learning_rate   | 0.0003    |
|    n_updates       | 53199     |
----------------------------------
=== Iterazione IRL 136 ===
Loss reward (iter 136): 4.924431324005127
=== Iterazione IRL 137 ===
Loss reward (iter 137): 3.8334908485412598
=== Iterazione IRL 138 ===
Loss reward (iter 138): 1.7111953496932983
=== Iterazione IRL 139 ===
Loss reward (iter 139): 0.17030751705169678
=== Iterazione IRL 140 ===
Loss reward (iter 140): 0.18234986066818237
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 130       |
|    time_elapsed    | 3         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.97e+03 |
|    critic_loss     | 210       |
|    ent_coef        | 1.27      |
|    ent_coef_loss   | 0.0366    |
|    learning_rate   | 0.0003    |
|    n_updates       | 53499     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 113       |
|    time_elapsed    | 7         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.97e+03 |
|    critic_loss     | 297       |
|    ent_coef        | 1.26      |
|    ent_coef_loss   | 0.000747  |
|    learning_rate   | 0.0003    |
|    n_updates       | 53899     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 108       |
|    time_elapsed    | 11        |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.98e+03 |
|    critic_loss     | 256       |
|    ent_coef        | 1.21      |
|    ent_coef_loss   | 0.0332    |
|    learning_rate   | 0.0003    |
|    n_updates       | 54299     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 106       |
|    time_elapsed    | 14        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.97e+03 |
|    critic_loss     | 230       |
|    ent_coef        | 1.21      |
|    ent_coef_loss   | 0.0427    |
|    learning_rate   | 0.0003    |
|    n_updates       | 54699     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 105       |
|    time_elapsed    | 18        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.96e+03 |
|    critic_loss     | 231       |
|    ent_coef        | 1.19      |
|    ent_coef_loss   | -0.0218   |
|    learning_rate   | 0.0003    |
|    n_updates       | 55099     |
----------------------------------
=== Iterazione IRL 141 ===
Loss reward (iter 141): -0.49434715509414673
=== Iterazione IRL 142 ===
Loss reward (iter 142): -1.279527187347412
=== Iterazione IRL 143 ===
Loss reward (iter 143): -1.9477148056030273
=== Iterazione IRL 144 ===
Loss reward (iter 144): -2.6217596530914307
=== Iterazione IRL 145 ===
Loss reward (iter 145): -3.1054139137268066
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 131       |
|    time_elapsed    | 3         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.97e+03 |
|    critic_loss     | 340       |
|    ent_coef        | 1.14      |
|    ent_coef_loss   | 0.00423   |
|    learning_rate   | 0.0003    |
|    n_updates       | 55399     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 113       |
|    time_elapsed    | 7         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.97e+03 |
|    critic_loss     | 246       |
|    ent_coef        | 1.13      |
|    ent_coef_loss   | -0.00713  |
|    learning_rate   | 0.0003    |
|    n_updates       | 55799     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 109       |
|    time_elapsed    | 11        |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.98e+03 |
|    critic_loss     | 269       |
|    ent_coef        | 1.1       |
|    ent_coef_loss   | -0.00178  |
|    learning_rate   | 0.0003    |
|    n_updates       | 56199     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 106       |
|    time_elapsed    | 14        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.98e+03 |
|    critic_loss     | 258       |
|    ent_coef        | 1.09      |
|    ent_coef_loss   | -0.000632 |
|    learning_rate   | 0.0003    |
|    n_updates       | 56599     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 105       |
|    time_elapsed    | 18        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.98e+03 |
|    critic_loss     | 274       |
|    ent_coef        | 1.1       |
|    ent_coef_loss   | -0.00254  |
|    learning_rate   | 0.0003    |
|    n_updates       | 56999     |
----------------------------------
=== Iterazione IRL 146 ===
Loss reward (iter 146): -0.17948704957962036
=== Iterazione IRL 147 ===
Loss reward (iter 147): 3.225592613220215
=== Iterazione IRL 148 ===
Loss reward (iter 148): 0.92364102602005
=== Iterazione IRL 149 ===
Loss reward (iter 149): 0.69686359167099
=== Iterazione IRL 150 ===
Loss reward (iter 150): 1.3433887958526611
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 131       |
|    time_elapsed    | 3         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.98e+03 |
|    critic_loss     | 266       |
|    ent_coef        | 1.05      |
|    ent_coef_loss   | -0.008    |
|    learning_rate   | 0.0003    |
|    n_updates       | 57299     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 114       |
|    time_elapsed    | 7         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.95e+03 |
|    critic_loss     | 238       |
|    ent_coef        | 1.07      |
|    ent_coef_loss   | -0.00915  |
|    learning_rate   | 0.0003    |
|    n_updates       | 57699     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 109       |
|    time_elapsed    | 10        |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.99e+03 |
|    critic_loss     | 276       |
|    ent_coef        | 1.07      |
|    ent_coef_loss   | -0.00801  |
|    learning_rate   | 0.0003    |
|    n_updates       | 58099     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 107       |
|    time_elapsed    | 14        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.97e+03 |
|    critic_loss     | 348       |
|    ent_coef        | 1.03      |
|    ent_coef_loss   | -0.0011   |
|    learning_rate   | 0.0003    |
|    n_updates       | 58499     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 108       |
|    time_elapsed    | 18        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.98e+03 |
|    critic_loss     | 265       |
|    ent_coef        | 1.01      |
|    ent_coef_loss   | 0.00178   |
|    learning_rate   | 0.0003    |
|    n_updates       | 58899     |
----------------------------------
=== Iterazione IRL 151 ===
Loss reward (iter 151): 4.942392349243164
=== Iterazione IRL 152 ===
Loss reward (iter 152): 5.7268266677856445
=== Iterazione IRL 153 ===
Loss reward (iter 153): 6.337847709655762
=== Iterazione IRL 154 ===
Loss reward (iter 154): 5.9254679679870605
=== Iterazione IRL 155 ===
Loss reward (iter 155): 4.399673938751221
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 153       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.96e+03 |
|    critic_loss     | 242       |
|    ent_coef        | 0.986     |
|    ent_coef_loss   | 0.000777  |
|    learning_rate   | 0.0003    |
|    n_updates       | 59199     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.97e+03 |
|    critic_loss     | 283       |
|    ent_coef        | 0.987     |
|    ent_coef_loss   | -0.00208  |
|    learning_rate   | 0.0003    |
|    n_updates       | 59599     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.97e+03 |
|    critic_loss     | 349       |
|    ent_coef        | 0.978     |
|    ent_coef_loss   | -0.000935 |
|    learning_rate   | 0.0003    |
|    n_updates       | 59999     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 122       |
|    time_elapsed    | 13        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.96e+03 |
|    critic_loss     | 345       |
|    ent_coef        | 0.966     |
|    ent_coef_loss   | -0.00791  |
|    learning_rate   | 0.0003    |
|    n_updates       | 60399     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 117       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.97e+03 |
|    critic_loss     | 259       |
|    ent_coef        | 0.983     |
|    ent_coef_loss   | 0.00132   |
|    learning_rate   | 0.0003    |
|    n_updates       | 60799     |
----------------------------------
=== Iterazione IRL 156 ===
Loss reward (iter 156): 10.282898902893066
=== Iterazione IRL 157 ===
Loss reward (iter 157): 8.41952133178711
=== Iterazione IRL 158 ===
Loss reward (iter 158): 7.607980728149414
=== Iterazione IRL 159 ===
Loss reward (iter 159): 9.260847091674805
=== Iterazione IRL 160 ===
Loss reward (iter 160): 6.377260208129883
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 132       |
|    time_elapsed    | 3         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.98e+03 |
|    critic_loss     | 324       |
|    ent_coef        | 1.02      |
|    ent_coef_loss   | 0.00246   |
|    learning_rate   | 0.0003    |
|    n_updates       | 61099     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 114       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.98e+03 |
|    critic_loss     | 286       |
|    ent_coef        | 1.03      |
|    ent_coef_loss   | 0.00305   |
|    learning_rate   | 0.0003    |
|    n_updates       | 61499     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 109       |
|    time_elapsed    | 10        |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.97e+03 |
|    critic_loss     | 195       |
|    ent_coef        | 1         |
|    ent_coef_loss   | 0.000224  |
|    learning_rate   | 0.0003    |
|    n_updates       | 61899     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 107       |
|    time_elapsed    | 14        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.95e+03 |
|    critic_loss     | 264       |
|    ent_coef        | 0.988     |
|    ent_coef_loss   | -0.000533 |
|    learning_rate   | 0.0003    |
|    n_updates       | 62299     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 106       |
|    time_elapsed    | 18        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.96e+03 |
|    critic_loss     | 239       |
|    ent_coef        | 1         |
|    ent_coef_loss   | -0.000145 |
|    learning_rate   | 0.0003    |
|    n_updates       | 62699     |
----------------------------------
=== Iterazione IRL 161 ===
Loss reward (iter 161): 9.536653518676758
=== Iterazione IRL 162 ===
Loss reward (iter 162): 9.019834518432617
=== Iterazione IRL 163 ===
Loss reward (iter 163): 10.173623085021973
=== Iterazione IRL 164 ===
Loss reward (iter 164): 7.5445876121521
=== Iterazione IRL 165 ===
Loss reward (iter 165): 7.281179428100586
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 131       |
|    time_elapsed    | 3         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.96e+03 |
|    critic_loss     | 215       |
|    ent_coef        | 1.04      |
|    ent_coef_loss   | -0.00304  |
|    learning_rate   | 0.0003    |
|    n_updates       | 62999     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 114       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.94e+03 |
|    critic_loss     | 247       |
|    ent_coef        | 1.03      |
|    ent_coef_loss   | 0.00664   |
|    learning_rate   | 0.0003    |
|    n_updates       | 63399     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 109       |
|    time_elapsed    | 10        |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.94e+03 |
|    critic_loss     | 259       |
|    ent_coef        | 1.01      |
|    ent_coef_loss   | 0.000744  |
|    learning_rate   | 0.0003    |
|    n_updates       | 63799     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 107       |
|    time_elapsed    | 14        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.95e+03 |
|    critic_loss     | 254       |
|    ent_coef        | 0.988     |
|    ent_coef_loss   | -0.00135  |
|    learning_rate   | 0.0003    |
|    n_updates       | 64199     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 106       |
|    time_elapsed    | 18        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.92e+03 |
|    critic_loss     | 328       |
|    ent_coef        | 0.997     |
|    ent_coef_loss   | 5.57e-05  |
|    learning_rate   | 0.0003    |
|    n_updates       | 64599     |
----------------------------------
=== Iterazione IRL 166 ===
Loss reward (iter 166): 5.502183437347412
=== Iterazione IRL 167 ===
Loss reward (iter 167): 6.17073917388916
=== Iterazione IRL 168 ===
Loss reward (iter 168): 5.51557731628418
=== Iterazione IRL 169 ===
Loss reward (iter 169): 4.676080226898193
=== Iterazione IRL 170 ===
Loss reward (iter 170): 4.559360027313232
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 131       |
|    time_elapsed    | 3         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.92e+03 |
|    critic_loss     | 245       |
|    ent_coef        | 1.01      |
|    ent_coef_loss   | -0.00105  |
|    learning_rate   | 0.0003    |
|    n_updates       | 64899     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 114       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.92e+03 |
|    critic_loss     | 169       |
|    ent_coef        | 1.01      |
|    ent_coef_loss   | -0.00109  |
|    learning_rate   | 0.0003    |
|    n_updates       | 65299     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 109       |
|    time_elapsed    | 10        |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.92e+03 |
|    critic_loss     | 287       |
|    ent_coef        | 0.992     |
|    ent_coef_loss   | -0.00129  |
|    learning_rate   | 0.0003    |
|    n_updates       | 65699     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 107       |
|    time_elapsed    | 14        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.92e+03 |
|    critic_loss     | 275       |
|    ent_coef        | 1         |
|    ent_coef_loss   | 1.16e-05  |
|    learning_rate   | 0.0003    |
|    n_updates       | 66099     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 106       |
|    time_elapsed    | 18        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.92e+03 |
|    critic_loss     | 340       |
|    ent_coef        | 0.998     |
|    ent_coef_loss   | 0.0001    |
|    learning_rate   | 0.0003    |
|    n_updates       | 66499     |
----------------------------------
=== Iterazione IRL 171 ===
Loss reward (iter 171): 7.6294636726379395
=== Iterazione IRL 172 ===
Loss reward (iter 172): 6.342497825622559
=== Iterazione IRL 173 ===
Loss reward (iter 173): 7.687428951263428
=== Iterazione IRL 174 ===
Loss reward (iter 174): 5.498294830322266
=== Iterazione IRL 175 ===
Loss reward (iter 175): 7.675327777862549
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 131       |
|    time_elapsed    | 3         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.93e+03 |
|    critic_loss     | 320       |
|    ent_coef        | 0.97      |
|    ent_coef_loss   | -0.00412  |
|    learning_rate   | 0.0003    |
|    n_updates       | 66799     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 114       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.89e+03 |
|    critic_loss     | 277       |
|    ent_coef        | 0.958     |
|    ent_coef_loss   | -0.00716  |
|    learning_rate   | 0.0003    |
|    n_updates       | 67199     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 110       |
|    time_elapsed    | 10        |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.91e+03 |
|    critic_loss     | 341       |
|    ent_coef        | 0.932     |
|    ent_coef_loss   | 0.000924  |
|    learning_rate   | 0.0003    |
|    n_updates       | 67599     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 112       |
|    time_elapsed    | 14        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.89e+03 |
|    critic_loss     | 216       |
|    ent_coef        | 0.919     |
|    ent_coef_loss   | -0.0124   |
|    learning_rate   | 0.0003    |
|    n_updates       | 67999     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 113       |
|    time_elapsed    | 17        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.92e+03 |
|    critic_loss     | 205       |
|    ent_coef        | 0.895     |
|    ent_coef_loss   | 0.0162    |
|    learning_rate   | 0.0003    |
|    n_updates       | 68399     |
----------------------------------
=== Iterazione IRL 176 ===
Loss reward (iter 176): 6.9060211181640625
=== Iterazione IRL 177 ===
Loss reward (iter 177): 5.250128269195557
=== Iterazione IRL 178 ===
Loss reward (iter 178): 1.9644279479980469
=== Iterazione IRL 179 ===
Loss reward (iter 179): 6.060849189758301
=== Iterazione IRL 180 ===
Loss reward (iter 180): 4.438255786895752
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 153       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.88e+03 |
|    critic_loss     | 293       |
|    ent_coef        | 0.859     |
|    ent_coef_loss   | -0.00848  |
|    learning_rate   | 0.0003    |
|    n_updates       | 68699     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.89e+03 |
|    critic_loss     | 308       |
|    ent_coef        | 0.843     |
|    ent_coef_loss   | -0.00447  |
|    learning_rate   | 0.0003    |
|    n_updates       | 69099     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.88e+03 |
|    critic_loss     | 304       |
|    ent_coef        | 0.829     |
|    ent_coef_loss   | 0.0299    |
|    learning_rate   | 0.0003    |
|    n_updates       | 69499     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.89e+03 |
|    critic_loss     | 274       |
|    ent_coef        | 0.85      |
|    ent_coef_loss   | -0.0267   |
|    learning_rate   | 0.0003    |
|    n_updates       | 69899     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 123       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.85e+03 |
|    critic_loss     | 245       |
|    ent_coef        | 0.821     |
|    ent_coef_loss   | -0.0264   |
|    learning_rate   | 0.0003    |
|    n_updates       | 70299     |
----------------------------------
=== Iterazione IRL 181 ===
Loss reward (iter 181): 3.4654252529144287
=== Iterazione IRL 182 ===
Loss reward (iter 182): 7.371047496795654
=== Iterazione IRL 183 ===
Loss reward (iter 183): 5.290596961975098
=== Iterazione IRL 184 ===
Loss reward (iter 184): 5.9368462562561035
=== Iterazione IRL 185 ===
Loss reward (iter 185): 4.593018054962158
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 153       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.85e+03 |
|    critic_loss     | 270       |
|    ent_coef        | 0.805     |
|    ent_coef_loss   | -0.0235   |
|    learning_rate   | 0.0003    |
|    n_updates       | 70599     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.87e+03 |
|    critic_loss     | 291       |
|    ent_coef        | 0.765     |
|    ent_coef_loss   | -0.0759   |
|    learning_rate   | 0.0003    |
|    n_updates       | 70999     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.84e+03 |
|    critic_loss     | 383       |
|    ent_coef        | 0.731     |
|    ent_coef_loss   | 0.000935  |
|    learning_rate   | 0.0003    |
|    n_updates       | 71399     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.84e+03 |
|    critic_loss     | 391       |
|    ent_coef        | 0.704     |
|    ent_coef_loss   | -0.0368   |
|    learning_rate   | 0.0003    |
|    n_updates       | 71799     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 123       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.84e+03 |
|    critic_loss     | 438       |
|    ent_coef        | 0.713     |
|    ent_coef_loss   | 0.0125    |
|    learning_rate   | 0.0003    |
|    n_updates       | 72199     |
----------------------------------
=== Iterazione IRL 186 ===
Loss reward (iter 186): 6.577953815460205
=== Iterazione IRL 187 ===
Loss reward (iter 187): 5.6421356201171875
=== Iterazione IRL 188 ===
Loss reward (iter 188): 4.586452007293701
=== Iterazione IRL 189 ===
Loss reward (iter 189): 4.36019229888916
=== Iterazione IRL 190 ===
Loss reward (iter 190): 5.302840709686279
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 153       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.85e+03 |
|    critic_loss     | 317       |
|    ent_coef        | 0.717     |
|    ent_coef_loss   | 0.0369    |
|    learning_rate   | 0.0003    |
|    n_updates       | 72499     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.82e+03 |
|    critic_loss     | 296       |
|    ent_coef        | 0.717     |
|    ent_coef_loss   | 0.0227    |
|    learning_rate   | 0.0003    |
|    n_updates       | 72899     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 120       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.81e+03 |
|    critic_loss     | 385       |
|    ent_coef        | 0.721     |
|    ent_coef_loss   | -0.0347   |
|    learning_rate   | 0.0003    |
|    n_updates       | 73299     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 114       |
|    time_elapsed    | 13        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.81e+03 |
|    critic_loss     | 424       |
|    ent_coef        | 0.736     |
|    ent_coef_loss   | -0.0496   |
|    learning_rate   | 0.0003    |
|    n_updates       | 73699     |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 20       |
|    fps             | 111      |
|    time_elapsed    | 17       |
|    total_timesteps | 2000     |
| train/             |          |
|    actor_loss      | -1.8e+03 |
|    critic_loss     | 331      |
|    ent_coef        | 0.745    |
|    ent_coef_loss   | 0.0444   |
|    learning_rate   | 0.0003   |
|    n_updates       | 74099    |
---------------------------------
=== Iterazione IRL 191 ===
Loss reward (iter 191): 6.8387932777404785
=== Iterazione IRL 192 ===
Loss reward (iter 192): 4.104398250579834
=== Iterazione IRL 193 ===
Loss reward (iter 193): 6.68095588684082
=== Iterazione IRL 194 ===
Loss reward (iter 194): 6.490727424621582
=== Iterazione IRL 195 ===
Loss reward (iter 195): 7.29327392578125
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 131       |
|    time_elapsed    | 3         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.79e+03 |
|    critic_loss     | 306       |
|    ent_coef        | 0.77      |
|    ent_coef_loss   | 0.0251    |
|    learning_rate   | 0.0003    |
|    n_updates       | 74399     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 114       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.78e+03 |
|    critic_loss     | 266       |
|    ent_coef        | 0.78      |
|    ent_coef_loss   | -0.00553  |
|    learning_rate   | 0.0003    |
|    n_updates       | 74799     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 109       |
|    time_elapsed    | 10        |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.78e+03 |
|    critic_loss     | 396       |
|    ent_coef        | 0.77      |
|    ent_coef_loss   | -0.023    |
|    learning_rate   | 0.0003    |
|    n_updates       | 75199     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 108       |
|    time_elapsed    | 14        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.77e+03 |
|    critic_loss     | 526       |
|    ent_coef        | 0.746     |
|    ent_coef_loss   | -0.0016   |
|    learning_rate   | 0.0003    |
|    n_updates       | 75599     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 109       |
|    time_elapsed    | 18        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.78e+03 |
|    critic_loss     | 255       |
|    ent_coef        | 0.706     |
|    ent_coef_loss   | 0.055     |
|    learning_rate   | 0.0003    |
|    n_updates       | 75999     |
----------------------------------
=== Iterazione IRL 196 ===
Loss reward (iter 196): 6.5548295974731445
=== Iterazione IRL 197 ===
Loss reward (iter 197): 4.809218883514404
=== Iterazione IRL 198 ===
Loss reward (iter 198): 7.043900489807129
=== Iterazione IRL 199 ===
Loss reward (iter 199): 5.8464484214782715
=== Iterazione IRL 200 ===
Loss reward (iter 200): 4.6336588859558105
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 153       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.77e+03 |
|    critic_loss     | 285       |
|    ent_coef        | 0.708     |
|    ent_coef_loss   | -0.0351   |
|    learning_rate   | 0.0003    |
|    n_updates       | 76299     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.76e+03 |
|    critic_loss     | 436       |
|    ent_coef        | 0.688     |
|    ent_coef_loss   | -0.0162   |
|    learning_rate   | 0.0003    |
|    n_updates       | 76699     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.74e+03 |
|    critic_loss     | 294       |
|    ent_coef        | 0.698     |
|    ent_coef_loss   | -0.0118   |
|    learning_rate   | 0.0003    |
|    n_updates       | 77099     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.74e+03 |
|    critic_loss     | 367       |
|    ent_coef        | 0.726     |
|    ent_coef_loss   | 0.0197    |
|    learning_rate   | 0.0003    |
|    n_updates       | 77499     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 123       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.75e+03 |
|    critic_loss     | 353       |
|    ent_coef        | 0.72      |
|    ent_coef_loss   | -0.0557   |
|    learning_rate   | 0.0003    |
|    n_updates       | 77899     |
----------------------------------
=== Iterazione IRL 201 ===
Loss reward (iter 201): 4.309047222137451
=== Iterazione IRL 202 ===
Loss reward (iter 202): 3.704519748687744
=== Iterazione IRL 203 ===
Loss reward (iter 203): 6.373420715332031
=== Iterazione IRL 204 ===
Loss reward (iter 204): 3.7339298725128174
=== Iterazione IRL 205 ===
Loss reward (iter 205): 2.2998104095458984
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 153       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.74e+03 |
|    critic_loss     | 320       |
|    ent_coef        | 0.717     |
|    ent_coef_loss   | -0.072    |
|    learning_rate   | 0.0003    |
|    n_updates       | 78199     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.74e+03 |
|    critic_loss     | 350       |
|    ent_coef        | 0.731     |
|    ent_coef_loss   | -0.0346   |
|    learning_rate   | 0.0003    |
|    n_updates       | 78599     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.73e+03 |
|    critic_loss     | 311       |
|    ent_coef        | 0.749     |
|    ent_coef_loss   | 0.016     |
|    learning_rate   | 0.0003    |
|    n_updates       | 78999     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.73e+03 |
|    critic_loss     | 357       |
|    ent_coef        | 0.752     |
|    ent_coef_loss   | -0.0248   |
|    learning_rate   | 0.0003    |
|    n_updates       | 79399     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 123       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.71e+03 |
|    critic_loss     | 364       |
|    ent_coef        | 0.764     |
|    ent_coef_loss   | 0.0323    |
|    learning_rate   | 0.0003    |
|    n_updates       | 79799     |
----------------------------------
=== Iterazione IRL 206 ===
Loss reward (iter 206): 6.848132133483887
=== Iterazione IRL 207 ===
Loss reward (iter 207): 5.722052097320557
=== Iterazione IRL 208 ===
Loss reward (iter 208): 5.600226402282715
=== Iterazione IRL 209 ===
Loss reward (iter 209): 5.311887741088867
=== Iterazione IRL 210 ===
Loss reward (iter 210): 6.688689231872559
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 153       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.69e+03 |
|    critic_loss     | 318       |
|    ent_coef        | 0.831     |
|    ent_coef_loss   | 0.0266    |
|    learning_rate   | 0.0003    |
|    n_updates       | 80099     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.69e+03 |
|    critic_loss     | 302       |
|    ent_coef        | 0.853     |
|    ent_coef_loss   | 0.0242    |
|    learning_rate   | 0.0003    |
|    n_updates       | 80499     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.68e+03 |
|    critic_loss     | 297       |
|    ent_coef        | 0.884     |
|    ent_coef_loss   | 0.0263    |
|    learning_rate   | 0.0003    |
|    n_updates       | 80899     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.65e+03 |
|    critic_loss     | 348       |
|    ent_coef        | 0.886     |
|    ent_coef_loss   | 0.0106    |
|    learning_rate   | 0.0003    |
|    n_updates       | 81299     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 123       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.65e+03 |
|    critic_loss     | 394       |
|    ent_coef        | 0.878     |
|    ent_coef_loss   | -0.00629  |
|    learning_rate   | 0.0003    |
|    n_updates       | 81699     |
----------------------------------
=== Iterazione IRL 211 ===
Loss reward (iter 211): 5.266026973724365
=== Iterazione IRL 212 ===
Loss reward (iter 212): 6.68195104598999
=== Iterazione IRL 213 ===
Loss reward (iter 213): 5.34050178527832
=== Iterazione IRL 214 ===
Loss reward (iter 214): 5.1305437088012695
=== Iterazione IRL 215 ===
Loss reward (iter 215): 5.139460563659668
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 153       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.68e+03 |
|    critic_loss     | 308       |
|    ent_coef        | 0.877     |
|    ent_coef_loss   | -0.00607  |
|    learning_rate   | 0.0003    |
|    n_updates       | 81999     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.65e+03 |
|    critic_loss     | 293       |
|    ent_coef        | 0.882     |
|    ent_coef_loss   | 0.00448   |
|    learning_rate   | 0.0003    |
|    n_updates       | 82399     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.69e+03 |
|    critic_loss     | 317       |
|    ent_coef        | 0.871     |
|    ent_coef_loss   | 0.00202   |
|    learning_rate   | 0.0003    |
|    n_updates       | 82799     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.66e+03 |
|    critic_loss     | 291       |
|    ent_coef        | 0.866     |
|    ent_coef_loss   | -0.000457 |
|    learning_rate   | 0.0003    |
|    n_updates       | 83199     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 123       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.63e+03 |
|    critic_loss     | 304       |
|    ent_coef        | 0.858     |
|    ent_coef_loss   | 0.0254    |
|    learning_rate   | 0.0003    |
|    n_updates       | 83599     |
----------------------------------
=== Iterazione IRL 216 ===
Loss reward (iter 216): 3.6812567710876465
=== Iterazione IRL 217 ===
Loss reward (iter 217): 5.510230541229248
=== Iterazione IRL 218 ===
Loss reward (iter 218): 5.6992902755737305
=== Iterazione IRL 219 ===
Loss reward (iter 219): 3.997885227203369
=== Iterazione IRL 220 ===
Loss reward (iter 220): 4.53438663482666
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.62e+03 |
|    critic_loss     | 363       |
|    ent_coef        | 0.887     |
|    ent_coef_loss   | 0.00957   |
|    learning_rate   | 0.0003    |
|    n_updates       | 83899     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.64e+03 |
|    critic_loss     | 314       |
|    ent_coef        | 0.899     |
|    ent_coef_loss   | -0.0127   |
|    learning_rate   | 0.0003    |
|    n_updates       | 84299     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.63e+03 |
|    critic_loss     | 351       |
|    ent_coef        | 0.888     |
|    ent_coef_loss   | -0.007    |
|    learning_rate   | 0.0003    |
|    n_updates       | 84699     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.64e+03 |
|    critic_loss     | 286       |
|    ent_coef        | 0.862     |
|    ent_coef_loss   | -0.0384   |
|    learning_rate   | 0.0003    |
|    n_updates       | 85099     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.59e+03 |
|    critic_loss     | 310       |
|    ent_coef        | 0.852     |
|    ent_coef_loss   | -0.0492   |
|    learning_rate   | 0.0003    |
|    n_updates       | 85499     |
----------------------------------
=== Iterazione IRL 221 ===
Loss reward (iter 221): 4.564010143280029
=== Iterazione IRL 222 ===
Loss reward (iter 222): 5.997783184051514
=== Iterazione IRL 223 ===
Loss reward (iter 223): 7.803651809692383
=== Iterazione IRL 224 ===
Loss reward (iter 224): 6.595649719238281
=== Iterazione IRL 225 ===
Loss reward (iter 225): 7.992931365966797
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.62e+03 |
|    critic_loss     | 233       |
|    ent_coef        | 0.856     |
|    ent_coef_loss   | 0.0372    |
|    learning_rate   | 0.0003    |
|    n_updates       | 85799     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 134       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.59e+03 |
|    critic_loss     | 293       |
|    ent_coef        | 0.866     |
|    ent_coef_loss   | -0.00668  |
|    learning_rate   | 0.0003    |
|    n_updates       | 86199     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.61e+03 |
|    critic_loss     | 331       |
|    ent_coef        | 0.84      |
|    ent_coef_loss   | 0.00724   |
|    learning_rate   | 0.0003    |
|    n_updates       | 86599     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 126       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.59e+03 |
|    critic_loss     | 345       |
|    ent_coef        | 0.832     |
|    ent_coef_loss   | -0.0294   |
|    learning_rate   | 0.0003    |
|    n_updates       | 86999     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.57e+03 |
|    critic_loss     | 326       |
|    ent_coef        | 0.803     |
|    ent_coef_loss   | 0.00274   |
|    learning_rate   | 0.0003    |
|    n_updates       | 87399     |
----------------------------------
=== Iterazione IRL 226 ===
Loss reward (iter 226): 7.81851053237915
=== Iterazione IRL 227 ===
Loss reward (iter 227): 6.1255879402160645
=== Iterazione IRL 228 ===
Loss reward (iter 228): 2.244954824447632
=== Iterazione IRL 229 ===
Loss reward (iter 229): 7.937873840332031
=== Iterazione IRL 230 ===
Loss reward (iter 230): 7.004633903503418
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.56e+03 |
|    critic_loss     | 281       |
|    ent_coef        | 0.796     |
|    ent_coef_loss   | -0.0421   |
|    learning_rate   | 0.0003    |
|    n_updates       | 87699     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 134       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.58e+03 |
|    critic_loss     | 291       |
|    ent_coef        | 0.836     |
|    ent_coef_loss   | 0.0115    |
|    learning_rate   | 0.0003    |
|    n_updates       | 88099     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.56e+03 |
|    critic_loss     | 369       |
|    ent_coef        | 0.847     |
|    ent_coef_loss   | -0.018    |
|    learning_rate   | 0.0003    |
|    n_updates       | 88499     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.57e+03 |
|    critic_loss     | 274       |
|    ent_coef        | 0.847     |
|    ent_coef_loss   | 0.0207    |
|    learning_rate   | 0.0003    |
|    n_updates       | 88899     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.57e+03 |
|    critic_loss     | 321       |
|    ent_coef        | 0.85      |
|    ent_coef_loss   | 0.0232    |
|    learning_rate   | 0.0003    |
|    n_updates       | 89299     |
----------------------------------
=== Iterazione IRL 231 ===
Loss reward (iter 231): 5.10151481628418
=== Iterazione IRL 232 ===
Loss reward (iter 232): 4.988016128540039
=== Iterazione IRL 233 ===
Loss reward (iter 233): 4.334183692932129
=== Iterazione IRL 234 ===
Loss reward (iter 234): 5.0459818840026855
=== Iterazione IRL 235 ===
Loss reward (iter 235): 4.398220062255859
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.53e+03 |
|    critic_loss     | 383       |
|    ent_coef        | 0.885     |
|    ent_coef_loss   | 0.0117    |
|    learning_rate   | 0.0003    |
|    n_updates       | 89599     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 134       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.58e+03 |
|    critic_loss     | 372       |
|    ent_coef        | 0.919     |
|    ent_coef_loss   | 0.00596   |
|    learning_rate   | 0.0003    |
|    n_updates       | 89999     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.55e+03 |
|    critic_loss     | 275       |
|    ent_coef        | 0.941     |
|    ent_coef_loss   | 0.0143    |
|    learning_rate   | 0.0003    |
|    n_updates       | 90399     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.54e+03 |
|    critic_loss     | 286       |
|    ent_coef        | 0.976     |
|    ent_coef_loss   | 0.00323   |
|    learning_rate   | 0.0003    |
|    n_updates       | 90799     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.56e+03 |
|    critic_loss     | 341       |
|    ent_coef        | 1         |
|    ent_coef_loss   | -0.000432 |
|    learning_rate   | 0.0003    |
|    n_updates       | 91199     |
----------------------------------
=== Iterazione IRL 236 ===
Loss reward (iter 236): 4.125259876251221
=== Iterazione IRL 237 ===
Loss reward (iter 237): 4.314584732055664
=== Iterazione IRL 238 ===
Loss reward (iter 238): 4.138471603393555
=== Iterazione IRL 239 ===
Loss reward (iter 239): 2.817478656768799
=== Iterazione IRL 240 ===
Loss reward (iter 240): 2.8321750164031982
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.54e+03 |
|    critic_loss     | 376       |
|    ent_coef        | 1.03      |
|    ent_coef_loss   | -0.00499  |
|    learning_rate   | 0.0003    |
|    n_updates       | 91499     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 134       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.51e+03 |
|    critic_loss     | 452       |
|    ent_coef        | 1.04      |
|    ent_coef_loss   | -0.00978  |
|    learning_rate   | 0.0003    |
|    n_updates       | 91899     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.53e+03 |
|    critic_loss     | 384       |
|    ent_coef        | 1.05      |
|    ent_coef_loss   | -0.00523  |
|    learning_rate   | 0.0003    |
|    n_updates       | 92299     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.52e+03 |
|    critic_loss     | 414       |
|    ent_coef        | 1.02      |
|    ent_coef_loss   | -0.00119  |
|    learning_rate   | 0.0003    |
|    n_updates       | 92699     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.52e+03 |
|    critic_loss     | 432       |
|    ent_coef        | 1.01      |
|    ent_coef_loss   | -0.0004   |
|    learning_rate   | 0.0003    |
|    n_updates       | 93099     |
----------------------------------
=== Iterazione IRL 241 ===
Loss reward (iter 241): 5.861180305480957
=== Iterazione IRL 242 ===
Loss reward (iter 242): 4.688081741333008
=== Iterazione IRL 243 ===
Loss reward (iter 243): 4.348354339599609
=== Iterazione IRL 244 ===
Loss reward (iter 244): 5.327016830444336
=== Iterazione IRL 245 ===
Loss reward (iter 245): 3.789294719696045
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.52e+03 |
|    critic_loss     | 328       |
|    ent_coef        | 1.01      |
|    ent_coef_loss   | 0.000266  |
|    learning_rate   | 0.0003    |
|    n_updates       | 93399     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 134       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.51e+03 |
|    critic_loss     | 272       |
|    ent_coef        | 1.02      |
|    ent_coef_loss   | -0.00156  |
|    learning_rate   | 0.0003    |
|    n_updates       | 93799     |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -1.5e+03 |
|    critic_loss     | 319      |
|    ent_coef        | 1.03     |
|    ent_coef_loss   | 0.000414 |
|    learning_rate   | 0.0003   |
|    n_updates       | 94199    |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.51e+03 |
|    critic_loss     | 317       |
|    ent_coef        | 1.02      |
|    ent_coef_loss   | 0.000958  |
|    learning_rate   | 0.0003    |
|    n_updates       | 94599     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.48e+03 |
|    critic_loss     | 363       |
|    ent_coef        | 1.04      |
|    ent_coef_loss   | 0.00365   |
|    learning_rate   | 0.0003    |
|    n_updates       | 94999     |
----------------------------------
=== Iterazione IRL 246 ===
Loss reward (iter 246): 5.117982864379883
=== Iterazione IRL 247 ===
Loss reward (iter 247): 4.28758430480957
=== Iterazione IRL 248 ===
Loss reward (iter 248): 2.3816943168640137
=== Iterazione IRL 249 ===
Loss reward (iter 249): 4.93466329574585
=== Iterazione IRL 250 ===
Loss reward (iter 250): 3.3849730491638184
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.52e+03 |
|    critic_loss     | 309       |
|    ent_coef        | 1.04      |
|    ent_coef_loss   | -0.000129 |
|    learning_rate   | 0.0003    |
|    n_updates       | 95299     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 134       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.49e+03 |
|    critic_loss     | 320       |
|    ent_coef        | 1.04      |
|    ent_coef_loss   | -0.00126  |
|    learning_rate   | 0.0003    |
|    n_updates       | 95699     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.51e+03 |
|    critic_loss     | 292       |
|    ent_coef        | 1.05      |
|    ent_coef_loss   | 0.00865   |
|    learning_rate   | 0.0003    |
|    n_updates       | 96099     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.48e+03 |
|    critic_loss     | 365       |
|    ent_coef        | 1.09      |
|    ent_coef_loss   | 0.00213   |
|    learning_rate   | 0.0003    |
|    n_updates       | 96499     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.47e+03 |
|    critic_loss     | 374       |
|    ent_coef        | 1.07      |
|    ent_coef_loss   | -0.00402  |
|    learning_rate   | 0.0003    |
|    n_updates       | 96899     |
----------------------------------
=== Iterazione IRL 251 ===
Loss reward (iter 251): 5.672948360443115
=== Iterazione IRL 252 ===
Loss reward (iter 252): 4.107840061187744
=== Iterazione IRL 253 ===
Loss reward (iter 253): 4.020044326782227
=== Iterazione IRL 254 ===
Loss reward (iter 254): 4.593794822692871
=== Iterazione IRL 255 ===
Loss reward (iter 255): 3.4386916160583496
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.48e+03 |
|    critic_loss     | 336       |
|    ent_coef        | 1.07      |
|    ent_coef_loss   | 0.00246   |
|    learning_rate   | 0.0003    |
|    n_updates       | 97199     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 134       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.46e+03 |
|    critic_loss     | 275       |
|    ent_coef        | 1.09      |
|    ent_coef_loss   | -0.0214   |
|    learning_rate   | 0.0003    |
|    n_updates       | 97599     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.49e+03 |
|    critic_loss     | 393       |
|    ent_coef        | 1.09      |
|    ent_coef_loss   | -0.00724  |
|    learning_rate   | 0.0003    |
|    n_updates       | 97999     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 126       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.46e+03 |
|    critic_loss     | 327       |
|    ent_coef        | 1.09      |
|    ent_coef_loss   | 0.000146  |
|    learning_rate   | 0.0003    |
|    n_updates       | 98399     |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 20       |
|    fps             | 124      |
|    time_elapsed    | 16       |
|    total_timesteps | 2000     |
| train/             |          |
|    actor_loss      | -1.5e+03 |
|    critic_loss     | 335      |
|    ent_coef        | 1.14     |
|    ent_coef_loss   | 0.0197   |
|    learning_rate   | 0.0003   |
|    n_updates       | 98799    |
---------------------------------
=== Iterazione IRL 256 ===
Loss reward (iter 256): 6.061897277832031
=== Iterazione IRL 257 ===
Loss reward (iter 257): 5.464325428009033
=== Iterazione IRL 258 ===
Loss reward (iter 258): 5.052585601806641
=== Iterazione IRL 259 ===
Loss reward (iter 259): 4.594950199127197
=== Iterazione IRL 260 ===
Loss reward (iter 260): 5.367948532104492
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.46e+03 |
|    critic_loss     | 356       |
|    ent_coef        | 1.13      |
|    ent_coef_loss   | 0.00907   |
|    learning_rate   | 0.0003    |
|    n_updates       | 99099     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 134       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.45e+03 |
|    critic_loss     | 340       |
|    ent_coef        | 1.14      |
|    ent_coef_loss   | 9.67e-05  |
|    learning_rate   | 0.0003    |
|    n_updates       | 99499     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.46e+03 |
|    critic_loss     | 290       |
|    ent_coef        | 1.13      |
|    ent_coef_loss   | -0.0182   |
|    learning_rate   | 0.0003    |
|    n_updates       | 99899     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 126       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.46e+03 |
|    critic_loss     | 277       |
|    ent_coef        | 1.11      |
|    ent_coef_loss   | 0.0129    |
|    learning_rate   | 0.0003    |
|    n_updates       | 100299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.45e+03 |
|    critic_loss     | 345       |
|    ent_coef        | 1.1       |
|    ent_coef_loss   | -0.0135   |
|    learning_rate   | 0.0003    |
|    n_updates       | 100699    |
----------------------------------
=== Iterazione IRL 261 ===
Loss reward (iter 261): 4.783563137054443
=== Iterazione IRL 262 ===
Loss reward (iter 262): 3.758568525314331
=== Iterazione IRL 263 ===
Loss reward (iter 263): 4.791998863220215
=== Iterazione IRL 264 ===
Loss reward (iter 264): 6.046848773956299
=== Iterazione IRL 265 ===
Loss reward (iter 265): 4.736525535583496
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.43e+03 |
|    critic_loss     | 277       |
|    ent_coef        | 1.11      |
|    ent_coef_loss   | -0.0181   |
|    learning_rate   | 0.0003    |
|    n_updates       | 100999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 134       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.45e+03 |
|    critic_loss     | 313       |
|    ent_coef        | 1.14      |
|    ent_coef_loss   | 0.0149    |
|    learning_rate   | 0.0003    |
|    n_updates       | 101399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.44e+03 |
|    critic_loss     | 367       |
|    ent_coef        | 1.16      |
|    ent_coef_loss   | 0.00519   |
|    learning_rate   | 0.0003    |
|    n_updates       | 101799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 126       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.42e+03 |
|    critic_loss     | 297       |
|    ent_coef        | 1.17      |
|    ent_coef_loss   | 0.0261    |
|    learning_rate   | 0.0003    |
|    n_updates       | 102199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.42e+03 |
|    critic_loss     | 400       |
|    ent_coef        | 1.17      |
|    ent_coef_loss   | -0.0083   |
|    learning_rate   | 0.0003    |
|    n_updates       | 102599    |
----------------------------------
=== Iterazione IRL 266 ===
Loss reward (iter 266): 5.507303237915039
=== Iterazione IRL 267 ===
Loss reward (iter 267): 3.837345600128174
=== Iterazione IRL 268 ===
Loss reward (iter 268): 4.644414901733398
=== Iterazione IRL 269 ===
Loss reward (iter 269): 4.918705463409424
=== Iterazione IRL 270 ===
Loss reward (iter 270): 4.299254894256592
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.43e+03 |
|    critic_loss     | 388       |
|    ent_coef        | 1.19      |
|    ent_coef_loss   | -0.0149   |
|    learning_rate   | 0.0003    |
|    n_updates       | 102899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 134       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.45e+03 |
|    critic_loss     | 285       |
|    ent_coef        | 1.21      |
|    ent_coef_loss   | 0.0472    |
|    learning_rate   | 0.0003    |
|    n_updates       | 103299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.43e+03 |
|    critic_loss     | 313       |
|    ent_coef        | 1.22      |
|    ent_coef_loss   | -0.00691  |
|    learning_rate   | 0.0003    |
|    n_updates       | 103699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.43e+03 |
|    critic_loss     | 298       |
|    ent_coef        | 1.22      |
|    ent_coef_loss   | 0.0189    |
|    learning_rate   | 0.0003    |
|    n_updates       | 104099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 123       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.42e+03 |
|    critic_loss     | 282       |
|    ent_coef        | 1.2       |
|    ent_coef_loss   | 0.00816   |
|    learning_rate   | 0.0003    |
|    n_updates       | 104499    |
----------------------------------
=== Iterazione IRL 271 ===
Loss reward (iter 271): 3.8516685962677
=== Iterazione IRL 272 ===
Loss reward (iter 272): 5.482729911804199
=== Iterazione IRL 273 ===
Loss reward (iter 273): 4.936026096343994
=== Iterazione IRL 274 ===
Loss reward (iter 274): 5.626935958862305
=== Iterazione IRL 275 ===
Loss reward (iter 275): 5.234950542449951
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.46e+03 |
|    critic_loss     | 311       |
|    ent_coef        | 1.19      |
|    ent_coef_loss   | 0.0092    |
|    learning_rate   | 0.0003    |
|    n_updates       | 104799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 130       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.43e+03 |
|    critic_loss     | 283       |
|    ent_coef        | 1.19      |
|    ent_coef_loss   | 0.000155  |
|    learning_rate   | 0.0003    |
|    n_updates       | 105199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 124       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.42e+03 |
|    critic_loss     | 293       |
|    ent_coef        | 1.19      |
|    ent_coef_loss   | -0.0106   |
|    learning_rate   | 0.0003    |
|    n_updates       | 105599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 122       |
|    time_elapsed    | 13        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.41e+03 |
|    critic_loss     | 241       |
|    ent_coef        | 1.21      |
|    ent_coef_loss   | 0.0386    |
|    learning_rate   | 0.0003    |
|    n_updates       | 105999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 120       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.44e+03 |
|    critic_loss     | 276       |
|    ent_coef        | 1.22      |
|    ent_coef_loss   | -0.00428  |
|    learning_rate   | 0.0003    |
|    n_updates       | 106399    |
----------------------------------
=== Iterazione IRL 276 ===
Loss reward (iter 276): 5.6724724769592285
=== Iterazione IRL 277 ===
Loss reward (iter 277): 5.004933834075928
=== Iterazione IRL 278 ===
Loss reward (iter 278): 3.2093381881713867
=== Iterazione IRL 279 ===
Loss reward (iter 279): 4.151248455047607
=== Iterazione IRL 280 ===
Loss reward (iter 280): 5.275139808654785
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 149       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.44e+03 |
|    critic_loss     | 215       |
|    ent_coef        | 1.21      |
|    ent_coef_loss   | -0.0103   |
|    learning_rate   | 0.0003    |
|    n_updates       | 106699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 129       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.43e+03 |
|    critic_loss     | 272       |
|    ent_coef        | 1.21      |
|    ent_coef_loss   | 0.00101   |
|    learning_rate   | 0.0003    |
|    n_updates       | 107099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 123       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.42e+03 |
|    critic_loss     | 301       |
|    ent_coef        | 1.25      |
|    ent_coef_loss   | 0.0293    |
|    learning_rate   | 0.0003    |
|    n_updates       | 107499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 121       |
|    time_elapsed    | 13        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.44e+03 |
|    critic_loss     | 271       |
|    ent_coef        | 1.23      |
|    ent_coef_loss   | -0.0212   |
|    learning_rate   | 0.0003    |
|    n_updates       | 107899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 119       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.41e+03 |
|    critic_loss     | 249       |
|    ent_coef        | 1.23      |
|    ent_coef_loss   | 0.0232    |
|    learning_rate   | 0.0003    |
|    n_updates       | 108299    |
----------------------------------
=== Iterazione IRL 281 ===
Loss reward (iter 281): 5.615909099578857
=== Iterazione IRL 282 ===
Loss reward (iter 282): 4.650191307067871
=== Iterazione IRL 283 ===
Loss reward (iter 283): 5.217012882232666
=== Iterazione IRL 284 ===
Loss reward (iter 284): 4.848248481750488
=== Iterazione IRL 285 ===
Loss reward (iter 285): 5.289644241333008
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 148       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.42e+03 |
|    critic_loss     | 270       |
|    ent_coef        | 1.23      |
|    ent_coef_loss   | 0.0216    |
|    learning_rate   | 0.0003    |
|    n_updates       | 108599    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 129      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -1.4e+03 |
|    critic_loss     | 300      |
|    ent_coef        | 1.24     |
|    ent_coef_loss   | -0.0125  |
|    learning_rate   | 0.0003   |
|    n_updates       | 108999   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 125       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.41e+03 |
|    critic_loss     | 352       |
|    ent_coef        | 1.26      |
|    ent_coef_loss   | 0.0127    |
|    learning_rate   | 0.0003    |
|    n_updates       | 109399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 123       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.43e+03 |
|    critic_loss     | 300       |
|    ent_coef        | 1.27      |
|    ent_coef_loss   | -0.0121   |
|    learning_rate   | 0.0003    |
|    n_updates       | 109799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.41e+03 |
|    critic_loss     | 304       |
|    ent_coef        | 1.27      |
|    ent_coef_loss   | 0.0199    |
|    learning_rate   | 0.0003    |
|    n_updates       | 110199    |
----------------------------------
=== Iterazione IRL 286 ===
Loss reward (iter 286): 4.310863018035889
=== Iterazione IRL 287 ===
Loss reward (iter 287): 4.778274059295654
=== Iterazione IRL 288 ===
Loss reward (iter 288): 4.527345657348633
=== Iterazione IRL 289 ===
Loss reward (iter 289): 5.041667461395264
=== Iterazione IRL 290 ===
Loss reward (iter 290): 4.7220330238342285
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.39e+03 |
|    critic_loss     | 264       |
|    ent_coef        | 1.27      |
|    ent_coef_loss   | -0.00438  |
|    learning_rate   | 0.0003    |
|    n_updates       | 110499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 134       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.43e+03 |
|    critic_loss     | 256       |
|    ent_coef        | 1.31      |
|    ent_coef_loss   | -0.0398   |
|    learning_rate   | 0.0003    |
|    n_updates       | 110899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.39e+03 |
|    critic_loss     | 289       |
|    ent_coef        | 1.31      |
|    ent_coef_loss   | -0.054    |
|    learning_rate   | 0.0003    |
|    n_updates       | 111299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.41e+03 |
|    critic_loss     | 302       |
|    ent_coef        | 1.31      |
|    ent_coef_loss   | -0.0331   |
|    learning_rate   | 0.0003    |
|    n_updates       | 111699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 123       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.42e+03 |
|    critic_loss     | 302       |
|    ent_coef        | 1.32      |
|    ent_coef_loss   | -0.0797   |
|    learning_rate   | 0.0003    |
|    n_updates       | 112099    |
----------------------------------
=== Iterazione IRL 291 ===
Loss reward (iter 291): 5.599844932556152
=== Iterazione IRL 292 ===
Loss reward (iter 292): 5.2010955810546875
=== Iterazione IRL 293 ===
Loss reward (iter 293): 4.738644599914551
=== Iterazione IRL 294 ===
Loss reward (iter 294): 5.61862325668335
=== Iterazione IRL 295 ===
Loss reward (iter 295): 4.733478546142578
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.42e+03 |
|    critic_loss     | 298       |
|    ent_coef        | 1.34      |
|    ent_coef_loss   | -0.0641   |
|    learning_rate   | 0.0003    |
|    n_updates       | 112399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.41e+03 |
|    critic_loss     | 295       |
|    ent_coef        | 1.37      |
|    ent_coef_loss   | 0.0219    |
|    learning_rate   | 0.0003    |
|    n_updates       | 112799    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -1.4e+03 |
|    critic_loss     | 244      |
|    ent_coef        | 1.38     |
|    ent_coef_loss   | 0.0133   |
|    learning_rate   | 0.0003   |
|    n_updates       | 113199   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.43e+03 |
|    critic_loss     | 246       |
|    ent_coef        | 1.4       |
|    ent_coef_loss   | -0.0177   |
|    learning_rate   | 0.0003    |
|    n_updates       | 113599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.39e+03 |
|    critic_loss     | 304       |
|    ent_coef        | 1.44      |
|    ent_coef_loss   | -0.0461   |
|    learning_rate   | 0.0003    |
|    n_updates       | 113999    |
----------------------------------
=== Iterazione IRL 296 ===
Loss reward (iter 296): 5.456568717956543
=== Iterazione IRL 297 ===
Loss reward (iter 297): 5.8121466636657715
=== Iterazione IRL 298 ===
Loss reward (iter 298): 5.466689109802246
=== Iterazione IRL 299 ===
Loss reward (iter 299): 5.603480339050293
=== Iterazione IRL 300 ===
Loss reward (iter 300): 5.600900173187256
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 149       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.4e+03  |
|    critic_loss     | 220       |
|    ent_coef        | 1.47      |
|    ent_coef_loss   | -0.000616 |
|    learning_rate   | 0.0003    |
|    n_updates       | 114299    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 129      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -1.4e+03 |
|    critic_loss     | 238      |
|    ent_coef        | 1.48     |
|    ent_coef_loss   | 0.0458   |
|    learning_rate   | 0.0003   |
|    n_updates       | 114699   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 123       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.41e+03 |
|    critic_loss     | 249       |
|    ent_coef        | 1.47      |
|    ent_coef_loss   | 0.0245    |
|    learning_rate   | 0.0003    |
|    n_updates       | 115099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 121       |
|    time_elapsed    | 13        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.39e+03 |
|    critic_loss     | 306       |
|    ent_coef        | 1.51      |
|    ent_coef_loss   | -0.0282   |
|    learning_rate   | 0.0003    |
|    n_updates       | 115499    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 20       |
|    fps             | 119      |
|    time_elapsed    | 16       |
|    total_timesteps | 2000     |
| train/             |          |
|    actor_loss      | -1.4e+03 |
|    critic_loss     | 209      |
|    ent_coef        | 1.52     |
|    ent_coef_loss   | 0.0774   |
|    learning_rate   | 0.0003   |
|    n_updates       | 115899   |
---------------------------------
=== Iterazione IRL 301 ===
Loss reward (iter 301): 5.732262134552002
=== Iterazione IRL 302 ===
Loss reward (iter 302): 5.960738182067871
=== Iterazione IRL 303 ===
Loss reward (iter 303): 5.746091842651367
=== Iterazione IRL 304 ===
Loss reward (iter 304): 6.025177955627441
=== Iterazione IRL 305 ===
Loss reward (iter 305): 5.594663619995117
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 149       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.41e+03 |
|    critic_loss     | 217       |
|    ent_coef        | 1.51      |
|    ent_coef_loss   | -0.014    |
|    learning_rate   | 0.0003    |
|    n_updates       | 116199    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 129      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -1.4e+03 |
|    critic_loss     | 285      |
|    ent_coef        | 1.55     |
|    ent_coef_loss   | -0.00185 |
|    learning_rate   | 0.0003   |
|    n_updates       | 116599   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 123       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.42e+03 |
|    critic_loss     | 285       |
|    ent_coef        | 1.54      |
|    ent_coef_loss   | 0.00697   |
|    learning_rate   | 0.0003    |
|    n_updates       | 116999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 121       |
|    time_elapsed    | 13        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.39e+03 |
|    critic_loss     | 355       |
|    ent_coef        | 1.55      |
|    ent_coef_loss   | -0.0654   |
|    learning_rate   | 0.0003    |
|    n_updates       | 117399    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 20       |
|    fps             | 120      |
|    time_elapsed    | 16       |
|    total_timesteps | 2000     |
| train/             |          |
|    actor_loss      | -1.4e+03 |
|    critic_loss     | 249      |
|    ent_coef        | 1.54     |
|    ent_coef_loss   | 0.0373   |
|    learning_rate   | 0.0003   |
|    n_updates       | 117799   |
---------------------------------
=== Iterazione IRL 306 ===
Loss reward (iter 306): 5.602853775024414
=== Iterazione IRL 307 ===
Loss reward (iter 307): 6.231400489807129
=== Iterazione IRL 308 ===
Loss reward (iter 308): 5.998591423034668
=== Iterazione IRL 309 ===
Loss reward (iter 309): 6.004801273345947
=== Iterazione IRL 310 ===
Loss reward (iter 310): 6.145451545715332
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 153       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.41e+03 |
|    critic_loss     | 307       |
|    ent_coef        | 1.53      |
|    ent_coef_loss   | 0.0199    |
|    learning_rate   | 0.0003    |
|    n_updates       | 118099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.41e+03 |
|    critic_loss     | 274       |
|    ent_coef        | 1.52      |
|    ent_coef_loss   | -0.0592   |
|    learning_rate   | 0.0003    |
|    n_updates       | 118499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.42e+03 |
|    critic_loss     | 266       |
|    ent_coef        | 1.54      |
|    ent_coef_loss   | 0.021     |
|    learning_rate   | 0.0003    |
|    n_updates       | 118899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.41e+03 |
|    critic_loss     | 221       |
|    ent_coef        | 1.57      |
|    ent_coef_loss   | 0.0759    |
|    learning_rate   | 0.0003    |
|    n_updates       | 119299    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 20       |
|    fps             | 123      |
|    time_elapsed    | 16       |
|    total_timesteps | 2000     |
| train/             |          |
|    actor_loss      | -1.4e+03 |
|    critic_loss     | 274      |
|    ent_coef        | 1.57     |
|    ent_coef_loss   | -0.00114 |
|    learning_rate   | 0.0003   |
|    n_updates       | 119699   |
---------------------------------
=== Iterazione IRL 311 ===
Loss reward (iter 311): 5.8026604652404785
=== Iterazione IRL 312 ===
Loss reward (iter 312): 6.154170513153076
=== Iterazione IRL 313 ===
Loss reward (iter 313): 4.932592868804932
=== Iterazione IRL 314 ===
Loss reward (iter 314): 5.747936725616455
=== Iterazione IRL 315 ===
Loss reward (iter 315): 5.066435813903809
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 153       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.42e+03 |
|    critic_loss     | 272       |
|    ent_coef        | 1.58      |
|    ent_coef_loss   | -0.141    |
|    learning_rate   | 0.0003    |
|    n_updates       | 119999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.41e+03 |
|    critic_loss     | 284       |
|    ent_coef        | 1.56      |
|    ent_coef_loss   | -0.00648  |
|    learning_rate   | 0.0003    |
|    n_updates       | 120399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.39e+03 |
|    critic_loss     | 265       |
|    ent_coef        | 1.57      |
|    ent_coef_loss   | 0.00995   |
|    learning_rate   | 0.0003    |
|    n_updates       | 120799    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 16       |
|    fps             | 124      |
|    time_elapsed    | 12       |
|    total_timesteps | 1600     |
| train/             |          |
|    actor_loss      | -1.4e+03 |
|    critic_loss     | 278      |
|    ent_coef        | 1.57     |
|    ent_coef_loss   | -0.0908  |
|    learning_rate   | 0.0003   |
|    n_updates       | 121199   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.43e+03 |
|    critic_loss     | 294       |
|    ent_coef        | 1.55      |
|    ent_coef_loss   | 0.0248    |
|    learning_rate   | 0.0003    |
|    n_updates       | 121599    |
----------------------------------
=== Iterazione IRL 316 ===
Loss reward (iter 316): 6.936237812042236
=== Iterazione IRL 317 ===
Loss reward (iter 317): 5.930613040924072
=== Iterazione IRL 318 ===
Loss reward (iter 318): 6.9712605476379395
=== Iterazione IRL 319 ===
Loss reward (iter 319): 7.190108299255371
=== Iterazione IRL 320 ===
Loss reward (iter 320): 6.4524617195129395
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -1.4e+03 |
|    critic_loss     | 189      |
|    ent_coef        | 1.56     |
|    ent_coef_loss   | 0.00149  |
|    learning_rate   | 0.0003   |
|    n_updates       | 121899   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.37e+03 |
|    critic_loss     | 294       |
|    ent_coef        | 1.58      |
|    ent_coef_loss   | -0.0332   |
|    learning_rate   | 0.0003    |
|    n_updates       | 122299    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -1.4e+03 |
|    critic_loss     | 263      |
|    ent_coef        | 1.58     |
|    ent_coef_loss   | -0.0199  |
|    learning_rate   | 0.0003   |
|    n_updates       | 122699   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.41e+03 |
|    critic_loss     | 243       |
|    ent_coef        | 1.58      |
|    ent_coef_loss   | 0.0576    |
|    learning_rate   | 0.0003    |
|    n_updates       | 123099    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 20       |
|    fps             | 122      |
|    time_elapsed    | 16       |
|    total_timesteps | 2000     |
| train/             |          |
|    actor_loss      | -1.4e+03 |
|    critic_loss     | 215      |
|    ent_coef        | 1.57     |
|    ent_coef_loss   | -0.0607  |
|    learning_rate   | 0.0003   |
|    n_updates       | 123499   |
---------------------------------
=== Iterazione IRL 321 ===
Loss reward (iter 321): 6.020491600036621
=== Iterazione IRL 322 ===
Loss reward (iter 322): 5.937275409698486
=== Iterazione IRL 323 ===
Loss reward (iter 323): 6.023255348205566
=== Iterazione IRL 324 ===
Loss reward (iter 324): 6.009040355682373
=== Iterazione IRL 325 ===
Loss reward (iter 325): 5.929617404937744
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.43e+03 |
|    critic_loss     | 225       |
|    ent_coef        | 1.57      |
|    ent_coef_loss   | -0.0638   |
|    learning_rate   | 0.0003    |
|    n_updates       | 123799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.42e+03 |
|    critic_loss     | 207       |
|    ent_coef        | 1.56      |
|    ent_coef_loss   | -0.0234   |
|    learning_rate   | 0.0003    |
|    n_updates       | 124199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.44e+03 |
|    critic_loss     | 266       |
|    ent_coef        | 1.52      |
|    ent_coef_loss   | -0.0138   |
|    learning_rate   | 0.0003    |
|    n_updates       | 124599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.41e+03 |
|    critic_loss     | 210       |
|    ent_coef        | 1.55      |
|    ent_coef_loss   | 0.0115    |
|    learning_rate   | 0.0003    |
|    n_updates       | 124999    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 20       |
|    fps             | 123      |
|    time_elapsed    | 16       |
|    total_timesteps | 2000     |
| train/             |          |
|    actor_loss      | -1.4e+03 |
|    critic_loss     | 236      |
|    ent_coef        | 1.58     |
|    ent_coef_loss   | -0.00847 |
|    learning_rate   | 0.0003   |
|    n_updates       | 125399   |
---------------------------------
=== Iterazione IRL 326 ===
Loss reward (iter 326): 6.155559539794922
=== Iterazione IRL 327 ===
Loss reward (iter 327): 6.85664176940918
=== Iterazione IRL 328 ===
Loss reward (iter 328): 5.964494705200195
=== Iterazione IRL 329 ===
Loss reward (iter 329): 6.946624279022217
=== Iterazione IRL 330 ===
Loss reward (iter 330): 6.631209850311279
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.42e+03 |
|    critic_loss     | 274       |
|    ent_coef        | 1.56      |
|    ent_coef_loss   | 0.00417   |
|    learning_rate   | 0.0003    |
|    n_updates       | 125699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.41e+03 |
|    critic_loss     | 205       |
|    ent_coef        | 1.55      |
|    ent_coef_loss   | 0.0241    |
|    learning_rate   | 0.0003    |
|    n_updates       | 126099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.42e+03 |
|    critic_loss     | 288       |
|    ent_coef        | 1.56      |
|    ent_coef_loss   | -0.0052   |
|    learning_rate   | 0.0003    |
|    n_updates       | 126499    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 16       |
|    fps             | 124      |
|    time_elapsed    | 12       |
|    total_timesteps | 1600     |
| train/             |          |
|    actor_loss      | -1.4e+03 |
|    critic_loss     | 233      |
|    ent_coef        | 1.58     |
|    ent_coef_loss   | -0.00327 |
|    learning_rate   | 0.0003   |
|    n_updates       | 126899   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.43e+03 |
|    critic_loss     | 332       |
|    ent_coef        | 1.57      |
|    ent_coef_loss   | -0.0342   |
|    learning_rate   | 0.0003    |
|    n_updates       | 127299    |
----------------------------------
=== Iterazione IRL 331 ===
Loss reward (iter 331): 6.176151752471924
=== Iterazione IRL 332 ===
Loss reward (iter 332): 6.227614402770996
=== Iterazione IRL 333 ===
Loss reward (iter 333): 6.962063789367676
=== Iterazione IRL 334 ===
Loss reward (iter 334): 6.747611999511719
=== Iterazione IRL 335 ===
Loss reward (iter 335): 6.447063446044922
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.44e+03 |
|    critic_loss     | 232       |
|    ent_coef        | 1.58      |
|    ent_coef_loss   | 0.0376    |
|    learning_rate   | 0.0003    |
|    n_updates       | 127599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.43e+03 |
|    critic_loss     | 237       |
|    ent_coef        | 1.55      |
|    ent_coef_loss   | 0.0664    |
|    learning_rate   | 0.0003    |
|    n_updates       | 127999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.41e+03 |
|    critic_loss     | 294       |
|    ent_coef        | 1.54      |
|    ent_coef_loss   | -0.0845   |
|    learning_rate   | 0.0003    |
|    n_updates       | 128399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.42e+03 |
|    critic_loss     | 247       |
|    ent_coef        | 1.53      |
|    ent_coef_loss   | -0.0018   |
|    learning_rate   | 0.0003    |
|    n_updates       | 128799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.44e+03 |
|    critic_loss     | 233       |
|    ent_coef        | 1.54      |
|    ent_coef_loss   | 0.084     |
|    learning_rate   | 0.0003    |
|    n_updates       | 129199    |
----------------------------------
=== Iterazione IRL 336 ===
Loss reward (iter 336): 6.0955071449279785
=== Iterazione IRL 337 ===
Loss reward (iter 337): 5.532214164733887
=== Iterazione IRL 338 ===
Loss reward (iter 338): 6.540670394897461
=== Iterazione IRL 339 ===
Loss reward (iter 339): 5.847572326660156
=== Iterazione IRL 340 ===
Loss reward (iter 340): 6.293277740478516
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.44e+03 |
|    critic_loss     | 275       |
|    ent_coef        | 1.54      |
|    ent_coef_loss   | -0.0141   |
|    learning_rate   | 0.0003    |
|    n_updates       | 129499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.44e+03 |
|    critic_loss     | 198       |
|    ent_coef        | 1.56      |
|    ent_coef_loss   | 0.0177    |
|    learning_rate   | 0.0003    |
|    n_updates       | 129899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.43e+03 |
|    critic_loss     | 328       |
|    ent_coef        | 1.53      |
|    ent_coef_loss   | 0.00799   |
|    learning_rate   | 0.0003    |
|    n_updates       | 130299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.43e+03 |
|    critic_loss     | 222       |
|    ent_coef        | 1.57      |
|    ent_coef_loss   | 0.0923    |
|    learning_rate   | 0.0003    |
|    n_updates       | 130699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.41e+03 |
|    critic_loss     | 257       |
|    ent_coef        | 1.57      |
|    ent_coef_loss   | -0.0949   |
|    learning_rate   | 0.0003    |
|    n_updates       | 131099    |
----------------------------------
=== Iterazione IRL 341 ===
Loss reward (iter 341): 6.147559642791748
=== Iterazione IRL 342 ===
Loss reward (iter 342): 6.000261306762695
=== Iterazione IRL 343 ===
Loss reward (iter 343): 6.158586502075195
=== Iterazione IRL 344 ===
Loss reward (iter 344): 5.9270148277282715
=== Iterazione IRL 345 ===
Loss reward (iter 345): 6.25833797454834
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.42e+03 |
|    critic_loss     | 273       |
|    ent_coef        | 1.56      |
|    ent_coef_loss   | -0.0163   |
|    learning_rate   | 0.0003    |
|    n_updates       | 131399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.42e+03 |
|    critic_loss     | 254       |
|    ent_coef        | 1.57      |
|    ent_coef_loss   | -0.0573   |
|    learning_rate   | 0.0003    |
|    n_updates       | 131799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.44e+03 |
|    critic_loss     | 184       |
|    ent_coef        | 1.57      |
|    ent_coef_loss   | 0.0265    |
|    learning_rate   | 0.0003    |
|    n_updates       | 132199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.43e+03 |
|    critic_loss     | 277       |
|    ent_coef        | 1.6       |
|    ent_coef_loss   | 0.0628    |
|    learning_rate   | 0.0003    |
|    n_updates       | 132599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.44e+03 |
|    critic_loss     | 290       |
|    ent_coef        | 1.58      |
|    ent_coef_loss   | 0.014     |
|    learning_rate   | 0.0003    |
|    n_updates       | 132999    |
----------------------------------
=== Iterazione IRL 346 ===
Loss reward (iter 346): 6.4022040367126465
=== Iterazione IRL 347 ===
Loss reward (iter 347): 6.27733039855957
=== Iterazione IRL 348 ===
Loss reward (iter 348): 5.8178582191467285
=== Iterazione IRL 349 ===
Loss reward (iter 349): 6.355781078338623
=== Iterazione IRL 350 ===
Loss reward (iter 350): 6.0232110023498535
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.43e+03 |
|    critic_loss     | 168       |
|    ent_coef        | 1.56      |
|    ent_coef_loss   | -0.0248   |
|    learning_rate   | 0.0003    |
|    n_updates       | 133299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.42e+03 |
|    critic_loss     | 263       |
|    ent_coef        | 1.56      |
|    ent_coef_loss   | -0.0295   |
|    learning_rate   | 0.0003    |
|    n_updates       | 133699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.42e+03 |
|    critic_loss     | 262       |
|    ent_coef        | 1.55      |
|    ent_coef_loss   | 0.0175    |
|    learning_rate   | 0.0003    |
|    n_updates       | 134099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.45e+03 |
|    critic_loss     | 208       |
|    ent_coef        | 1.56      |
|    ent_coef_loss   | -0.0552   |
|    learning_rate   | 0.0003    |
|    n_updates       | 134499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.42e+03 |
|    critic_loss     | 240       |
|    ent_coef        | 1.59      |
|    ent_coef_loss   | -0.00655  |
|    learning_rate   | 0.0003    |
|    n_updates       | 134899    |
----------------------------------
=== Iterazione IRL 351 ===
Loss reward (iter 351): 6.39932107925415
=== Iterazione IRL 352 ===
Loss reward (iter 352): 6.4705352783203125
=== Iterazione IRL 353 ===
Loss reward (iter 353): 6.582144260406494
=== Iterazione IRL 354 ===
Loss reward (iter 354): 5.459941387176514
=== Iterazione IRL 355 ===
Loss reward (iter 355): 6.466853141784668
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.46e+03 |
|    critic_loss     | 256       |
|    ent_coef        | 1.58      |
|    ent_coef_loss   | 0.0882    |
|    learning_rate   | 0.0003    |
|    n_updates       | 135199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.47e+03 |
|    critic_loss     | 229       |
|    ent_coef        | 1.6       |
|    ent_coef_loss   | 0.068     |
|    learning_rate   | 0.0003    |
|    n_updates       | 135599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.44e+03 |
|    critic_loss     | 248       |
|    ent_coef        | 1.59      |
|    ent_coef_loss   | -0.00237  |
|    learning_rate   | 0.0003    |
|    n_updates       | 135999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.43e+03 |
|    critic_loss     | 310       |
|    ent_coef        | 1.6       |
|    ent_coef_loss   | -0.0693   |
|    learning_rate   | 0.0003    |
|    n_updates       | 136399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.45e+03 |
|    critic_loss     | 286       |
|    ent_coef        | 1.6       |
|    ent_coef_loss   | 0.0206    |
|    learning_rate   | 0.0003    |
|    n_updates       | 136799    |
----------------------------------
=== Iterazione IRL 356 ===
Loss reward (iter 356): 6.214670181274414
=== Iterazione IRL 357 ===
Loss reward (iter 357): 6.115985870361328
=== Iterazione IRL 358 ===
Loss reward (iter 358): 6.120004653930664
=== Iterazione IRL 359 ===
Loss reward (iter 359): 5.85880708694458
=== Iterazione IRL 360 ===
Loss reward (iter 360): 5.868494987487793
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.44e+03 |
|    critic_loss     | 291       |
|    ent_coef        | 1.61      |
|    ent_coef_loss   | 0.0314    |
|    learning_rate   | 0.0003    |
|    n_updates       | 137099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.45e+03 |
|    critic_loss     | 197       |
|    ent_coef        | 1.6       |
|    ent_coef_loss   | 0.0795    |
|    learning_rate   | 0.0003    |
|    n_updates       | 137499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.46e+03 |
|    critic_loss     | 252       |
|    ent_coef        | 1.62      |
|    ent_coef_loss   | 0.117     |
|    learning_rate   | 0.0003    |
|    n_updates       | 137899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.42e+03 |
|    critic_loss     | 230       |
|    ent_coef        | 1.61      |
|    ent_coef_loss   | -0.085    |
|    learning_rate   | 0.0003    |
|    n_updates       | 138299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.45e+03 |
|    critic_loss     | 252       |
|    ent_coef        | 1.61      |
|    ent_coef_loss   | -0.0417   |
|    learning_rate   | 0.0003    |
|    n_updates       | 138699    |
----------------------------------
=== Iterazione IRL 361 ===
Loss reward (iter 361): 5.852100372314453
=== Iterazione IRL 362 ===
Loss reward (iter 362): 6.6160736083984375
=== Iterazione IRL 363 ===
Loss reward (iter 363): 5.923072338104248
=== Iterazione IRL 364 ===
Loss reward (iter 364): 6.108581066131592
=== Iterazione IRL 365 ===
Loss reward (iter 365): 5.241148471832275
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.47e+03 |
|    critic_loss     | 221       |
|    ent_coef        | 1.6       |
|    ent_coef_loss   | 0.0867    |
|    learning_rate   | 0.0003    |
|    n_updates       | 138999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.44e+03 |
|    critic_loss     | 241       |
|    ent_coef        | 1.58      |
|    ent_coef_loss   | 0.0477    |
|    learning_rate   | 0.0003    |
|    n_updates       | 139399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.47e+03 |
|    critic_loss     | 134       |
|    ent_coef        | 1.61      |
|    ent_coef_loss   | 0.0651    |
|    learning_rate   | 0.0003    |
|    n_updates       | 139799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.44e+03 |
|    critic_loss     | 181       |
|    ent_coef        | 1.61      |
|    ent_coef_loss   | 0.0496    |
|    learning_rate   | 0.0003    |
|    n_updates       | 140199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.46e+03 |
|    critic_loss     | 259       |
|    ent_coef        | 1.62      |
|    ent_coef_loss   | 0.00989   |
|    learning_rate   | 0.0003    |
|    n_updates       | 140599    |
----------------------------------
=== Iterazione IRL 366 ===
Loss reward (iter 366): 5.514262676239014
=== Iterazione IRL 367 ===
Loss reward (iter 367): 6.148628234863281
=== Iterazione IRL 368 ===
Loss reward (iter 368): 6.0976881980896
=== Iterazione IRL 369 ===
Loss reward (iter 369): 6.209421157836914
=== Iterazione IRL 370 ===
Loss reward (iter 370): 5.748894691467285
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.45e+03 |
|    critic_loss     | 187       |
|    ent_coef        | 1.63      |
|    ent_coef_loss   | -0.0146   |
|    learning_rate   | 0.0003    |
|    n_updates       | 140899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.45e+03 |
|    critic_loss     | 210       |
|    ent_coef        | 1.63      |
|    ent_coef_loss   | -0.0609   |
|    learning_rate   | 0.0003    |
|    n_updates       | 141299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.44e+03 |
|    critic_loss     | 210       |
|    ent_coef        | 1.64      |
|    ent_coef_loss   | -0.0269   |
|    learning_rate   | 0.0003    |
|    n_updates       | 141699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.46e+03 |
|    critic_loss     | 197       |
|    ent_coef        | 1.62      |
|    ent_coef_loss   | -0.0227   |
|    learning_rate   | 0.0003    |
|    n_updates       | 142099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.47e+03 |
|    critic_loss     | 213       |
|    ent_coef        | 1.62      |
|    ent_coef_loss   | 0.0164    |
|    learning_rate   | 0.0003    |
|    n_updates       | 142499    |
----------------------------------
=== Iterazione IRL 371 ===
Loss reward (iter 371): 6.509603500366211
=== Iterazione IRL 372 ===
Loss reward (iter 372): 6.188910961151123
=== Iterazione IRL 373 ===
Loss reward (iter 373): 6.011528968811035
=== Iterazione IRL 374 ===
Loss reward (iter 374): 6.038501262664795
=== Iterazione IRL 375 ===
Loss reward (iter 375): 6.177996635437012
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.47e+03 |
|    critic_loss     | 205       |
|    ent_coef        | 1.62      |
|    ent_coef_loss   | 0.0403    |
|    learning_rate   | 0.0003    |
|    n_updates       | 142799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.46e+03 |
|    critic_loss     | 198       |
|    ent_coef        | 1.65      |
|    ent_coef_loss   | -0.108    |
|    learning_rate   | 0.0003    |
|    n_updates       | 143199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.47e+03 |
|    critic_loss     | 239       |
|    ent_coef        | 1.7       |
|    ent_coef_loss   | -0.0323   |
|    learning_rate   | 0.0003    |
|    n_updates       | 143599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.47e+03 |
|    critic_loss     | 195       |
|    ent_coef        | 1.69      |
|    ent_coef_loss   | 0.028     |
|    learning_rate   | 0.0003    |
|    n_updates       | 143999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.45e+03 |
|    critic_loss     | 252       |
|    ent_coef        | 1.7       |
|    ent_coef_loss   | 0.0458    |
|    learning_rate   | 0.0003    |
|    n_updates       | 144399    |
----------------------------------
=== Iterazione IRL 376 ===
Loss reward (iter 376): 5.360278129577637
=== Iterazione IRL 377 ===
Loss reward (iter 377): 6.532392978668213
=== Iterazione IRL 378 ===
Loss reward (iter 378): 6.1640424728393555
=== Iterazione IRL 379 ===
Loss reward (iter 379): 5.790305137634277
=== Iterazione IRL 380 ===
Loss reward (iter 380): 5.3786725997924805
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.48e+03 |
|    critic_loss     | 242       |
|    ent_coef        | 1.7       |
|    ent_coef_loss   | 0.138     |
|    learning_rate   | 0.0003    |
|    n_updates       | 144699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.45e+03 |
|    critic_loss     | 273       |
|    ent_coef        | 1.68      |
|    ent_coef_loss   | -0.0501   |
|    learning_rate   | 0.0003    |
|    n_updates       | 145099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.48e+03 |
|    critic_loss     | 196       |
|    ent_coef        | 1.69      |
|    ent_coef_loss   | 0.0107    |
|    learning_rate   | 0.0003    |
|    n_updates       | 145499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.46e+03 |
|    critic_loss     | 238       |
|    ent_coef        | 1.65      |
|    ent_coef_loss   | -0.0209   |
|    learning_rate   | 0.0003    |
|    n_updates       | 145899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.48e+03 |
|    critic_loss     | 299       |
|    ent_coef        | 1.67      |
|    ent_coef_loss   | -0.0494   |
|    learning_rate   | 0.0003    |
|    n_updates       | 146299    |
----------------------------------
=== Iterazione IRL 381 ===
Loss reward (iter 381): 6.282706260681152
=== Iterazione IRL 382 ===
Loss reward (iter 382): 5.883918285369873
=== Iterazione IRL 383 ===
Loss reward (iter 383): 6.122254848480225
=== Iterazione IRL 384 ===
Loss reward (iter 384): 5.54841947555542
=== Iterazione IRL 385 ===
Loss reward (iter 385): 6.138763427734375
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.47e+03 |
|    critic_loss     | 186       |
|    ent_coef        | 1.67      |
|    ent_coef_loss   | -0.00916  |
|    learning_rate   | 0.0003    |
|    n_updates       | 146599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.46e+03 |
|    critic_loss     | 214       |
|    ent_coef        | 1.69      |
|    ent_coef_loss   | -0.000121 |
|    learning_rate   | 0.0003    |
|    n_updates       | 146999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.49e+03 |
|    critic_loss     | 200       |
|    ent_coef        | 1.7       |
|    ent_coef_loss   | -0.00997  |
|    learning_rate   | 0.0003    |
|    n_updates       | 147399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.47e+03 |
|    critic_loss     | 144       |
|    ent_coef        | 1.71      |
|    ent_coef_loss   | 0.0748    |
|    learning_rate   | 0.0003    |
|    n_updates       | 147799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 123       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.49e+03 |
|    critic_loss     | 220       |
|    ent_coef        | 1.7       |
|    ent_coef_loss   | 0.0346    |
|    learning_rate   | 0.0003    |
|    n_updates       | 148199    |
----------------------------------
=== Iterazione IRL 386 ===
Loss reward (iter 386): 5.632445335388184
=== Iterazione IRL 387 ===
Loss reward (iter 387): 5.4139018058776855
=== Iterazione IRL 388 ===
Loss reward (iter 388): 6.048384189605713
=== Iterazione IRL 389 ===
Loss reward (iter 389): 5.698892593383789
=== Iterazione IRL 390 ===
Loss reward (iter 390): 6.35508394241333
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.48e+03 |
|    critic_loss     | 333       |
|    ent_coef        | 1.67      |
|    ent_coef_loss   | -0.0245   |
|    learning_rate   | 0.0003    |
|    n_updates       | 148499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.47e+03 |
|    critic_loss     | 262       |
|    ent_coef        | 1.68      |
|    ent_coef_loss   | -0.0106   |
|    learning_rate   | 0.0003    |
|    n_updates       | 148899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.49e+03 |
|    critic_loss     | 199       |
|    ent_coef        | 1.67      |
|    ent_coef_loss   | 0.0681    |
|    learning_rate   | 0.0003    |
|    n_updates       | 149299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.49e+03 |
|    critic_loss     | 197       |
|    ent_coef        | 1.67      |
|    ent_coef_loss   | -0.154    |
|    learning_rate   | 0.0003    |
|    n_updates       | 149699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 123       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.48e+03 |
|    critic_loss     | 212       |
|    ent_coef        | 1.67      |
|    ent_coef_loss   | 0.00239   |
|    learning_rate   | 0.0003    |
|    n_updates       | 150099    |
----------------------------------
=== Iterazione IRL 391 ===
Loss reward (iter 391): 5.689887046813965
=== Iterazione IRL 392 ===
Loss reward (iter 392): 5.986361980438232
=== Iterazione IRL 393 ===
Loss reward (iter 393): 6.449646472930908
=== Iterazione IRL 394 ===
Loss reward (iter 394): 5.476686477661133
=== Iterazione IRL 395 ===
Loss reward (iter 395): 5.5076727867126465
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.49e+03 |
|    critic_loss     | 226       |
|    ent_coef        | 1.64      |
|    ent_coef_loss   | -0.0735   |
|    learning_rate   | 0.0003    |
|    n_updates       | 150399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 134       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.48e+03 |
|    critic_loss     | 192       |
|    ent_coef        | 1.63      |
|    ent_coef_loss   | -0.126    |
|    learning_rate   | 0.0003    |
|    n_updates       | 150799    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -1.5e+03 |
|    critic_loss     | 164      |
|    ent_coef        | 1.62     |
|    ent_coef_loss   | 0.0325   |
|    learning_rate   | 0.0003   |
|    n_updates       | 151199   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.48e+03 |
|    critic_loss     | 202       |
|    ent_coef        | 1.61      |
|    ent_coef_loss   | -0.0686   |
|    learning_rate   | 0.0003    |
|    n_updates       | 151599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 123       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.48e+03 |
|    critic_loss     | 227       |
|    ent_coef        | 1.6       |
|    ent_coef_loss   | -0.0438   |
|    learning_rate   | 0.0003    |
|    n_updates       | 151999    |
----------------------------------
=== Iterazione IRL 396 ===
Loss reward (iter 396): 4.90077018737793
=== Iterazione IRL 397 ===
Loss reward (iter 397): 5.5726823806762695
=== Iterazione IRL 398 ===
Loss reward (iter 398): 5.056948661804199
=== Iterazione IRL 399 ===
Loss reward (iter 399): 5.19525671005249
=== Iterazione IRL 400 ===
Loss reward (iter 400): 5.245217323303223
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.49e+03 |
|    critic_loss     | 214       |
|    ent_coef        | 1.59      |
|    ent_coef_loss   | 0.0212    |
|    learning_rate   | 0.0003    |
|    n_updates       | 152299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.49e+03 |
|    critic_loss     | 287       |
|    ent_coef        | 1.58      |
|    ent_coef_loss   | -0.08     |
|    learning_rate   | 0.0003    |
|    n_updates       | 152699    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -1.5e+03 |
|    critic_loss     | 158      |
|    ent_coef        | 1.59     |
|    ent_coef_loss   | 0.000994 |
|    learning_rate   | 0.0003   |
|    n_updates       | 153099   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.49e+03 |
|    critic_loss     | 195       |
|    ent_coef        | 1.59      |
|    ent_coef_loss   | -0.02     |
|    learning_rate   | 0.0003    |
|    n_updates       | 153499    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 20       |
|    fps             | 124      |
|    time_elapsed    | 16       |
|    total_timesteps | 2000     |
| train/             |          |
|    actor_loss      | -1.5e+03 |
|    critic_loss     | 202      |
|    ent_coef        | 1.55     |
|    ent_coef_loss   | 0.0117   |
|    learning_rate   | 0.0003   |
|    n_updates       | 153899   |
---------------------------------
=== Iterazione IRL 401 ===
Loss reward (iter 401): 5.514331340789795
=== Iterazione IRL 402 ===
Loss reward (iter 402): 5.626364707946777
=== Iterazione IRL 403 ===
Loss reward (iter 403): 6.520338535308838
=== Iterazione IRL 404 ===
Loss reward (iter 404): 4.938132286071777
=== Iterazione IRL 405 ===
Loss reward (iter 405): 5.911182403564453
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.49e+03 |
|    critic_loss     | 185       |
|    ent_coef        | 1.51      |
|    ent_coef_loss   | -0.0549   |
|    learning_rate   | 0.0003    |
|    n_updates       | 154199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 134       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.51e+03 |
|    critic_loss     | 199       |
|    ent_coef        | 1.5       |
|    ent_coef_loss   | -0.0492   |
|    learning_rate   | 0.0003    |
|    n_updates       | 154599    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -1.5e+03 |
|    critic_loss     | 243      |
|    ent_coef        | 1.52     |
|    ent_coef_loss   | 0.0299   |
|    learning_rate   | 0.0003   |
|    n_updates       | 154999   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 126       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.51e+03 |
|    critic_loss     | 244       |
|    ent_coef        | 1.51      |
|    ent_coef_loss   | 0.039     |
|    learning_rate   | 0.0003    |
|    n_updates       | 155399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.51e+03 |
|    critic_loss     | 208       |
|    ent_coef        | 1.51      |
|    ent_coef_loss   | -0.013    |
|    learning_rate   | 0.0003    |
|    n_updates       | 155799    |
----------------------------------
=== Iterazione IRL 406 ===
Loss reward (iter 406): 6.1619110107421875
=== Iterazione IRL 407 ===
Loss reward (iter 407): 5.798548221588135
=== Iterazione IRL 408 ===
Loss reward (iter 408): 5.130136013031006
=== Iterazione IRL 409 ===
Loss reward (iter 409): 5.121287822723389
=== Iterazione IRL 410 ===
Loss reward (iter 410): 5.703975677490234
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.49e+03 |
|    critic_loss     | 192       |
|    ent_coef        | 1.47      |
|    ent_coef_loss   | 0.0381    |
|    learning_rate   | 0.0003    |
|    n_updates       | 156099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.51e+03 |
|    critic_loss     | 256       |
|    ent_coef        | 1.48      |
|    ent_coef_loss   | 0.0456    |
|    learning_rate   | 0.0003    |
|    n_updates       | 156499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.49e+03 |
|    critic_loss     | 212       |
|    ent_coef        | 1.49      |
|    ent_coef_loss   | 0.0162    |
|    learning_rate   | 0.0003    |
|    n_updates       | 156899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.51e+03 |
|    critic_loss     | 174       |
|    ent_coef        | 1.48      |
|    ent_coef_loss   | 0.082     |
|    learning_rate   | 0.0003    |
|    n_updates       | 157299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 123       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.51e+03 |
|    critic_loss     | 179       |
|    ent_coef        | 1.48      |
|    ent_coef_loss   | 0.0288    |
|    learning_rate   | 0.0003    |
|    n_updates       | 157699    |
----------------------------------
=== Iterazione IRL 411 ===
Loss reward (iter 411): 5.5329694747924805
=== Iterazione IRL 412 ===
Loss reward (iter 412): 5.099702835083008
=== Iterazione IRL 413 ===
Loss reward (iter 413): 5.956963062286377
=== Iterazione IRL 414 ===
Loss reward (iter 414): 6.098767280578613
=== Iterazione IRL 415 ===
Loss reward (iter 415): 5.570823669433594
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.52e+03 |
|    critic_loss     | 153       |
|    ent_coef        | 1.47      |
|    ent_coef_loss   | 0.000162  |
|    learning_rate   | 0.0003    |
|    n_updates       | 157999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.51e+03 |
|    critic_loss     | 184       |
|    ent_coef        | 1.46      |
|    ent_coef_loss   | 0.0227    |
|    learning_rate   | 0.0003    |
|    n_updates       | 158399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.51e+03 |
|    critic_loss     | 224       |
|    ent_coef        | 1.46      |
|    ent_coef_loss   | 0.022     |
|    learning_rate   | 0.0003    |
|    n_updates       | 158799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.51e+03 |
|    critic_loss     | 156       |
|    ent_coef        | 1.44      |
|    ent_coef_loss   | 0.0333    |
|    learning_rate   | 0.0003    |
|    n_updates       | 159199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 123       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.52e+03 |
|    critic_loss     | 186       |
|    ent_coef        | 1.42      |
|    ent_coef_loss   | 0.0324    |
|    learning_rate   | 0.0003    |
|    n_updates       | 159599    |
----------------------------------
=== Iterazione IRL 416 ===
Loss reward (iter 416): 6.477121829986572
=== Iterazione IRL 417 ===
Loss reward (iter 417): 5.3329243659973145
=== Iterazione IRL 418 ===
Loss reward (iter 418): 5.027935981750488
=== Iterazione IRL 419 ===
Loss reward (iter 419): 5.756408214569092
=== Iterazione IRL 420 ===
Loss reward (iter 420): 5.707638263702393
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.53e+03 |
|    critic_loss     | 139       |
|    ent_coef        | 1.41      |
|    ent_coef_loss   | 0.0138    |
|    learning_rate   | 0.0003    |
|    n_updates       | 159899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.52e+03 |
|    critic_loss     | 164       |
|    ent_coef        | 1.43      |
|    ent_coef_loss   | -0.0257   |
|    learning_rate   | 0.0003    |
|    n_updates       | 160299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.51e+03 |
|    critic_loss     | 189       |
|    ent_coef        | 1.44      |
|    ent_coef_loss   | 0.000946  |
|    learning_rate   | 0.0003    |
|    n_updates       | 160699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.51e+03 |
|    critic_loss     | 171       |
|    ent_coef        | 1.44      |
|    ent_coef_loss   | -0.041    |
|    learning_rate   | 0.0003    |
|    n_updates       | 161099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 123       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.51e+03 |
|    critic_loss     | 167       |
|    ent_coef        | 1.46      |
|    ent_coef_loss   | -0.00745  |
|    learning_rate   | 0.0003    |
|    n_updates       | 161499    |
----------------------------------
=== Iterazione IRL 421 ===
Loss reward (iter 421): 5.736999988555908
=== Iterazione IRL 422 ===
Loss reward (iter 422): 5.605297088623047
=== Iterazione IRL 423 ===
Loss reward (iter 423): 5.679092884063721
=== Iterazione IRL 424 ===
Loss reward (iter 424): 5.426560878753662
=== Iterazione IRL 425 ===
Loss reward (iter 425): 5.6180644035339355
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.51e+03 |
|    critic_loss     | 233       |
|    ent_coef        | 1.47      |
|    ent_coef_loss   | -0.0129   |
|    learning_rate   | 0.0003    |
|    n_updates       | 161799    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -1.5e+03 |
|    critic_loss     | 158      |
|    ent_coef        | 1.48     |
|    ent_coef_loss   | 0.00962  |
|    learning_rate   | 0.0003   |
|    n_updates       | 162199   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.53e+03 |
|    critic_loss     | 195       |
|    ent_coef        | 1.45      |
|    ent_coef_loss   | 0.0548    |
|    learning_rate   | 0.0003    |
|    n_updates       | 162599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.52e+03 |
|    critic_loss     | 212       |
|    ent_coef        | 1.46      |
|    ent_coef_loss   | 0.037     |
|    learning_rate   | 0.0003    |
|    n_updates       | 162999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 123       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.51e+03 |
|    critic_loss     | 154       |
|    ent_coef        | 1.46      |
|    ent_coef_loss   | -0.0291   |
|    learning_rate   | 0.0003    |
|    n_updates       | 163399    |
----------------------------------
=== Iterazione IRL 426 ===
Loss reward (iter 426): 5.73956298828125
=== Iterazione IRL 427 ===
Loss reward (iter 427): 6.190688133239746
=== Iterazione IRL 428 ===
Loss reward (iter 428): 5.8392415046691895
=== Iterazione IRL 429 ===
Loss reward (iter 429): 5.4892659187316895
=== Iterazione IRL 430 ===
Loss reward (iter 430): 6.22624397277832
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -1.5e+03 |
|    critic_loss     | 198      |
|    ent_coef        | 1.46     |
|    ent_coef_loss   | -0.018   |
|    learning_rate   | 0.0003   |
|    n_updates       | 163699   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.53e+03 |
|    critic_loss     | 159       |
|    ent_coef        | 1.48      |
|    ent_coef_loss   | -0.0204   |
|    learning_rate   | 0.0003    |
|    n_updates       | 164099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.51e+03 |
|    critic_loss     | 197       |
|    ent_coef        | 1.48      |
|    ent_coef_loss   | 0.0245    |
|    learning_rate   | 0.0003    |
|    n_updates       | 164499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.52e+03 |
|    critic_loss     | 153       |
|    ent_coef        | 1.5       |
|    ent_coef_loss   | -0.0228   |
|    learning_rate   | 0.0003    |
|    n_updates       | 164899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 123       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.51e+03 |
|    critic_loss     | 193       |
|    ent_coef        | 1.49      |
|    ent_coef_loss   | -0.033    |
|    learning_rate   | 0.0003    |
|    n_updates       | 165299    |
----------------------------------
=== Iterazione IRL 431 ===
Loss reward (iter 431): 5.245211601257324
=== Iterazione IRL 432 ===
Loss reward (iter 432): 5.109602928161621
=== Iterazione IRL 433 ===
Loss reward (iter 433): 4.90646505355835
=== Iterazione IRL 434 ===
Loss reward (iter 434): 5.811131477355957
=== Iterazione IRL 435 ===
Loss reward (iter 435): 5.347778797149658
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.51e+03 |
|    critic_loss     | 143       |
|    ent_coef        | 1.49      |
|    ent_coef_loss   | 0.0487    |
|    learning_rate   | 0.0003    |
|    n_updates       | 165599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 134       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.52e+03 |
|    critic_loss     | 148       |
|    ent_coef        | 1.5       |
|    ent_coef_loss   | 0.0591    |
|    learning_rate   | 0.0003    |
|    n_updates       | 165999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.52e+03 |
|    critic_loss     | 170       |
|    ent_coef        | 1.5       |
|    ent_coef_loss   | -0.0603   |
|    learning_rate   | 0.0003    |
|    n_updates       | 166399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 126       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.51e+03 |
|    critic_loss     | 246       |
|    ent_coef        | 1.52      |
|    ent_coef_loss   | 0.0158    |
|    learning_rate   | 0.0003    |
|    n_updates       | 166799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.51e+03 |
|    critic_loss     | 119       |
|    ent_coef        | 1.51      |
|    ent_coef_loss   | 0.0351    |
|    learning_rate   | 0.0003    |
|    n_updates       | 167199    |
----------------------------------
=== Iterazione IRL 436 ===
Loss reward (iter 436): 5.493040084838867
=== Iterazione IRL 437 ===
Loss reward (iter 437): 5.189039707183838
=== Iterazione IRL 438 ===
Loss reward (iter 438): 4.998837947845459
=== Iterazione IRL 439 ===
Loss reward (iter 439): 5.744811058044434
=== Iterazione IRL 440 ===
Loss reward (iter 440): 4.508118629455566
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.51e+03 |
|    critic_loss     | 193       |
|    ent_coef        | 1.51      |
|    ent_coef_loss   | 0.00379   |
|    learning_rate   | 0.0003    |
|    n_updates       | 167499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 134       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.51e+03 |
|    critic_loss     | 199       |
|    ent_coef        | 1.48      |
|    ent_coef_loss   | 0.016     |
|    learning_rate   | 0.0003    |
|    n_updates       | 167899    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -1.5e+03 |
|    critic_loss     | 198      |
|    ent_coef        | 1.46     |
|    ent_coef_loss   | 0.0333   |
|    learning_rate   | 0.0003   |
|    n_updates       | 168299   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 126       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.52e+03 |
|    critic_loss     | 190       |
|    ent_coef        | 1.47      |
|    ent_coef_loss   | 0.0129    |
|    learning_rate   | 0.0003    |
|    n_updates       | 168699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.51e+03 |
|    critic_loss     | 188       |
|    ent_coef        | 1.51      |
|    ent_coef_loss   | -0.0337   |
|    learning_rate   | 0.0003    |
|    n_updates       | 169099    |
----------------------------------
=== Iterazione IRL 441 ===
Loss reward (iter 441): 6.5775556564331055
=== Iterazione IRL 442 ===
Loss reward (iter 442): 5.78446102142334
=== Iterazione IRL 443 ===
Loss reward (iter 443): 5.946808338165283
=== Iterazione IRL 444 ===
Loss reward (iter 444): 6.313331604003906
=== Iterazione IRL 445 ===
Loss reward (iter 445): 5.721198081970215
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.52e+03 |
|    critic_loss     | 166       |
|    ent_coef        | 1.51      |
|    ent_coef_loss   | 0.0161    |
|    learning_rate   | 0.0003    |
|    n_updates       | 169399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 134       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.54e+03 |
|    critic_loss     | 175       |
|    ent_coef        | 1.5       |
|    ent_coef_loss   | 0.0185    |
|    learning_rate   | 0.0003    |
|    n_updates       | 169799    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -1.5e+03 |
|    critic_loss     | 212      |
|    ent_coef        | 1.5      |
|    ent_coef_loss   | -0.0967  |
|    learning_rate   | 0.0003   |
|    n_updates       | 170199   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 126       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.51e+03 |
|    critic_loss     | 132       |
|    ent_coef        | 1.52      |
|    ent_coef_loss   | 0.00127   |
|    learning_rate   | 0.0003    |
|    n_updates       | 170599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.51e+03 |
|    critic_loss     | 146       |
|    ent_coef        | 1.54      |
|    ent_coef_loss   | -0.0283   |
|    learning_rate   | 0.0003    |
|    n_updates       | 170999    |
----------------------------------
=== Iterazione IRL 446 ===
Loss reward (iter 446): 5.4809722900390625
=== Iterazione IRL 447 ===
Loss reward (iter 447): 5.415596961975098
=== Iterazione IRL 448 ===
Loss reward (iter 448): 5.618716239929199
=== Iterazione IRL 449 ===
Loss reward (iter 449): 5.643826961517334
=== Iterazione IRL 450 ===
Loss reward (iter 450): 4.578261375427246
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.52e+03 |
|    critic_loss     | 203       |
|    ent_coef        | 1.54      |
|    ent_coef_loss   | -0.0124   |
|    learning_rate   | 0.0003    |
|    n_updates       | 171299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 134       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.52e+03 |
|    critic_loss     | 213       |
|    ent_coef        | 1.54      |
|    ent_coef_loss   | -0.0622   |
|    learning_rate   | 0.0003    |
|    n_updates       | 171699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.51e+03 |
|    critic_loss     | 125       |
|    ent_coef        | 1.52      |
|    ent_coef_loss   | 0.0572    |
|    learning_rate   | 0.0003    |
|    n_updates       | 172099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 126       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.51e+03 |
|    critic_loss     | 244       |
|    ent_coef        | 1.52      |
|    ent_coef_loss   | 0.0107    |
|    learning_rate   | 0.0003    |
|    n_updates       | 172499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.51e+03 |
|    critic_loss     | 242       |
|    ent_coef        | 1.51      |
|    ent_coef_loss   | -0.0361   |
|    learning_rate   | 0.0003    |
|    n_updates       | 172899    |
----------------------------------
=== Iterazione IRL 451 ===
Loss reward (iter 451): 4.170595645904541
=== Iterazione IRL 452 ===
Loss reward (iter 452): 6.011247158050537
=== Iterazione IRL 453 ===
Loss reward (iter 453): 4.762668609619141
=== Iterazione IRL 454 ===
Loss reward (iter 454): 5.8059163093566895
=== Iterazione IRL 455 ===
Loss reward (iter 455): 5.069575309753418
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.52e+03 |
|    critic_loss     | 193       |
|    ent_coef        | 1.51      |
|    ent_coef_loss   | 0.0286    |
|    learning_rate   | 0.0003    |
|    n_updates       | 173199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 134       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.51e+03 |
|    critic_loss     | 189       |
|    ent_coef        | 1.46      |
|    ent_coef_loss   | -0.023    |
|    learning_rate   | 0.0003    |
|    n_updates       | 173599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.52e+03 |
|    critic_loss     | 163       |
|    ent_coef        | 1.46      |
|    ent_coef_loss   | -0.000497 |
|    learning_rate   | 0.0003    |
|    n_updates       | 173999    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 16       |
|    fps             | 126      |
|    time_elapsed    | 12       |
|    total_timesteps | 1600     |
| train/             |          |
|    actor_loss      | -1.5e+03 |
|    critic_loss     | 155      |
|    ent_coef        | 1.46     |
|    ent_coef_loss   | 0.0203   |
|    learning_rate   | 0.0003   |
|    n_updates       | 174399   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.53e+03 |
|    critic_loss     | 234       |
|    ent_coef        | 1.47      |
|    ent_coef_loss   | -0.039    |
|    learning_rate   | 0.0003    |
|    n_updates       | 174799    |
----------------------------------
=== Iterazione IRL 456 ===
Loss reward (iter 456): 5.450056076049805
=== Iterazione IRL 457 ===
Loss reward (iter 457): 5.529065132141113
=== Iterazione IRL 458 ===
Loss reward (iter 458): 5.706391334533691
=== Iterazione IRL 459 ===
Loss reward (iter 459): 5.26675271987915
=== Iterazione IRL 460 ===
Loss reward (iter 460): 4.7295002937316895
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.53e+03 |
|    critic_loss     | 161       |
|    ent_coef        | 1.49      |
|    ent_coef_loss   | -0.081    |
|    learning_rate   | 0.0003    |
|    n_updates       | 175099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 134       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.52e+03 |
|    critic_loss     | 224       |
|    ent_coef        | 1.49      |
|    ent_coef_loss   | 0.0571    |
|    learning_rate   | 0.0003    |
|    n_updates       | 175499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.54e+03 |
|    critic_loss     | 139       |
|    ent_coef        | 1.51      |
|    ent_coef_loss   | 0.0322    |
|    learning_rate   | 0.0003    |
|    n_updates       | 175899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 126       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.52e+03 |
|    critic_loss     | 268       |
|    ent_coef        | 1.5       |
|    ent_coef_loss   | 0.0457    |
|    learning_rate   | 0.0003    |
|    n_updates       | 176299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.53e+03 |
|    critic_loss     | 144       |
|    ent_coef        | 1.48      |
|    ent_coef_loss   | 0.00542   |
|    learning_rate   | 0.0003    |
|    n_updates       | 176699    |
----------------------------------
=== Iterazione IRL 461 ===
Loss reward (iter 461): 4.4298834800720215
=== Iterazione IRL 462 ===
Loss reward (iter 462): 5.755154609680176
=== Iterazione IRL 463 ===
Loss reward (iter 463): 4.982237815856934
=== Iterazione IRL 464 ===
Loss reward (iter 464): 4.748170375823975
=== Iterazione IRL 465 ===
Loss reward (iter 465): 4.360901832580566
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 153       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.53e+03 |
|    critic_loss     | 151       |
|    ent_coef        | 1.49      |
|    ent_coef_loss   | -0.0321   |
|    learning_rate   | 0.0003    |
|    n_updates       | 176999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.53e+03 |
|    critic_loss     | 154       |
|    ent_coef        | 1.48      |
|    ent_coef_loss   | -0.0478   |
|    learning_rate   | 0.0003    |
|    n_updates       | 177399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.52e+03 |
|    critic_loss     | 218       |
|    ent_coef        | 1.48      |
|    ent_coef_loss   | 0.0119    |
|    learning_rate   | 0.0003    |
|    n_updates       | 177799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.51e+03 |
|    critic_loss     | 244       |
|    ent_coef        | 1.48      |
|    ent_coef_loss   | 0.00336   |
|    learning_rate   | 0.0003    |
|    n_updates       | 178199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 123       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.52e+03 |
|    critic_loss     | 122       |
|    ent_coef        | 1.47      |
|    ent_coef_loss   | 0.0141    |
|    learning_rate   | 0.0003    |
|    n_updates       | 178599    |
----------------------------------
=== Iterazione IRL 466 ===
Loss reward (iter 466): 4.017375469207764
=== Iterazione IRL 467 ===
Loss reward (iter 467): 4.067812442779541
=== Iterazione IRL 468 ===
Loss reward (iter 468): 4.556613445281982
=== Iterazione IRL 469 ===
Loss reward (iter 469): 4.814472675323486
=== Iterazione IRL 470 ===
Loss reward (iter 470): 5.424437046051025
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.54e+03 |
|    critic_loss     | 218       |
|    ent_coef        | 1.46      |
|    ent_coef_loss   | 0.00209   |
|    learning_rate   | 0.0003    |
|    n_updates       | 178899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.53e+03 |
|    critic_loss     | 174       |
|    ent_coef        | 1.48      |
|    ent_coef_loss   | -0.0476   |
|    learning_rate   | 0.0003    |
|    n_updates       | 179299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.53e+03 |
|    critic_loss     | 148       |
|    ent_coef        | 1.5       |
|    ent_coef_loss   | -0.00104  |
|    learning_rate   | 0.0003    |
|    n_updates       | 179699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.53e+03 |
|    critic_loss     | 227       |
|    ent_coef        | 1.5       |
|    ent_coef_loss   | 0.000422  |
|    learning_rate   | 0.0003    |
|    n_updates       | 180099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.53e+03 |
|    critic_loss     | 279       |
|    ent_coef        | 1.47      |
|    ent_coef_loss   | 0.00909   |
|    learning_rate   | 0.0003    |
|    n_updates       | 180499    |
----------------------------------
=== Iterazione IRL 471 ===
Loss reward (iter 471): 4.618559837341309
=== Iterazione IRL 472 ===
Loss reward (iter 472): 5.304385185241699
=== Iterazione IRL 473 ===
Loss reward (iter 473): 5.238033294677734
=== Iterazione IRL 474 ===
Loss reward (iter 474): 5.0168256759643555
=== Iterazione IRL 475 ===
Loss reward (iter 475): 5.043819904327393
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.53e+03 |
|    critic_loss     | 200       |
|    ent_coef        | 1.44      |
|    ent_coef_loss   | 0.0317    |
|    learning_rate   | 0.0003    |
|    n_updates       | 180799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.52e+03 |
|    critic_loss     | 221       |
|    ent_coef        | 1.45      |
|    ent_coef_loss   | 0.00209   |
|    learning_rate   | 0.0003    |
|    n_updates       | 181199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.53e+03 |
|    critic_loss     | 159       |
|    ent_coef        | 1.44      |
|    ent_coef_loss   | -0.0555   |
|    learning_rate   | 0.0003    |
|    n_updates       | 181599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.54e+03 |
|    critic_loss     | 143       |
|    ent_coef        | 1.46      |
|    ent_coef_loss   | -0.0262   |
|    learning_rate   | 0.0003    |
|    n_updates       | 181999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 123       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.53e+03 |
|    critic_loss     | 192       |
|    ent_coef        | 1.44      |
|    ent_coef_loss   | -0.0557   |
|    learning_rate   | 0.0003    |
|    n_updates       | 182399    |
----------------------------------
=== Iterazione IRL 476 ===
Loss reward (iter 476): 5.180211067199707
=== Iterazione IRL 477 ===
Loss reward (iter 477): 5.108829975128174
=== Iterazione IRL 478 ===
Loss reward (iter 478): 4.315284729003906
=== Iterazione IRL 479 ===
Loss reward (iter 479): 4.98234224319458
=== Iterazione IRL 480 ===
Loss reward (iter 480): 4.797735691070557
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.53e+03 |
|    critic_loss     | 171       |
|    ent_coef        | 1.43      |
|    ent_coef_loss   | 0.0435    |
|    learning_rate   | 0.0003    |
|    n_updates       | 182699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.53e+03 |
|    critic_loss     | 163       |
|    ent_coef        | 1.43      |
|    ent_coef_loss   | 0.0107    |
|    learning_rate   | 0.0003    |
|    n_updates       | 183099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.52e+03 |
|    critic_loss     | 173       |
|    ent_coef        | 1.45      |
|    ent_coef_loss   | 0.0497    |
|    learning_rate   | 0.0003    |
|    n_updates       | 183499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.52e+03 |
|    critic_loss     | 189       |
|    ent_coef        | 1.45      |
|    ent_coef_loss   | -0.0335   |
|    learning_rate   | 0.0003    |
|    n_updates       | 183899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.53e+03 |
|    critic_loss     | 193       |
|    ent_coef        | 1.46      |
|    ent_coef_loss   | -0.0539   |
|    learning_rate   | 0.0003    |
|    n_updates       | 184299    |
----------------------------------
=== Iterazione IRL 481 ===
Loss reward (iter 481): 5.029513835906982
=== Iterazione IRL 482 ===
Loss reward (iter 482): 4.973034858703613
=== Iterazione IRL 483 ===
Loss reward (iter 483): 4.496278762817383
=== Iterazione IRL 484 ===
Loss reward (iter 484): 5.476737022399902
=== Iterazione IRL 485 ===
Loss reward (iter 485): 4.614684104919434
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.52e+03 |
|    critic_loss     | 163       |
|    ent_coef        | 1.46      |
|    ent_coef_loss   | -0.0268   |
|    learning_rate   | 0.0003    |
|    n_updates       | 184599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.52e+03 |
|    critic_loss     | 201       |
|    ent_coef        | 1.47      |
|    ent_coef_loss   | -0.0409   |
|    learning_rate   | 0.0003    |
|    n_updates       | 184999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.53e+03 |
|    critic_loss     | 195       |
|    ent_coef        | 1.47      |
|    ent_coef_loss   | 0.0613    |
|    learning_rate   | 0.0003    |
|    n_updates       | 185399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.53e+03 |
|    critic_loss     | 156       |
|    ent_coef        | 1.47      |
|    ent_coef_loss   | 0.0222    |
|    learning_rate   | 0.0003    |
|    n_updates       | 185799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.51e+03 |
|    critic_loss     | 179       |
|    ent_coef        | 1.48      |
|    ent_coef_loss   | 0.0384    |
|    learning_rate   | 0.0003    |
|    n_updates       | 186199    |
----------------------------------
=== Iterazione IRL 486 ===
Loss reward (iter 486): 4.9725542068481445
=== Iterazione IRL 487 ===
Loss reward (iter 487): 6.63171911239624
=== Iterazione IRL 488 ===
Loss reward (iter 488): 5.565220355987549
=== Iterazione IRL 489 ===
Loss reward (iter 489): 5.023278713226318
=== Iterazione IRL 490 ===
Loss reward (iter 490): 5.759156227111816
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.54e+03 |
|    critic_loss     | 181       |
|    ent_coef        | 1.46      |
|    ent_coef_loss   | 0.0392    |
|    learning_rate   | 0.0003    |
|    n_updates       | 186499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.54e+03 |
|    critic_loss     | 201       |
|    ent_coef        | 1.46      |
|    ent_coef_loss   | 0.00062   |
|    learning_rate   | 0.0003    |
|    n_updates       | 186899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.54e+03 |
|    critic_loss     | 162       |
|    ent_coef        | 1.46      |
|    ent_coef_loss   | 0.0369    |
|    learning_rate   | 0.0003    |
|    n_updates       | 187299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.53e+03 |
|    critic_loss     | 171       |
|    ent_coef        | 1.47      |
|    ent_coef_loss   | 0.0824    |
|    learning_rate   | 0.0003    |
|    n_updates       | 187699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.51e+03 |
|    critic_loss     | 214       |
|    ent_coef        | 1.44      |
|    ent_coef_loss   | -0.0331   |
|    learning_rate   | 0.0003    |
|    n_updates       | 188099    |
----------------------------------
=== Iterazione IRL 491 ===
Loss reward (iter 491): 4.670346260070801
=== Iterazione IRL 492 ===
Loss reward (iter 492): 4.777207374572754
=== Iterazione IRL 493 ===
Loss reward (iter 493): 5.151817321777344
=== Iterazione IRL 494 ===
Loss reward (iter 494): 4.736682415008545
=== Iterazione IRL 495 ===
Loss reward (iter 495): 5.04148006439209
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.52e+03 |
|    critic_loss     | 214       |
|    ent_coef        | 1.45      |
|    ent_coef_loss   | -0.0142   |
|    learning_rate   | 0.0003    |
|    n_updates       | 188399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 134       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.53e+03 |
|    critic_loss     | 208       |
|    ent_coef        | 1.42      |
|    ent_coef_loss   | -0.00139  |
|    learning_rate   | 0.0003    |
|    n_updates       | 188799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.53e+03 |
|    critic_loss     | 213       |
|    ent_coef        | 1.42      |
|    ent_coef_loss   | -0.0227   |
|    learning_rate   | 0.0003    |
|    n_updates       | 189199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 126       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.53e+03 |
|    critic_loss     | 143       |
|    ent_coef        | 1.41      |
|    ent_coef_loss   | 0.0197    |
|    learning_rate   | 0.0003    |
|    n_updates       | 189599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.52e+03 |
|    critic_loss     | 188       |
|    ent_coef        | 1.43      |
|    ent_coef_loss   | -0.0193   |
|    learning_rate   | 0.0003    |
|    n_updates       | 189999    |
----------------------------------
=== Iterazione IRL 496 ===
Loss reward (iter 496): 5.0312299728393555
=== Iterazione IRL 497 ===
Loss reward (iter 497): 5.960226535797119
=== Iterazione IRL 498 ===
Loss reward (iter 498): 4.589268684387207
=== Iterazione IRL 499 ===
Loss reward (iter 499): 5.1548027992248535
=== Iterazione IRL 500 ===
Loss reward (iter 500): 4.318286895751953
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.54e+03 |
|    critic_loss     | 196       |
|    ent_coef        | 1.46      |
|    ent_coef_loss   | 0.0524    |
|    learning_rate   | 0.0003    |
|    n_updates       | 190299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 134       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.55e+03 |
|    critic_loss     | 141       |
|    ent_coef        | 1.44      |
|    ent_coef_loss   | 0.0584    |
|    learning_rate   | 0.0003    |
|    n_updates       | 190699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.52e+03 |
|    critic_loss     | 203       |
|    ent_coef        | 1.43      |
|    ent_coef_loss   | -0.0673   |
|    learning_rate   | 0.0003    |
|    n_updates       | 191099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.53e+03 |
|    critic_loss     | 152       |
|    ent_coef        | 1.42      |
|    ent_coef_loss   | 0.0316    |
|    learning_rate   | 0.0003    |
|    n_updates       | 191499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.55e+03 |
|    critic_loss     | 155       |
|    ent_coef        | 1.42      |
|    ent_coef_loss   | 0.0109    |
|    learning_rate   | 0.0003    |
|    n_updates       | 191899    |
----------------------------------
=== Iterazione IRL 501 ===
Loss reward (iter 501): 5.606612205505371
=== Iterazione IRL 502 ===
Loss reward (iter 502): 5.820681095123291
=== Iterazione IRL 503 ===
Loss reward (iter 503): 5.122682571411133
=== Iterazione IRL 504 ===
Loss reward (iter 504): 4.833922386169434
=== Iterazione IRL 505 ===
Loss reward (iter 505): 5.025721549987793
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.53e+03 |
|    critic_loss     | 174       |
|    ent_coef        | 1.44      |
|    ent_coef_loss   | -0.0277   |
|    learning_rate   | 0.0003    |
|    n_updates       | 192199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 134       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.53e+03 |
|    critic_loss     | 153       |
|    ent_coef        | 1.46      |
|    ent_coef_loss   | -0.0106   |
|    learning_rate   | 0.0003    |
|    n_updates       | 192599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.51e+03 |
|    critic_loss     | 254       |
|    ent_coef        | 1.46      |
|    ent_coef_loss   | -0.0694   |
|    learning_rate   | 0.0003    |
|    n_updates       | 192999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.53e+03 |
|    critic_loss     | 118       |
|    ent_coef        | 1.45      |
|    ent_coef_loss   | -0.00586  |
|    learning_rate   | 0.0003    |
|    n_updates       | 193399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.55e+03 |
|    critic_loss     | 123       |
|    ent_coef        | 1.43      |
|    ent_coef_loss   | 0.0426    |
|    learning_rate   | 0.0003    |
|    n_updates       | 193799    |
----------------------------------
=== Iterazione IRL 506 ===
Loss reward (iter 506): 4.420295238494873
=== Iterazione IRL 507 ===
Loss reward (iter 507): 5.607152938842773
=== Iterazione IRL 508 ===
Loss reward (iter 508): 5.884992599487305
=== Iterazione IRL 509 ===
Loss reward (iter 509): 4.480888843536377
=== Iterazione IRL 510 ===
Loss reward (iter 510): 5.2244038581848145
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.53e+03 |
|    critic_loss     | 241       |
|    ent_coef        | 1.43      |
|    ent_coef_loss   | 0.0216    |
|    learning_rate   | 0.0003    |
|    n_updates       | 194099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.52e+03 |
|    critic_loss     | 180       |
|    ent_coef        | 1.44      |
|    ent_coef_loss   | -0.0274   |
|    learning_rate   | 0.0003    |
|    n_updates       | 194499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.53e+03 |
|    critic_loss     | 126       |
|    ent_coef        | 1.47      |
|    ent_coef_loss   | -0.0127   |
|    learning_rate   | 0.0003    |
|    n_updates       | 194899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.53e+03 |
|    critic_loss     | 178       |
|    ent_coef        | 1.47      |
|    ent_coef_loss   | 0.0523    |
|    learning_rate   | 0.0003    |
|    n_updates       | 195299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.51e+03 |
|    critic_loss     | 202       |
|    ent_coef        | 1.45      |
|    ent_coef_loss   | -0.0284   |
|    learning_rate   | 0.0003    |
|    n_updates       | 195699    |
----------------------------------
=== Iterazione IRL 511 ===
Loss reward (iter 511): 6.320590019226074
=== Iterazione IRL 512 ===
Loss reward (iter 512): 5.521517753601074
=== Iterazione IRL 513 ===
Loss reward (iter 513): 5.106432914733887
=== Iterazione IRL 514 ===
Loss reward (iter 514): 5.310465335845947
=== Iterazione IRL 515 ===
Loss reward (iter 515): 6.356685161590576
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.52e+03 |
|    critic_loss     | 126       |
|    ent_coef        | 1.42      |
|    ent_coef_loss   | 0.0214    |
|    learning_rate   | 0.0003    |
|    n_updates       | 195999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.53e+03 |
|    critic_loss     | 211       |
|    ent_coef        | 1.41      |
|    ent_coef_loss   | 0.028     |
|    learning_rate   | 0.0003    |
|    n_updates       | 196399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.53e+03 |
|    critic_loss     | 156       |
|    ent_coef        | 1.41      |
|    ent_coef_loss   | 0.0458    |
|    learning_rate   | 0.0003    |
|    n_updates       | 196799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.52e+03 |
|    critic_loss     | 203       |
|    ent_coef        | 1.42      |
|    ent_coef_loss   | -0.00663  |
|    learning_rate   | 0.0003    |
|    n_updates       | 197199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.54e+03 |
|    critic_loss     | 172       |
|    ent_coef        | 1.4       |
|    ent_coef_loss   | -0.0112   |
|    learning_rate   | 0.0003    |
|    n_updates       | 197599    |
----------------------------------
=== Iterazione IRL 516 ===
Loss reward (iter 516): 5.069159984588623
=== Iterazione IRL 517 ===
Loss reward (iter 517): 4.663578987121582
=== Iterazione IRL 518 ===
Loss reward (iter 518): 4.622159004211426
=== Iterazione IRL 519 ===
Loss reward (iter 519): 5.012680530548096
=== Iterazione IRL 520 ===
Loss reward (iter 520): 4.171388149261475
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.53e+03 |
|    critic_loss     | 183       |
|    ent_coef        | 1.42      |
|    ent_coef_loss   | -0.00934  |
|    learning_rate   | 0.0003    |
|    n_updates       | 197899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 134       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.53e+03 |
|    critic_loss     | 216       |
|    ent_coef        | 1.43      |
|    ent_coef_loss   | -0.00356  |
|    learning_rate   | 0.0003    |
|    n_updates       | 198299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.53e+03 |
|    critic_loss     | 142       |
|    ent_coef        | 1.45      |
|    ent_coef_loss   | 0.0056    |
|    learning_rate   | 0.0003    |
|    n_updates       | 198699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.52e+03 |
|    critic_loss     | 196       |
|    ent_coef        | 1.47      |
|    ent_coef_loss   | -0.0365   |
|    learning_rate   | 0.0003    |
|    n_updates       | 199099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.53e+03 |
|    critic_loss     | 184       |
|    ent_coef        | 1.48      |
|    ent_coef_loss   | -0.0121   |
|    learning_rate   | 0.0003    |
|    n_updates       | 199499    |
----------------------------------
=== Iterazione IRL 521 ===
Loss reward (iter 521): 4.930974960327148
=== Iterazione IRL 522 ===
Loss reward (iter 522): 4.575448036193848
=== Iterazione IRL 523 ===
Loss reward (iter 523): 4.857060432434082
=== Iterazione IRL 524 ===
Loss reward (iter 524): 4.689082145690918
=== Iterazione IRL 525 ===
Loss reward (iter 525): 5.080592632293701
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.54e+03 |
|    critic_loss     | 173       |
|    ent_coef        | 1.48      |
|    ent_coef_loss   | -0.024    |
|    learning_rate   | 0.0003    |
|    n_updates       | 199799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.53e+03 |
|    critic_loss     | 154       |
|    ent_coef        | 1.51      |
|    ent_coef_loss   | 0.0095    |
|    learning_rate   | 0.0003    |
|    n_updates       | 200199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.55e+03 |
|    critic_loss     | 193       |
|    ent_coef        | 1.5       |
|    ent_coef_loss   | -0.0328   |
|    learning_rate   | 0.0003    |
|    n_updates       | 200599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.54e+03 |
|    critic_loss     | 158       |
|    ent_coef        | 1.5       |
|    ent_coef_loss   | 0.0608    |
|    learning_rate   | 0.0003    |
|    n_updates       | 200999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.52e+03 |
|    critic_loss     | 257       |
|    ent_coef        | 1.5       |
|    ent_coef_loss   | 0.0143    |
|    learning_rate   | 0.0003    |
|    n_updates       | 201399    |
----------------------------------
=== Iterazione IRL 526 ===
Loss reward (iter 526): 5.4174933433532715
=== Iterazione IRL 527 ===
Loss reward (iter 527): 4.7455291748046875
=== Iterazione IRL 528 ===
Loss reward (iter 528): 5.228765964508057
=== Iterazione IRL 529 ===
Loss reward (iter 529): 5.024969100952148
=== Iterazione IRL 530 ===
Loss reward (iter 530): 4.555399417877197
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.53e+03 |
|    critic_loss     | 132       |
|    ent_coef        | 1.51      |
|    ent_coef_loss   | 0.0055    |
|    learning_rate   | 0.0003    |
|    n_updates       | 201699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.54e+03 |
|    critic_loss     | 164       |
|    ent_coef        | 1.48      |
|    ent_coef_loss   | 0.0244    |
|    learning_rate   | 0.0003    |
|    n_updates       | 202099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.52e+03 |
|    critic_loss     | 220       |
|    ent_coef        | 1.48      |
|    ent_coef_loss   | 0.00756   |
|    learning_rate   | 0.0003    |
|    n_updates       | 202499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.54e+03 |
|    critic_loss     | 101       |
|    ent_coef        | 1.52      |
|    ent_coef_loss   | 0.0092    |
|    learning_rate   | 0.0003    |
|    n_updates       | 202899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.53e+03 |
|    critic_loss     | 189       |
|    ent_coef        | 1.5       |
|    ent_coef_loss   | 0.035     |
|    learning_rate   | 0.0003    |
|    n_updates       | 203299    |
----------------------------------
=== Iterazione IRL 531 ===
Loss reward (iter 531): 5.5636796951293945
=== Iterazione IRL 532 ===
Loss reward (iter 532): 5.507244110107422
=== Iterazione IRL 533 ===
Loss reward (iter 533): 5.724934101104736
=== Iterazione IRL 534 ===
Loss reward (iter 534): 4.832574844360352
=== Iterazione IRL 535 ===
Loss reward (iter 535): 5.827065467834473
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.54e+03 |
|    critic_loss     | 166       |
|    ent_coef        | 1.49      |
|    ent_coef_loss   | -0.109    |
|    learning_rate   | 0.0003    |
|    n_updates       | 203599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 134       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.53e+03 |
|    critic_loss     | 266       |
|    ent_coef        | 1.51      |
|    ent_coef_loss   | 0.0113    |
|    learning_rate   | 0.0003    |
|    n_updates       | 203999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.55e+03 |
|    critic_loss     | 170       |
|    ent_coef        | 1.52      |
|    ent_coef_loss   | -0.00632  |
|    learning_rate   | 0.0003    |
|    n_updates       | 204399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.53e+03 |
|    critic_loss     | 143       |
|    ent_coef        | 1.53      |
|    ent_coef_loss   | 0.00771   |
|    learning_rate   | 0.0003    |
|    n_updates       | 204799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.53e+03 |
|    critic_loss     | 137       |
|    ent_coef        | 1.52      |
|    ent_coef_loss   | 0.025     |
|    learning_rate   | 0.0003    |
|    n_updates       | 205199    |
----------------------------------
=== Iterazione IRL 536 ===
Loss reward (iter 536): 5.406798362731934
=== Iterazione IRL 537 ===
Loss reward (iter 537): 4.239691734313965
=== Iterazione IRL 538 ===
Loss reward (iter 538): 4.860316753387451
=== Iterazione IRL 539 ===
Loss reward (iter 539): 4.639471054077148
=== Iterazione IRL 540 ===
Loss reward (iter 540): 5.136309623718262
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.54e+03 |
|    critic_loss     | 162       |
|    ent_coef        | 1.53      |
|    ent_coef_loss   | 0.000539  |
|    learning_rate   | 0.0003    |
|    n_updates       | 205499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 134       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.54e+03 |
|    critic_loss     | 142       |
|    ent_coef        | 1.51      |
|    ent_coef_loss   | -0.00123  |
|    learning_rate   | 0.0003    |
|    n_updates       | 205899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.55e+03 |
|    critic_loss     | 173       |
|    ent_coef        | 1.5       |
|    ent_coef_loss   | 0.0226    |
|    learning_rate   | 0.0003    |
|    n_updates       | 206299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.55e+03 |
|    critic_loss     | 197       |
|    ent_coef        | 1.49      |
|    ent_coef_loss   | -0.0309   |
|    learning_rate   | 0.0003    |
|    n_updates       | 206699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.54e+03 |
|    critic_loss     | 143       |
|    ent_coef        | 1.5       |
|    ent_coef_loss   | -0.0922   |
|    learning_rate   | 0.0003    |
|    n_updates       | 207099    |
----------------------------------
=== Iterazione IRL 541 ===
Loss reward (iter 541): 5.2814412117004395
=== Iterazione IRL 542 ===
Loss reward (iter 542): 5.209760665893555
=== Iterazione IRL 543 ===
Loss reward (iter 543): 5.642691612243652
=== Iterazione IRL 544 ===
Loss reward (iter 544): 4.8533854484558105
=== Iterazione IRL 545 ===
Loss reward (iter 545): 5.232717037200928
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.55e+03 |
|    critic_loss     | 115       |
|    ent_coef        | 1.52      |
|    ent_coef_loss   | -0.0138   |
|    learning_rate   | 0.0003    |
|    n_updates       | 207399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.55e+03 |
|    critic_loss     | 192       |
|    ent_coef        | 1.54      |
|    ent_coef_loss   | -0.0561   |
|    learning_rate   | 0.0003    |
|    n_updates       | 207799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.55e+03 |
|    critic_loss     | 149       |
|    ent_coef        | 1.55      |
|    ent_coef_loss   | 0.0409    |
|    learning_rate   | 0.0003    |
|    n_updates       | 208199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.55e+03 |
|    critic_loss     | 125       |
|    ent_coef        | 1.54      |
|    ent_coef_loss   | 0.0712    |
|    learning_rate   | 0.0003    |
|    n_updates       | 208599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.54e+03 |
|    critic_loss     | 226       |
|    ent_coef        | 1.53      |
|    ent_coef_loss   | -0.023    |
|    learning_rate   | 0.0003    |
|    n_updates       | 208999    |
----------------------------------
=== Iterazione IRL 546 ===
Loss reward (iter 546): 4.656011581420898
=== Iterazione IRL 547 ===
Loss reward (iter 547): 3.2486324310302734
=== Iterazione IRL 548 ===
Loss reward (iter 548): 3.5970213413238525
=== Iterazione IRL 549 ===
Loss reward (iter 549): 4.534250736236572
=== Iterazione IRL 550 ===
Loss reward (iter 550): 3.973001480102539
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.55e+03 |
|    critic_loss     | 180       |
|    ent_coef        | 1.53      |
|    ent_coef_loss   | -0.0021   |
|    learning_rate   | 0.0003    |
|    n_updates       | 209299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.55e+03 |
|    critic_loss     | 129       |
|    ent_coef        | 1.53      |
|    ent_coef_loss   | 0.0438    |
|    learning_rate   | 0.0003    |
|    n_updates       | 209699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.53e+03 |
|    critic_loss     | 204       |
|    ent_coef        | 1.52      |
|    ent_coef_loss   | 0.0102    |
|    learning_rate   | 0.0003    |
|    n_updates       | 210099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.53e+03 |
|    critic_loss     | 139       |
|    ent_coef        | 1.51      |
|    ent_coef_loss   | -0.00546  |
|    learning_rate   | 0.0003    |
|    n_updates       | 210499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.54e+03 |
|    critic_loss     | 183       |
|    ent_coef        | 1.5       |
|    ent_coef_loss   | -0.0256   |
|    learning_rate   | 0.0003    |
|    n_updates       | 210899    |
----------------------------------
=== Iterazione IRL 551 ===
Loss reward (iter 551): 5.059116363525391
=== Iterazione IRL 552 ===
Loss reward (iter 552): 5.141005992889404
=== Iterazione IRL 553 ===
Loss reward (iter 553): 5.942628860473633
=== Iterazione IRL 554 ===
Loss reward (iter 554): 4.632473945617676
=== Iterazione IRL 555 ===
Loss reward (iter 555): 4.765438079833984
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.55e+03 |
|    critic_loss     | 160       |
|    ent_coef        | 1.51      |
|    ent_coef_loss   | -0.0221   |
|    learning_rate   | 0.0003    |
|    n_updates       | 211199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.54e+03 |
|    critic_loss     | 142       |
|    ent_coef        | 1.51      |
|    ent_coef_loss   | 0.0985    |
|    learning_rate   | 0.0003    |
|    n_updates       | 211599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.56e+03 |
|    critic_loss     | 157       |
|    ent_coef        | 1.53      |
|    ent_coef_loss   | 0.0189    |
|    learning_rate   | 0.0003    |
|    n_updates       | 211999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.54e+03 |
|    critic_loss     | 116       |
|    ent_coef        | 1.53      |
|    ent_coef_loss   | -0.0165   |
|    learning_rate   | 0.0003    |
|    n_updates       | 212399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.56e+03 |
|    critic_loss     | 177       |
|    ent_coef        | 1.51      |
|    ent_coef_loss   | -0.0181   |
|    learning_rate   | 0.0003    |
|    n_updates       | 212799    |
----------------------------------
=== Iterazione IRL 556 ===
Loss reward (iter 556): 5.273550510406494
=== Iterazione IRL 557 ===
Loss reward (iter 557): 5.670507907867432
=== Iterazione IRL 558 ===
Loss reward (iter 558): 5.111989974975586
=== Iterazione IRL 559 ===
Loss reward (iter 559): 4.845422744750977
=== Iterazione IRL 560 ===
Loss reward (iter 560): 5.33885383605957
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.55e+03 |
|    critic_loss     | 232       |
|    ent_coef        | 1.53      |
|    ent_coef_loss   | 0.0588    |
|    learning_rate   | 0.0003    |
|    n_updates       | 213099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 134       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.56e+03 |
|    critic_loss     | 158       |
|    ent_coef        | 1.55      |
|    ent_coef_loss   | -0.073    |
|    learning_rate   | 0.0003    |
|    n_updates       | 213499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.54e+03 |
|    critic_loss     | 141       |
|    ent_coef        | 1.56      |
|    ent_coef_loss   | 0.025     |
|    learning_rate   | 0.0003    |
|    n_updates       | 213899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.55e+03 |
|    critic_loss     | 202       |
|    ent_coef        | 1.54      |
|    ent_coef_loss   | 0.0937    |
|    learning_rate   | 0.0003    |
|    n_updates       | 214299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.56e+03 |
|    critic_loss     | 113       |
|    ent_coef        | 1.51      |
|    ent_coef_loss   | 0.0197    |
|    learning_rate   | 0.0003    |
|    n_updates       | 214699    |
----------------------------------
=== Iterazione IRL 561 ===
Loss reward (iter 561): 5.035921096801758
=== Iterazione IRL 562 ===
Loss reward (iter 562): 5.5230584144592285
=== Iterazione IRL 563 ===
Loss reward (iter 563): 4.985191822052002
=== Iterazione IRL 564 ===
Loss reward (iter 564): 5.624605178833008
=== Iterazione IRL 565 ===
Loss reward (iter 565): 4.697671890258789
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.55e+03 |
|    critic_loss     | 192       |
|    ent_coef        | 1.53      |
|    ent_coef_loss   | -0.008    |
|    learning_rate   | 0.0003    |
|    n_updates       | 214999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 134       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.55e+03 |
|    critic_loss     | 164       |
|    ent_coef        | 1.55      |
|    ent_coef_loss   | 0.0199    |
|    learning_rate   | 0.0003    |
|    n_updates       | 215399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.55e+03 |
|    critic_loss     | 233       |
|    ent_coef        | 1.58      |
|    ent_coef_loss   | -0.0604   |
|    learning_rate   | 0.0003    |
|    n_updates       | 215799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.55e+03 |
|    critic_loss     | 125       |
|    ent_coef        | 1.56      |
|    ent_coef_loss   | 0.0352    |
|    learning_rate   | 0.0003    |
|    n_updates       | 216199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.56e+03 |
|    critic_loss     | 216       |
|    ent_coef        | 1.58      |
|    ent_coef_loss   | -0.0131   |
|    learning_rate   | 0.0003    |
|    n_updates       | 216599    |
----------------------------------
=== Iterazione IRL 566 ===
Loss reward (iter 566): 4.158172130584717
=== Iterazione IRL 567 ===
Loss reward (iter 567): 4.841485023498535
=== Iterazione IRL 568 ===
Loss reward (iter 568): 4.890957832336426
=== Iterazione IRL 569 ===
Loss reward (iter 569): 5.674182891845703
=== Iterazione IRL 570 ===
Loss reward (iter 570): 5.351397514343262
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.55e+03 |
|    critic_loss     | 210       |
|    ent_coef        | 1.58      |
|    ent_coef_loss   | -0.0544   |
|    learning_rate   | 0.0003    |
|    n_updates       | 216899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.56e+03 |
|    critic_loss     | 167       |
|    ent_coef        | 1.56      |
|    ent_coef_loss   | -0.00947  |
|    learning_rate   | 0.0003    |
|    n_updates       | 217299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.56e+03 |
|    critic_loss     | 136       |
|    ent_coef        | 1.57      |
|    ent_coef_loss   | -0.0541   |
|    learning_rate   | 0.0003    |
|    n_updates       | 217699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.54e+03 |
|    critic_loss     | 244       |
|    ent_coef        | 1.54      |
|    ent_coef_loss   | -0.026    |
|    learning_rate   | 0.0003    |
|    n_updates       | 218099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.58e+03 |
|    critic_loss     | 153       |
|    ent_coef        | 1.54      |
|    ent_coef_loss   | 0.0297    |
|    learning_rate   | 0.0003    |
|    n_updates       | 218499    |
----------------------------------
=== Iterazione IRL 571 ===
Loss reward (iter 571): 6.155246257781982
=== Iterazione IRL 572 ===
Loss reward (iter 572): 5.761114597320557
=== Iterazione IRL 573 ===
Loss reward (iter 573): 5.191678047180176
=== Iterazione IRL 574 ===
Loss reward (iter 574): 5.331315994262695
=== Iterazione IRL 575 ===
Loss reward (iter 575): 5.281038284301758
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.56e+03 |
|    critic_loss     | 170       |
|    ent_coef        | 1.55      |
|    ent_coef_loss   | -0.0682   |
|    learning_rate   | 0.0003    |
|    n_updates       | 218799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.58e+03 |
|    critic_loss     | 149       |
|    ent_coef        | 1.54      |
|    ent_coef_loss   | -0.00169  |
|    learning_rate   | 0.0003    |
|    n_updates       | 219199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.56e+03 |
|    critic_loss     | 132       |
|    ent_coef        | 1.56      |
|    ent_coef_loss   | -0.0167   |
|    learning_rate   | 0.0003    |
|    n_updates       | 219599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.56e+03 |
|    critic_loss     | 207       |
|    ent_coef        | 1.56      |
|    ent_coef_loss   | -0.0278   |
|    learning_rate   | 0.0003    |
|    n_updates       | 219999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.57e+03 |
|    critic_loss     | 208       |
|    ent_coef        | 1.57      |
|    ent_coef_loss   | -0.00563  |
|    learning_rate   | 0.0003    |
|    n_updates       | 220399    |
----------------------------------
=== Iterazione IRL 576 ===
Loss reward (iter 576): 4.770480155944824
=== Iterazione IRL 577 ===
Loss reward (iter 577): 4.781495094299316
=== Iterazione IRL 578 ===
Loss reward (iter 578): 4.419567108154297
=== Iterazione IRL 579 ===
Loss reward (iter 579): 4.600248336791992
=== Iterazione IRL 580 ===
Loss reward (iter 580): 4.250956058502197
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.57e+03 |
|    critic_loss     | 230       |
|    ent_coef        | 1.58      |
|    ent_coef_loss   | -0.018    |
|    learning_rate   | 0.0003    |
|    n_updates       | 220699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 134       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.55e+03 |
|    critic_loss     | 160       |
|    ent_coef        | 1.6       |
|    ent_coef_loss   | -0.00207  |
|    learning_rate   | 0.0003    |
|    n_updates       | 221099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.57e+03 |
|    critic_loss     | 146       |
|    ent_coef        | 1.61      |
|    ent_coef_loss   | 0.0136    |
|    learning_rate   | 0.0003    |
|    n_updates       | 221499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.57e+03 |
|    critic_loss     | 169       |
|    ent_coef        | 1.59      |
|    ent_coef_loss   | 0.12      |
|    learning_rate   | 0.0003    |
|    n_updates       | 221899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.56e+03 |
|    critic_loss     | 167       |
|    ent_coef        | 1.59      |
|    ent_coef_loss   | -0.00605  |
|    learning_rate   | 0.0003    |
|    n_updates       | 222299    |
----------------------------------
=== Iterazione IRL 581 ===
Loss reward (iter 581): 4.565547466278076
=== Iterazione IRL 582 ===
Loss reward (iter 582): 4.415279388427734
=== Iterazione IRL 583 ===
Loss reward (iter 583): 5.592218399047852
=== Iterazione IRL 584 ===
Loss reward (iter 584): 4.2811808586120605
=== Iterazione IRL 585 ===
Loss reward (iter 585): 4.38891077041626
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.56e+03 |
|    critic_loss     | 164       |
|    ent_coef        | 1.61      |
|    ent_coef_loss   | -0.033    |
|    learning_rate   | 0.0003    |
|    n_updates       | 222599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.59e+03 |
|    critic_loss     | 130       |
|    ent_coef        | 1.6       |
|    ent_coef_loss   | 0.0233    |
|    learning_rate   | 0.0003    |
|    n_updates       | 222999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.57e+03 |
|    critic_loss     | 152       |
|    ent_coef        | 1.6       |
|    ent_coef_loss   | 0.0171    |
|    learning_rate   | 0.0003    |
|    n_updates       | 223399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.58e+03 |
|    critic_loss     | 135       |
|    ent_coef        | 1.61      |
|    ent_coef_loss   | 0.0333    |
|    learning_rate   | 0.0003    |
|    n_updates       | 223799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.58e+03 |
|    critic_loss     | 193       |
|    ent_coef        | 1.61      |
|    ent_coef_loss   | 0.0276    |
|    learning_rate   | 0.0003    |
|    n_updates       | 224199    |
----------------------------------
=== Iterazione IRL 586 ===
Loss reward (iter 586): 4.856451034545898
=== Iterazione IRL 587 ===
Loss reward (iter 587): 5.103703022003174
=== Iterazione IRL 588 ===
Loss reward (iter 588): 4.87009859085083
=== Iterazione IRL 589 ===
Loss reward (iter 589): 4.699312686920166
=== Iterazione IRL 590 ===
Loss reward (iter 590): 4.431494235992432
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.58e+03 |
|    critic_loss     | 131       |
|    ent_coef        | 1.62      |
|    ent_coef_loss   | 0.0386    |
|    learning_rate   | 0.0003    |
|    n_updates       | 224499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 134       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.59e+03 |
|    critic_loss     | 139       |
|    ent_coef        | 1.65      |
|    ent_coef_loss   | 0.0156    |
|    learning_rate   | 0.0003    |
|    n_updates       | 224899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.57e+03 |
|    critic_loss     | 339       |
|    ent_coef        | 1.64      |
|    ent_coef_loss   | 0.0401    |
|    learning_rate   | 0.0003    |
|    n_updates       | 225299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.56e+03 |
|    critic_loss     | 192       |
|    ent_coef        | 1.63      |
|    ent_coef_loss   | -0.0341   |
|    learning_rate   | 0.0003    |
|    n_updates       | 225699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.58e+03 |
|    critic_loss     | 133       |
|    ent_coef        | 1.64      |
|    ent_coef_loss   | 0.0677    |
|    learning_rate   | 0.0003    |
|    n_updates       | 226099    |
----------------------------------
=== Iterazione IRL 591 ===
Loss reward (iter 591): 4.005331993103027
=== Iterazione IRL 592 ===
Loss reward (iter 592): 5.047669410705566
=== Iterazione IRL 593 ===
Loss reward (iter 593): 4.708835601806641
=== Iterazione IRL 594 ===
Loss reward (iter 594): 5.230988025665283
=== Iterazione IRL 595 ===
Loss reward (iter 595): 4.7096357345581055
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.58e+03 |
|    critic_loss     | 182       |
|    ent_coef        | 1.61      |
|    ent_coef_loss   | 0.00305   |
|    learning_rate   | 0.0003    |
|    n_updates       | 226399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 134       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.59e+03 |
|    critic_loss     | 149       |
|    ent_coef        | 1.62      |
|    ent_coef_loss   | 0.0121    |
|    learning_rate   | 0.0003    |
|    n_updates       | 226799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.58e+03 |
|    critic_loss     | 146       |
|    ent_coef        | 1.63      |
|    ent_coef_loss   | -0.00668  |
|    learning_rate   | 0.0003    |
|    n_updates       | 227199    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 16       |
|    fps             | 125      |
|    time_elapsed    | 12       |
|    total_timesteps | 1600     |
| train/             |          |
|    actor_loss      | -1.6e+03 |
|    critic_loss     | 140      |
|    ent_coef        | 1.64     |
|    ent_coef_loss   | 0.171    |
|    learning_rate   | 0.0003   |
|    n_updates       | 227599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 20       |
|    fps             | 124      |
|    time_elapsed    | 16       |
|    total_timesteps | 2000     |
| train/             |          |
|    actor_loss      | -1.6e+03 |
|    critic_loss     | 118      |
|    ent_coef        | 1.64     |
|    ent_coef_loss   | -0.00181 |
|    learning_rate   | 0.0003   |
|    n_updates       | 227999   |
---------------------------------
=== Iterazione IRL 596 ===
Loss reward (iter 596): 4.271997928619385
=== Iterazione IRL 597 ===
Loss reward (iter 597): 5.1636505126953125
=== Iterazione IRL 598 ===
Loss reward (iter 598): 4.991059303283691
=== Iterazione IRL 599 ===
Loss reward (iter 599): 4.987026691436768
=== Iterazione IRL 600 ===
Loss reward (iter 600): 4.728731632232666
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.57e+03 |
|    critic_loss     | 164       |
|    ent_coef        | 1.65      |
|    ent_coef_loss   | -0.0371   |
|    learning_rate   | 0.0003    |
|    n_updates       | 228299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 134       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.59e+03 |
|    critic_loss     | 151       |
|    ent_coef        | 1.64      |
|    ent_coef_loss   | 0.0749    |
|    learning_rate   | 0.0003    |
|    n_updates       | 228699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.59e+03 |
|    critic_loss     | 176       |
|    ent_coef        | 1.62      |
|    ent_coef_loss   | 0.000324  |
|    learning_rate   | 0.0003    |
|    n_updates       | 229099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.59e+03 |
|    critic_loss     | 143       |
|    ent_coef        | 1.62      |
|    ent_coef_loss   | 0.0129    |
|    learning_rate   | 0.0003    |
|    n_updates       | 229499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.61e+03 |
|    critic_loss     | 118       |
|    ent_coef        | 1.62      |
|    ent_coef_loss   | -0.0118   |
|    learning_rate   | 0.0003    |
|    n_updates       | 229899    |
----------------------------------
=== Iterazione IRL 601 ===
Loss reward (iter 601): 5.611893653869629
=== Iterazione IRL 602 ===
Loss reward (iter 602): 4.958427906036377
=== Iterazione IRL 603 ===
Loss reward (iter 603): 4.912542819976807
=== Iterazione IRL 604 ===
Loss reward (iter 604): 5.133697032928467
=== Iterazione IRL 605 ===
Loss reward (iter 605): 4.658642292022705
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.59e+03 |
|    critic_loss     | 120       |
|    ent_coef        | 1.61      |
|    ent_coef_loss   | 0.00067   |
|    learning_rate   | 0.0003    |
|    n_updates       | 230199    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -1.6e+03 |
|    critic_loss     | 186      |
|    ent_coef        | 1.64     |
|    ent_coef_loss   | -0.023   |
|    learning_rate   | 0.0003   |
|    n_updates       | 230599   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -1.6e+03 |
|    critic_loss     | 180      |
|    ent_coef        | 1.66     |
|    ent_coef_loss   | 0.00447  |
|    learning_rate   | 0.0003   |
|    n_updates       | 230999   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.59e+03 |
|    critic_loss     | 153       |
|    ent_coef        | 1.66      |
|    ent_coef_loss   | 0.0501    |
|    learning_rate   | 0.0003    |
|    n_updates       | 231399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.59e+03 |
|    critic_loss     | 210       |
|    ent_coef        | 1.63      |
|    ent_coef_loss   | 0.00785   |
|    learning_rate   | 0.0003    |
|    n_updates       | 231799    |
----------------------------------
=== Iterazione IRL 606 ===
Loss reward (iter 606): 4.320747375488281
=== Iterazione IRL 607 ===
Loss reward (iter 607): 4.8034443855285645
=== Iterazione IRL 608 ===
Loss reward (iter 608): 4.386373996734619
=== Iterazione IRL 609 ===
Loss reward (iter 609): 5.20757532119751
=== Iterazione IRL 610 ===
Loss reward (iter 610): 4.50464391708374
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.58e+03 |
|    critic_loss     | 181       |
|    ent_coef        | 1.61      |
|    ent_coef_loss   | 0.0107    |
|    learning_rate   | 0.0003    |
|    n_updates       | 232099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 134       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.61e+03 |
|    critic_loss     | 167       |
|    ent_coef        | 1.64      |
|    ent_coef_loss   | 0.00144   |
|    learning_rate   | 0.0003    |
|    n_updates       | 232499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.61e+03 |
|    critic_loss     | 169       |
|    ent_coef        | 1.64      |
|    ent_coef_loss   | -0.0204   |
|    learning_rate   | 0.0003    |
|    n_updates       | 232899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.61e+03 |
|    critic_loss     | 151       |
|    ent_coef        | 1.65      |
|    ent_coef_loss   | 0.0753    |
|    learning_rate   | 0.0003    |
|    n_updates       | 233299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.59e+03 |
|    critic_loss     | 234       |
|    ent_coef        | 1.67      |
|    ent_coef_loss   | -0.0393   |
|    learning_rate   | 0.0003    |
|    n_updates       | 233699    |
----------------------------------
=== Iterazione IRL 611 ===
Loss reward (iter 611): 5.38030481338501
=== Iterazione IRL 612 ===
Loss reward (iter 612): 4.912896633148193
=== Iterazione IRL 613 ===
Loss reward (iter 613): 4.21830415725708
=== Iterazione IRL 614 ===
Loss reward (iter 614): 5.34581995010376
=== Iterazione IRL 615 ===
Loss reward (iter 615): 4.985657691955566
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -1.6e+03 |
|    critic_loss     | 175      |
|    ent_coef        | 1.66     |
|    ent_coef_loss   | 0.0319   |
|    learning_rate   | 0.0003   |
|    n_updates       | 233999   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 134       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.62e+03 |
|    critic_loss     | 151       |
|    ent_coef        | 1.67      |
|    ent_coef_loss   | -0.00347  |
|    learning_rate   | 0.0003    |
|    n_updates       | 234399    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -1.6e+03 |
|    critic_loss     | 143      |
|    ent_coef        | 1.68     |
|    ent_coef_loss   | 0.03     |
|    learning_rate   | 0.0003   |
|    n_updates       | 234799   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.61e+03 |
|    critic_loss     | 138       |
|    ent_coef        | 1.68      |
|    ent_coef_loss   | -0.00503  |
|    learning_rate   | 0.0003    |
|    n_updates       | 235199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.62e+03 |
|    critic_loss     | 168       |
|    ent_coef        | 1.67      |
|    ent_coef_loss   | 3.94e-05  |
|    learning_rate   | 0.0003    |
|    n_updates       | 235599    |
----------------------------------
=== Iterazione IRL 616 ===
Loss reward (iter 616): 4.7496161460876465
=== Iterazione IRL 617 ===
Loss reward (iter 617): 3.46345853805542
=== Iterazione IRL 618 ===
Loss reward (iter 618): 5.225258827209473
=== Iterazione IRL 619 ===
Loss reward (iter 619): 4.8581953048706055
=== Iterazione IRL 620 ===
Loss reward (iter 620): 4.018598556518555
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.62e+03 |
|    critic_loss     | 136       |
|    ent_coef        | 1.68      |
|    ent_coef_loss   | 0.0724    |
|    learning_rate   | 0.0003    |
|    n_updates       | 235899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.62e+03 |
|    critic_loss     | 167       |
|    ent_coef        | 1.69      |
|    ent_coef_loss   | 0.0286    |
|    learning_rate   | 0.0003    |
|    n_updates       | 236299    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -1.6e+03 |
|    critic_loss     | 200      |
|    ent_coef        | 1.69     |
|    ent_coef_loss   | -0.132   |
|    learning_rate   | 0.0003   |
|    n_updates       | 236699   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.62e+03 |
|    critic_loss     | 205       |
|    ent_coef        | 1.68      |
|    ent_coef_loss   | -0.0037   |
|    learning_rate   | 0.0003    |
|    n_updates       | 237099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.63e+03 |
|    critic_loss     | 157       |
|    ent_coef        | 1.69      |
|    ent_coef_loss   | 0.0168    |
|    learning_rate   | 0.0003    |
|    n_updates       | 237499    |
----------------------------------
=== Iterazione IRL 621 ===
Loss reward (iter 621): 4.764678001403809
=== Iterazione IRL 622 ===
Loss reward (iter 622): 4.303702354431152
=== Iterazione IRL 623 ===
Loss reward (iter 623): 5.06490421295166
=== Iterazione IRL 624 ===
Loss reward (iter 624): 4.2842559814453125
=== Iterazione IRL 625 ===
Loss reward (iter 625): 4.219692707061768
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -1.6e+03 |
|    critic_loss     | 141      |
|    ent_coef        | 1.7      |
|    ent_coef_loss   | -0.0667  |
|    learning_rate   | 0.0003   |
|    n_updates       | 237799   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.62e+03 |
|    critic_loss     | 219       |
|    ent_coef        | 1.7       |
|    ent_coef_loss   | 0.0924    |
|    learning_rate   | 0.0003    |
|    n_updates       | 238199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.62e+03 |
|    critic_loss     | 100       |
|    ent_coef        | 1.7       |
|    ent_coef_loss   | 0.0302    |
|    learning_rate   | 0.0003    |
|    n_updates       | 238599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.62e+03 |
|    critic_loss     | 125       |
|    ent_coef        | 1.71      |
|    ent_coef_loss   | 0.0778    |
|    learning_rate   | 0.0003    |
|    n_updates       | 238999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.62e+03 |
|    critic_loss     | 136       |
|    ent_coef        | 1.71      |
|    ent_coef_loss   | -0.0248   |
|    learning_rate   | 0.0003    |
|    n_updates       | 239399    |
----------------------------------
=== Iterazione IRL 626 ===
Loss reward (iter 626): 5.069711208343506
=== Iterazione IRL 627 ===
Loss reward (iter 627): 4.980592727661133
=== Iterazione IRL 628 ===
Loss reward (iter 628): 3.7512733936309814
=== Iterazione IRL 629 ===
Loss reward (iter 629): 5.313196182250977
=== Iterazione IRL 630 ===
Loss reward (iter 630): 4.639376640319824
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.64e+03 |
|    critic_loss     | 129       |
|    ent_coef        | 1.7       |
|    ent_coef_loss   | 0.00193   |
|    learning_rate   | 0.0003    |
|    n_updates       | 239699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 134       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.63e+03 |
|    critic_loss     | 212       |
|    ent_coef        | 1.71      |
|    ent_coef_loss   | -0.0115   |
|    learning_rate   | 0.0003    |
|    n_updates       | 240099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.63e+03 |
|    critic_loss     | 162       |
|    ent_coef        | 1.71      |
|    ent_coef_loss   | 0.0519    |
|    learning_rate   | 0.0003    |
|    n_updates       | 240499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.62e+03 |
|    critic_loss     | 148       |
|    ent_coef        | 1.71      |
|    ent_coef_loss   | -0.113    |
|    learning_rate   | 0.0003    |
|    n_updates       | 240899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.63e+03 |
|    critic_loss     | 139       |
|    ent_coef        | 1.71      |
|    ent_coef_loss   | -0.0643   |
|    learning_rate   | 0.0003    |
|    n_updates       | 241299    |
----------------------------------
=== Iterazione IRL 631 ===
Loss reward (iter 631): 4.246708869934082
=== Iterazione IRL 632 ===
Loss reward (iter 632): 4.721604824066162
=== Iterazione IRL 633 ===
Loss reward (iter 633): 4.004467010498047
=== Iterazione IRL 634 ===
Loss reward (iter 634): 4.003715515136719
=== Iterazione IRL 635 ===
Loss reward (iter 635): 4.614505767822266
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.65e+03 |
|    critic_loss     | 129       |
|    ent_coef        | 1.73      |
|    ent_coef_loss   | -0.0127   |
|    learning_rate   | 0.0003    |
|    n_updates       | 241599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 134       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.64e+03 |
|    critic_loss     | 212       |
|    ent_coef        | 1.73      |
|    ent_coef_loss   | 0.0456    |
|    learning_rate   | 0.0003    |
|    n_updates       | 241999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.65e+03 |
|    critic_loss     | 197       |
|    ent_coef        | 1.72      |
|    ent_coef_loss   | -0.0104   |
|    learning_rate   | 0.0003    |
|    n_updates       | 242399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.64e+03 |
|    critic_loss     | 103       |
|    ent_coef        | 1.73      |
|    ent_coef_loss   | 0.0216    |
|    learning_rate   | 0.0003    |
|    n_updates       | 242799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.64e+03 |
|    critic_loss     | 166       |
|    ent_coef        | 1.74      |
|    ent_coef_loss   | 0.0163    |
|    learning_rate   | 0.0003    |
|    n_updates       | 243199    |
----------------------------------
=== Iterazione IRL 636 ===
Loss reward (iter 636): 4.507846355438232
=== Iterazione IRL 637 ===
Loss reward (iter 637): 4.417543888092041
=== Iterazione IRL 638 ===
Loss reward (iter 638): 4.04083251953125
=== Iterazione IRL 639 ===
Loss reward (iter 639): 4.645998477935791
=== Iterazione IRL 640 ===
Loss reward (iter 640): 4.168371200561523
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.64e+03 |
|    critic_loss     | 199       |
|    ent_coef        | 1.74      |
|    ent_coef_loss   | -0.028    |
|    learning_rate   | 0.0003    |
|    n_updates       | 243499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 134       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.64e+03 |
|    critic_loss     | 104       |
|    ent_coef        | 1.75      |
|    ent_coef_loss   | -0.00818  |
|    learning_rate   | 0.0003    |
|    n_updates       | 243899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.66e+03 |
|    critic_loss     | 196       |
|    ent_coef        | 1.74      |
|    ent_coef_loss   | 0.0342    |
|    learning_rate   | 0.0003    |
|    n_updates       | 244299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.66e+03 |
|    critic_loss     | 109       |
|    ent_coef        | 1.74      |
|    ent_coef_loss   | 0.0533    |
|    learning_rate   | 0.0003    |
|    n_updates       | 244699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.66e+03 |
|    critic_loss     | 136       |
|    ent_coef        | 1.75      |
|    ent_coef_loss   | -0.0104   |
|    learning_rate   | 0.0003    |
|    n_updates       | 245099    |
----------------------------------
=== Iterazione IRL 641 ===
Loss reward (iter 641): 4.696539878845215
=== Iterazione IRL 642 ===
Loss reward (iter 642): 4.098275184631348
=== Iterazione IRL 643 ===
Loss reward (iter 643): 3.7867913246154785
=== Iterazione IRL 644 ===
Loss reward (iter 644): 4.508688449859619
=== Iterazione IRL 645 ===
Loss reward (iter 645): 4.888217926025391
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.64e+03 |
|    critic_loss     | 166       |
|    ent_coef        | 1.76      |
|    ent_coef_loss   | -0.048    |
|    learning_rate   | 0.0003    |
|    n_updates       | 245399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 134       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.66e+03 |
|    critic_loss     | 137       |
|    ent_coef        | 1.76      |
|    ent_coef_loss   | -0.00786  |
|    learning_rate   | 0.0003    |
|    n_updates       | 245799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.66e+03 |
|    critic_loss     | 151       |
|    ent_coef        | 1.76      |
|    ent_coef_loss   | 0.025     |
|    learning_rate   | 0.0003    |
|    n_updates       | 246199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.66e+03 |
|    critic_loss     | 140       |
|    ent_coef        | 1.76      |
|    ent_coef_loss   | -0.00748  |
|    learning_rate   | 0.0003    |
|    n_updates       | 246599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.66e+03 |
|    critic_loss     | 231       |
|    ent_coef        | 1.77      |
|    ent_coef_loss   | -0.00806  |
|    learning_rate   | 0.0003    |
|    n_updates       | 246999    |
----------------------------------
=== Iterazione IRL 646 ===
Loss reward (iter 646): 4.904491901397705
=== Iterazione IRL 647 ===
Loss reward (iter 647): 5.017800807952881
=== Iterazione IRL 648 ===
Loss reward (iter 648): 4.7301926612854
=== Iterazione IRL 649 ===
Loss reward (iter 649): 5.198015213012695
=== Iterazione IRL 650 ===
Loss reward (iter 650): 5.361875057220459
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.67e+03 |
|    critic_loss     | 153       |
|    ent_coef        | 1.78      |
|    ent_coef_loss   | 0.0343    |
|    learning_rate   | 0.0003    |
|    n_updates       | 247299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 134       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.67e+03 |
|    critic_loss     | 157       |
|    ent_coef        | 1.78      |
|    ent_coef_loss   | -0.0678   |
|    learning_rate   | 0.0003    |
|    n_updates       | 247699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.66e+03 |
|    critic_loss     | 110       |
|    ent_coef        | 1.75      |
|    ent_coef_loss   | -0.042    |
|    learning_rate   | 0.0003    |
|    n_updates       | 248099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.68e+03 |
|    critic_loss     | 113       |
|    ent_coef        | 1.74      |
|    ent_coef_loss   | 0.0693    |
|    learning_rate   | 0.0003    |
|    n_updates       | 248499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.66e+03 |
|    critic_loss     | 149       |
|    ent_coef        | 1.75      |
|    ent_coef_loss   | -0.121    |
|    learning_rate   | 0.0003    |
|    n_updates       | 248899    |
----------------------------------
=== Iterazione IRL 651 ===
Loss reward (iter 651): 4.953273773193359
=== Iterazione IRL 652 ===
Loss reward (iter 652): 4.582528591156006
=== Iterazione IRL 653 ===
Loss reward (iter 653): 4.404428482055664
=== Iterazione IRL 654 ===
Loss reward (iter 654): 4.663607120513916
=== Iterazione IRL 655 ===
Loss reward (iter 655): 4.515099048614502
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.67e+03 |
|    critic_loss     | 122       |
|    ent_coef        | 1.75      |
|    ent_coef_loss   | 0.0546    |
|    learning_rate   | 0.0003    |
|    n_updates       | 249199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.67e+03 |
|    critic_loss     | 104       |
|    ent_coef        | 1.75      |
|    ent_coef_loss   | -0.0426   |
|    learning_rate   | 0.0003    |
|    n_updates       | 249599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.68e+03 |
|    critic_loss     | 112       |
|    ent_coef        | 1.74      |
|    ent_coef_loss   | 0.023     |
|    learning_rate   | 0.0003    |
|    n_updates       | 249999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.68e+03 |
|    critic_loss     | 106       |
|    ent_coef        | 1.75      |
|    ent_coef_loss   | -0.0841   |
|    learning_rate   | 0.0003    |
|    n_updates       | 250399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.67e+03 |
|    critic_loss     | 189       |
|    ent_coef        | 1.77      |
|    ent_coef_loss   | -0.0653   |
|    learning_rate   | 0.0003    |
|    n_updates       | 250799    |
----------------------------------
=== Iterazione IRL 656 ===
Loss reward (iter 656): 4.531475067138672
=== Iterazione IRL 657 ===
Loss reward (iter 657): 4.682114124298096
=== Iterazione IRL 658 ===
Loss reward (iter 658): 4.509748935699463
=== Iterazione IRL 659 ===
Loss reward (iter 659): 5.716541767120361
=== Iterazione IRL 660 ===
Loss reward (iter 660): 5.9718546867370605
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.67e+03 |
|    critic_loss     | 148       |
|    ent_coef        | 1.75      |
|    ent_coef_loss   | 0.00572   |
|    learning_rate   | 0.0003    |
|    n_updates       | 251099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.69e+03 |
|    critic_loss     | 114       |
|    ent_coef        | 1.76      |
|    ent_coef_loss   | 0.0147    |
|    learning_rate   | 0.0003    |
|    n_updates       | 251499    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -1.7e+03 |
|    critic_loss     | 127      |
|    ent_coef        | 1.79     |
|    ent_coef_loss   | 0.11     |
|    learning_rate   | 0.0003   |
|    n_updates       | 251899   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.68e+03 |
|    critic_loss     | 175       |
|    ent_coef        | 1.77      |
|    ent_coef_loss   | -0.0653   |
|    learning_rate   | 0.0003    |
|    n_updates       | 252299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.68e+03 |
|    critic_loss     | 164       |
|    ent_coef        | 1.76      |
|    ent_coef_loss   | -0.0419   |
|    learning_rate   | 0.0003    |
|    n_updates       | 252699    |
----------------------------------
=== Iterazione IRL 661 ===
Loss reward (iter 661): 5.783110618591309
=== Iterazione IRL 662 ===
Loss reward (iter 662): 4.458028316497803
=== Iterazione IRL 663 ===
Loss reward (iter 663): 4.654355049133301
=== Iterazione IRL 664 ===
Loss reward (iter 664): 5.754998207092285
=== Iterazione IRL 665 ===
Loss reward (iter 665): 4.533441543579102
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.68e+03 |
|    critic_loss     | 172       |
|    ent_coef        | 1.76      |
|    ent_coef_loss   | -0.00574  |
|    learning_rate   | 0.0003    |
|    n_updates       | 252999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 134       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.71e+03 |
|    critic_loss     | 200       |
|    ent_coef        | 1.76      |
|    ent_coef_loss   | 0.00794   |
|    learning_rate   | 0.0003    |
|    n_updates       | 253399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.69e+03 |
|    critic_loss     | 156       |
|    ent_coef        | 1.76      |
|    ent_coef_loss   | 0.0506    |
|    learning_rate   | 0.0003    |
|    n_updates       | 253799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.71e+03 |
|    critic_loss     | 140       |
|    ent_coef        | 1.75      |
|    ent_coef_loss   | 0.0667    |
|    learning_rate   | 0.0003    |
|    n_updates       | 254199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.71e+03 |
|    critic_loss     | 163       |
|    ent_coef        | 1.75      |
|    ent_coef_loss   | -0.0124   |
|    learning_rate   | 0.0003    |
|    n_updates       | 254599    |
----------------------------------
=== Iterazione IRL 666 ===
Loss reward (iter 666): 4.864258766174316
=== Iterazione IRL 667 ===
Loss reward (iter 667): 4.864616870880127
=== Iterazione IRL 668 ===
Loss reward (iter 668): 4.629489898681641
=== Iterazione IRL 669 ===
Loss reward (iter 669): 4.9177093505859375
=== Iterazione IRL 670 ===
Loss reward (iter 670): 4.997757434844971
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.71e+03 |
|    critic_loss     | 122       |
|    ent_coef        | 1.75      |
|    ent_coef_loss   | 0.0381    |
|    learning_rate   | 0.0003    |
|    n_updates       | 254899    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -1.7e+03 |
|    critic_loss     | 240      |
|    ent_coef        | 1.77     |
|    ent_coef_loss   | -0.109   |
|    learning_rate   | 0.0003   |
|    n_updates       | 255299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -1.7e+03 |
|    critic_loss     | 116      |
|    ent_coef        | 1.79     |
|    ent_coef_loss   | -0.0374  |
|    learning_rate   | 0.0003   |
|    n_updates       | 255699   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.71e+03 |
|    critic_loss     | 131       |
|    ent_coef        | 1.77      |
|    ent_coef_loss   | 0.0147    |
|    learning_rate   | 0.0003    |
|    n_updates       | 256099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.71e+03 |
|    critic_loss     | 147       |
|    ent_coef        | 1.79      |
|    ent_coef_loss   | -0.0444   |
|    learning_rate   | 0.0003    |
|    n_updates       | 256499    |
----------------------------------
=== Iterazione IRL 671 ===
Loss reward (iter 671): 6.101121425628662
=== Iterazione IRL 672 ===
Loss reward (iter 672): 5.604928970336914
=== Iterazione IRL 673 ===
Loss reward (iter 673): 5.194596290588379
=== Iterazione IRL 674 ===
Loss reward (iter 674): 5.440070152282715
=== Iterazione IRL 675 ===
Loss reward (iter 675): 5.491050720214844
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.71e+03 |
|    critic_loss     | 113       |
|    ent_coef        | 1.79      |
|    ent_coef_loss   | 0.0191    |
|    learning_rate   | 0.0003    |
|    n_updates       | 256799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 134       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.73e+03 |
|    critic_loss     | 137       |
|    ent_coef        | 1.8       |
|    ent_coef_loss   | -0.0241   |
|    learning_rate   | 0.0003    |
|    n_updates       | 257199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.72e+03 |
|    critic_loss     | 104       |
|    ent_coef        | 1.8       |
|    ent_coef_loss   | -0.0233   |
|    learning_rate   | 0.0003    |
|    n_updates       | 257599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.72e+03 |
|    critic_loss     | 177       |
|    ent_coef        | 1.8       |
|    ent_coef_loss   | 0.0478    |
|    learning_rate   | 0.0003    |
|    n_updates       | 257999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.73e+03 |
|    critic_loss     | 103       |
|    ent_coef        | 1.84      |
|    ent_coef_loss   | -0.00465  |
|    learning_rate   | 0.0003    |
|    n_updates       | 258399    |
----------------------------------
=== Iterazione IRL 676 ===
Loss reward (iter 676): 5.430940628051758
=== Iterazione IRL 677 ===
Loss reward (iter 677): 4.71842098236084
=== Iterazione IRL 678 ===
Loss reward (iter 678): 5.248912334442139
=== Iterazione IRL 679 ===
Loss reward (iter 679): 4.544369697570801
=== Iterazione IRL 680 ===
Loss reward (iter 680): 5.839289665222168
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.74e+03 |
|    critic_loss     | 139       |
|    ent_coef        | 1.83      |
|    ent_coef_loss   | 0.0408    |
|    learning_rate   | 0.0003    |
|    n_updates       | 258699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 134       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.73e+03 |
|    critic_loss     | 136       |
|    ent_coef        | 1.84      |
|    ent_coef_loss   | 0.0384    |
|    learning_rate   | 0.0003    |
|    n_updates       | 259099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.73e+03 |
|    critic_loss     | 162       |
|    ent_coef        | 1.82      |
|    ent_coef_loss   | 0.00472   |
|    learning_rate   | 0.0003    |
|    n_updates       | 259499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.74e+03 |
|    critic_loss     | 140       |
|    ent_coef        | 1.83      |
|    ent_coef_loss   | -0.0652   |
|    learning_rate   | 0.0003    |
|    n_updates       | 259899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.73e+03 |
|    critic_loss     | 118       |
|    ent_coef        | 1.84      |
|    ent_coef_loss   | 0.00129   |
|    learning_rate   | 0.0003    |
|    n_updates       | 260299    |
----------------------------------
=== Iterazione IRL 681 ===
Loss reward (iter 681): 5.599374771118164
=== Iterazione IRL 682 ===
Loss reward (iter 682): 6.100875377655029
=== Iterazione IRL 683 ===
Loss reward (iter 683): 3.7084383964538574
=== Iterazione IRL 684 ===
Loss reward (iter 684): 5.211717128753662
=== Iterazione IRL 685 ===
Loss reward (iter 685): 5.040472030639648
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.73e+03 |
|    critic_loss     | 189       |
|    ent_coef        | 1.84      |
|    ent_coef_loss   | -0.0888   |
|    learning_rate   | 0.0003    |
|    n_updates       | 260599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.75e+03 |
|    critic_loss     | 93.4      |
|    ent_coef        | 1.86      |
|    ent_coef_loss   | -0.0163   |
|    learning_rate   | 0.0003    |
|    n_updates       | 260999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.75e+03 |
|    critic_loss     | 188       |
|    ent_coef        | 1.86      |
|    ent_coef_loss   | -0.0512   |
|    learning_rate   | 0.0003    |
|    n_updates       | 261399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.75e+03 |
|    critic_loss     | 142       |
|    ent_coef        | 1.85      |
|    ent_coef_loss   | -0.0182   |
|    learning_rate   | 0.0003    |
|    n_updates       | 261799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.75e+03 |
|    critic_loss     | 108       |
|    ent_coef        | 1.86      |
|    ent_coef_loss   | -0.0767   |
|    learning_rate   | 0.0003    |
|    n_updates       | 262199    |
----------------------------------
=== Iterazione IRL 686 ===
Loss reward (iter 686): 6.957563400268555
=== Iterazione IRL 687 ===
Loss reward (iter 687): 4.428282737731934
=== Iterazione IRL 688 ===
Loss reward (iter 688): 5.456826210021973
=== Iterazione IRL 689 ===
Loss reward (iter 689): 6.298585891723633
=== Iterazione IRL 690 ===
Loss reward (iter 690): 5.928493499755859
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.73e+03 |
|    critic_loss     | 166       |
|    ent_coef        | 1.88      |
|    ent_coef_loss   | -0.102    |
|    learning_rate   | 0.0003    |
|    n_updates       | 262499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.75e+03 |
|    critic_loss     | 148       |
|    ent_coef        | 1.86      |
|    ent_coef_loss   | 0.0899    |
|    learning_rate   | 0.0003    |
|    n_updates       | 262899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.74e+03 |
|    critic_loss     | 163       |
|    ent_coef        | 1.88      |
|    ent_coef_loss   | -0.0475   |
|    learning_rate   | 0.0003    |
|    n_updates       | 263299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.77e+03 |
|    critic_loss     | 166       |
|    ent_coef        | 1.89      |
|    ent_coef_loss   | -0.0842   |
|    learning_rate   | 0.0003    |
|    n_updates       | 263699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.76e+03 |
|    critic_loss     | 112       |
|    ent_coef        | 1.9       |
|    ent_coef_loss   | 0.0333    |
|    learning_rate   | 0.0003    |
|    n_updates       | 264099    |
----------------------------------
=== Iterazione IRL 691 ===
Loss reward (iter 691): 5.27397346496582
=== Iterazione IRL 692 ===
Loss reward (iter 692): 5.837688446044922
=== Iterazione IRL 693 ===
Loss reward (iter 693): 5.454086780548096
=== Iterazione IRL 694 ===
Loss reward (iter 694): 4.883814811706543
=== Iterazione IRL 695 ===
Loss reward (iter 695): 4.90657377243042
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.76e+03 |
|    critic_loss     | 134       |
|    ent_coef        | 1.88      |
|    ent_coef_loss   | 0.00169   |
|    learning_rate   | 0.0003    |
|    n_updates       | 264399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.75e+03 |
|    critic_loss     | 137       |
|    ent_coef        | 1.9       |
|    ent_coef_loss   | -0.03     |
|    learning_rate   | 0.0003    |
|    n_updates       | 264799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.78e+03 |
|    critic_loss     | 105       |
|    ent_coef        | 1.89      |
|    ent_coef_loss   | 0.0594    |
|    learning_rate   | 0.0003    |
|    n_updates       | 265199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.77e+03 |
|    critic_loss     | 83.9      |
|    ent_coef        | 1.89      |
|    ent_coef_loss   | 0.00285   |
|    learning_rate   | 0.0003    |
|    n_updates       | 265599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.77e+03 |
|    critic_loss     | 156       |
|    ent_coef        | 1.87      |
|    ent_coef_loss   | 0.0821    |
|    learning_rate   | 0.0003    |
|    n_updates       | 265999    |
----------------------------------
=== Iterazione IRL 696 ===
Loss reward (iter 696): 4.854194641113281
=== Iterazione IRL 697 ===
Loss reward (iter 697): 4.931535720825195
=== Iterazione IRL 698 ===
Loss reward (iter 698): 4.662014961242676
=== Iterazione IRL 699 ===
Loss reward (iter 699): 4.886918067932129
=== Iterazione IRL 700 ===
Loss reward (iter 700): 5.742363929748535
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.77e+03 |
|    critic_loss     | 165       |
|    ent_coef        | 1.86      |
|    ent_coef_loss   | -0.0303   |
|    learning_rate   | 0.0003    |
|    n_updates       | 266299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.77e+03 |
|    critic_loss     | 145       |
|    ent_coef        | 1.87      |
|    ent_coef_loss   | -0.0306   |
|    learning_rate   | 0.0003    |
|    n_updates       | 266699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.77e+03 |
|    critic_loss     | 129       |
|    ent_coef        | 1.89      |
|    ent_coef_loss   | 0.000524  |
|    learning_rate   | 0.0003    |
|    n_updates       | 267099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.78e+03 |
|    critic_loss     | 156       |
|    ent_coef        | 1.9       |
|    ent_coef_loss   | 0.0259    |
|    learning_rate   | 0.0003    |
|    n_updates       | 267499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.78e+03 |
|    critic_loss     | 152       |
|    ent_coef        | 1.91      |
|    ent_coef_loss   | -0.00677  |
|    learning_rate   | 0.0003    |
|    n_updates       | 267899    |
----------------------------------
=== Iterazione IRL 701 ===
Loss reward (iter 701): 4.904375076293945
=== Iterazione IRL 702 ===
Loss reward (iter 702): 5.301795959472656
=== Iterazione IRL 703 ===
Loss reward (iter 703): 4.501397132873535
=== Iterazione IRL 704 ===
Loss reward (iter 704): 5.524171352386475
=== Iterazione IRL 705 ===
Loss reward (iter 705): 4.625043869018555
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.78e+03 |
|    critic_loss     | 144       |
|    ent_coef        | 1.9       |
|    ent_coef_loss   | 0.0193    |
|    learning_rate   | 0.0003    |
|    n_updates       | 268199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.79e+03 |
|    critic_loss     | 177       |
|    ent_coef        | 1.89      |
|    ent_coef_loss   | 0.0369    |
|    learning_rate   | 0.0003    |
|    n_updates       | 268599    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -1.8e+03 |
|    critic_loss     | 138      |
|    ent_coef        | 1.88     |
|    ent_coef_loss   | 0.0267   |
|    learning_rate   | 0.0003   |
|    n_updates       | 268999   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.78e+03 |
|    critic_loss     | 186       |
|    ent_coef        | 1.92      |
|    ent_coef_loss   | 0.0271    |
|    learning_rate   | 0.0003    |
|    n_updates       | 269399    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 20       |
|    fps             | 124      |
|    time_elapsed    | 16       |
|    total_timesteps | 2000     |
| train/             |          |
|    actor_loss      | -1.8e+03 |
|    critic_loss     | 128      |
|    ent_coef        | 1.91     |
|    ent_coef_loss   | 0.0875   |
|    learning_rate   | 0.0003   |
|    n_updates       | 269799   |
---------------------------------
=== Iterazione IRL 706 ===
Loss reward (iter 706): 5.072466850280762
=== Iterazione IRL 707 ===
Loss reward (iter 707): 5.415355205535889
=== Iterazione IRL 708 ===
Loss reward (iter 708): 4.614706516265869
=== Iterazione IRL 709 ===
Loss reward (iter 709): 5.54752779006958
=== Iterazione IRL 710 ===
Loss reward (iter 710): 5.329580307006836
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -1.8e+03 |
|    critic_loss     | 112      |
|    ent_coef        | 1.9      |
|    ent_coef_loss   | -0.0173  |
|    learning_rate   | 0.0003   |
|    n_updates       | 270099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -1.8e+03 |
|    critic_loss     | 127      |
|    ent_coef        | 1.89     |
|    ent_coef_loss   | -0.0317  |
|    learning_rate   | 0.0003   |
|    n_updates       | 270499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -1.8e+03 |
|    critic_loss     | 120      |
|    ent_coef        | 1.9      |
|    ent_coef_loss   | 0.0241   |
|    learning_rate   | 0.0003   |
|    n_updates       | 270899   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 16       |
|    fps             | 125      |
|    time_elapsed    | 12       |
|    total_timesteps | 1600     |
| train/             |          |
|    actor_loss      | -1.8e+03 |
|    critic_loss     | 157      |
|    ent_coef        | 1.91     |
|    ent_coef_loss   | -0.0696  |
|    learning_rate   | 0.0003   |
|    n_updates       | 271299   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.78e+03 |
|    critic_loss     | 181       |
|    ent_coef        | 1.91      |
|    ent_coef_loss   | -0.0207   |
|    learning_rate   | 0.0003    |
|    n_updates       | 271699    |
----------------------------------
=== Iterazione IRL 711 ===
Loss reward (iter 711): 4.657825946807861
=== Iterazione IRL 712 ===
Loss reward (iter 712): 5.402407169342041
=== Iterazione IRL 713 ===
Loss reward (iter 713): 5.006496906280518
=== Iterazione IRL 714 ===
Loss reward (iter 714): 5.412463188171387
=== Iterazione IRL 715 ===
Loss reward (iter 715): 4.915637969970703
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -1.8e+03 |
|    critic_loss     | 158      |
|    ent_coef        | 1.93     |
|    ent_coef_loss   | 0.00773  |
|    learning_rate   | 0.0003   |
|    n_updates       | 271999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -1.8e+03 |
|    critic_loss     | 112      |
|    ent_coef        | 1.93     |
|    ent_coef_loss   | -0.0123  |
|    learning_rate   | 0.0003   |
|    n_updates       | 272399   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.81e+03 |
|    critic_loss     | 140       |
|    ent_coef        | 1.93      |
|    ent_coef_loss   | 0.0456    |
|    learning_rate   | 0.0003    |
|    n_updates       | 272799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.82e+03 |
|    critic_loss     | 191       |
|    ent_coef        | 1.95      |
|    ent_coef_loss   | -0.0085   |
|    learning_rate   | 0.0003    |
|    n_updates       | 273199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.81e+03 |
|    critic_loss     | 111       |
|    ent_coef        | 1.95      |
|    ent_coef_loss   | 0.0575    |
|    learning_rate   | 0.0003    |
|    n_updates       | 273599    |
----------------------------------
=== Iterazione IRL 716 ===
Loss reward (iter 716): 4.473108291625977
=== Iterazione IRL 717 ===
Loss reward (iter 717): 4.640795707702637
=== Iterazione IRL 718 ===
Loss reward (iter 718): 5.078245639801025
=== Iterazione IRL 719 ===
Loss reward (iter 719): 5.319649696350098
=== Iterazione IRL 720 ===
Loss reward (iter 720): 4.25736141204834
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -1.8e+03 |
|    critic_loss     | 147      |
|    ent_coef        | 1.94     |
|    ent_coef_loss   | 0.017    |
|    learning_rate   | 0.0003   |
|    n_updates       | 273899   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.82e+03 |
|    critic_loss     | 149       |
|    ent_coef        | 1.91      |
|    ent_coef_loss   | 0.0704    |
|    learning_rate   | 0.0003    |
|    n_updates       | 274299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.79e+03 |
|    critic_loss     | 149       |
|    ent_coef        | 1.9       |
|    ent_coef_loss   | -0.0614   |
|    learning_rate   | 0.0003    |
|    n_updates       | 274699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.81e+03 |
|    critic_loss     | 213       |
|    ent_coef        | 1.91      |
|    ent_coef_loss   | -0.115    |
|    learning_rate   | 0.0003    |
|    n_updates       | 275099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.81e+03 |
|    critic_loss     | 132       |
|    ent_coef        | 1.93      |
|    ent_coef_loss   | 0.0296    |
|    learning_rate   | 0.0003    |
|    n_updates       | 275499    |
----------------------------------
=== Iterazione IRL 721 ===
Loss reward (iter 721): 4.889429092407227
=== Iterazione IRL 722 ===
Loss reward (iter 722): 6.199212074279785
=== Iterazione IRL 723 ===
Loss reward (iter 723): 5.266087532043457
=== Iterazione IRL 724 ===
Loss reward (iter 724): 5.807807922363281
=== Iterazione IRL 725 ===
Loss reward (iter 725): 5.434858322143555
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.83e+03 |
|    critic_loss     | 155       |
|    ent_coef        | 1.95      |
|    ent_coef_loss   | -0.0999   |
|    learning_rate   | 0.0003    |
|    n_updates       | 275799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.83e+03 |
|    critic_loss     | 193       |
|    ent_coef        | 1.97      |
|    ent_coef_loss   | 0.0273    |
|    learning_rate   | 0.0003    |
|    n_updates       | 276199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.84e+03 |
|    critic_loss     | 148       |
|    ent_coef        | 1.97      |
|    ent_coef_loss   | -0.05     |
|    learning_rate   | 0.0003    |
|    n_updates       | 276599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.81e+03 |
|    critic_loss     | 132       |
|    ent_coef        | 1.98      |
|    ent_coef_loss   | 0.0537    |
|    learning_rate   | 0.0003    |
|    n_updates       | 276999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.84e+03 |
|    critic_loss     | 133       |
|    ent_coef        | 1.97      |
|    ent_coef_loss   | 0.0857    |
|    learning_rate   | 0.0003    |
|    n_updates       | 277399    |
----------------------------------
=== Iterazione IRL 726 ===
Loss reward (iter 726): 5.325506210327148
=== Iterazione IRL 727 ===
Loss reward (iter 727): 5.445641994476318
=== Iterazione IRL 728 ===
Loss reward (iter 728): 5.165689945220947
=== Iterazione IRL 729 ===
Loss reward (iter 729): 5.118914604187012
=== Iterazione IRL 730 ===
Loss reward (iter 730): 5.342706680297852
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.83e+03 |
|    critic_loss     | 147       |
|    ent_coef        | 1.98      |
|    ent_coef_loss   | -0.0256   |
|    learning_rate   | 0.0003    |
|    n_updates       | 277699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.83e+03 |
|    critic_loss     | 125       |
|    ent_coef        | 1.96      |
|    ent_coef_loss   | 0.00278   |
|    learning_rate   | 0.0003    |
|    n_updates       | 278099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.84e+03 |
|    critic_loss     | 176       |
|    ent_coef        | 1.94      |
|    ent_coef_loss   | -0.0282   |
|    learning_rate   | 0.0003    |
|    n_updates       | 278499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.86e+03 |
|    critic_loss     | 167       |
|    ent_coef        | 1.95      |
|    ent_coef_loss   | 0.0203    |
|    learning_rate   | 0.0003    |
|    n_updates       | 278899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.84e+03 |
|    critic_loss     | 158       |
|    ent_coef        | 1.98      |
|    ent_coef_loss   | -0.000972 |
|    learning_rate   | 0.0003    |
|    n_updates       | 279299    |
----------------------------------
=== Iterazione IRL 731 ===
Loss reward (iter 731): 5.36115837097168
=== Iterazione IRL 732 ===
Loss reward (iter 732): 4.858461380004883
=== Iterazione IRL 733 ===
Loss reward (iter 733): 5.307100296020508
=== Iterazione IRL 734 ===
Loss reward (iter 734): 5.358395099639893
=== Iterazione IRL 735 ===
Loss reward (iter 735): 5.162734031677246
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.85e+03 |
|    critic_loss     | 167       |
|    ent_coef        | 1.98      |
|    ent_coef_loss   | 0.085     |
|    learning_rate   | 0.0003    |
|    n_updates       | 279599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.85e+03 |
|    critic_loss     | 195       |
|    ent_coef        | 2         |
|    ent_coef_loss   | -0.0131   |
|    learning_rate   | 0.0003    |
|    n_updates       | 279999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.86e+03 |
|    critic_loss     | 140       |
|    ent_coef        | 2         |
|    ent_coef_loss   | -0.0771   |
|    learning_rate   | 0.0003    |
|    n_updates       | 280399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.87e+03 |
|    critic_loss     | 126       |
|    ent_coef        | 2.01      |
|    ent_coef_loss   | -0.097    |
|    learning_rate   | 0.0003    |
|    n_updates       | 280799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.85e+03 |
|    critic_loss     | 152       |
|    ent_coef        | 2.01      |
|    ent_coef_loss   | -0.052    |
|    learning_rate   | 0.0003    |
|    n_updates       | 281199    |
----------------------------------
=== Iterazione IRL 736 ===
Loss reward (iter 736): 5.286386489868164
=== Iterazione IRL 737 ===
Loss reward (iter 737): 5.565261363983154
=== Iterazione IRL 738 ===
Loss reward (iter 738): 4.8289289474487305
=== Iterazione IRL 739 ===
Loss reward (iter 739): 5.590995788574219
=== Iterazione IRL 740 ===
Loss reward (iter 740): 5.5489983558654785
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.85e+03 |
|    critic_loss     | 177       |
|    ent_coef        | 2.03      |
|    ent_coef_loss   | 0.0554    |
|    learning_rate   | 0.0003    |
|    n_updates       | 281499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 134       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.86e+03 |
|    critic_loss     | 125       |
|    ent_coef        | 2.02      |
|    ent_coef_loss   | 0.0476    |
|    learning_rate   | 0.0003    |
|    n_updates       | 281899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.87e+03 |
|    critic_loss     | 157       |
|    ent_coef        | 2.02      |
|    ent_coef_loss   | 0.107     |
|    learning_rate   | 0.0003    |
|    n_updates       | 282299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.86e+03 |
|    critic_loss     | 146       |
|    ent_coef        | 2.03      |
|    ent_coef_loss   | 0.00876   |
|    learning_rate   | 0.0003    |
|    n_updates       | 282699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.86e+03 |
|    critic_loss     | 119       |
|    ent_coef        | 2.03      |
|    ent_coef_loss   | 0.0268    |
|    learning_rate   | 0.0003    |
|    n_updates       | 283099    |
----------------------------------
=== Iterazione IRL 741 ===
Loss reward (iter 741): 5.406698703765869
=== Iterazione IRL 742 ===
Loss reward (iter 742): 5.977304458618164
=== Iterazione IRL 743 ===
Loss reward (iter 743): 5.528026103973389
=== Iterazione IRL 744 ===
Loss reward (iter 744): 5.473578453063965
=== Iterazione IRL 745 ===
Loss reward (iter 745): 5.330542087554932
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.85e+03 |
|    critic_loss     | 159       |
|    ent_coef        | 2.03      |
|    ent_coef_loss   | -0.128    |
|    learning_rate   | 0.0003    |
|    n_updates       | 283399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 134       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.87e+03 |
|    critic_loss     | 161       |
|    ent_coef        | 2.01      |
|    ent_coef_loss   | -0.0493   |
|    learning_rate   | 0.0003    |
|    n_updates       | 283799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.88e+03 |
|    critic_loss     | 115       |
|    ent_coef        | 2.03      |
|    ent_coef_loss   | -0.0142   |
|    learning_rate   | 0.0003    |
|    n_updates       | 284199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.87e+03 |
|    critic_loss     | 158       |
|    ent_coef        | 2.05      |
|    ent_coef_loss   | -0.0461   |
|    learning_rate   | 0.0003    |
|    n_updates       | 284599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 123       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.86e+03 |
|    critic_loss     | 124       |
|    ent_coef        | 2.05      |
|    ent_coef_loss   | 0.00993   |
|    learning_rate   | 0.0003    |
|    n_updates       | 284999    |
----------------------------------
=== Iterazione IRL 746 ===
Loss reward (iter 746): 4.876468658447266
=== Iterazione IRL 747 ===
Loss reward (iter 747): 4.941521167755127
=== Iterazione IRL 748 ===
Loss reward (iter 748): 5.168276309967041
=== Iterazione IRL 749 ===
Loss reward (iter 749): 4.474969387054443
=== Iterazione IRL 750 ===
Loss reward (iter 750): 5.390196323394775
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.87e+03 |
|    critic_loss     | 197       |
|    ent_coef        | 2.02      |
|    ent_coef_loss   | 0.0448    |
|    learning_rate   | 0.0003    |
|    n_updates       | 285299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.89e+03 |
|    critic_loss     | 178       |
|    ent_coef        | 2.02      |
|    ent_coef_loss   | 0.08      |
|    learning_rate   | 0.0003    |
|    n_updates       | 285699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.88e+03 |
|    critic_loss     | 159       |
|    ent_coef        | 2.02      |
|    ent_coef_loss   | 0.0418    |
|    learning_rate   | 0.0003    |
|    n_updates       | 286099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.89e+03 |
|    critic_loss     | 181       |
|    ent_coef        | 2.03      |
|    ent_coef_loss   | 0.0722    |
|    learning_rate   | 0.0003    |
|    n_updates       | 286499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 123       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.89e+03 |
|    critic_loss     | 131       |
|    ent_coef        | 2.05      |
|    ent_coef_loss   | 0.026     |
|    learning_rate   | 0.0003    |
|    n_updates       | 286899    |
----------------------------------
=== Iterazione IRL 751 ===
Loss reward (iter 751): 5.647963047027588
=== Iterazione IRL 752 ===
Loss reward (iter 752): 5.350239276885986
=== Iterazione IRL 753 ===
Loss reward (iter 753): 5.064664363861084
=== Iterazione IRL 754 ===
Loss reward (iter 754): 5.53863000869751
=== Iterazione IRL 755 ===
Loss reward (iter 755): 4.857089996337891
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.89e+03 |
|    critic_loss     | 108       |
|    ent_coef        | 2.05      |
|    ent_coef_loss   | 0.00817   |
|    learning_rate   | 0.0003    |
|    n_updates       | 287199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 134       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.89e+03 |
|    critic_loss     | 112       |
|    ent_coef        | 2.06      |
|    ent_coef_loss   | -0.0441   |
|    learning_rate   | 0.0003    |
|    n_updates       | 287599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.89e+03 |
|    critic_loss     | 165       |
|    ent_coef        | 2.07      |
|    ent_coef_loss   | -0.0677   |
|    learning_rate   | 0.0003    |
|    n_updates       | 287999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.89e+03 |
|    critic_loss     | 124       |
|    ent_coef        | 2.05      |
|    ent_coef_loss   | -0.00712  |
|    learning_rate   | 0.0003    |
|    n_updates       | 288399    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 20       |
|    fps             | 124      |
|    time_elapsed    | 16       |
|    total_timesteps | 2000     |
| train/             |          |
|    actor_loss      | -1.9e+03 |
|    critic_loss     | 142      |
|    ent_coef        | 2.04     |
|    ent_coef_loss   | -0.0299  |
|    learning_rate   | 0.0003   |
|    n_updates       | 288799   |
---------------------------------
=== Iterazione IRL 756 ===
Loss reward (iter 756): 5.8507537841796875
=== Iterazione IRL 757 ===
Loss reward (iter 757): 6.169335842132568
=== Iterazione IRL 758 ===
Loss reward (iter 758): 5.676872253417969
=== Iterazione IRL 759 ===
Loss reward (iter 759): 5.779747009277344
=== Iterazione IRL 760 ===
Loss reward (iter 760): 5.941800594329834
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.91e+03 |
|    critic_loss     | 152       |
|    ent_coef        | 2.04      |
|    ent_coef_loss   | -0.0933   |
|    learning_rate   | 0.0003    |
|    n_updates       | 289099    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 134      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -1.9e+03 |
|    critic_loss     | 111      |
|    ent_coef        | 2.05     |
|    ent_coef_loss   | -0.00972 |
|    learning_rate   | 0.0003   |
|    n_updates       | 289499   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.91e+03 |
|    critic_loss     | 159       |
|    ent_coef        | 2.05      |
|    ent_coef_loss   | 0.0539    |
|    learning_rate   | 0.0003    |
|    n_updates       | 289899    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 16       |
|    fps             | 125      |
|    time_elapsed    | 12       |
|    total_timesteps | 1600     |
| train/             |          |
|    actor_loss      | -1.9e+03 |
|    critic_loss     | 178      |
|    ent_coef        | 2.07     |
|    ent_coef_loss   | 0.00814  |
|    learning_rate   | 0.0003   |
|    n_updates       | 290299   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.91e+03 |
|    critic_loss     | 212       |
|    ent_coef        | 2.09      |
|    ent_coef_loss   | -0.01     |
|    learning_rate   | 0.0003    |
|    n_updates       | 290699    |
----------------------------------
=== Iterazione IRL 761 ===
Loss reward (iter 761): 5.5135321617126465
=== Iterazione IRL 762 ===
Loss reward (iter 762): 5.492888927459717
=== Iterazione IRL 763 ===
Loss reward (iter 763): 5.210984706878662
=== Iterazione IRL 764 ===
Loss reward (iter 764): 5.368954181671143
=== Iterazione IRL 765 ===
Loss reward (iter 765): 5.252825736999512
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.92e+03 |
|    critic_loss     | 134       |
|    ent_coef        | 2.1       |
|    ent_coef_loss   | -0.0218   |
|    learning_rate   | 0.0003    |
|    n_updates       | 290999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 134       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.91e+03 |
|    critic_loss     | 175       |
|    ent_coef        | 2.08      |
|    ent_coef_loss   | -0.0223   |
|    learning_rate   | 0.0003    |
|    n_updates       | 291399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.92e+03 |
|    critic_loss     | 197       |
|    ent_coef        | 2.07      |
|    ent_coef_loss   | -0.00974  |
|    learning_rate   | 0.0003    |
|    n_updates       | 291799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.89e+03 |
|    critic_loss     | 142       |
|    ent_coef        | 2.09      |
|    ent_coef_loss   | -0.0583   |
|    learning_rate   | 0.0003    |
|    n_updates       | 292199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.92e+03 |
|    critic_loss     | 167       |
|    ent_coef        | 2.08      |
|    ent_coef_loss   | 0.0142    |
|    learning_rate   | 0.0003    |
|    n_updates       | 292599    |
----------------------------------
=== Iterazione IRL 766 ===
Loss reward (iter 766): 5.194581985473633
=== Iterazione IRL 767 ===
Loss reward (iter 767): 6.057021141052246
=== Iterazione IRL 768 ===
Loss reward (iter 768): 5.211681365966797
=== Iterazione IRL 769 ===
Loss reward (iter 769): 6.200927734375
=== Iterazione IRL 770 ===
Loss reward (iter 770): 5.009072780609131
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.92e+03 |
|    critic_loss     | 151       |
|    ent_coef        | 2.09      |
|    ent_coef_loss   | 0.00566   |
|    learning_rate   | 0.0003    |
|    n_updates       | 292899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.92e+03 |
|    critic_loss     | 173       |
|    ent_coef        | 2.09      |
|    ent_coef_loss   | -0.0297   |
|    learning_rate   | 0.0003    |
|    n_updates       | 293299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.94e+03 |
|    critic_loss     | 117       |
|    ent_coef        | 2.09      |
|    ent_coef_loss   | 0.0216    |
|    learning_rate   | 0.0003    |
|    n_updates       | 293699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.94e+03 |
|    critic_loss     | 115       |
|    ent_coef        | 2.09      |
|    ent_coef_loss   | 0.0342    |
|    learning_rate   | 0.0003    |
|    n_updates       | 294099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.94e+03 |
|    critic_loss     | 133       |
|    ent_coef        | 2.1       |
|    ent_coef_loss   | 0.0322    |
|    learning_rate   | 0.0003    |
|    n_updates       | 294499    |
----------------------------------
=== Iterazione IRL 771 ===
Loss reward (iter 771): 5.625168800354004
=== Iterazione IRL 772 ===
Loss reward (iter 772): 5.729471206665039
=== Iterazione IRL 773 ===
Loss reward (iter 773): 5.364135265350342
=== Iterazione IRL 774 ===
Loss reward (iter 774): 5.187655925750732
=== Iterazione IRL 775 ===
Loss reward (iter 775): 5.506653308868408
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.94e+03 |
|    critic_loss     | 194       |
|    ent_coef        | 2.12      |
|    ent_coef_loss   | -0.0496   |
|    learning_rate   | 0.0003    |
|    n_updates       | 294799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.93e+03 |
|    critic_loss     | 134       |
|    ent_coef        | 2.12      |
|    ent_coef_loss   | 0.0126    |
|    learning_rate   | 0.0003    |
|    n_updates       | 295199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.95e+03 |
|    critic_loss     | 205       |
|    ent_coef        | 2.11      |
|    ent_coef_loss   | -0.0281   |
|    learning_rate   | 0.0003    |
|    n_updates       | 295599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.93e+03 |
|    critic_loss     | 123       |
|    ent_coef        | 2.11      |
|    ent_coef_loss   | 0.0741    |
|    learning_rate   | 0.0003    |
|    n_updates       | 295999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.94e+03 |
|    critic_loss     | 166       |
|    ent_coef        | 2.12      |
|    ent_coef_loss   | -0.095    |
|    learning_rate   | 0.0003    |
|    n_updates       | 296399    |
----------------------------------
=== Iterazione IRL 776 ===
Loss reward (iter 776): 6.000896453857422
=== Iterazione IRL 777 ===
Loss reward (iter 777): 5.3025736808776855
=== Iterazione IRL 778 ===
Loss reward (iter 778): 4.772353172302246
=== Iterazione IRL 779 ===
Loss reward (iter 779): 4.755959510803223
=== Iterazione IRL 780 ===
Loss reward (iter 780): 5.698065757751465
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.95e+03 |
|    critic_loss     | 202       |
|    ent_coef        | 2.1       |
|    ent_coef_loss   | -0.0137   |
|    learning_rate   | 0.0003    |
|    n_updates       | 296699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.94e+03 |
|    critic_loss     | 199       |
|    ent_coef        | 2.14      |
|    ent_coef_loss   | -0.0821   |
|    learning_rate   | 0.0003    |
|    n_updates       | 297099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.95e+03 |
|    critic_loss     | 167       |
|    ent_coef        | 2.14      |
|    ent_coef_loss   | 0.0711    |
|    learning_rate   | 0.0003    |
|    n_updates       | 297499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.96e+03 |
|    critic_loss     | 133       |
|    ent_coef        | 2.13      |
|    ent_coef_loss   | 0.166     |
|    learning_rate   | 0.0003    |
|    n_updates       | 297899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.96e+03 |
|    critic_loss     | 184       |
|    ent_coef        | 2.13      |
|    ent_coef_loss   | -0.0337   |
|    learning_rate   | 0.0003    |
|    n_updates       | 298299    |
----------------------------------
=== Iterazione IRL 781 ===
Loss reward (iter 781): 5.973904132843018
=== Iterazione IRL 782 ===
Loss reward (iter 782): 5.652991771697998
=== Iterazione IRL 783 ===
Loss reward (iter 783): 5.940782070159912
=== Iterazione IRL 784 ===
Loss reward (iter 784): 5.525729179382324
=== Iterazione IRL 785 ===
Loss reward (iter 785): 5.937422752380371
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.96e+03 |
|    critic_loss     | 143       |
|    ent_coef        | 2.13      |
|    ent_coef_loss   | -0.013    |
|    learning_rate   | 0.0003    |
|    n_updates       | 298599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 134       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.94e+03 |
|    critic_loss     | 191       |
|    ent_coef        | 2.14      |
|    ent_coef_loss   | 0.0157    |
|    learning_rate   | 0.0003    |
|    n_updates       | 298999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.95e+03 |
|    critic_loss     | 206       |
|    ent_coef        | 2.14      |
|    ent_coef_loss   | -0.0803   |
|    learning_rate   | 0.0003    |
|    n_updates       | 299399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.97e+03 |
|    critic_loss     | 158       |
|    ent_coef        | 2.14      |
|    ent_coef_loss   | -0.0876   |
|    learning_rate   | 0.0003    |
|    n_updates       | 299799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.96e+03 |
|    critic_loss     | 210       |
|    ent_coef        | 2.17      |
|    ent_coef_loss   | 0.202     |
|    learning_rate   | 0.0003    |
|    n_updates       | 300199    |
----------------------------------
=== Iterazione IRL 786 ===
Loss reward (iter 786): 5.799478054046631
=== Iterazione IRL 787 ===
Loss reward (iter 787): 6.089034557342529
=== Iterazione IRL 788 ===
Loss reward (iter 788): 5.416487216949463
=== Iterazione IRL 789 ===
Loss reward (iter 789): 5.737143039703369
=== Iterazione IRL 790 ===
Loss reward (iter 790): 5.1201605796813965
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.97e+03 |
|    critic_loss     | 179       |
|    ent_coef        | 2.18      |
|    ent_coef_loss   | -0.00558  |
|    learning_rate   | 0.0003    |
|    n_updates       | 300499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.98e+03 |
|    critic_loss     | 184       |
|    ent_coef        | 2.19      |
|    ent_coef_loss   | 0.132     |
|    learning_rate   | 0.0003    |
|    n_updates       | 300899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.97e+03 |
|    critic_loss     | 180       |
|    ent_coef        | 2.15      |
|    ent_coef_loss   | 0.00886   |
|    learning_rate   | 0.0003    |
|    n_updates       | 301299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.98e+03 |
|    critic_loss     | 143       |
|    ent_coef        | 2.16      |
|    ent_coef_loss   | -0.0305   |
|    learning_rate   | 0.0003    |
|    n_updates       | 301699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.98e+03 |
|    critic_loss     | 142       |
|    ent_coef        | 2.15      |
|    ent_coef_loss   | -0.109    |
|    learning_rate   | 0.0003    |
|    n_updates       | 302099    |
----------------------------------
=== Iterazione IRL 791 ===
Loss reward (iter 791): 5.92434549331665
=== Iterazione IRL 792 ===
Loss reward (iter 792): 5.306185722351074
=== Iterazione IRL 793 ===
Loss reward (iter 793): 5.245386123657227
=== Iterazione IRL 794 ===
Loss reward (iter 794): 6.346214771270752
=== Iterazione IRL 795 ===
Loss reward (iter 795): 5.5728302001953125
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.99e+03 |
|    critic_loss     | 124       |
|    ent_coef        | 2.16      |
|    ent_coef_loss   | -0.074    |
|    learning_rate   | 0.0003    |
|    n_updates       | 302399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 134       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.98e+03 |
|    critic_loss     | 162       |
|    ent_coef        | 2.16      |
|    ent_coef_loss   | -0.0538   |
|    learning_rate   | 0.0003    |
|    n_updates       | 302799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.97e+03 |
|    critic_loss     | 237       |
|    ent_coef        | 2.17      |
|    ent_coef_loss   | -0.0426   |
|    learning_rate   | 0.0003    |
|    n_updates       | 303199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.99e+03 |
|    critic_loss     | 144       |
|    ent_coef        | 2.17      |
|    ent_coef_loss   | 0.0567    |
|    learning_rate   | 0.0003    |
|    n_updates       | 303599    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 20       |
|    fps             | 123      |
|    time_elapsed    | 16       |
|    total_timesteps | 2000     |
| train/             |          |
|    actor_loss      | -2e+03   |
|    critic_loss     | 180      |
|    ent_coef        | 2.16     |
|    ent_coef_loss   | 0.00947  |
|    learning_rate   | 0.0003   |
|    n_updates       | 303999   |
---------------------------------
=== Iterazione IRL 796 ===
Loss reward (iter 796): 5.682486057281494
=== Iterazione IRL 797 ===
Loss reward (iter 797): 5.538886070251465
=== Iterazione IRL 798 ===
Loss reward (iter 798): 5.747303009033203
=== Iterazione IRL 799 ===
Loss reward (iter 799): 5.90964937210083
=== Iterazione IRL 800 ===
Loss reward (iter 800): 5.9883952140808105
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.98e+03 |
|    critic_loss     | 165       |
|    ent_coef        | 2.16      |
|    ent_coef_loss   | -0.0268   |
|    learning_rate   | 0.0003    |
|    n_updates       | 304299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.98e+03 |
|    critic_loss     | 135       |
|    ent_coef        | 2.17      |
|    ent_coef_loss   | -0.0145   |
|    learning_rate   | 0.0003    |
|    n_updates       | 304699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.97e+03 |
|    critic_loss     | 197       |
|    ent_coef        | 2.18      |
|    ent_coef_loss   | -0.0569   |
|    learning_rate   | 0.0003    |
|    n_updates       | 305099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.99e+03 |
|    critic_loss     | 222       |
|    ent_coef        | 2.18      |
|    ent_coef_loss   | -0.0367   |
|    learning_rate   | 0.0003    |
|    n_updates       | 305499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 123       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.98e+03 |
|    critic_loss     | 126       |
|    ent_coef        | 2.19      |
|    ent_coef_loss   | 0.00353   |
|    learning_rate   | 0.0003    |
|    n_updates       | 305899    |
----------------------------------
=== Iterazione IRL 801 ===
Loss reward (iter 801): 6.078259468078613
=== Iterazione IRL 802 ===
Loss reward (iter 802): 5.810436248779297
=== Iterazione IRL 803 ===
Loss reward (iter 803): 5.712189674377441
=== Iterazione IRL 804 ===
Loss reward (iter 804): 5.609715461730957
=== Iterazione IRL 805 ===
Loss reward (iter 805): 5.606517314910889
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2e+03   |
|    critic_loss     | 146      |
|    ent_coef        | 2.19     |
|    ent_coef_loss   | 0.0175   |
|    learning_rate   | 0.0003   |
|    n_updates       | 306199   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 134       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.01e+03 |
|    critic_loss     | 112       |
|    ent_coef        | 2.18      |
|    ent_coef_loss   | 0.0342    |
|    learning_rate   | 0.0003    |
|    n_updates       | 306599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.01e+03 |
|    critic_loss     | 144       |
|    ent_coef        | 2.18      |
|    ent_coef_loss   | -0.041    |
|    learning_rate   | 0.0003    |
|    n_updates       | 306999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.98e+03 |
|    critic_loss     | 173       |
|    ent_coef        | 2.18      |
|    ent_coef_loss   | -0.104    |
|    learning_rate   | 0.0003    |
|    n_updates       | 307399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.02e+03 |
|    critic_loss     | 209       |
|    ent_coef        | 2.17      |
|    ent_coef_loss   | 0.0898    |
|    learning_rate   | 0.0003    |
|    n_updates       | 307799    |
----------------------------------
=== Iterazione IRL 806 ===
Loss reward (iter 806): 4.7491841316223145
=== Iterazione IRL 807 ===
Loss reward (iter 807): 5.696982383728027
=== Iterazione IRL 808 ===
Loss reward (iter 808): 5.523466110229492
=== Iterazione IRL 809 ===
Loss reward (iter 809): 5.400776386260986
=== Iterazione IRL 810 ===
Loss reward (iter 810): 5.295258045196533
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.02e+03 |
|    critic_loss     | 93.3      |
|    ent_coef        | 2.18      |
|    ent_coef_loss   | 0.0495    |
|    learning_rate   | 0.0003    |
|    n_updates       | 308099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 134       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.01e+03 |
|    critic_loss     | 155       |
|    ent_coef        | 2.2       |
|    ent_coef_loss   | 0.122     |
|    learning_rate   | 0.0003    |
|    n_updates       | 308499    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 128      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -2e+03   |
|    critic_loss     | 209      |
|    ent_coef        | 2.2      |
|    ent_coef_loss   | -0.00693 |
|    learning_rate   | 0.0003   |
|    n_updates       | 308899   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.01e+03 |
|    critic_loss     | 174       |
|    ent_coef        | 2.2       |
|    ent_coef_loss   | -0.065    |
|    learning_rate   | 0.0003    |
|    n_updates       | 309299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.03e+03 |
|    critic_loss     | 125       |
|    ent_coef        | 2.18      |
|    ent_coef_loss   | 0.00318   |
|    learning_rate   | 0.0003    |
|    n_updates       | 309699    |
----------------------------------
=== Iterazione IRL 811 ===
Loss reward (iter 811): 5.424052715301514
=== Iterazione IRL 812 ===
Loss reward (iter 812): 6.119537353515625
=== Iterazione IRL 813 ===
Loss reward (iter 813): 5.410031795501709
=== Iterazione IRL 814 ===
Loss reward (iter 814): 5.390221118927002
=== Iterazione IRL 815 ===
Loss reward (iter 815): 5.777352809906006
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.02e+03 |
|    critic_loss     | 188       |
|    ent_coef        | 2.2       |
|    ent_coef_loss   | 0.0219    |
|    learning_rate   | 0.0003    |
|    n_updates       | 309999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 134       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.02e+03 |
|    critic_loss     | 167       |
|    ent_coef        | 2.2       |
|    ent_coef_loss   | 0.00737   |
|    learning_rate   | 0.0003    |
|    n_updates       | 310399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.03e+03 |
|    critic_loss     | 200       |
|    ent_coef        | 2.18      |
|    ent_coef_loss   | 0.0376    |
|    learning_rate   | 0.0003    |
|    n_updates       | 310799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.03e+03 |
|    critic_loss     | 179       |
|    ent_coef        | 2.18      |
|    ent_coef_loss   | 0.0306    |
|    learning_rate   | 0.0003    |
|    n_updates       | 311199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.02e+03 |
|    critic_loss     | 167       |
|    ent_coef        | 2.19      |
|    ent_coef_loss   | -0.0382   |
|    learning_rate   | 0.0003    |
|    n_updates       | 311599    |
----------------------------------
=== Iterazione IRL 816 ===
Loss reward (iter 816): 5.959134578704834
=== Iterazione IRL 817 ===
Loss reward (iter 817): 5.765796661376953
=== Iterazione IRL 818 ===
Loss reward (iter 818): 5.6452531814575195
=== Iterazione IRL 819 ===
Loss reward (iter 819): 5.94638729095459
=== Iterazione IRL 820 ===
Loss reward (iter 820): 5.858721733093262
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.04e+03 |
|    critic_loss     | 149       |
|    ent_coef        | 2.2       |
|    ent_coef_loss   | -0.0545   |
|    learning_rate   | 0.0003    |
|    n_updates       | 311899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 134       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.03e+03 |
|    critic_loss     | 185       |
|    ent_coef        | 2.2       |
|    ent_coef_loss   | -0.116    |
|    learning_rate   | 0.0003    |
|    n_updates       | 312299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.03e+03 |
|    critic_loss     | 125       |
|    ent_coef        | 2.19      |
|    ent_coef_loss   | -0.0446   |
|    learning_rate   | 0.0003    |
|    n_updates       | 312699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.04e+03 |
|    critic_loss     | 148       |
|    ent_coef        | 2.19      |
|    ent_coef_loss   | -0.025    |
|    learning_rate   | 0.0003    |
|    n_updates       | 313099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.05e+03 |
|    critic_loss     | 135       |
|    ent_coef        | 2.2       |
|    ent_coef_loss   | 0.00027   |
|    learning_rate   | 0.0003    |
|    n_updates       | 313499    |
----------------------------------
=== Iterazione IRL 821 ===
Loss reward (iter 821): 5.832071304321289
=== Iterazione IRL 822 ===
Loss reward (iter 822): 5.876450538635254
=== Iterazione IRL 823 ===
Loss reward (iter 823): 5.74302864074707
=== Iterazione IRL 824 ===
Loss reward (iter 824): 5.870781421661377
=== Iterazione IRL 825 ===
Loss reward (iter 825): 5.660289287567139
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.03e+03 |
|    critic_loss     | 125       |
|    ent_coef        | 2.22      |
|    ent_coef_loss   | 0.0726    |
|    learning_rate   | 0.0003    |
|    n_updates       | 313799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 134       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.05e+03 |
|    critic_loss     | 142       |
|    ent_coef        | 2.24      |
|    ent_coef_loss   | -0.117    |
|    learning_rate   | 0.0003    |
|    n_updates       | 314199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.04e+03 |
|    critic_loss     | 155       |
|    ent_coef        | 2.2       |
|    ent_coef_loss   | 0.0685    |
|    learning_rate   | 0.0003    |
|    n_updates       | 314599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.04e+03 |
|    critic_loss     | 168       |
|    ent_coef        | 2.2       |
|    ent_coef_loss   | 0.0019    |
|    learning_rate   | 0.0003    |
|    n_updates       | 314999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.02e+03 |
|    critic_loss     | 282       |
|    ent_coef        | 2.21      |
|    ent_coef_loss   | 0.0227    |
|    learning_rate   | 0.0003    |
|    n_updates       | 315399    |
----------------------------------
=== Iterazione IRL 826 ===
Loss reward (iter 826): 5.948814868927002
=== Iterazione IRL 827 ===
Loss reward (iter 827): 5.688117980957031
=== Iterazione IRL 828 ===
Loss reward (iter 828): 5.567397117614746
=== Iterazione IRL 829 ===
Loss reward (iter 829): 5.552238464355469
=== Iterazione IRL 830 ===
Loss reward (iter 830): 5.497122287750244
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.05e+03 |
|    critic_loss     | 107       |
|    ent_coef        | 2.23      |
|    ent_coef_loss   | 0.0111    |
|    learning_rate   | 0.0003    |
|    n_updates       | 315699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.04e+03 |
|    critic_loss     | 143       |
|    ent_coef        | 2.24      |
|    ent_coef_loss   | 0.0927    |
|    learning_rate   | 0.0003    |
|    n_updates       | 316099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.04e+03 |
|    critic_loss     | 174       |
|    ent_coef        | 2.25      |
|    ent_coef_loss   | -0.0118   |
|    learning_rate   | 0.0003    |
|    n_updates       | 316499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.08e+03 |
|    critic_loss     | 119       |
|    ent_coef        | 2.25      |
|    ent_coef_loss   | -0.0109   |
|    learning_rate   | 0.0003    |
|    n_updates       | 316899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.06e+03 |
|    critic_loss     | 154       |
|    ent_coef        | 2.25      |
|    ent_coef_loss   | 0.036     |
|    learning_rate   | 0.0003    |
|    n_updates       | 317299    |
----------------------------------
=== Iterazione IRL 831 ===
Loss reward (iter 831): 5.252704620361328
=== Iterazione IRL 832 ===
Loss reward (iter 832): 5.7769856452941895
=== Iterazione IRL 833 ===
Loss reward (iter 833): 5.891091823577881
=== Iterazione IRL 834 ===
Loss reward (iter 834): 5.553928375244141
=== Iterazione IRL 835 ===
Loss reward (iter 835): 5.786857604980469
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.05e+03 |
|    critic_loss     | 111       |
|    ent_coef        | 2.24      |
|    ent_coef_loss   | 0.118     |
|    learning_rate   | 0.0003    |
|    n_updates       | 317599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.07e+03 |
|    critic_loss     | 140       |
|    ent_coef        | 2.24      |
|    ent_coef_loss   | 0.101     |
|    learning_rate   | 0.0003    |
|    n_updates       | 317999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.07e+03 |
|    critic_loss     | 153       |
|    ent_coef        | 2.23      |
|    ent_coef_loss   | 0.0895    |
|    learning_rate   | 0.0003    |
|    n_updates       | 318399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.06e+03 |
|    critic_loss     | 183       |
|    ent_coef        | 2.25      |
|    ent_coef_loss   | -0.146    |
|    learning_rate   | 0.0003    |
|    n_updates       | 318799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.06e+03 |
|    critic_loss     | 130       |
|    ent_coef        | 2.25      |
|    ent_coef_loss   | -0.0144   |
|    learning_rate   | 0.0003    |
|    n_updates       | 319199    |
----------------------------------
=== Iterazione IRL 836 ===
Loss reward (iter 836): 5.7351789474487305
=== Iterazione IRL 837 ===
Loss reward (iter 837): 5.781154632568359
=== Iterazione IRL 838 ===
Loss reward (iter 838): 5.300432205200195
=== Iterazione IRL 839 ===
Loss reward (iter 839): 5.408434867858887
=== Iterazione IRL 840 ===
Loss reward (iter 840): 5.376064777374268
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.09e+03 |
|    critic_loss     | 134       |
|    ent_coef        | 2.25      |
|    ent_coef_loss   | -0.0368   |
|    learning_rate   | 0.0003    |
|    n_updates       | 319499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.08e+03 |
|    critic_loss     | 153       |
|    ent_coef        | 2.23      |
|    ent_coef_loss   | -0.027    |
|    learning_rate   | 0.0003    |
|    n_updates       | 319899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.05e+03 |
|    critic_loss     | 172       |
|    ent_coef        | 2.23      |
|    ent_coef_loss   | 0.00179   |
|    learning_rate   | 0.0003    |
|    n_updates       | 320299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.08e+03 |
|    critic_loss     | 194       |
|    ent_coef        | 2.22      |
|    ent_coef_loss   | -0.144    |
|    learning_rate   | 0.0003    |
|    n_updates       | 320699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.06e+03 |
|    critic_loss     | 106       |
|    ent_coef        | 2.25      |
|    ent_coef_loss   | -0.00126  |
|    learning_rate   | 0.0003    |
|    n_updates       | 321099    |
----------------------------------
=== Iterazione IRL 841 ===
Loss reward (iter 841): 6.432068824768066
=== Iterazione IRL 842 ===
Loss reward (iter 842): 6.112157821655273
=== Iterazione IRL 843 ===
Loss reward (iter 843): 6.31382942199707
=== Iterazione IRL 844 ===
Loss reward (iter 844): 6.055251598358154
=== Iterazione IRL 845 ===
Loss reward (iter 845): 5.981633186340332
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.09e+03 |
|    critic_loss     | 136       |
|    ent_coef        | 2.26      |
|    ent_coef_loss   | 0.138     |
|    learning_rate   | 0.0003    |
|    n_updates       | 321399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.07e+03 |
|    critic_loss     | 152       |
|    ent_coef        | 2.25      |
|    ent_coef_loss   | -0.00949  |
|    learning_rate   | 0.0003    |
|    n_updates       | 321799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.07e+03 |
|    critic_loss     | 222       |
|    ent_coef        | 2.27      |
|    ent_coef_loss   | -0.0272   |
|    learning_rate   | 0.0003    |
|    n_updates       | 322199    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 16       |
|    fps             | 125      |
|    time_elapsed    | 12       |
|    total_timesteps | 1600     |
| train/             |          |
|    actor_loss      | -2.1e+03 |
|    critic_loss     | 125      |
|    ent_coef        | 2.3      |
|    ent_coef_loss   | 0.0256   |
|    learning_rate   | 0.0003   |
|    n_updates       | 322599   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 123       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.06e+03 |
|    critic_loss     | 172       |
|    ent_coef        | 2.28      |
|    ent_coef_loss   | -0.00584  |
|    learning_rate   | 0.0003    |
|    n_updates       | 322999    |
----------------------------------
=== Iterazione IRL 846 ===
Loss reward (iter 846): 5.188967227935791
=== Iterazione IRL 847 ===
Loss reward (iter 847): 5.132494926452637
=== Iterazione IRL 848 ===
Loss reward (iter 848): 5.814815521240234
=== Iterazione IRL 849 ===
Loss reward (iter 849): 5.635411262512207
=== Iterazione IRL 850 ===
Loss reward (iter 850): 5.747848987579346
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.07e+03 |
|    critic_loss     | 296       |
|    ent_coef        | 2.3       |
|    ent_coef_loss   | -0.0573   |
|    learning_rate   | 0.0003    |
|    n_updates       | 323299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.08e+03 |
|    critic_loss     | 202       |
|    ent_coef        | 2.29      |
|    ent_coef_loss   | 0.0878    |
|    learning_rate   | 0.0003    |
|    n_updates       | 323699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.08e+03 |
|    critic_loss     | 136       |
|    ent_coef        | 2.3       |
|    ent_coef_loss   | -0.0262   |
|    learning_rate   | 0.0003    |
|    n_updates       | 324099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.08e+03 |
|    critic_loss     | 235       |
|    ent_coef        | 2.29      |
|    ent_coef_loss   | -0.0283   |
|    learning_rate   | 0.0003    |
|    n_updates       | 324499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.09e+03 |
|    critic_loss     | 131       |
|    ent_coef        | 2.31      |
|    ent_coef_loss   | 0.166     |
|    learning_rate   | 0.0003    |
|    n_updates       | 324899    |
----------------------------------
=== Iterazione IRL 851 ===
Loss reward (iter 851): 5.942594528198242
=== Iterazione IRL 852 ===
Loss reward (iter 852): 5.554190635681152
=== Iterazione IRL 853 ===
Loss reward (iter 853): 5.507669925689697
=== Iterazione IRL 854 ===
Loss reward (iter 854): 5.7944560050964355
=== Iterazione IRL 855 ===
Loss reward (iter 855): 5.68070650100708
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.11e+03 |
|    critic_loss     | 153       |
|    ent_coef        | 2.32      |
|    ent_coef_loss   | -0.0161   |
|    learning_rate   | 0.0003    |
|    n_updates       | 325199    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 5        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.1e+03 |
|    critic_loss     | 186      |
|    ent_coef        | 2.31     |
|    ent_coef_loss   | -0.179   |
|    learning_rate   | 0.0003   |
|    n_updates       | 325599   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.09e+03 |
|    critic_loss     | 143       |
|    ent_coef        | 2.28      |
|    ent_coef_loss   | -0.13     |
|    learning_rate   | 0.0003    |
|    n_updates       | 325999    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 16       |
|    fps             | 124      |
|    time_elapsed    | 12       |
|    total_timesteps | 1600     |
| train/             |          |
|    actor_loss      | -2.1e+03 |
|    critic_loss     | 155      |
|    ent_coef        | 2.24     |
|    ent_coef_loss   | -0.11    |
|    learning_rate   | 0.0003   |
|    n_updates       | 326399   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 123       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.11e+03 |
|    critic_loss     | 158       |
|    ent_coef        | 2.26      |
|    ent_coef_loss   | -0.101    |
|    learning_rate   | 0.0003    |
|    n_updates       | 326799    |
----------------------------------
=== Iterazione IRL 856 ===
Loss reward (iter 856): 5.833431720733643
=== Iterazione IRL 857 ===
Loss reward (iter 857): 5.668931484222412
=== Iterazione IRL 858 ===
Loss reward (iter 858): 6.102634906768799
=== Iterazione IRL 859 ===
Loss reward (iter 859): 5.711395263671875
=== Iterazione IRL 860 ===
Loss reward (iter 860): 5.7112274169921875
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.11e+03 |
|    critic_loss     | 173       |
|    ent_coef        | 2.29      |
|    ent_coef_loss   | -0.0262   |
|    learning_rate   | 0.0003    |
|    n_updates       | 327099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.11e+03 |
|    critic_loss     | 95.9      |
|    ent_coef        | 2.29      |
|    ent_coef_loss   | 0.063     |
|    learning_rate   | 0.0003    |
|    n_updates       | 327499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.11e+03 |
|    critic_loss     | 166       |
|    ent_coef        | 2.32      |
|    ent_coef_loss   | -0.0678   |
|    learning_rate   | 0.0003    |
|    n_updates       | 327899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.11e+03 |
|    critic_loss     | 135       |
|    ent_coef        | 2.33      |
|    ent_coef_loss   | 0.0321    |
|    learning_rate   | 0.0003    |
|    n_updates       | 328299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.11e+03 |
|    critic_loss     | 158       |
|    ent_coef        | 2.35      |
|    ent_coef_loss   | -0.0328   |
|    learning_rate   | 0.0003    |
|    n_updates       | 328699    |
----------------------------------
=== Iterazione IRL 861 ===
Loss reward (iter 861): 5.770142078399658
=== Iterazione IRL 862 ===
Loss reward (iter 862): 4.982366561889648
=== Iterazione IRL 863 ===
Loss reward (iter 863): 5.871895790100098
=== Iterazione IRL 864 ===
Loss reward (iter 864): 5.854152679443359
=== Iterazione IRL 865 ===
Loss reward (iter 865): 5.860299110412598
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.11e+03 |
|    critic_loss     | 155       |
|    ent_coef        | 2.31      |
|    ent_coef_loss   | -0.0677   |
|    learning_rate   | 0.0003    |
|    n_updates       | 328999    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.1e+03 |
|    critic_loss     | 148      |
|    ent_coef        | 2.32     |
|    ent_coef_loss   | -0.109   |
|    learning_rate   | 0.0003   |
|    n_updates       | 329399   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.12e+03 |
|    critic_loss     | 123       |
|    ent_coef        | 2.33      |
|    ent_coef_loss   | 0.14      |
|    learning_rate   | 0.0003    |
|    n_updates       | 329799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.13e+03 |
|    critic_loss     | 113       |
|    ent_coef        | 2.33      |
|    ent_coef_loss   | 0.157     |
|    learning_rate   | 0.0003    |
|    n_updates       | 330199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.14e+03 |
|    critic_loss     | 147       |
|    ent_coef        | 2.34      |
|    ent_coef_loss   | 0.098     |
|    learning_rate   | 0.0003    |
|    n_updates       | 330599    |
----------------------------------
=== Iterazione IRL 866 ===
Loss reward (iter 866): 6.311119556427002
=== Iterazione IRL 867 ===
Loss reward (iter 867): 6.221851348876953
=== Iterazione IRL 868 ===
Loss reward (iter 868): 5.667016983032227
=== Iterazione IRL 869 ===
Loss reward (iter 869): 6.324461460113525
=== Iterazione IRL 870 ===
Loss reward (iter 870): 5.82479190826416
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.12e+03 |
|    critic_loss     | 166       |
|    ent_coef        | 2.36      |
|    ent_coef_loss   | -0.175    |
|    learning_rate   | 0.0003    |
|    n_updates       | 330899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.12e+03 |
|    critic_loss     | 121       |
|    ent_coef        | 2.33      |
|    ent_coef_loss   | -0.0544   |
|    learning_rate   | 0.0003    |
|    n_updates       | 331299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.12e+03 |
|    critic_loss     | 129       |
|    ent_coef        | 2.33      |
|    ent_coef_loss   | 0.0947    |
|    learning_rate   | 0.0003    |
|    n_updates       | 331699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.14e+03 |
|    critic_loss     | 194       |
|    ent_coef        | 2.34      |
|    ent_coef_loss   | -0.0122   |
|    learning_rate   | 0.0003    |
|    n_updates       | 332099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.13e+03 |
|    critic_loss     | 168       |
|    ent_coef        | 2.33      |
|    ent_coef_loss   | 0.0531    |
|    learning_rate   | 0.0003    |
|    n_updates       | 332499    |
----------------------------------
=== Iterazione IRL 871 ===
Loss reward (iter 871): 6.256832122802734
=== Iterazione IRL 872 ===
Loss reward (iter 872): 5.888181686401367
=== Iterazione IRL 873 ===
Loss reward (iter 873): 5.9610772132873535
=== Iterazione IRL 874 ===
Loss reward (iter 874): 6.491809368133545
=== Iterazione IRL 875 ===
Loss reward (iter 875): 6.039863586425781
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.1e+03 |
|    critic_loss     | 128      |
|    ent_coef        | 2.33     |
|    ent_coef_loss   | -0.0144  |
|    learning_rate   | 0.0003   |
|    n_updates       | 332799   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.12e+03 |
|    critic_loss     | 174       |
|    ent_coef        | 2.33      |
|    ent_coef_loss   | -0.0148   |
|    learning_rate   | 0.0003    |
|    n_updates       | 333199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.15e+03 |
|    critic_loss     | 163       |
|    ent_coef        | 2.35      |
|    ent_coef_loss   | -0.0625   |
|    learning_rate   | 0.0003    |
|    n_updates       | 333599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.15e+03 |
|    critic_loss     | 148       |
|    ent_coef        | 2.36      |
|    ent_coef_loss   | 0.115     |
|    learning_rate   | 0.0003    |
|    n_updates       | 333999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 123       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.13e+03 |
|    critic_loss     | 161       |
|    ent_coef        | 2.35      |
|    ent_coef_loss   | -0.0465   |
|    learning_rate   | 0.0003    |
|    n_updates       | 334399    |
----------------------------------
=== Iterazione IRL 876 ===
Loss reward (iter 876): 5.913508415222168
=== Iterazione IRL 877 ===
Loss reward (iter 877): 6.232708930969238
=== Iterazione IRL 878 ===
Loss reward (iter 878): 6.200922012329102
=== Iterazione IRL 879 ===
Loss reward (iter 879): 5.6697516441345215
=== Iterazione IRL 880 ===
Loss reward (iter 880): 5.594299793243408
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 153       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.13e+03 |
|    critic_loss     | 237       |
|    ent_coef        | 2.36      |
|    ent_coef_loss   | -0.0373   |
|    learning_rate   | 0.0003    |
|    n_updates       | 334699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.15e+03 |
|    critic_loss     | 122       |
|    ent_coef        | 2.36      |
|    ent_coef_loss   | -0.0191   |
|    learning_rate   | 0.0003    |
|    n_updates       | 335099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.15e+03 |
|    critic_loss     | 182       |
|    ent_coef        | 2.34      |
|    ent_coef_loss   | -0.0094   |
|    learning_rate   | 0.0003    |
|    n_updates       | 335499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.14e+03 |
|    critic_loss     | 180       |
|    ent_coef        | 2.31      |
|    ent_coef_loss   | -0.0664   |
|    learning_rate   | 0.0003    |
|    n_updates       | 335899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 123       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.12e+03 |
|    critic_loss     | 181       |
|    ent_coef        | 2.32      |
|    ent_coef_loss   | -0.0449   |
|    learning_rate   | 0.0003    |
|    n_updates       | 336299    |
----------------------------------
=== Iterazione IRL 881 ===
Loss reward (iter 881): 5.521759986877441
=== Iterazione IRL 882 ===
Loss reward (iter 882): 6.53742790222168
=== Iterazione IRL 883 ===
Loss reward (iter 883): 6.377086162567139
=== Iterazione IRL 884 ===
Loss reward (iter 884): 6.3131103515625
=== Iterazione IRL 885 ===
Loss reward (iter 885): 5.6692094802856445
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 153       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.14e+03 |
|    critic_loss     | 165       |
|    ent_coef        | 2.33      |
|    ent_coef_loss   | -0.0661   |
|    learning_rate   | 0.0003    |
|    n_updates       | 336599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.16e+03 |
|    critic_loss     | 161       |
|    ent_coef        | 2.35      |
|    ent_coef_loss   | 0.0114    |
|    learning_rate   | 0.0003    |
|    n_updates       | 336999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.16e+03 |
|    critic_loss     | 117       |
|    ent_coef        | 2.35      |
|    ent_coef_loss   | 0.0142    |
|    learning_rate   | 0.0003    |
|    n_updates       | 337399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.16e+03 |
|    critic_loss     | 151       |
|    ent_coef        | 2.34      |
|    ent_coef_loss   | -0.0352   |
|    learning_rate   | 0.0003    |
|    n_updates       | 337799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 123       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.16e+03 |
|    critic_loss     | 206       |
|    ent_coef        | 2.34      |
|    ent_coef_loss   | 0.0376    |
|    learning_rate   | 0.0003    |
|    n_updates       | 338199    |
----------------------------------
=== Iterazione IRL 886 ===
Loss reward (iter 886): 6.0378031730651855
=== Iterazione IRL 887 ===
Loss reward (iter 887): 6.096710681915283
=== Iterazione IRL 888 ===
Loss reward (iter 888): 6.106396198272705
=== Iterazione IRL 889 ===
Loss reward (iter 889): 6.282436370849609
=== Iterazione IRL 890 ===
Loss reward (iter 890): 6.231299877166748
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.16e+03 |
|    critic_loss     | 189       |
|    ent_coef        | 2.36      |
|    ent_coef_loss   | 0.0855    |
|    learning_rate   | 0.0003    |
|    n_updates       | 338499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.16e+03 |
|    critic_loss     | 171       |
|    ent_coef        | 2.36      |
|    ent_coef_loss   | -0.011    |
|    learning_rate   | 0.0003    |
|    n_updates       | 338899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.15e+03 |
|    critic_loss     | 149       |
|    ent_coef        | 2.36      |
|    ent_coef_loss   | -0.0692   |
|    learning_rate   | 0.0003    |
|    n_updates       | 339299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.16e+03 |
|    critic_loss     | 129       |
|    ent_coef        | 2.36      |
|    ent_coef_loss   | 0.145     |
|    learning_rate   | 0.0003    |
|    n_updates       | 339699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.16e+03 |
|    critic_loss     | 154       |
|    ent_coef        | 2.37      |
|    ent_coef_loss   | 0.0401    |
|    learning_rate   | 0.0003    |
|    n_updates       | 340099    |
----------------------------------
=== Iterazione IRL 891 ===
Loss reward (iter 891): 6.26917839050293
=== Iterazione IRL 892 ===
Loss reward (iter 892): 5.753678798675537
=== Iterazione IRL 893 ===
Loss reward (iter 893): 5.658395290374756
=== Iterazione IRL 894 ===
Loss reward (iter 894): 5.6844706535339355
=== Iterazione IRL 895 ===
Loss reward (iter 895): 6.124270439147949
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.17e+03 |
|    critic_loss     | 172       |
|    ent_coef        | 2.39      |
|    ent_coef_loss   | 0.078     |
|    learning_rate   | 0.0003    |
|    n_updates       | 340399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.17e+03 |
|    critic_loss     | 133       |
|    ent_coef        | 2.39      |
|    ent_coef_loss   | 0.0237    |
|    learning_rate   | 0.0003    |
|    n_updates       | 340799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.17e+03 |
|    critic_loss     | 150       |
|    ent_coef        | 2.4       |
|    ent_coef_loss   | -0.114    |
|    learning_rate   | 0.0003    |
|    n_updates       | 341199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.17e+03 |
|    critic_loss     | 177       |
|    ent_coef        | 2.41      |
|    ent_coef_loss   | 0.0331    |
|    learning_rate   | 0.0003    |
|    n_updates       | 341599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.17e+03 |
|    critic_loss     | 195       |
|    ent_coef        | 2.38      |
|    ent_coef_loss   | -0.121    |
|    learning_rate   | 0.0003    |
|    n_updates       | 341999    |
----------------------------------
=== Iterazione IRL 896 ===
Loss reward (iter 896): 6.046769618988037
=== Iterazione IRL 897 ===
Loss reward (iter 897): 6.012722015380859
=== Iterazione IRL 898 ===
Loss reward (iter 898): 5.940492153167725
=== Iterazione IRL 899 ===
Loss reward (iter 899): 5.621891498565674
=== Iterazione IRL 900 ===
Loss reward (iter 900): 5.934257984161377
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.17e+03 |
|    critic_loss     | 175       |
|    ent_coef        | 2.4       |
|    ent_coef_loss   | 0.0996    |
|    learning_rate   | 0.0003    |
|    n_updates       | 342299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.16e+03 |
|    critic_loss     | 166       |
|    ent_coef        | 2.36      |
|    ent_coef_loss   | -0.046    |
|    learning_rate   | 0.0003    |
|    n_updates       | 342699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.16e+03 |
|    critic_loss     | 159       |
|    ent_coef        | 2.37      |
|    ent_coef_loss   | -0.194    |
|    learning_rate   | 0.0003    |
|    n_updates       | 343099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.18e+03 |
|    critic_loss     | 148       |
|    ent_coef        | 2.38      |
|    ent_coef_loss   | -0.0868   |
|    learning_rate   | 0.0003    |
|    n_updates       | 343499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.17e+03 |
|    critic_loss     | 131       |
|    ent_coef        | 2.39      |
|    ent_coef_loss   | -0.0851   |
|    learning_rate   | 0.0003    |
|    n_updates       | 343899    |
----------------------------------
=== Iterazione IRL 901 ===
Loss reward (iter 901): 6.4216461181640625
=== Iterazione IRL 902 ===
Loss reward (iter 902): 5.091245174407959
=== Iterazione IRL 903 ===
Loss reward (iter 903): 6.060996055603027
=== Iterazione IRL 904 ===
Loss reward (iter 904): 5.934810161590576
=== Iterazione IRL 905 ===
Loss reward (iter 905): 5.79948091506958
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.19e+03 |
|    critic_loss     | 144       |
|    ent_coef        | 2.39      |
|    ent_coef_loss   | 0.0718    |
|    learning_rate   | 0.0003    |
|    n_updates       | 344199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.19e+03 |
|    critic_loss     | 175       |
|    ent_coef        | 2.42      |
|    ent_coef_loss   | 0.0678    |
|    learning_rate   | 0.0003    |
|    n_updates       | 344599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.19e+03 |
|    critic_loss     | 136       |
|    ent_coef        | 2.42      |
|    ent_coef_loss   | 0.0262    |
|    learning_rate   | 0.0003    |
|    n_updates       | 344999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.18e+03 |
|    critic_loss     | 125       |
|    ent_coef        | 2.41      |
|    ent_coef_loss   | 0.018     |
|    learning_rate   | 0.0003    |
|    n_updates       | 345399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.17e+03 |
|    critic_loss     | 152       |
|    ent_coef        | 2.41      |
|    ent_coef_loss   | -0.00889  |
|    learning_rate   | 0.0003    |
|    n_updates       | 345799    |
----------------------------------
=== Iterazione IRL 906 ===
Loss reward (iter 906): 6.875143527984619
=== Iterazione IRL 907 ===
Loss reward (iter 907): 6.05877161026001
=== Iterazione IRL 908 ===
Loss reward (iter 908): 6.476597785949707
=== Iterazione IRL 909 ===
Loss reward (iter 909): 5.931859970092773
=== Iterazione IRL 910 ===
Loss reward (iter 910): 6.026968955993652
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.19e+03 |
|    critic_loss     | 140       |
|    ent_coef        | 2.4       |
|    ent_coef_loss   | 0.0173    |
|    learning_rate   | 0.0003    |
|    n_updates       | 346099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.21e+03 |
|    critic_loss     | 132       |
|    ent_coef        | 2.4       |
|    ent_coef_loss   | 0.018     |
|    learning_rate   | 0.0003    |
|    n_updates       | 346499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.19e+03 |
|    critic_loss     | 166       |
|    ent_coef        | 2.41      |
|    ent_coef_loss   | -0.109    |
|    learning_rate   | 0.0003    |
|    n_updates       | 346899    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 16       |
|    fps             | 124      |
|    time_elapsed    | 12       |
|    total_timesteps | 1600     |
| train/             |          |
|    actor_loss      | -2.2e+03 |
|    critic_loss     | 94.9     |
|    ent_coef        | 2.38     |
|    ent_coef_loss   | 0.126    |
|    learning_rate   | 0.0003   |
|    n_updates       | 347299   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.22e+03 |
|    critic_loss     | 146       |
|    ent_coef        | 2.41      |
|    ent_coef_loss   | -0.0319   |
|    learning_rate   | 0.0003    |
|    n_updates       | 347699    |
----------------------------------
=== Iterazione IRL 911 ===
Loss reward (iter 911): 6.156018257141113
=== Iterazione IRL 912 ===
Loss reward (iter 912): 6.594582557678223
=== Iterazione IRL 913 ===
Loss reward (iter 913): 6.513554573059082
=== Iterazione IRL 914 ===
Loss reward (iter 914): 6.307015419006348
=== Iterazione IRL 915 ===
Loss reward (iter 915): 6.047841548919678
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.2e+03 |
|    critic_loss     | 212      |
|    ent_coef        | 2.4      |
|    ent_coef_loss   | 0.107    |
|    learning_rate   | 0.0003   |
|    n_updates       | 347999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.2e+03 |
|    critic_loss     | 131      |
|    ent_coef        | 2.43     |
|    ent_coef_loss   | 0.0442   |
|    learning_rate   | 0.0003   |
|    n_updates       | 348399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -2.2e+03 |
|    critic_loss     | 202      |
|    ent_coef        | 2.44     |
|    ent_coef_loss   | -0.131   |
|    learning_rate   | 0.0003   |
|    n_updates       | 348799   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.22e+03 |
|    critic_loss     | 120       |
|    ent_coef        | 2.45      |
|    ent_coef_loss   | 0.0531    |
|    learning_rate   | 0.0003    |
|    n_updates       | 349199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.21e+03 |
|    critic_loss     | 193       |
|    ent_coef        | 2.44      |
|    ent_coef_loss   | 0.0083    |
|    learning_rate   | 0.0003    |
|    n_updates       | 349599    |
----------------------------------
=== Iterazione IRL 916 ===
Loss reward (iter 916): 6.472813129425049
=== Iterazione IRL 917 ===
Loss reward (iter 917): 6.312822341918945
=== Iterazione IRL 918 ===
Loss reward (iter 918): 6.352945804595947
=== Iterazione IRL 919 ===
Loss reward (iter 919): 6.632672309875488
=== Iterazione IRL 920 ===
Loss reward (iter 920): 6.151240348815918
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.21e+03 |
|    critic_loss     | 143       |
|    ent_coef        | 2.45      |
|    ent_coef_loss   | -0.0491   |
|    learning_rate   | 0.0003    |
|    n_updates       | 349899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.23e+03 |
|    critic_loss     | 118       |
|    ent_coef        | 2.43      |
|    ent_coef_loss   | 0.0352    |
|    learning_rate   | 0.0003    |
|    n_updates       | 350299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.19e+03 |
|    critic_loss     | 209       |
|    ent_coef        | 2.41      |
|    ent_coef_loss   | -0.0876   |
|    learning_rate   | 0.0003    |
|    n_updates       | 350699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.21e+03 |
|    critic_loss     | 152       |
|    ent_coef        | 2.43      |
|    ent_coef_loss   | 0.0321    |
|    learning_rate   | 0.0003    |
|    n_updates       | 351099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.21e+03 |
|    critic_loss     | 151       |
|    ent_coef        | 2.43      |
|    ent_coef_loss   | -0.0216   |
|    learning_rate   | 0.0003    |
|    n_updates       | 351499    |
----------------------------------
=== Iterazione IRL 921 ===
Loss reward (iter 921): 6.315981388092041
=== Iterazione IRL 922 ===
Loss reward (iter 922): 6.1354193687438965
=== Iterazione IRL 923 ===
Loss reward (iter 923): 6.360477447509766
=== Iterazione IRL 924 ===
Loss reward (iter 924): 5.714686393737793
=== Iterazione IRL 925 ===
Loss reward (iter 925): 6.007955551147461
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.22e+03 |
|    critic_loss     | 126       |
|    ent_coef        | 2.43      |
|    ent_coef_loss   | -0.0137   |
|    learning_rate   | 0.0003    |
|    n_updates       | 351799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.23e+03 |
|    critic_loss     | 139       |
|    ent_coef        | 2.45      |
|    ent_coef_loss   | 0.0884    |
|    learning_rate   | 0.0003    |
|    n_updates       | 352199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.23e+03 |
|    critic_loss     | 191       |
|    ent_coef        | 2.45      |
|    ent_coef_loss   | 0.102     |
|    learning_rate   | 0.0003    |
|    n_updates       | 352599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 123       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.22e+03 |
|    critic_loss     | 164       |
|    ent_coef        | 2.43      |
|    ent_coef_loss   | -0.0446   |
|    learning_rate   | 0.0003    |
|    n_updates       | 352999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.23e+03 |
|    critic_loss     | 145       |
|    ent_coef        | 2.44      |
|    ent_coef_loss   | 0.179     |
|    learning_rate   | 0.0003    |
|    n_updates       | 353399    |
----------------------------------
=== Iterazione IRL 926 ===
Loss reward (iter 926): 6.152283668518066
=== Iterazione IRL 927 ===
Loss reward (iter 927): 6.106714248657227
=== Iterazione IRL 928 ===
Loss reward (iter 928): 5.127206325531006
=== Iterazione IRL 929 ===
Loss reward (iter 929): 6.317873001098633
=== Iterazione IRL 930 ===
Loss reward (iter 930): 5.859831809997559
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.25e+03 |
|    critic_loss     | 179       |
|    ent_coef        | 2.42      |
|    ent_coef_loss   | 0.0322    |
|    learning_rate   | 0.0003    |
|    n_updates       | 353699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.22e+03 |
|    critic_loss     | 174       |
|    ent_coef        | 2.44      |
|    ent_coef_loss   | 0.145     |
|    learning_rate   | 0.0003    |
|    n_updates       | 354099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.22e+03 |
|    critic_loss     | 243       |
|    ent_coef        | 2.44      |
|    ent_coef_loss   | 0.0534    |
|    learning_rate   | 0.0003    |
|    n_updates       | 354499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.24e+03 |
|    critic_loss     | 160       |
|    ent_coef        | 2.44      |
|    ent_coef_loss   | -0.0454   |
|    learning_rate   | 0.0003    |
|    n_updates       | 354899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.24e+03 |
|    critic_loss     | 131       |
|    ent_coef        | 2.45      |
|    ent_coef_loss   | -0.00894  |
|    learning_rate   | 0.0003    |
|    n_updates       | 355299    |
----------------------------------
=== Iterazione IRL 931 ===
Loss reward (iter 931): 5.814689636230469
=== Iterazione IRL 932 ===
Loss reward (iter 932): 6.028409957885742
=== Iterazione IRL 933 ===
Loss reward (iter 933): 6.304556846618652
=== Iterazione IRL 934 ===
Loss reward (iter 934): 6.3474249839782715
=== Iterazione IRL 935 ===
Loss reward (iter 935): 6.118791580200195
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 153       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.25e+03 |
|    critic_loss     | 183       |
|    ent_coef        | 2.47      |
|    ent_coef_loss   | -0.0182   |
|    learning_rate   | 0.0003    |
|    n_updates       | 355599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.24e+03 |
|    critic_loss     | 139       |
|    ent_coef        | 2.46      |
|    ent_coef_loss   | 0.0906    |
|    learning_rate   | 0.0003    |
|    n_updates       | 355999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.22e+03 |
|    critic_loss     | 191       |
|    ent_coef        | 2.45      |
|    ent_coef_loss   | -0.000489 |
|    learning_rate   | 0.0003    |
|    n_updates       | 356399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.26e+03 |
|    critic_loss     | 125       |
|    ent_coef        | 2.45      |
|    ent_coef_loss   | -0.0156   |
|    learning_rate   | 0.0003    |
|    n_updates       | 356799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 123       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.23e+03 |
|    critic_loss     | 154       |
|    ent_coef        | 2.44      |
|    ent_coef_loss   | 0.176     |
|    learning_rate   | 0.0003    |
|    n_updates       | 357199    |
----------------------------------
=== Iterazione IRL 936 ===
Loss reward (iter 936): 6.197295665740967
=== Iterazione IRL 937 ===
Loss reward (iter 937): 6.0039520263671875
=== Iterazione IRL 938 ===
Loss reward (iter 938): 5.535363674163818
=== Iterazione IRL 939 ===
Loss reward (iter 939): 6.130110740661621
=== Iterazione IRL 940 ===
Loss reward (iter 940): 6.071179389953613
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.26e+03 |
|    critic_loss     | 142       |
|    ent_coef        | 2.48      |
|    ent_coef_loss   | -0.0469   |
|    learning_rate   | 0.0003    |
|    n_updates       | 357499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.25e+03 |
|    critic_loss     | 194       |
|    ent_coef        | 2.46      |
|    ent_coef_loss   | 0.0786    |
|    learning_rate   | 0.0003    |
|    n_updates       | 357899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.24e+03 |
|    critic_loss     | 192       |
|    ent_coef        | 2.47      |
|    ent_coef_loss   | -0.0224   |
|    learning_rate   | 0.0003    |
|    n_updates       | 358299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.25e+03 |
|    critic_loss     | 175       |
|    ent_coef        | 2.47      |
|    ent_coef_loss   | 0.1       |
|    learning_rate   | 0.0003    |
|    n_updates       | 358699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 123       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.28e+03 |
|    critic_loss     | 151       |
|    ent_coef        | 2.45      |
|    ent_coef_loss   | 0.0234    |
|    learning_rate   | 0.0003    |
|    n_updates       | 359099    |
----------------------------------
=== Iterazione IRL 941 ===
Loss reward (iter 941): 6.346308708190918
=== Iterazione IRL 942 ===
Loss reward (iter 942): 6.2790045738220215
=== Iterazione IRL 943 ===
Loss reward (iter 943): 6.160581588745117
=== Iterazione IRL 944 ===
Loss reward (iter 944): 6.284419059753418
=== Iterazione IRL 945 ===
Loss reward (iter 945): 6.441853046417236
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.26e+03 |
|    critic_loss     | 109       |
|    ent_coef        | 2.45      |
|    ent_coef_loss   | 0.0161    |
|    learning_rate   | 0.0003    |
|    n_updates       | 359399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.27e+03 |
|    critic_loss     | 152       |
|    ent_coef        | 2.45      |
|    ent_coef_loss   | 0.0475    |
|    learning_rate   | 0.0003    |
|    n_updates       | 359799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.25e+03 |
|    critic_loss     | 127       |
|    ent_coef        | 2.47      |
|    ent_coef_loss   | 0.0127    |
|    learning_rate   | 0.0003    |
|    n_updates       | 360199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.25e+03 |
|    critic_loss     | 224       |
|    ent_coef        | 2.49      |
|    ent_coef_loss   | 0.0226    |
|    learning_rate   | 0.0003    |
|    n_updates       | 360599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 124       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.27e+03 |
|    critic_loss     | 177       |
|    ent_coef        | 2.48      |
|    ent_coef_loss   | -0.0833   |
|    learning_rate   | 0.0003    |
|    n_updates       | 360999    |
----------------------------------
=== Iterazione IRL 946 ===
Loss reward (iter 946): 5.9019060134887695
=== Iterazione IRL 947 ===
Loss reward (iter 947): 6.852141380310059
=== Iterazione IRL 948 ===
Loss reward (iter 948): 6.349453449249268
=== Iterazione IRL 949 ===
Loss reward (iter 949): 6.176869869232178
=== Iterazione IRL 950 ===
Loss reward (iter 950): 6.022091865539551
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 154       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.25e+03 |
|    critic_loss     | 124       |
|    ent_coef        | 2.49      |
|    ent_coef_loss   | 0.01      |
|    learning_rate   | 0.0003    |
|    n_updates       | 361299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.24e+03 |
|    critic_loss     | 260       |
|    ent_coef        | 2.5       |
|    ent_coef_loss   | 0.11      |
|    learning_rate   | 0.0003    |
|    n_updates       | 361699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.25e+03 |
|    critic_loss     | 165       |
|    ent_coef        | 2.52      |
|    ent_coef_loss   | -0.0784   |
|    learning_rate   | 0.0003    |
|    n_updates       | 362099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.26e+03 |
|    critic_loss     | 172       |
|    ent_coef        | 2.51      |
|    ent_coef_loss   | 0.084     |
|    learning_rate   | 0.0003    |
|    n_updates       | 362499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 123       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.26e+03 |
|    critic_loss     | 217       |
|    ent_coef        | 2.52      |
|    ent_coef_loss   | 0.0399    |
|    learning_rate   | 0.0003    |
|    n_updates       | 362899    |
----------------------------------
=== Iterazione IRL 951 ===
Loss reward (iter 951): 6.209444522857666
=== Iterazione IRL 952 ===
Loss reward (iter 952): 6.356208801269531
=== Iterazione IRL 953 ===
Loss reward (iter 953): 6.25009822845459
=== Iterazione IRL 954 ===
Loss reward (iter 954): 6.150463581085205
=== Iterazione IRL 955 ===
Loss reward (iter 955): 8.167844772338867
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 153       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.25e+03 |
|    critic_loss     | 154       |
|    ent_coef        | 2.51      |
|    ent_coef_loss   | -0.02     |
|    learning_rate   | 0.0003    |
|    n_updates       | 363199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.27e+03 |
|    critic_loss     | 226       |
|    ent_coef        | 2.52      |
|    ent_coef_loss   | -0.0737   |
|    learning_rate   | 0.0003    |
|    n_updates       | 363599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.28e+03 |
|    critic_loss     | 143       |
|    ent_coef        | 2.51      |
|    ent_coef_loss   | 0.0834    |
|    learning_rate   | 0.0003    |
|    n_updates       | 363999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.28e+03 |
|    critic_loss     | 180       |
|    ent_coef        | 2.53      |
|    ent_coef_loss   | -0.0286   |
|    learning_rate   | 0.0003    |
|    n_updates       | 364399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 123       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.28e+03 |
|    critic_loss     | 158       |
|    ent_coef        | 2.5       |
|    ent_coef_loss   | -0.2      |
|    learning_rate   | 0.0003    |
|    n_updates       | 364799    |
----------------------------------
=== Iterazione IRL 956 ===
Loss reward (iter 956): 6.23969841003418
=== Iterazione IRL 957 ===
Loss reward (iter 957): 7.636721611022949
=== Iterazione IRL 958 ===
Loss reward (iter 958): 7.500163555145264
=== Iterazione IRL 959 ===
Loss reward (iter 959): 7.487611293792725
=== Iterazione IRL 960 ===
Loss reward (iter 960): 6.49825382232666
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 153       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.27e+03 |
|    critic_loss     | 113       |
|    ent_coef        | 2.48      |
|    ent_coef_loss   | -0.00815  |
|    learning_rate   | 0.0003    |
|    n_updates       | 365099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.27e+03 |
|    critic_loss     | 154       |
|    ent_coef        | 2.49      |
|    ent_coef_loss   | -0.0987   |
|    learning_rate   | 0.0003    |
|    n_updates       | 365499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.26e+03 |
|    critic_loss     | 136       |
|    ent_coef        | 2.49      |
|    ent_coef_loss   | -0.0193   |
|    learning_rate   | 0.0003    |
|    n_updates       | 365899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.28e+03 |
|    critic_loss     | 180       |
|    ent_coef        | 2.51      |
|    ent_coef_loss   | -0.118    |
|    learning_rate   | 0.0003    |
|    n_updates       | 366299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 123       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.28e+03 |
|    critic_loss     | 138       |
|    ent_coef        | 2.5       |
|    ent_coef_loss   | 0.0736    |
|    learning_rate   | 0.0003    |
|    n_updates       | 366699    |
----------------------------------
=== Iterazione IRL 961 ===
Loss reward (iter 961): 6.673996448516846
=== Iterazione IRL 962 ===
Loss reward (iter 962): 6.58833646774292
=== Iterazione IRL 963 ===
Loss reward (iter 963): 6.4840874671936035
=== Iterazione IRL 964 ===
Loss reward (iter 964): 6.290759563446045
=== Iterazione IRL 965 ===
Loss reward (iter 965): 6.301077842712402
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 153       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.29e+03 |
|    critic_loss     | 223       |
|    ent_coef        | 2.54      |
|    ent_coef_loss   | 0.13      |
|    learning_rate   | 0.0003    |
|    n_updates       | 366999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.26e+03 |
|    critic_loss     | 160       |
|    ent_coef        | 2.57      |
|    ent_coef_loss   | 0.0221    |
|    learning_rate   | 0.0003    |
|    n_updates       | 367399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.29e+03 |
|    critic_loss     | 165       |
|    ent_coef        | 2.56      |
|    ent_coef_loss   | -0.0546   |
|    learning_rate   | 0.0003    |
|    n_updates       | 367799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.29e+03 |
|    critic_loss     | 112       |
|    ent_coef        | 2.53      |
|    ent_coef_loss   | 0.1       |
|    learning_rate   | 0.0003    |
|    n_updates       | 368199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 123       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.29e+03 |
|    critic_loss     | 191       |
|    ent_coef        | 2.51      |
|    ent_coef_loss   | 0.0545    |
|    learning_rate   | 0.0003    |
|    n_updates       | 368599    |
----------------------------------
=== Iterazione IRL 966 ===
Loss reward (iter 966): 6.698333263397217
=== Iterazione IRL 967 ===
Loss reward (iter 967): 6.323063373565674
=== Iterazione IRL 968 ===
Loss reward (iter 968): 6.321563720703125
=== Iterazione IRL 969 ===
Loss reward (iter 969): 5.788618087768555
=== Iterazione IRL 970 ===
Loss reward (iter 970): 6.49676513671875
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 153       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.28e+03 |
|    critic_loss     | 124       |
|    ent_coef        | 2.49      |
|    ent_coef_loss   | -0.117    |
|    learning_rate   | 0.0003    |
|    n_updates       | 368899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.28e+03 |
|    critic_loss     | 172       |
|    ent_coef        | 2.52      |
|    ent_coef_loss   | -0.0594   |
|    learning_rate   | 0.0003    |
|    n_updates       | 369299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.29e+03 |
|    critic_loss     | 129       |
|    ent_coef        | 2.55      |
|    ent_coef_loss   | 0.00838   |
|    learning_rate   | 0.0003    |
|    n_updates       | 369699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.29e+03 |
|    critic_loss     | 146       |
|    ent_coef        | 2.54      |
|    ent_coef_loss   | 0.0905    |
|    learning_rate   | 0.0003    |
|    n_updates       | 370099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 123       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.31e+03 |
|    critic_loss     | 175       |
|    ent_coef        | 2.53      |
|    ent_coef_loss   | 0.0627    |
|    learning_rate   | 0.0003    |
|    n_updates       | 370499    |
----------------------------------
=== Iterazione IRL 971 ===
Loss reward (iter 971): 6.4219136238098145
=== Iterazione IRL 972 ===
Loss reward (iter 972): 6.2200927734375
=== Iterazione IRL 973 ===
Loss reward (iter 973): 6.851450443267822
=== Iterazione IRL 974 ===
Loss reward (iter 974): 6.132214069366455
=== Iterazione IRL 975 ===
Loss reward (iter 975): 6.151061058044434
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 154      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.3e+03 |
|    critic_loss     | 125      |
|    ent_coef        | 2.54     |
|    ent_coef_loss   | 0.0455   |
|    learning_rate   | 0.0003   |
|    n_updates       | 370799   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.29e+03 |
|    critic_loss     | 101       |
|    ent_coef        | 2.53      |
|    ent_coef_loss   | -0.0271   |
|    learning_rate   | 0.0003    |
|    n_updates       | 371199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 128       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.29e+03 |
|    critic_loss     | 149       |
|    ent_coef        | 2.54      |
|    ent_coef_loss   | -0.0252   |
|    learning_rate   | 0.0003    |
|    n_updates       | 371599    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 16       |
|    fps             | 125      |
|    time_elapsed    | 12       |
|    total_timesteps | 1600     |
| train/             |          |
|    actor_loss      | -2.3e+03 |
|    critic_loss     | 142      |
|    ent_coef        | 2.54     |
|    ent_coef_loss   | 0.0218   |
|    learning_rate   | 0.0003   |
|    n_updates       | 371999   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 123       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.31e+03 |
|    critic_loss     | 136       |
|    ent_coef        | 2.52      |
|    ent_coef_loss   | -0.0538   |
|    learning_rate   | 0.0003    |
|    n_updates       | 372399    |
----------------------------------
=== Iterazione IRL 976 ===
Loss reward (iter 976): 5.910538673400879
=== Iterazione IRL 977 ===
Loss reward (iter 977): 6.557188987731934
=== Iterazione IRL 978 ===
Loss reward (iter 978): 6.506451606750488
=== Iterazione IRL 979 ===
Loss reward (iter 979): 5.652249336242676
=== Iterazione IRL 980 ===
Loss reward (iter 980): 6.6235246658325195
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.3e+03 |
|    critic_loss     | 153      |
|    ent_coef        | 2.54     |
|    ent_coef_loss   | 0.0156   |
|    learning_rate   | 0.0003   |
|    n_updates       | 372699   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.31e+03 |
|    critic_loss     | 150       |
|    ent_coef        | 2.55      |
|    ent_coef_loss   | 0.0864    |
|    learning_rate   | 0.0003    |
|    n_updates       | 373099    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -2.3e+03 |
|    critic_loss     | 183      |
|    ent_coef        | 2.54     |
|    ent_coef_loss   | -0.0252  |
|    learning_rate   | 0.0003   |
|    n_updates       | 373499   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 123       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.32e+03 |
|    critic_loss     | 89.9      |
|    ent_coef        | 2.56      |
|    ent_coef_loss   | -0.0318   |
|    learning_rate   | 0.0003    |
|    n_updates       | 373899    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 20       |
|    fps             | 122      |
|    time_elapsed    | 16       |
|    total_timesteps | 2000     |
| train/             |          |
|    actor_loss      | -2.3e+03 |
|    critic_loss     | 144      |
|    ent_coef        | 2.54     |
|    ent_coef_loss   | 0.0641   |
|    learning_rate   | 0.0003   |
|    n_updates       | 374299   |
---------------------------------
=== Iterazione IRL 981 ===
Loss reward (iter 981): 6.260983467102051
=== Iterazione IRL 982 ===
Loss reward (iter 982): 6.301835060119629
=== Iterazione IRL 983 ===
Loss reward (iter 983): 6.659266471862793
=== Iterazione IRL 984 ===
Loss reward (iter 984): 6.308627605438232
=== Iterazione IRL 985 ===
Loss reward (iter 985): 6.221404075622559
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.31e+03 |
|    critic_loss     | 196       |
|    ent_coef        | 2.55      |
|    ent_coef_loss   | -0.0304   |
|    learning_rate   | 0.0003    |
|    n_updates       | 374599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.31e+03 |
|    critic_loss     | 181       |
|    ent_coef        | 2.55      |
|    ent_coef_loss   | 0.0515    |
|    learning_rate   | 0.0003    |
|    n_updates       | 374999    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -2.3e+03 |
|    critic_loss     | 200      |
|    ent_coef        | 2.54     |
|    ent_coef_loss   | -0.0372  |
|    learning_rate   | 0.0003   |
|    n_updates       | 375399   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 123       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.31e+03 |
|    critic_loss     | 126       |
|    ent_coef        | 2.57      |
|    ent_coef_loss   | -0.051    |
|    learning_rate   | 0.0003    |
|    n_updates       | 375799    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 20       |
|    fps             | 122      |
|    time_elapsed    | 16       |
|    total_timesteps | 2000     |
| train/             |          |
|    actor_loss      | -2.3e+03 |
|    critic_loss     | 158      |
|    ent_coef        | 2.56     |
|    ent_coef_loss   | -0.0823  |
|    learning_rate   | 0.0003   |
|    n_updates       | 376199   |
---------------------------------
=== Iterazione IRL 986 ===
Loss reward (iter 986): 6.331249237060547
=== Iterazione IRL 987 ===
Loss reward (iter 987): 6.413944721221924
=== Iterazione IRL 988 ===
Loss reward (iter 988): 6.235926628112793
=== Iterazione IRL 989 ===
Loss reward (iter 989): 6.245800495147705
=== Iterazione IRL 990 ===
Loss reward (iter 990): 6.059096336364746
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.31e+03 |
|    critic_loss     | 103       |
|    ent_coef        | 2.58      |
|    ent_coef_loss   | 0.169     |
|    learning_rate   | 0.0003    |
|    n_updates       | 376499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.33e+03 |
|    critic_loss     | 145       |
|    ent_coef        | 2.56      |
|    ent_coef_loss   | 0.00485   |
|    learning_rate   | 0.0003    |
|    n_updates       | 376899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.32e+03 |
|    critic_loss     | 194       |
|    ent_coef        | 2.54      |
|    ent_coef_loss   | -0.0168   |
|    learning_rate   | 0.0003    |
|    n_updates       | 377299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 123       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.32e+03 |
|    critic_loss     | 163       |
|    ent_coef        | 2.55      |
|    ent_coef_loss   | -0.136    |
|    learning_rate   | 0.0003    |
|    n_updates       | 377699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.33e+03 |
|    critic_loss     | 175       |
|    ent_coef        | 2.57      |
|    ent_coef_loss   | 0.162     |
|    learning_rate   | 0.0003    |
|    n_updates       | 378099    |
----------------------------------
=== Iterazione IRL 991 ===
Loss reward (iter 991): 5.735852241516113
=== Iterazione IRL 992 ===
Loss reward (iter 992): 5.808162212371826
=== Iterazione IRL 993 ===
Loss reward (iter 993): 6.538987636566162
=== Iterazione IRL 994 ===
Loss reward (iter 994): 5.987332820892334
=== Iterazione IRL 995 ===
Loss reward (iter 995): 5.958177089691162
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.32e+03 |
|    critic_loss     | 165       |
|    ent_coef        | 2.55      |
|    ent_coef_loss   | -0.0915   |
|    learning_rate   | 0.0003    |
|    n_updates       | 378399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.31e+03 |
|    critic_loss     | 128       |
|    ent_coef        | 2.56      |
|    ent_coef_loss   | -0.053    |
|    learning_rate   | 0.0003    |
|    n_updates       | 378799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.33e+03 |
|    critic_loss     | 188       |
|    ent_coef        | 2.58      |
|    ent_coef_loss   | 0.0532    |
|    learning_rate   | 0.0003    |
|    n_updates       | 379199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 123       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.33e+03 |
|    critic_loss     | 102       |
|    ent_coef        | 2.57      |
|    ent_coef_loss   | 0.0109    |
|    learning_rate   | 0.0003    |
|    n_updates       | 379599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.34e+03 |
|    critic_loss     | 223       |
|    ent_coef        | 2.57      |
|    ent_coef_loss   | 0.0421    |
|    learning_rate   | 0.0003    |
|    n_updates       | 379999    |
----------------------------------
=== Iterazione IRL 996 ===
Loss reward (iter 996): 6.611696243286133
=== Iterazione IRL 997 ===
Loss reward (iter 997): 6.403165817260742
=== Iterazione IRL 998 ===
Loss reward (iter 998): 5.739845275878906
=== Iterazione IRL 999 ===
Loss reward (iter 999): 6.207187652587891
=== Iterazione IRL 1000 ===
Loss reward (iter 1000): 5.9906158447265625
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.33e+03 |
|    critic_loss     | 141       |
|    ent_coef        | 2.58      |
|    ent_coef_loss   | -0.0348   |
|    learning_rate   | 0.0003    |
|    n_updates       | 380299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.33e+03 |
|    critic_loss     | 175       |
|    ent_coef        | 2.58      |
|    ent_coef_loss   | -0.108    |
|    learning_rate   | 0.0003    |
|    n_updates       | 380699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.32e+03 |
|    critic_loss     | 214       |
|    ent_coef        | 2.59      |
|    ent_coef_loss   | -0.147    |
|    learning_rate   | 0.0003    |
|    n_updates       | 381099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.35e+03 |
|    critic_loss     | 171       |
|    ent_coef        | 2.58      |
|    ent_coef_loss   | -0.0123   |
|    learning_rate   | 0.0003    |
|    n_updates       | 381499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.33e+03 |
|    critic_loss     | 247       |
|    ent_coef        | 2.58      |
|    ent_coef_loss   | -0.0405   |
|    learning_rate   | 0.0003    |
|    n_updates       | 381899    |
----------------------------------
=== Iterazione IRL 1001 ===
Loss reward (iter 1001): 6.347865104675293
=== Iterazione IRL 1002 ===
Loss reward (iter 1002): 6.473175048828125
=== Iterazione IRL 1003 ===
Loss reward (iter 1003): 6.322615146636963
=== Iterazione IRL 1004 ===
Loss reward (iter 1004): 6.524233818054199
=== Iterazione IRL 1005 ===
Loss reward (iter 1005): 6.3642778396606445
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.35e+03 |
|    critic_loss     | 177       |
|    ent_coef        | 2.61      |
|    ent_coef_loss   | 0.0225    |
|    learning_rate   | 0.0003    |
|    n_updates       | 382199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.35e+03 |
|    critic_loss     | 143       |
|    ent_coef        | 2.58      |
|    ent_coef_loss   | 0.137     |
|    learning_rate   | 0.0003    |
|    n_updates       | 382599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.34e+03 |
|    critic_loss     | 143       |
|    ent_coef        | 2.58      |
|    ent_coef_loss   | -0.0935   |
|    learning_rate   | 0.0003    |
|    n_updates       | 382999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 123       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.35e+03 |
|    critic_loss     | 143       |
|    ent_coef        | 2.6       |
|    ent_coef_loss   | -0.0355   |
|    learning_rate   | 0.0003    |
|    n_updates       | 383399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.33e+03 |
|    critic_loss     | 210       |
|    ent_coef        | 2.59      |
|    ent_coef_loss   | -0.111    |
|    learning_rate   | 0.0003    |
|    n_updates       | 383799    |
----------------------------------
=== Iterazione IRL 1006 ===
Loss reward (iter 1006): 6.364603042602539
=== Iterazione IRL 1007 ===
Loss reward (iter 1007): 6.144740581512451
=== Iterazione IRL 1008 ===
Loss reward (iter 1008): 6.548232555389404
=== Iterazione IRL 1009 ===
Loss reward (iter 1009): 6.431638240814209
=== Iterazione IRL 1010 ===
Loss reward (iter 1010): 5.861033916473389
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.36e+03 |
|    critic_loss     | 164       |
|    ent_coef        | 2.56      |
|    ent_coef_loss   | 0.023     |
|    learning_rate   | 0.0003    |
|    n_updates       | 384099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.35e+03 |
|    critic_loss     | 141       |
|    ent_coef        | 2.57      |
|    ent_coef_loss   | -0.0471   |
|    learning_rate   | 0.0003    |
|    n_updates       | 384499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.35e+03 |
|    critic_loss     | 131       |
|    ent_coef        | 2.58      |
|    ent_coef_loss   | 0.189     |
|    learning_rate   | 0.0003    |
|    n_updates       | 384899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 123       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.35e+03 |
|    critic_loss     | 159       |
|    ent_coef        | 2.58      |
|    ent_coef_loss   | -0.0732   |
|    learning_rate   | 0.0003    |
|    n_updates       | 385299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.34e+03 |
|    critic_loss     | 163       |
|    ent_coef        | 2.56      |
|    ent_coef_loss   | -0.106    |
|    learning_rate   | 0.0003    |
|    n_updates       | 385699    |
----------------------------------
=== Iterazione IRL 1011 ===
Loss reward (iter 1011): 6.955780982971191
=== Iterazione IRL 1012 ===
Loss reward (iter 1012): 6.610097408294678
=== Iterazione IRL 1013 ===
Loss reward (iter 1013): 6.497491836547852
=== Iterazione IRL 1014 ===
Loss reward (iter 1014): 6.666362285614014
=== Iterazione IRL 1015 ===
Loss reward (iter 1015): 6.523599624633789
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.36e+03 |
|    critic_loss     | 136       |
|    ent_coef        | 2.58      |
|    ent_coef_loss   | -0.0193   |
|    learning_rate   | 0.0003    |
|    n_updates       | 385999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.35e+03 |
|    critic_loss     | 116       |
|    ent_coef        | 2.6       |
|    ent_coef_loss   | 0.109     |
|    learning_rate   | 0.0003    |
|    n_updates       | 386399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.36e+03 |
|    critic_loss     | 147       |
|    ent_coef        | 2.59      |
|    ent_coef_loss   | 0.113     |
|    learning_rate   | 0.0003    |
|    n_updates       | 386799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 123       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.35e+03 |
|    critic_loss     | 181       |
|    ent_coef        | 2.63      |
|    ent_coef_loss   | -0.113    |
|    learning_rate   | 0.0003    |
|    n_updates       | 387199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.35e+03 |
|    critic_loss     | 200       |
|    ent_coef        | 2.62      |
|    ent_coef_loss   | -0.0135   |
|    learning_rate   | 0.0003    |
|    n_updates       | 387599    |
----------------------------------
=== Iterazione IRL 1016 ===
Loss reward (iter 1016): 6.285381317138672
=== Iterazione IRL 1017 ===
Loss reward (iter 1017): 6.054476261138916
=== Iterazione IRL 1018 ===
Loss reward (iter 1018): 6.229480743408203
=== Iterazione IRL 1019 ===
Loss reward (iter 1019): 6.124642848968506
=== Iterazione IRL 1020 ===
Loss reward (iter 1020): 5.075631618499756
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.37e+03 |
|    critic_loss     | 204       |
|    ent_coef        | 2.6       |
|    ent_coef_loss   | 0.036     |
|    learning_rate   | 0.0003    |
|    n_updates       | 387899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.37e+03 |
|    critic_loss     | 138       |
|    ent_coef        | 2.58      |
|    ent_coef_loss   | 0.111     |
|    learning_rate   | 0.0003    |
|    n_updates       | 388299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.36e+03 |
|    critic_loss     | 150       |
|    ent_coef        | 2.58      |
|    ent_coef_loss   | 0.043     |
|    learning_rate   | 0.0003    |
|    n_updates       | 388699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 123       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.33e+03 |
|    critic_loss     | 286       |
|    ent_coef        | 2.57      |
|    ent_coef_loss   | -0.043    |
|    learning_rate   | 0.0003    |
|    n_updates       | 389099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.35e+03 |
|    critic_loss     | 109       |
|    ent_coef        | 2.59      |
|    ent_coef_loss   | 0.105     |
|    learning_rate   | 0.0003    |
|    n_updates       | 389499    |
----------------------------------
=== Iterazione IRL 1021 ===
Loss reward (iter 1021): 6.529384613037109
=== Iterazione IRL 1022 ===
Loss reward (iter 1022): 6.332790851593018
=== Iterazione IRL 1023 ===
Loss reward (iter 1023): 6.283649444580078
=== Iterazione IRL 1024 ===
Loss reward (iter 1024): 6.216983318328857
=== Iterazione IRL 1025 ===
Loss reward (iter 1025): 6.519230842590332
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 153       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.37e+03 |
|    critic_loss     | 134       |
|    ent_coef        | 2.63      |
|    ent_coef_loss   | -0.0572   |
|    learning_rate   | 0.0003    |
|    n_updates       | 389799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.35e+03 |
|    critic_loss     | 133       |
|    ent_coef        | 2.63      |
|    ent_coef_loss   | 0.0394    |
|    learning_rate   | 0.0003    |
|    n_updates       | 390199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.36e+03 |
|    critic_loss     | 142       |
|    ent_coef        | 2.6       |
|    ent_coef_loss   | 0.043     |
|    learning_rate   | 0.0003    |
|    n_updates       | 390599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.35e+03 |
|    critic_loss     | 166       |
|    ent_coef        | 2.59      |
|    ent_coef_loss   | -0.153    |
|    learning_rate   | 0.0003    |
|    n_updates       | 390999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 123       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.37e+03 |
|    critic_loss     | 176       |
|    ent_coef        | 2.58      |
|    ent_coef_loss   | -0.088    |
|    learning_rate   | 0.0003    |
|    n_updates       | 391399    |
----------------------------------
=== Iterazione IRL 1026 ===
Loss reward (iter 1026): 5.933511257171631
=== Iterazione IRL 1027 ===
Loss reward (iter 1027): 6.169173240661621
=== Iterazione IRL 1028 ===
Loss reward (iter 1028): 6.298220157623291
=== Iterazione IRL 1029 ===
Loss reward (iter 1029): 6.2046074867248535
=== Iterazione IRL 1030 ===
Loss reward (iter 1030): 5.935538291931152
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 153       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.38e+03 |
|    critic_loss     | 176       |
|    ent_coef        | 2.59      |
|    ent_coef_loss   | 0.0863    |
|    learning_rate   | 0.0003    |
|    n_updates       | 391699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.37e+03 |
|    critic_loss     | 222       |
|    ent_coef        | 2.59      |
|    ent_coef_loss   | 0.00194   |
|    learning_rate   | 0.0003    |
|    n_updates       | 392099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.36e+03 |
|    critic_loss     | 147       |
|    ent_coef        | 2.61      |
|    ent_coef_loss   | 0.0428    |
|    learning_rate   | 0.0003    |
|    n_updates       | 392499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.37e+03 |
|    critic_loss     | 146       |
|    ent_coef        | 2.61      |
|    ent_coef_loss   | -0.077    |
|    learning_rate   | 0.0003    |
|    n_updates       | 392899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 123       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.36e+03 |
|    critic_loss     | 193       |
|    ent_coef        | 2.61      |
|    ent_coef_loss   | -0.0182   |
|    learning_rate   | 0.0003    |
|    n_updates       | 393299    |
----------------------------------
=== Iterazione IRL 1031 ===
Loss reward (iter 1031): 6.141938209533691
=== Iterazione IRL 1032 ===
Loss reward (iter 1032): 6.42555046081543
=== Iterazione IRL 1033 ===
Loss reward (iter 1033): 6.5988688468933105
=== Iterazione IRL 1034 ===
Loss reward (iter 1034): 6.244801044464111
=== Iterazione IRL 1035 ===
Loss reward (iter 1035): 6.164050579071045
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 153       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.37e+03 |
|    critic_loss     | 149       |
|    ent_coef        | 2.6       |
|    ent_coef_loss   | -0.022    |
|    learning_rate   | 0.0003    |
|    n_updates       | 393599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.37e+03 |
|    critic_loss     | 154       |
|    ent_coef        | 2.61      |
|    ent_coef_loss   | 0.0146    |
|    learning_rate   | 0.0003    |
|    n_updates       | 393999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.37e+03 |
|    critic_loss     | 145       |
|    ent_coef        | 2.62      |
|    ent_coef_loss   | 0.00425   |
|    learning_rate   | 0.0003    |
|    n_updates       | 394399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.38e+03 |
|    critic_loss     | 124       |
|    ent_coef        | 2.62      |
|    ent_coef_loss   | 0.0163    |
|    learning_rate   | 0.0003    |
|    n_updates       | 394799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.37e+03 |
|    critic_loss     | 147       |
|    ent_coef        | 2.6       |
|    ent_coef_loss   | 0.133     |
|    learning_rate   | 0.0003    |
|    n_updates       | 395199    |
----------------------------------
=== Iterazione IRL 1036 ===
Loss reward (iter 1036): 6.3850812911987305
=== Iterazione IRL 1037 ===
Loss reward (iter 1037): 6.394431114196777
=== Iterazione IRL 1038 ===
Loss reward (iter 1038): 6.280389785766602
=== Iterazione IRL 1039 ===
Loss reward (iter 1039): 6.247762203216553
=== Iterazione IRL 1040 ===
Loss reward (iter 1040): 6.263943672180176
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.37e+03 |
|    critic_loss     | 158       |
|    ent_coef        | 2.63      |
|    ent_coef_loss   | 0.00795   |
|    learning_rate   | 0.0003    |
|    n_updates       | 395499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.39e+03 |
|    critic_loss     | 137       |
|    ent_coef        | 2.65      |
|    ent_coef_loss   | 0.0161    |
|    learning_rate   | 0.0003    |
|    n_updates       | 395899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.39e+03 |
|    critic_loss     | 134       |
|    ent_coef        | 2.64      |
|    ent_coef_loss   | 0.04      |
|    learning_rate   | 0.0003    |
|    n_updates       | 396299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 123       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.38e+03 |
|    critic_loss     | 158       |
|    ent_coef        | 2.64      |
|    ent_coef_loss   | 0.11      |
|    learning_rate   | 0.0003    |
|    n_updates       | 396699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.38e+03 |
|    critic_loss     | 139       |
|    ent_coef        | 2.64      |
|    ent_coef_loss   | 0.116     |
|    learning_rate   | 0.0003    |
|    n_updates       | 397099    |
----------------------------------
=== Iterazione IRL 1041 ===
Loss reward (iter 1041): 6.47952127456665
=== Iterazione IRL 1042 ===
Loss reward (iter 1042): 6.472049713134766
=== Iterazione IRL 1043 ===
Loss reward (iter 1043): 6.063239574432373
=== Iterazione IRL 1044 ===
Loss reward (iter 1044): 6.26453161239624
=== Iterazione IRL 1045 ===
Loss reward (iter 1045): 6.025222301483154
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.38e+03 |
|    critic_loss     | 214       |
|    ent_coef        | 2.66      |
|    ent_coef_loss   | -0.0133   |
|    learning_rate   | 0.0003    |
|    n_updates       | 397399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.38e+03 |
|    critic_loss     | 141       |
|    ent_coef        | 2.66      |
|    ent_coef_loss   | -0.0212   |
|    learning_rate   | 0.0003    |
|    n_updates       | 397799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.37e+03 |
|    critic_loss     | 130       |
|    ent_coef        | 2.63      |
|    ent_coef_loss   | 0.121     |
|    learning_rate   | 0.0003    |
|    n_updates       | 398199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.39e+03 |
|    critic_loss     | 177       |
|    ent_coef        | 2.63      |
|    ent_coef_loss   | -0.0235   |
|    learning_rate   | 0.0003    |
|    n_updates       | 398599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 123       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.38e+03 |
|    critic_loss     | 119       |
|    ent_coef        | 2.64      |
|    ent_coef_loss   | 0.001     |
|    learning_rate   | 0.0003    |
|    n_updates       | 398999    |
----------------------------------
=== Iterazione IRL 1046 ===
Loss reward (iter 1046): 6.209017753601074
=== Iterazione IRL 1047 ===
Loss reward (iter 1047): 6.026120185852051
=== Iterazione IRL 1048 ===
Loss reward (iter 1048): 6.438888072967529
=== Iterazione IRL 1049 ===
Loss reward (iter 1049): 6.250521183013916
=== Iterazione IRL 1050 ===
Loss reward (iter 1050): 6.322272777557373
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 153       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.38e+03 |
|    critic_loss     | 147       |
|    ent_coef        | 2.64      |
|    ent_coef_loss   | -0.00552  |
|    learning_rate   | 0.0003    |
|    n_updates       | 399299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.38e+03 |
|    critic_loss     | 139       |
|    ent_coef        | 2.63      |
|    ent_coef_loss   | 0.00977   |
|    learning_rate   | 0.0003    |
|    n_updates       | 399699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.39e+03 |
|    critic_loss     | 144       |
|    ent_coef        | 2.64      |
|    ent_coef_loss   | -0.133    |
|    learning_rate   | 0.0003    |
|    n_updates       | 400099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.39e+03 |
|    critic_loss     | 155       |
|    ent_coef        | 2.66      |
|    ent_coef_loss   | 0.0391    |
|    learning_rate   | 0.0003    |
|    n_updates       | 400499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 123       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.38e+03 |
|    critic_loss     | 121       |
|    ent_coef        | 2.63      |
|    ent_coef_loss   | -0.0605   |
|    learning_rate   | 0.0003    |
|    n_updates       | 400899    |
----------------------------------
=== Iterazione IRL 1051 ===
Loss reward (iter 1051): 6.322591304779053
=== Iterazione IRL 1052 ===
Loss reward (iter 1052): 6.55930757522583
=== Iterazione IRL 1053 ===
Loss reward (iter 1053): 6.552525043487549
=== Iterazione IRL 1054 ===
Loss reward (iter 1054): 6.232263088226318
=== Iterazione IRL 1055 ===
Loss reward (iter 1055): 6.333649635314941
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 153       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.38e+03 |
|    critic_loss     | 173       |
|    ent_coef        | 2.64      |
|    ent_coef_loss   | -0.146    |
|    learning_rate   | 0.0003    |
|    n_updates       | 401199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.39e+03 |
|    critic_loss     | 176       |
|    ent_coef        | 2.64      |
|    ent_coef_loss   | 0.125     |
|    learning_rate   | 0.0003    |
|    n_updates       | 401599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.38e+03 |
|    critic_loss     | 174       |
|    ent_coef        | 2.65      |
|    ent_coef_loss   | 0.0729    |
|    learning_rate   | 0.0003    |
|    n_updates       | 401999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.39e+03 |
|    critic_loss     | 134       |
|    ent_coef        | 2.67      |
|    ent_coef_loss   | 0.0145    |
|    learning_rate   | 0.0003    |
|    n_updates       | 402399    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 20       |
|    fps             | 123      |
|    time_elapsed    | 16       |
|    total_timesteps | 2000     |
| train/             |          |
|    actor_loss      | -2.4e+03 |
|    critic_loss     | 123      |
|    ent_coef        | 2.65     |
|    ent_coef_loss   | -0.00887 |
|    learning_rate   | 0.0003   |
|    n_updates       | 402799   |
---------------------------------
=== Iterazione IRL 1056 ===
Loss reward (iter 1056): 6.524363040924072
=== Iterazione IRL 1057 ===
Loss reward (iter 1057): 5.941717624664307
=== Iterazione IRL 1058 ===
Loss reward (iter 1058): 6.304330825805664
=== Iterazione IRL 1059 ===
Loss reward (iter 1059): 6.3877153396606445
=== Iterazione IRL 1060 ===
Loss reward (iter 1060): 6.457495212554932
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 153       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.38e+03 |
|    critic_loss     | 177       |
|    ent_coef        | 2.67      |
|    ent_coef_loss   | 0.171     |
|    learning_rate   | 0.0003    |
|    n_updates       | 403099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.41e+03 |
|    critic_loss     | 108       |
|    ent_coef        | 2.66      |
|    ent_coef_loss   | -0.0579   |
|    learning_rate   | 0.0003    |
|    n_updates       | 403499    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -2.4e+03 |
|    critic_loss     | 123      |
|    ent_coef        | 2.65     |
|    ent_coef_loss   | 0.135    |
|    learning_rate   | 0.0003   |
|    n_updates       | 403899   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.41e+03 |
|    critic_loss     | 162       |
|    ent_coef        | 2.63      |
|    ent_coef_loss   | -0.0897   |
|    learning_rate   | 0.0003    |
|    n_updates       | 404299    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 20       |
|    fps             | 123      |
|    time_elapsed    | 16       |
|    total_timesteps | 2000     |
| train/             |          |
|    actor_loss      | -2.4e+03 |
|    critic_loss     | 158      |
|    ent_coef        | 2.64     |
|    ent_coef_loss   | 0.13     |
|    learning_rate   | 0.0003   |
|    n_updates       | 404699   |
---------------------------------
=== Iterazione IRL 1061 ===
Loss reward (iter 1061): 6.334859848022461
=== Iterazione IRL 1062 ===
Loss reward (iter 1062): 6.322325229644775
=== Iterazione IRL 1063 ===
Loss reward (iter 1063): 6.442809104919434
=== Iterazione IRL 1064 ===
Loss reward (iter 1064): 6.198282718658447
=== Iterazione IRL 1065 ===
Loss reward (iter 1065): 6.665520668029785
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.4e+03 |
|    critic_loss     | 210      |
|    ent_coef        | 2.65     |
|    ent_coef_loss   | 0.0841   |
|    learning_rate   | 0.0003   |
|    n_updates       | 404999   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.39e+03 |
|    critic_loss     | 170       |
|    ent_coef        | 2.65      |
|    ent_coef_loss   | -0.0144   |
|    learning_rate   | 0.0003    |
|    n_updates       | 405399    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -2.4e+03 |
|    critic_loss     | 139      |
|    ent_coef        | 2.67     |
|    ent_coef_loss   | -0.00939 |
|    learning_rate   | 0.0003   |
|    n_updates       | 405799   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.41e+03 |
|    critic_loss     | 208       |
|    ent_coef        | 2.69      |
|    ent_coef_loss   | -0.0168   |
|    learning_rate   | 0.0003    |
|    n_updates       | 406199    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 20       |
|    fps             | 123      |
|    time_elapsed    | 16       |
|    total_timesteps | 2000     |
| train/             |          |
|    actor_loss      | -2.4e+03 |
|    critic_loss     | 217      |
|    ent_coef        | 2.66     |
|    ent_coef_loss   | -0.0156  |
|    learning_rate   | 0.0003   |
|    n_updates       | 406599   |
---------------------------------
=== Iterazione IRL 1066 ===
Loss reward (iter 1066): 6.487537384033203
=== Iterazione IRL 1067 ===
Loss reward (iter 1067): 6.650907039642334
=== Iterazione IRL 1068 ===
Loss reward (iter 1068): 6.483013153076172
=== Iterazione IRL 1069 ===
Loss reward (iter 1069): 6.328742980957031
=== Iterazione IRL 1070 ===
Loss reward (iter 1070): 6.522727012634277
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.38e+03 |
|    critic_loss     | 148       |
|    ent_coef        | 2.67      |
|    ent_coef_loss   | 0.0519    |
|    learning_rate   | 0.0003    |
|    n_updates       | 406899    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.4e+03 |
|    critic_loss     | 151      |
|    ent_coef        | 2.68     |
|    ent_coef_loss   | 0.116    |
|    learning_rate   | 0.0003   |
|    n_updates       | 407299   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.41e+03 |
|    critic_loss     | 148       |
|    ent_coef        | 2.69      |
|    ent_coef_loss   | -0.0534   |
|    learning_rate   | 0.0003    |
|    n_updates       | 407699    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 16       |
|    fps             | 124      |
|    time_elapsed    | 12       |
|    total_timesteps | 1600     |
| train/             |          |
|    actor_loss      | -2.4e+03 |
|    critic_loss     | 141      |
|    ent_coef        | 2.69     |
|    ent_coef_loss   | 0.0322   |
|    learning_rate   | 0.0003   |
|    n_updates       | 408099   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.41e+03 |
|    critic_loss     | 122       |
|    ent_coef        | 2.69      |
|    ent_coef_loss   | 0.0219    |
|    learning_rate   | 0.0003    |
|    n_updates       | 408499    |
----------------------------------
=== Iterazione IRL 1071 ===
Loss reward (iter 1071): 6.23408317565918
=== Iterazione IRL 1072 ===
Loss reward (iter 1072): 5.7248711585998535
=== Iterazione IRL 1073 ===
Loss reward (iter 1073): 6.70316219329834
=== Iterazione IRL 1074 ===
Loss reward (iter 1074): 6.752253532409668
=== Iterazione IRL 1075 ===
Loss reward (iter 1075): 6.406652450561523
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.4e+03 |
|    critic_loss     | 197      |
|    ent_coef        | 2.65     |
|    ent_coef_loss   | 0.0319   |
|    learning_rate   | 0.0003   |
|    n_updates       | 408799   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.42e+03 |
|    critic_loss     | 102       |
|    ent_coef        | 2.65      |
|    ent_coef_loss   | 0.00685   |
|    learning_rate   | 0.0003    |
|    n_updates       | 409199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.41e+03 |
|    critic_loss     | 182       |
|    ent_coef        | 2.68      |
|    ent_coef_loss   | 0.0153    |
|    learning_rate   | 0.0003    |
|    n_updates       | 409599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.41e+03 |
|    critic_loss     | 205       |
|    ent_coef        | 2.69      |
|    ent_coef_loss   | -0.0462   |
|    learning_rate   | 0.0003    |
|    n_updates       | 409999    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 20       |
|    fps             | 122      |
|    time_elapsed    | 16       |
|    total_timesteps | 2000     |
| train/             |          |
|    actor_loss      | -2.4e+03 |
|    critic_loss     | 170      |
|    ent_coef        | 2.66     |
|    ent_coef_loss   | -0.0274  |
|    learning_rate   | 0.0003   |
|    n_updates       | 410399   |
---------------------------------
=== Iterazione IRL 1076 ===
Loss reward (iter 1076): 6.255856037139893
=== Iterazione IRL 1077 ===
Loss reward (iter 1077): 6.292936325073242
=== Iterazione IRL 1078 ===
Loss reward (iter 1078): 6.178354740142822
=== Iterazione IRL 1079 ===
Loss reward (iter 1079): 6.633195877075195
=== Iterazione IRL 1080 ===
Loss reward (iter 1080): 6.322122573852539
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.4e+03 |
|    critic_loss     | 146      |
|    ent_coef        | 2.67     |
|    ent_coef_loss   | -0.0829  |
|    learning_rate   | 0.0003   |
|    n_updates       | 410699   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.42e+03 |
|    critic_loss     | 132       |
|    ent_coef        | 2.66      |
|    ent_coef_loss   | 0.0056    |
|    learning_rate   | 0.0003    |
|    n_updates       | 411099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.42e+03 |
|    critic_loss     | 129       |
|    ent_coef        | 2.67      |
|    ent_coef_loss   | 0.0796    |
|    learning_rate   | 0.0003    |
|    n_updates       | 411499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.41e+03 |
|    critic_loss     | 155       |
|    ent_coef        | 2.67      |
|    ent_coef_loss   | -0.0685   |
|    learning_rate   | 0.0003    |
|    n_updates       | 411899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.42e+03 |
|    critic_loss     | 122       |
|    ent_coef        | 2.65      |
|    ent_coef_loss   | -0.0911   |
|    learning_rate   | 0.0003    |
|    n_updates       | 412299    |
----------------------------------
=== Iterazione IRL 1081 ===
Loss reward (iter 1081): 7.194521903991699
=== Iterazione IRL 1082 ===
Loss reward (iter 1082): 6.776237487792969
=== Iterazione IRL 1083 ===
Loss reward (iter 1083): 6.56182336807251
=== Iterazione IRL 1084 ===
Loss reward (iter 1084): 6.49890661239624
=== Iterazione IRL 1085 ===
Loss reward (iter 1085): 6.117211818695068
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.41e+03 |
|    critic_loss     | 162       |
|    ent_coef        | 2.65      |
|    ent_coef_loss   | -0.146    |
|    learning_rate   | 0.0003    |
|    n_updates       | 412599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.41e+03 |
|    critic_loss     | 177       |
|    ent_coef        | 2.67      |
|    ent_coef_loss   | -0.0605   |
|    learning_rate   | 0.0003    |
|    n_updates       | 412999    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -2.4e+03 |
|    critic_loss     | 180      |
|    ent_coef        | 2.69     |
|    ent_coef_loss   | -0.0335  |
|    learning_rate   | 0.0003   |
|    n_updates       | 413399   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.41e+03 |
|    critic_loss     | 171       |
|    ent_coef        | 2.67      |
|    ent_coef_loss   | -0.0819   |
|    learning_rate   | 0.0003    |
|    n_updates       | 413799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 123       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.43e+03 |
|    critic_loss     | 109       |
|    ent_coef        | 2.65      |
|    ent_coef_loss   | 0.0449    |
|    learning_rate   | 0.0003    |
|    n_updates       | 414199    |
----------------------------------
=== Iterazione IRL 1086 ===
Loss reward (iter 1086): 6.528287887573242
=== Iterazione IRL 1087 ===
Loss reward (iter 1087): 6.230650424957275
=== Iterazione IRL 1088 ===
Loss reward (iter 1088): 6.351265907287598
=== Iterazione IRL 1089 ===
Loss reward (iter 1089): 6.52873420715332
=== Iterazione IRL 1090 ===
Loss reward (iter 1090): 6.463783264160156
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 153       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.44e+03 |
|    critic_loss     | 103       |
|    ent_coef        | 2.64      |
|    ent_coef_loss   | -0.0752   |
|    learning_rate   | 0.0003    |
|    n_updates       | 414499    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.4e+03 |
|    critic_loss     | 119      |
|    ent_coef        | 2.67     |
|    ent_coef_loss   | -0.0466  |
|    learning_rate   | 0.0003   |
|    n_updates       | 414899   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.42e+03 |
|    critic_loss     | 189       |
|    ent_coef        | 2.68      |
|    ent_coef_loss   | -0.014    |
|    learning_rate   | 0.0003    |
|    n_updates       | 415299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.42e+03 |
|    critic_loss     | 144       |
|    ent_coef        | 2.71      |
|    ent_coef_loss   | -0.0194   |
|    learning_rate   | 0.0003    |
|    n_updates       | 415699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.43e+03 |
|    critic_loss     | 150       |
|    ent_coef        | 2.71      |
|    ent_coef_loss   | 0.0334    |
|    learning_rate   | 0.0003    |
|    n_updates       | 416099    |
----------------------------------
=== Iterazione IRL 1091 ===
Loss reward (iter 1091): 6.687253475189209
=== Iterazione IRL 1092 ===
Loss reward (iter 1092): 6.419027805328369
=== Iterazione IRL 1093 ===
Loss reward (iter 1093): 6.117578029632568
=== Iterazione IRL 1094 ===
Loss reward (iter 1094): 6.585724353790283
=== Iterazione IRL 1095 ===
Loss reward (iter 1095): 6.535147666931152
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.42e+03 |
|    critic_loss     | 107       |
|    ent_coef        | 2.7       |
|    ent_coef_loss   | -0.0353   |
|    learning_rate   | 0.0003    |
|    n_updates       | 416399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.42e+03 |
|    critic_loss     | 175       |
|    ent_coef        | 2.68      |
|    ent_coef_loss   | 0.107     |
|    learning_rate   | 0.0003    |
|    n_updates       | 416799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.42e+03 |
|    critic_loss     | 190       |
|    ent_coef        | 2.71      |
|    ent_coef_loss   | -0.116    |
|    learning_rate   | 0.0003    |
|    n_updates       | 417199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 123       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.43e+03 |
|    critic_loss     | 124       |
|    ent_coef        | 2.72      |
|    ent_coef_loss   | -0.0851   |
|    learning_rate   | 0.0003    |
|    n_updates       | 417599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.43e+03 |
|    critic_loss     | 196       |
|    ent_coef        | 2.71      |
|    ent_coef_loss   | 0.00557   |
|    learning_rate   | 0.0003    |
|    n_updates       | 417999    |
----------------------------------
=== Iterazione IRL 1096 ===
Loss reward (iter 1096): 6.677552700042725
=== Iterazione IRL 1097 ===
Loss reward (iter 1097): 6.2607831954956055
=== Iterazione IRL 1098 ===
Loss reward (iter 1098): 6.680373191833496
=== Iterazione IRL 1099 ===
Loss reward (iter 1099): 6.592078685760498
=== Iterazione IRL 1100 ===
Loss reward (iter 1100): 6.61515998840332
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.42e+03 |
|    critic_loss     | 132       |
|    ent_coef        | 2.72      |
|    ent_coef_loss   | -0.00987  |
|    learning_rate   | 0.0003    |
|    n_updates       | 418299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.42e+03 |
|    critic_loss     | 168       |
|    ent_coef        | 2.7       |
|    ent_coef_loss   | 0.0769    |
|    learning_rate   | 0.0003    |
|    n_updates       | 418699    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -2.4e+03 |
|    critic_loss     | 245      |
|    ent_coef        | 2.69     |
|    ent_coef_loss   | -0.0397  |
|    learning_rate   | 0.0003   |
|    n_updates       | 419099   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 123       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.42e+03 |
|    critic_loss     | 159       |
|    ent_coef        | 2.71      |
|    ent_coef_loss   | -0.132    |
|    learning_rate   | 0.0003    |
|    n_updates       | 419499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.42e+03 |
|    critic_loss     | 148       |
|    ent_coef        | 2.71      |
|    ent_coef_loss   | -0.0285   |
|    learning_rate   | 0.0003    |
|    n_updates       | 419899    |
----------------------------------
=== Iterazione IRL 1101 ===
Loss reward (iter 1101): 7.444706439971924
=== Iterazione IRL 1102 ===
Loss reward (iter 1102): 6.797469139099121
=== Iterazione IRL 1103 ===
Loss reward (iter 1103): 6.502161979675293
=== Iterazione IRL 1104 ===
Loss reward (iter 1104): 6.41859769821167
=== Iterazione IRL 1105 ===
Loss reward (iter 1105): 6.245534420013428
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.43e+03 |
|    critic_loss     | 124       |
|    ent_coef        | 2.71      |
|    ent_coef_loss   | 0.0339    |
|    learning_rate   | 0.0003    |
|    n_updates       | 420199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.43e+03 |
|    critic_loss     | 199       |
|    ent_coef        | 2.7       |
|    ent_coef_loss   | -0.0763   |
|    learning_rate   | 0.0003    |
|    n_updates       | 420599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.44e+03 |
|    critic_loss     | 138       |
|    ent_coef        | 2.71      |
|    ent_coef_loss   | 0.0526    |
|    learning_rate   | 0.0003    |
|    n_updates       | 420999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 123       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.44e+03 |
|    critic_loss     | 126       |
|    ent_coef        | 2.67      |
|    ent_coef_loss   | 0.057     |
|    learning_rate   | 0.0003    |
|    n_updates       | 421399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.42e+03 |
|    critic_loss     | 156       |
|    ent_coef        | 2.67      |
|    ent_coef_loss   | 0.0964    |
|    learning_rate   | 0.0003    |
|    n_updates       | 421799    |
----------------------------------
=== Iterazione IRL 1106 ===
Loss reward (iter 1106): 6.633345603942871
=== Iterazione IRL 1107 ===
Loss reward (iter 1107): 6.713716506958008
=== Iterazione IRL 1108 ===
Loss reward (iter 1108): 6.581599235534668
=== Iterazione IRL 1109 ===
Loss reward (iter 1109): 6.313811779022217
=== Iterazione IRL 1110 ===
Loss reward (iter 1110): 6.590141296386719
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.43e+03 |
|    critic_loss     | 166       |
|    ent_coef        | 2.68      |
|    ent_coef_loss   | -0.0564   |
|    learning_rate   | 0.0003    |
|    n_updates       | 422099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.43e+03 |
|    critic_loss     | 121       |
|    ent_coef        | 2.65      |
|    ent_coef_loss   | 0.0815    |
|    learning_rate   | 0.0003    |
|    n_updates       | 422499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.44e+03 |
|    critic_loss     | 97        |
|    ent_coef        | 2.67      |
|    ent_coef_loss   | 0.115     |
|    learning_rate   | 0.0003    |
|    n_updates       | 422899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 123       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.44e+03 |
|    critic_loss     | 145       |
|    ent_coef        | 2.68      |
|    ent_coef_loss   | -0.0093   |
|    learning_rate   | 0.0003    |
|    n_updates       | 423299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.44e+03 |
|    critic_loss     | 120       |
|    ent_coef        | 2.67      |
|    ent_coef_loss   | -0.08     |
|    learning_rate   | 0.0003    |
|    n_updates       | 423699    |
----------------------------------
=== Iterazione IRL 1111 ===
Loss reward (iter 1111): 6.224936008453369
=== Iterazione IRL 1112 ===
Loss reward (iter 1112): 7.2808051109313965
=== Iterazione IRL 1113 ===
Loss reward (iter 1113): 6.739424228668213
=== Iterazione IRL 1114 ===
Loss reward (iter 1114): 6.69835901260376
=== Iterazione IRL 1115 ===
Loss reward (iter 1115): 6.255927562713623
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.44e+03 |
|    critic_loss     | 119       |
|    ent_coef        | 2.67      |
|    ent_coef_loss   | -0.111    |
|    learning_rate   | 0.0003    |
|    n_updates       | 423999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.42e+03 |
|    critic_loss     | 187       |
|    ent_coef        | 2.7       |
|    ent_coef_loss   | -0.103    |
|    learning_rate   | 0.0003    |
|    n_updates       | 424399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.45e+03 |
|    critic_loss     | 159       |
|    ent_coef        | 2.7       |
|    ent_coef_loss   | 0.027     |
|    learning_rate   | 0.0003    |
|    n_updates       | 424799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 123       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.45e+03 |
|    critic_loss     | 131       |
|    ent_coef        | 2.69      |
|    ent_coef_loss   | 0.0108    |
|    learning_rate   | 0.0003    |
|    n_updates       | 425199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.44e+03 |
|    critic_loss     | 120       |
|    ent_coef        | 2.68      |
|    ent_coef_loss   | -0.00843  |
|    learning_rate   | 0.0003    |
|    n_updates       | 425599    |
----------------------------------
=== Iterazione IRL 1116 ===
Loss reward (iter 1116): 6.81394100189209
=== Iterazione IRL 1117 ===
Loss reward (iter 1117): 6.308947563171387
=== Iterazione IRL 1118 ===
Loss reward (iter 1118): 6.589840412139893
=== Iterazione IRL 1119 ===
Loss reward (iter 1119): 6.5683488845825195
=== Iterazione IRL 1120 ===
Loss reward (iter 1120): 6.51231050491333
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.44e+03 |
|    critic_loss     | 169       |
|    ent_coef        | 2.68      |
|    ent_coef_loss   | 0.0672    |
|    learning_rate   | 0.0003    |
|    n_updates       | 425899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.44e+03 |
|    critic_loss     | 92.3      |
|    ent_coef        | 2.68      |
|    ent_coef_loss   | 0.0443    |
|    learning_rate   | 0.0003    |
|    n_updates       | 426299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.45e+03 |
|    critic_loss     | 108       |
|    ent_coef        | 2.68      |
|    ent_coef_loss   | -0.0986   |
|    learning_rate   | 0.0003    |
|    n_updates       | 426699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 123       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.42e+03 |
|    critic_loss     | 287       |
|    ent_coef        | 2.71      |
|    ent_coef_loss   | -0.0459   |
|    learning_rate   | 0.0003    |
|    n_updates       | 427099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.44e+03 |
|    critic_loss     | 146       |
|    ent_coef        | 2.68      |
|    ent_coef_loss   | -0.00291  |
|    learning_rate   | 0.0003    |
|    n_updates       | 427499    |
----------------------------------
=== Iterazione IRL 1121 ===
Loss reward (iter 1121): 6.633208274841309
=== Iterazione IRL 1122 ===
Loss reward (iter 1122): 6.710063934326172
=== Iterazione IRL 1123 ===
Loss reward (iter 1123): 6.639089584350586
=== Iterazione IRL 1124 ===
Loss reward (iter 1124): 6.852158546447754
=== Iterazione IRL 1125 ===
Loss reward (iter 1125): 6.572879314422607
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.43e+03 |
|    critic_loss     | 160       |
|    ent_coef        | 2.69      |
|    ent_coef_loss   | 0.0313    |
|    learning_rate   | 0.0003    |
|    n_updates       | 427799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.43e+03 |
|    critic_loss     | 130       |
|    ent_coef        | 2.7       |
|    ent_coef_loss   | -0.047    |
|    learning_rate   | 0.0003    |
|    n_updates       | 428199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.44e+03 |
|    critic_loss     | 107       |
|    ent_coef        | 2.71      |
|    ent_coef_loss   | 0.0136    |
|    learning_rate   | 0.0003    |
|    n_updates       | 428599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 123       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.45e+03 |
|    critic_loss     | 122       |
|    ent_coef        | 2.7       |
|    ent_coef_loss   | 0.0443    |
|    learning_rate   | 0.0003    |
|    n_updates       | 428999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.45e+03 |
|    critic_loss     | 196       |
|    ent_coef        | 2.73      |
|    ent_coef_loss   | -0.0965   |
|    learning_rate   | 0.0003    |
|    n_updates       | 429399    |
----------------------------------
=== Iterazione IRL 1126 ===
Loss reward (iter 1126): 6.1111531257629395
=== Iterazione IRL 1127 ===
Loss reward (iter 1127): 7.393590927124023
=== Iterazione IRL 1128 ===
Loss reward (iter 1128): 6.573128700256348
=== Iterazione IRL 1129 ===
Loss reward (iter 1129): 6.560919761657715
=== Iterazione IRL 1130 ===
Loss reward (iter 1130): 6.518263339996338
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.43e+03 |
|    critic_loss     | 123       |
|    ent_coef        | 2.73      |
|    ent_coef_loss   | -0.0434   |
|    learning_rate   | 0.0003    |
|    n_updates       | 429699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.44e+03 |
|    critic_loss     | 132       |
|    ent_coef        | 2.74      |
|    ent_coef_loss   | -0.152    |
|    learning_rate   | 0.0003    |
|    n_updates       | 430099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.42e+03 |
|    critic_loss     | 245       |
|    ent_coef        | 2.69      |
|    ent_coef_loss   | -0.165    |
|    learning_rate   | 0.0003    |
|    n_updates       | 430499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.44e+03 |
|    critic_loss     | 192       |
|    ent_coef        | 2.7       |
|    ent_coef_loss   | -0.0689   |
|    learning_rate   | 0.0003    |
|    n_updates       | 430899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 123       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.45e+03 |
|    critic_loss     | 125       |
|    ent_coef        | 2.73      |
|    ent_coef_loss   | -0.106    |
|    learning_rate   | 0.0003    |
|    n_updates       | 431299    |
----------------------------------
=== Iterazione IRL 1131 ===
Loss reward (iter 1131): 6.247618198394775
=== Iterazione IRL 1132 ===
Loss reward (iter 1132): 6.428493499755859
=== Iterazione IRL 1133 ===
Loss reward (iter 1133): 6.157293319702148
=== Iterazione IRL 1134 ===
Loss reward (iter 1134): 6.396051406860352
=== Iterazione IRL 1135 ===
Loss reward (iter 1135): 6.205848217010498
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 153       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.45e+03 |
|    critic_loss     | 158       |
|    ent_coef        | 2.75      |
|    ent_coef_loss   | 0.0201    |
|    learning_rate   | 0.0003    |
|    n_updates       | 431599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.45e+03 |
|    critic_loss     | 111       |
|    ent_coef        | 2.73      |
|    ent_coef_loss   | -0.0278   |
|    learning_rate   | 0.0003    |
|    n_updates       | 431999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.46e+03 |
|    critic_loss     | 140       |
|    ent_coef        | 2.71      |
|    ent_coef_loss   | -0.00673  |
|    learning_rate   | 0.0003    |
|    n_updates       | 432399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.41e+03 |
|    critic_loss     | 210       |
|    ent_coef        | 2.71      |
|    ent_coef_loss   | -0.151    |
|    learning_rate   | 0.0003    |
|    n_updates       | 432799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 123       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.44e+03 |
|    critic_loss     | 179       |
|    ent_coef        | 2.71      |
|    ent_coef_loss   | -0.125    |
|    learning_rate   | 0.0003    |
|    n_updates       | 433199    |
----------------------------------
=== Iterazione IRL 1136 ===
Loss reward (iter 1136): 6.533474922180176
=== Iterazione IRL 1137 ===
Loss reward (iter 1137): 6.250283718109131
=== Iterazione IRL 1138 ===
Loss reward (iter 1138): 6.161340713500977
=== Iterazione IRL 1139 ===
Loss reward (iter 1139): 6.387543678283691
=== Iterazione IRL 1140 ===
Loss reward (iter 1140): 6.47226619720459
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 153       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.44e+03 |
|    critic_loss     | 123       |
|    ent_coef        | 2.69      |
|    ent_coef_loss   | 0.0693    |
|    learning_rate   | 0.0003    |
|    n_updates       | 433499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.43e+03 |
|    critic_loss     | 187       |
|    ent_coef        | 2.69      |
|    ent_coef_loss   | -0.123    |
|    learning_rate   | 0.0003    |
|    n_updates       | 433899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.44e+03 |
|    critic_loss     | 170       |
|    ent_coef        | 2.7       |
|    ent_coef_loss   | -0.0662   |
|    learning_rate   | 0.0003    |
|    n_updates       | 434299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.45e+03 |
|    critic_loss     | 113       |
|    ent_coef        | 2.7       |
|    ent_coef_loss   | 0.0968    |
|    learning_rate   | 0.0003    |
|    n_updates       | 434699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 123       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.45e+03 |
|    critic_loss     | 122       |
|    ent_coef        | 2.71      |
|    ent_coef_loss   | -0.118    |
|    learning_rate   | 0.0003    |
|    n_updates       | 435099    |
----------------------------------
=== Iterazione IRL 1141 ===
Loss reward (iter 1141): 6.230522632598877
=== Iterazione IRL 1142 ===
Loss reward (iter 1142): 6.177185535430908
=== Iterazione IRL 1143 ===
Loss reward (iter 1143): 6.2026143074035645
=== Iterazione IRL 1144 ===
Loss reward (iter 1144): 5.055325508117676
=== Iterazione IRL 1145 ===
Loss reward (iter 1145): 6.597735404968262
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 153       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.45e+03 |
|    critic_loss     | 166       |
|    ent_coef        | 2.7       |
|    ent_coef_loss   | -0.155    |
|    learning_rate   | 0.0003    |
|    n_updates       | 435399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.46e+03 |
|    critic_loss     | 139       |
|    ent_coef        | 2.72      |
|    ent_coef_loss   | 0.000677  |
|    learning_rate   | 0.0003    |
|    n_updates       | 435799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.45e+03 |
|    critic_loss     | 148       |
|    ent_coef        | 2.72      |
|    ent_coef_loss   | -0.0458   |
|    learning_rate   | 0.0003    |
|    n_updates       | 436199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.46e+03 |
|    critic_loss     | 103       |
|    ent_coef        | 2.7       |
|    ent_coef_loss   | 0.0196    |
|    learning_rate   | 0.0003    |
|    n_updates       | 436599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 123       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.46e+03 |
|    critic_loss     | 134       |
|    ent_coef        | 2.72      |
|    ent_coef_loss   | 0.0145    |
|    learning_rate   | 0.0003    |
|    n_updates       | 436999    |
----------------------------------
=== Iterazione IRL 1146 ===
Loss reward (iter 1146): 6.946308135986328
=== Iterazione IRL 1147 ===
Loss reward (iter 1147): 6.415715217590332
=== Iterazione IRL 1148 ===
Loss reward (iter 1148): 6.840083122253418
=== Iterazione IRL 1149 ===
Loss reward (iter 1149): 6.421144008636475
=== Iterazione IRL 1150 ===
Loss reward (iter 1150): 6.494853496551514
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 153       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.44e+03 |
|    critic_loss     | 172       |
|    ent_coef        | 2.71      |
|    ent_coef_loss   | -0.058    |
|    learning_rate   | 0.0003    |
|    n_updates       | 437299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.47e+03 |
|    critic_loss     | 88.8      |
|    ent_coef        | 2.72      |
|    ent_coef_loss   | 0.174     |
|    learning_rate   | 0.0003    |
|    n_updates       | 437699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.45e+03 |
|    critic_loss     | 133       |
|    ent_coef        | 2.7       |
|    ent_coef_loss   | -0.0233   |
|    learning_rate   | 0.0003    |
|    n_updates       | 438099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.46e+03 |
|    critic_loss     | 130       |
|    ent_coef        | 2.69      |
|    ent_coef_loss   | -0.0597   |
|    learning_rate   | 0.0003    |
|    n_updates       | 438499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 123       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.46e+03 |
|    critic_loss     | 151       |
|    ent_coef        | 2.68      |
|    ent_coef_loss   | -0.0175   |
|    learning_rate   | 0.0003    |
|    n_updates       | 438899    |
----------------------------------
=== Iterazione IRL 1151 ===
Loss reward (iter 1151): 6.560505390167236
=== Iterazione IRL 1152 ===
Loss reward (iter 1152): 6.544005393981934
=== Iterazione IRL 1153 ===
Loss reward (iter 1153): 5.51708459854126
=== Iterazione IRL 1154 ===
Loss reward (iter 1154): 7.260880947113037
=== Iterazione IRL 1155 ===
Loss reward (iter 1155): 6.973852634429932
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 153       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.46e+03 |
|    critic_loss     | 156       |
|    ent_coef        | 2.7       |
|    ent_coef_loss   | 0.00412   |
|    learning_rate   | 0.0003    |
|    n_updates       | 439199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.46e+03 |
|    critic_loss     | 120       |
|    ent_coef        | 2.71      |
|    ent_coef_loss   | 0.125     |
|    learning_rate   | 0.0003    |
|    n_updates       | 439599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.46e+03 |
|    critic_loss     | 153       |
|    ent_coef        | 2.72      |
|    ent_coef_loss   | -0.0844   |
|    learning_rate   | 0.0003    |
|    n_updates       | 439999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.47e+03 |
|    critic_loss     | 111       |
|    ent_coef        | 2.72      |
|    ent_coef_loss   | -0.00709  |
|    learning_rate   | 0.0003    |
|    n_updates       | 440399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 123       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.45e+03 |
|    critic_loss     | 125       |
|    ent_coef        | 2.73      |
|    ent_coef_loss   | -0.0551   |
|    learning_rate   | 0.0003    |
|    n_updates       | 440799    |
----------------------------------
=== Iterazione IRL 1156 ===
Loss reward (iter 1156): 6.843689441680908
=== Iterazione IRL 1157 ===
Loss reward (iter 1157): 6.632796287536621
=== Iterazione IRL 1158 ===
Loss reward (iter 1158): 6.643355846405029
=== Iterazione IRL 1159 ===
Loss reward (iter 1159): 6.7298359870910645
=== Iterazione IRL 1160 ===
Loss reward (iter 1160): 6.70665168762207
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 153       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.48e+03 |
|    critic_loss     | 162       |
|    ent_coef        | 2.75      |
|    ent_coef_loss   | 0.0377    |
|    learning_rate   | 0.0003    |
|    n_updates       | 441099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.46e+03 |
|    critic_loss     | 190       |
|    ent_coef        | 2.73      |
|    ent_coef_loss   | 0.0623    |
|    learning_rate   | 0.0003    |
|    n_updates       | 441499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.47e+03 |
|    critic_loss     | 114       |
|    ent_coef        | 2.72      |
|    ent_coef_loss   | -0.0615   |
|    learning_rate   | 0.0003    |
|    n_updates       | 441899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.46e+03 |
|    critic_loss     | 163       |
|    ent_coef        | 2.71      |
|    ent_coef_loss   | -0.216    |
|    learning_rate   | 0.0003    |
|    n_updates       | 442299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 123       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.47e+03 |
|    critic_loss     | 103       |
|    ent_coef        | 2.72      |
|    ent_coef_loss   | 0.0111    |
|    learning_rate   | 0.0003    |
|    n_updates       | 442699    |
----------------------------------
=== Iterazione IRL 1161 ===
Loss reward (iter 1161): 6.69757604598999
=== Iterazione IRL 1162 ===
Loss reward (iter 1162): 6.826480865478516
=== Iterazione IRL 1163 ===
Loss reward (iter 1163): 6.899206638336182
=== Iterazione IRL 1164 ===
Loss reward (iter 1164): 6.6373291015625
=== Iterazione IRL 1165 ===
Loss reward (iter 1165): 6.599136829376221
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 153       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.46e+03 |
|    critic_loss     | 119       |
|    ent_coef        | 2.69      |
|    ent_coef_loss   | -0.0832   |
|    learning_rate   | 0.0003    |
|    n_updates       | 442999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.47e+03 |
|    critic_loss     | 128       |
|    ent_coef        | 2.7       |
|    ent_coef_loss   | 0.0554    |
|    learning_rate   | 0.0003    |
|    n_updates       | 443399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.46e+03 |
|    critic_loss     | 131       |
|    ent_coef        | 2.71      |
|    ent_coef_loss   | -0.0936   |
|    learning_rate   | 0.0003    |
|    n_updates       | 443799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.46e+03 |
|    critic_loss     | 126       |
|    ent_coef        | 2.7       |
|    ent_coef_loss   | 0.0395    |
|    learning_rate   | 0.0003    |
|    n_updates       | 444199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 123       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.47e+03 |
|    critic_loss     | 119       |
|    ent_coef        | 2.68      |
|    ent_coef_loss   | 0.0697    |
|    learning_rate   | 0.0003    |
|    n_updates       | 444599    |
----------------------------------
=== Iterazione IRL 1166 ===
Loss reward (iter 1166): 6.569809913635254
=== Iterazione IRL 1167 ===
Loss reward (iter 1167): 6.6662983894348145
=== Iterazione IRL 1168 ===
Loss reward (iter 1168): 6.736052513122559
=== Iterazione IRL 1169 ===
Loss reward (iter 1169): 6.491770267486572
=== Iterazione IRL 1170 ===
Loss reward (iter 1170): 6.917629241943359
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 153       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.46e+03 |
|    critic_loss     | 259       |
|    ent_coef        | 2.69      |
|    ent_coef_loss   | -0.107    |
|    learning_rate   | 0.0003    |
|    n_updates       | 444899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.47e+03 |
|    critic_loss     | 161       |
|    ent_coef        | 2.69      |
|    ent_coef_loss   | 0.055     |
|    learning_rate   | 0.0003    |
|    n_updates       | 445299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.47e+03 |
|    critic_loss     | 140       |
|    ent_coef        | 2.69      |
|    ent_coef_loss   | 0.0646    |
|    learning_rate   | 0.0003    |
|    n_updates       | 445699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.46e+03 |
|    critic_loss     | 151       |
|    ent_coef        | 2.67      |
|    ent_coef_loss   | 0.0117    |
|    learning_rate   | 0.0003    |
|    n_updates       | 446099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.47e+03 |
|    critic_loss     | 106       |
|    ent_coef        | 2.68      |
|    ent_coef_loss   | 0.0164    |
|    learning_rate   | 0.0003    |
|    n_updates       | 446499    |
----------------------------------
=== Iterazione IRL 1171 ===
Loss reward (iter 1171): 6.834805011749268
=== Iterazione IRL 1172 ===
Loss reward (iter 1172): 6.46458101272583
=== Iterazione IRL 1173 ===
Loss reward (iter 1173): 6.494256019592285
=== Iterazione IRL 1174 ===
Loss reward (iter 1174): 6.498018741607666
=== Iterazione IRL 1175 ===
Loss reward (iter 1175): 6.4501261711120605
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.46e+03 |
|    critic_loss     | 135       |
|    ent_coef        | 2.69      |
|    ent_coef_loss   | 0.0951    |
|    learning_rate   | 0.0003    |
|    n_updates       | 446799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.47e+03 |
|    critic_loss     | 137       |
|    ent_coef        | 2.69      |
|    ent_coef_loss   | 0.0575    |
|    learning_rate   | 0.0003    |
|    n_updates       | 447199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.47e+03 |
|    critic_loss     | 138       |
|    ent_coef        | 2.69      |
|    ent_coef_loss   | -0.0835   |
|    learning_rate   | 0.0003    |
|    n_updates       | 447599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 123       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.46e+03 |
|    critic_loss     | 132       |
|    ent_coef        | 2.71      |
|    ent_coef_loss   | 0.141     |
|    learning_rate   | 0.0003    |
|    n_updates       | 447999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.48e+03 |
|    critic_loss     | 169       |
|    ent_coef        | 2.72      |
|    ent_coef_loss   | 0.0934    |
|    learning_rate   | 0.0003    |
|    n_updates       | 448399    |
----------------------------------
=== Iterazione IRL 1176 ===
Loss reward (iter 1176): 6.434823513031006
=== Iterazione IRL 1177 ===
Loss reward (iter 1177): 6.608074188232422
=== Iterazione IRL 1178 ===
Loss reward (iter 1178): 6.496988773345947
=== Iterazione IRL 1179 ===
Loss reward (iter 1179): 6.439218997955322
=== Iterazione IRL 1180 ===
Loss reward (iter 1180): 6.529159069061279
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.47e+03 |
|    critic_loss     | 118       |
|    ent_coef        | 2.71      |
|    ent_coef_loss   | 0.0166    |
|    learning_rate   | 0.0003    |
|    n_updates       | 448699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.47e+03 |
|    critic_loss     | 130       |
|    ent_coef        | 2.7       |
|    ent_coef_loss   | -0.124    |
|    learning_rate   | 0.0003    |
|    n_updates       | 449099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.46e+03 |
|    critic_loss     | 213       |
|    ent_coef        | 2.69      |
|    ent_coef_loss   | -0.0776   |
|    learning_rate   | 0.0003    |
|    n_updates       | 449499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 123       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.48e+03 |
|    critic_loss     | 115       |
|    ent_coef        | 2.7       |
|    ent_coef_loss   | 0.0611    |
|    learning_rate   | 0.0003    |
|    n_updates       | 449899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.48e+03 |
|    critic_loss     | 121       |
|    ent_coef        | 2.73      |
|    ent_coef_loss   | 0.00223   |
|    learning_rate   | 0.0003    |
|    n_updates       | 450299    |
----------------------------------
=== Iterazione IRL 1181 ===
Loss reward (iter 1181): 6.471429824829102
=== Iterazione IRL 1182 ===
Loss reward (iter 1182): 6.590946674346924
=== Iterazione IRL 1183 ===
Loss reward (iter 1183): 6.356064319610596
=== Iterazione IRL 1184 ===
Loss reward (iter 1184): 6.628668785095215
=== Iterazione IRL 1185 ===
Loss reward (iter 1185): 5.129133701324463
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 153       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.48e+03 |
|    critic_loss     | 216       |
|    ent_coef        | 2.72      |
|    ent_coef_loss   | -0.0556   |
|    learning_rate   | 0.0003    |
|    n_updates       | 450599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.46e+03 |
|    critic_loss     | 216       |
|    ent_coef        | 2.69      |
|    ent_coef_loss   | -0.189    |
|    learning_rate   | 0.0003    |
|    n_updates       | 450999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.48e+03 |
|    critic_loss     | 116       |
|    ent_coef        | 2.72      |
|    ent_coef_loss   | 0.181     |
|    learning_rate   | 0.0003    |
|    n_updates       | 451399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.46e+03 |
|    critic_loss     | 141       |
|    ent_coef        | 2.7       |
|    ent_coef_loss   | -0.0487   |
|    learning_rate   | 0.0003    |
|    n_updates       | 451799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 123       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.48e+03 |
|    critic_loss     | 157       |
|    ent_coef        | 2.71      |
|    ent_coef_loss   | -0.0622   |
|    learning_rate   | 0.0003    |
|    n_updates       | 452199    |
----------------------------------
=== Iterazione IRL 1186 ===
Loss reward (iter 1186): 6.742099761962891
=== Iterazione IRL 1187 ===
Loss reward (iter 1187): 6.761828899383545
=== Iterazione IRL 1188 ===
Loss reward (iter 1188): 7.317612171173096
=== Iterazione IRL 1189 ===
Loss reward (iter 1189): 6.736372470855713
=== Iterazione IRL 1190 ===
Loss reward (iter 1190): 6.762930870056152
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.48e+03 |
|    critic_loss     | 157       |
|    ent_coef        | 2.7       |
|    ent_coef_loss   | -0.0454   |
|    learning_rate   | 0.0003    |
|    n_updates       | 452499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.46e+03 |
|    critic_loss     | 110       |
|    ent_coef        | 2.72      |
|    ent_coef_loss   | -0.0338   |
|    learning_rate   | 0.0003    |
|    n_updates       | 452899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.48e+03 |
|    critic_loss     | 175       |
|    ent_coef        | 2.71      |
|    ent_coef_loss   | 0.222     |
|    learning_rate   | 0.0003    |
|    n_updates       | 453299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 123       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.47e+03 |
|    critic_loss     | 103       |
|    ent_coef        | 2.7       |
|    ent_coef_loss   | 0.0184    |
|    learning_rate   | 0.0003    |
|    n_updates       | 453699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.48e+03 |
|    critic_loss     | 114       |
|    ent_coef        | 2.71      |
|    ent_coef_loss   | -0.0333   |
|    learning_rate   | 0.0003    |
|    n_updates       | 454099    |
----------------------------------
=== Iterazione IRL 1191 ===
Loss reward (iter 1191): 6.7763352394104
=== Iterazione IRL 1192 ===
Loss reward (iter 1192): 6.62036657333374
=== Iterazione IRL 1193 ===
Loss reward (iter 1193): 6.4008941650390625
=== Iterazione IRL 1194 ===
Loss reward (iter 1194): 6.598353385925293
=== Iterazione IRL 1195 ===
Loss reward (iter 1195): 6.697936534881592
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.49e+03 |
|    critic_loss     | 154       |
|    ent_coef        | 2.7       |
|    ent_coef_loss   | -0.0337   |
|    learning_rate   | 0.0003    |
|    n_updates       | 454399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.48e+03 |
|    critic_loss     | 148       |
|    ent_coef        | 2.73      |
|    ent_coef_loss   | 0.0345    |
|    learning_rate   | 0.0003    |
|    n_updates       | 454799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.48e+03 |
|    critic_loss     | 153       |
|    ent_coef        | 2.73      |
|    ent_coef_loss   | 0.109     |
|    learning_rate   | 0.0003    |
|    n_updates       | 455199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 123       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.49e+03 |
|    critic_loss     | 152       |
|    ent_coef        | 2.71      |
|    ent_coef_loss   | 0.0233    |
|    learning_rate   | 0.0003    |
|    n_updates       | 455599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.48e+03 |
|    critic_loss     | 166       |
|    ent_coef        | 2.72      |
|    ent_coef_loss   | -0.0643   |
|    learning_rate   | 0.0003    |
|    n_updates       | 455999    |
----------------------------------
=== Iterazione IRL 1196 ===
Loss reward (iter 1196): 6.183234691619873
=== Iterazione IRL 1197 ===
Loss reward (iter 1197): 6.148004055023193
=== Iterazione IRL 1198 ===
Loss reward (iter 1198): 6.712498664855957
=== Iterazione IRL 1199 ===
Loss reward (iter 1199): 6.287500858306885
=== Iterazione IRL 1200 ===
Loss reward (iter 1200): 6.348512172698975
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.49e+03 |
|    critic_loss     | 151       |
|    ent_coef        | 2.72      |
|    ent_coef_loss   | 0.00708   |
|    learning_rate   | 0.0003    |
|    n_updates       | 456299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.48e+03 |
|    critic_loss     | 163       |
|    ent_coef        | 2.73      |
|    ent_coef_loss   | -0.0958   |
|    learning_rate   | 0.0003    |
|    n_updates       | 456699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.47e+03 |
|    critic_loss     | 134       |
|    ent_coef        | 2.73      |
|    ent_coef_loss   | -0.0801   |
|    learning_rate   | 0.0003    |
|    n_updates       | 457099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 123       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.49e+03 |
|    critic_loss     | 120       |
|    ent_coef        | 2.71      |
|    ent_coef_loss   | 0.0991    |
|    learning_rate   | 0.0003    |
|    n_updates       | 457499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.49e+03 |
|    critic_loss     | 95        |
|    ent_coef        | 2.7       |
|    ent_coef_loss   | 0.00444   |
|    learning_rate   | 0.0003    |
|    n_updates       | 457899    |
----------------------------------
=== Iterazione IRL 1201 ===
Loss reward (iter 1201): 6.515705585479736
=== Iterazione IRL 1202 ===
Loss reward (iter 1202): 6.253172874450684
=== Iterazione IRL 1203 ===
Loss reward (iter 1203): 6.383269309997559
=== Iterazione IRL 1204 ===
Loss reward (iter 1204): 6.209483623504639
=== Iterazione IRL 1205 ===
Loss reward (iter 1205): 6.341156005859375
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 153       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.47e+03 |
|    critic_loss     | 185       |
|    ent_coef        | 2.68      |
|    ent_coef_loss   | -0.0689   |
|    learning_rate   | 0.0003    |
|    n_updates       | 458199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.48e+03 |
|    critic_loss     | 136       |
|    ent_coef        | 2.67      |
|    ent_coef_loss   | -0.0142   |
|    learning_rate   | 0.0003    |
|    n_updates       | 458599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.49e+03 |
|    critic_loss     | 131       |
|    ent_coef        | 2.69      |
|    ent_coef_loss   | 0.0648    |
|    learning_rate   | 0.0003    |
|    n_updates       | 458999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.48e+03 |
|    critic_loss     | 110       |
|    ent_coef        | 2.69      |
|    ent_coef_loss   | 0.0477    |
|    learning_rate   | 0.0003    |
|    n_updates       | 459399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 123       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.49e+03 |
|    critic_loss     | 110       |
|    ent_coef        | 2.7       |
|    ent_coef_loss   | -0.0702   |
|    learning_rate   | 0.0003    |
|    n_updates       | 459799    |
----------------------------------
=== Iterazione IRL 1206 ===
Loss reward (iter 1206): 6.455110549926758
=== Iterazione IRL 1207 ===
Loss reward (iter 1207): 6.29379940032959
=== Iterazione IRL 1208 ===
Loss reward (iter 1208): 6.527220249176025
=== Iterazione IRL 1209 ===
Loss reward (iter 1209): 6.092019081115723
=== Iterazione IRL 1210 ===
Loss reward (iter 1210): 6.488032341003418
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 153       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.48e+03 |
|    critic_loss     | 144       |
|    ent_coef        | 2.68      |
|    ent_coef_loss   | -0.199    |
|    learning_rate   | 0.0003    |
|    n_updates       | 460099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.49e+03 |
|    critic_loss     | 130       |
|    ent_coef        | 2.7       |
|    ent_coef_loss   | -0.00214  |
|    learning_rate   | 0.0003    |
|    n_updates       | 460499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.49e+03 |
|    critic_loss     | 110       |
|    ent_coef        | 2.69      |
|    ent_coef_loss   | -0.0177   |
|    learning_rate   | 0.0003    |
|    n_updates       | 460899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.49e+03 |
|    critic_loss     | 170       |
|    ent_coef        | 2.7       |
|    ent_coef_loss   | -0.00884  |
|    learning_rate   | 0.0003    |
|    n_updates       | 461299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 123       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.49e+03 |
|    critic_loss     | 116       |
|    ent_coef        | 2.69      |
|    ent_coef_loss   | 0.0678    |
|    learning_rate   | 0.0003    |
|    n_updates       | 461699    |
----------------------------------
=== Iterazione IRL 1211 ===
Loss reward (iter 1211): 6.466480731964111
=== Iterazione IRL 1212 ===
Loss reward (iter 1212): 6.5800323486328125
=== Iterazione IRL 1213 ===
Loss reward (iter 1213): 6.642852306365967
=== Iterazione IRL 1214 ===
Loss reward (iter 1214): 6.529432773590088
=== Iterazione IRL 1215 ===
Loss reward (iter 1215): 6.286363124847412
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 153       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.49e+03 |
|    critic_loss     | 203       |
|    ent_coef        | 2.72      |
|    ent_coef_loss   | 0.0772    |
|    learning_rate   | 0.0003    |
|    n_updates       | 461999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.48e+03 |
|    critic_loss     | 139       |
|    ent_coef        | 2.72      |
|    ent_coef_loss   | -0.0137   |
|    learning_rate   | 0.0003    |
|    n_updates       | 462399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.48e+03 |
|    critic_loss     | 125       |
|    ent_coef        | 2.7       |
|    ent_coef_loss   | -0.07     |
|    learning_rate   | 0.0003    |
|    n_updates       | 462799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.48e+03 |
|    critic_loss     | 110       |
|    ent_coef        | 2.69      |
|    ent_coef_loss   | -0.075    |
|    learning_rate   | 0.0003    |
|    n_updates       | 463199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 123       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.49e+03 |
|    critic_loss     | 132       |
|    ent_coef        | 2.68      |
|    ent_coef_loss   | 0.156     |
|    learning_rate   | 0.0003    |
|    n_updates       | 463599    |
----------------------------------
=== Iterazione IRL 1216 ===
Loss reward (iter 1216): 6.348692417144775
=== Iterazione IRL 1217 ===
Loss reward (iter 1217): 6.910467147827148
=== Iterazione IRL 1218 ===
Loss reward (iter 1218): 6.491867542266846
=== Iterazione IRL 1219 ===
Loss reward (iter 1219): 6.529476165771484
=== Iterazione IRL 1220 ===
Loss reward (iter 1220): 6.521948337554932
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 153       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.49e+03 |
|    critic_loss     | 116       |
|    ent_coef        | 2.68      |
|    ent_coef_loss   | 0.00674   |
|    learning_rate   | 0.0003    |
|    n_updates       | 463899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.49e+03 |
|    critic_loss     | 148       |
|    ent_coef        | 2.69      |
|    ent_coef_loss   | -0.0443   |
|    learning_rate   | 0.0003    |
|    n_updates       | 464299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.47e+03 |
|    critic_loss     | 116       |
|    ent_coef        | 2.7       |
|    ent_coef_loss   | 0.109     |
|    learning_rate   | 0.0003    |
|    n_updates       | 464699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.49e+03 |
|    critic_loss     | 133       |
|    ent_coef        | 2.7       |
|    ent_coef_loss   | -0.0162   |
|    learning_rate   | 0.0003    |
|    n_updates       | 465099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.48e+03 |
|    critic_loss     | 143       |
|    ent_coef        | 2.72      |
|    ent_coef_loss   | -0.143    |
|    learning_rate   | 0.0003    |
|    n_updates       | 465499    |
----------------------------------
=== Iterazione IRL 1221 ===
Loss reward (iter 1221): 6.806705474853516
=== Iterazione IRL 1222 ===
Loss reward (iter 1222): 6.646542549133301
=== Iterazione IRL 1223 ===
Loss reward (iter 1223): 6.730712413787842
=== Iterazione IRL 1224 ===
Loss reward (iter 1224): 6.6768646240234375
=== Iterazione IRL 1225 ===
Loss reward (iter 1225): 6.639357089996338
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.49e+03 |
|    critic_loss     | 130       |
|    ent_coef        | 2.7       |
|    ent_coef_loss   | 0.128     |
|    learning_rate   | 0.0003    |
|    n_updates       | 465799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.47e+03 |
|    critic_loss     | 115       |
|    ent_coef        | 2.69      |
|    ent_coef_loss   | 0.0297    |
|    learning_rate   | 0.0003    |
|    n_updates       | 466199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.48e+03 |
|    critic_loss     | 178       |
|    ent_coef        | 2.68      |
|    ent_coef_loss   | -0.0839   |
|    learning_rate   | 0.0003    |
|    n_updates       | 466599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 123       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.49e+03 |
|    critic_loss     | 117       |
|    ent_coef        | 2.67      |
|    ent_coef_loss   | 0.019     |
|    learning_rate   | 0.0003    |
|    n_updates       | 466999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.48e+03 |
|    critic_loss     | 126       |
|    ent_coef        | 2.72      |
|    ent_coef_loss   | 0.00619   |
|    learning_rate   | 0.0003    |
|    n_updates       | 467399    |
----------------------------------
=== Iterazione IRL 1226 ===
Loss reward (iter 1226): 6.679070472717285
=== Iterazione IRL 1227 ===
Loss reward (iter 1227): 6.568154811859131
=== Iterazione IRL 1228 ===
Loss reward (iter 1228): 6.684284210205078
=== Iterazione IRL 1229 ===
Loss reward (iter 1229): 6.986616134643555
=== Iterazione IRL 1230 ===
Loss reward (iter 1230): 6.805045127868652
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.49e+03 |
|    critic_loss     | 127       |
|    ent_coef        | 2.71      |
|    ent_coef_loss   | 0.0472    |
|    learning_rate   | 0.0003    |
|    n_updates       | 467699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.49e+03 |
|    critic_loss     | 149       |
|    ent_coef        | 2.7       |
|    ent_coef_loss   | 0.0375    |
|    learning_rate   | 0.0003    |
|    n_updates       | 468099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.49e+03 |
|    critic_loss     | 106       |
|    ent_coef        | 2.69      |
|    ent_coef_loss   | 0.172     |
|    learning_rate   | 0.0003    |
|    n_updates       | 468499    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 16       |
|    fps             | 123      |
|    time_elapsed    | 12       |
|    total_timesteps | 1600     |
| train/             |          |
|    actor_loss      | -2.5e+03 |
|    critic_loss     | 157      |
|    ent_coef        | 2.68     |
|    ent_coef_loss   | 0.096    |
|    learning_rate   | 0.0003   |
|    n_updates       | 468899   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.49e+03 |
|    critic_loss     | 145       |
|    ent_coef        | 2.66      |
|    ent_coef_loss   | -0.0872   |
|    learning_rate   | 0.0003    |
|    n_updates       | 469299    |
----------------------------------
=== Iterazione IRL 1231 ===
Loss reward (iter 1231): 6.410848617553711
=== Iterazione IRL 1232 ===
Loss reward (iter 1232): 6.583566665649414
=== Iterazione IRL 1233 ===
Loss reward (iter 1233): 6.470674514770508
=== Iterazione IRL 1234 ===
Loss reward (iter 1234): 6.213124752044678
=== Iterazione IRL 1235 ===
Loss reward (iter 1235): 6.944138050079346
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.48e+03 |
|    critic_loss     | 161       |
|    ent_coef        | 2.67      |
|    ent_coef_loss   | -0.0742   |
|    learning_rate   | 0.0003    |
|    n_updates       | 469599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.49e+03 |
|    critic_loss     | 183       |
|    ent_coef        | 2.69      |
|    ent_coef_loss   | -0.122    |
|    learning_rate   | 0.0003    |
|    n_updates       | 469999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.49e+03 |
|    critic_loss     | 121       |
|    ent_coef        | 2.69      |
|    ent_coef_loss   | 0.0182    |
|    learning_rate   | 0.0003    |
|    n_updates       | 470399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 123       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.49e+03 |
|    critic_loss     | 106       |
|    ent_coef        | 2.69      |
|    ent_coef_loss   | 0.109     |
|    learning_rate   | 0.0003    |
|    n_updates       | 470799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.49e+03 |
|    critic_loss     | 158       |
|    ent_coef        | 2.7       |
|    ent_coef_loss   | -0.0205   |
|    learning_rate   | 0.0003    |
|    n_updates       | 471199    |
----------------------------------
=== Iterazione IRL 1236 ===
Loss reward (iter 1236): 6.821722507476807
=== Iterazione IRL 1237 ===
Loss reward (iter 1237): 6.6132307052612305
=== Iterazione IRL 1238 ===
Loss reward (iter 1238): 5.915392875671387
=== Iterazione IRL 1239 ===
Loss reward (iter 1239): 6.6903395652771
=== Iterazione IRL 1240 ===
Loss reward (iter 1240): 6.097060203552246
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.49e+03 |
|    critic_loss     | 122       |
|    ent_coef        | 2.7       |
|    ent_coef_loss   | 0.0224    |
|    learning_rate   | 0.0003    |
|    n_updates       | 471499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.49e+03 |
|    critic_loss     | 156       |
|    ent_coef        | 2.68      |
|    ent_coef_loss   | -0.0278   |
|    learning_rate   | 0.0003    |
|    n_updates       | 471899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.49e+03 |
|    critic_loss     | 164       |
|    ent_coef        | 2.68      |
|    ent_coef_loss   | -0.0328   |
|    learning_rate   | 0.0003    |
|    n_updates       | 472299    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 16       |
|    fps             | 123      |
|    time_elapsed    | 12       |
|    total_timesteps | 1600     |
| train/             |          |
|    actor_loss      | -2.5e+03 |
|    critic_loss     | 130      |
|    ent_coef        | 2.66     |
|    ent_coef_loss   | 0.0221   |
|    learning_rate   | 0.0003   |
|    n_updates       | 472699   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 20       |
|    fps             | 122      |
|    time_elapsed    | 16       |
|    total_timesteps | 2000     |
| train/             |          |
|    actor_loss      | -2.5e+03 |
|    critic_loss     | 115      |
|    ent_coef        | 2.71     |
|    ent_coef_loss   | 0.233    |
|    learning_rate   | 0.0003   |
|    n_updates       | 473099   |
---------------------------------
=== Iterazione IRL 1241 ===
Loss reward (iter 1241): 7.099358558654785
=== Iterazione IRL 1242 ===
Loss reward (iter 1242): 7.349671840667725
=== Iterazione IRL 1243 ===
Loss reward (iter 1243): 6.332613945007324
=== Iterazione IRL 1244 ===
Loss reward (iter 1244): 6.877662658691406
=== Iterazione IRL 1245 ===
Loss reward (iter 1245): 6.496204376220703
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.5e+03 |
|    critic_loss     | 121      |
|    ent_coef        | 2.7      |
|    ent_coef_loss   | 0.0247   |
|    learning_rate   | 0.0003   |
|    n_updates       | 473399   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 110       |
|    ent_coef        | 2.68      |
|    ent_coef_loss   | 0.129     |
|    learning_rate   | 0.0003    |
|    n_updates       | 473799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.48e+03 |
|    critic_loss     | 192       |
|    ent_coef        | 2.69      |
|    ent_coef_loss   | -0.0587   |
|    learning_rate   | 0.0003    |
|    n_updates       | 474199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 123       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.49e+03 |
|    critic_loss     | 133       |
|    ent_coef        | 2.69      |
|    ent_coef_loss   | 0.0441    |
|    learning_rate   | 0.0003    |
|    n_updates       | 474599    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 20       |
|    fps             | 122      |
|    time_elapsed    | 16       |
|    total_timesteps | 2000     |
| train/             |          |
|    actor_loss      | -2.5e+03 |
|    critic_loss     | 98.9     |
|    ent_coef        | 2.69     |
|    ent_coef_loss   | 0.0171   |
|    learning_rate   | 0.0003   |
|    n_updates       | 474999   |
---------------------------------
=== Iterazione IRL 1246 ===
Loss reward (iter 1246): 6.715100288391113
=== Iterazione IRL 1247 ===
Loss reward (iter 1247): 6.486697673797607
=== Iterazione IRL 1248 ===
Loss reward (iter 1248): 6.525161266326904
=== Iterazione IRL 1249 ===
Loss reward (iter 1249): 6.765330791473389
=== Iterazione IRL 1250 ===
Loss reward (iter 1250): 6.7441630363464355
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.49e+03 |
|    critic_loss     | 165       |
|    ent_coef        | 2.68      |
|    ent_coef_loss   | 0.00581   |
|    learning_rate   | 0.0003    |
|    n_updates       | 475299    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.5e+03 |
|    critic_loss     | 112      |
|    ent_coef        | 2.69     |
|    ent_coef_loss   | -0.178   |
|    learning_rate   | 0.0003   |
|    n_updates       | 475699   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.49e+03 |
|    critic_loss     | 125       |
|    ent_coef        | 2.72      |
|    ent_coef_loss   | -0.0953   |
|    learning_rate   | 0.0003    |
|    n_updates       | 476099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 123       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.48e+03 |
|    critic_loss     | 124       |
|    ent_coef        | 2.72      |
|    ent_coef_loss   | 0.0898    |
|    learning_rate   | 0.0003    |
|    n_updates       | 476499    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 20       |
|    fps             | 122      |
|    time_elapsed    | 16       |
|    total_timesteps | 2000     |
| train/             |          |
|    actor_loss      | -2.5e+03 |
|    critic_loss     | 90.1     |
|    ent_coef        | 2.73     |
|    ent_coef_loss   | -0.0662  |
|    learning_rate   | 0.0003   |
|    n_updates       | 476899   |
---------------------------------
=== Iterazione IRL 1251 ===
Loss reward (iter 1251): 6.640676498413086
=== Iterazione IRL 1252 ===
Loss reward (iter 1252): 6.561520099639893
=== Iterazione IRL 1253 ===
Loss reward (iter 1253): 6.617438316345215
=== Iterazione IRL 1254 ===
Loss reward (iter 1254): 6.629425048828125
=== Iterazione IRL 1255 ===
Loss reward (iter 1255): 6.306197166442871
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.49e+03 |
|    critic_loss     | 111       |
|    ent_coef        | 2.74      |
|    ent_coef_loss   | -0.211    |
|    learning_rate   | 0.0003    |
|    n_updates       | 477199    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.5e+03 |
|    critic_loss     | 119      |
|    ent_coef        | 2.76     |
|    ent_coef_loss   | 0.0822   |
|    learning_rate   | 0.0003   |
|    n_updates       | 477599   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.49e+03 |
|    critic_loss     | 146       |
|    ent_coef        | 2.73      |
|    ent_coef_loss   | 0.0331    |
|    learning_rate   | 0.0003    |
|    n_updates       | 477999    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 16       |
|    fps             | 123      |
|    time_elapsed    | 12       |
|    total_timesteps | 1600     |
| train/             |          |
|    actor_loss      | -2.5e+03 |
|    critic_loss     | 81.2     |
|    ent_coef        | 2.72     |
|    ent_coef_loss   | 0.087    |
|    learning_rate   | 0.0003   |
|    n_updates       | 478399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 20       |
|    fps             | 122      |
|    time_elapsed    | 16       |
|    total_timesteps | 2000     |
| train/             |          |
|    actor_loss      | -2.5e+03 |
|    critic_loss     | 157      |
|    ent_coef        | 2.71     |
|    ent_coef_loss   | -0.0236  |
|    learning_rate   | 0.0003   |
|    n_updates       | 478799   |
---------------------------------
=== Iterazione IRL 1256 ===
Loss reward (iter 1256): 6.658533573150635
=== Iterazione IRL 1257 ===
Loss reward (iter 1257): 6.296151161193848
=== Iterazione IRL 1258 ===
Loss reward (iter 1258): 6.64046049118042
=== Iterazione IRL 1259 ===
Loss reward (iter 1259): 6.39829158782959
=== Iterazione IRL 1260 ===
Loss reward (iter 1260): 6.247283935546875
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.49e+03 |
|    critic_loss     | 136       |
|    ent_coef        | 2.71      |
|    ent_coef_loss   | 0.0379    |
|    learning_rate   | 0.0003    |
|    n_updates       | 479099    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.5e+03 |
|    critic_loss     | 144      |
|    ent_coef        | 2.71     |
|    ent_coef_loss   | -0.179   |
|    learning_rate   | 0.0003   |
|    n_updates       | 479499   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 107       |
|    ent_coef        | 2.69      |
|    ent_coef_loss   | 0.0426    |
|    learning_rate   | 0.0003    |
|    n_updates       | 479899    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 16       |
|    fps             | 124      |
|    time_elapsed    | 12       |
|    total_timesteps | 1600     |
| train/             |          |
|    actor_loss      | -2.5e+03 |
|    critic_loss     | 147      |
|    ent_coef        | 2.72     |
|    ent_coef_loss   | -0.0749  |
|    learning_rate   | 0.0003   |
|    n_updates       | 480299   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 20       |
|    fps             | 122      |
|    time_elapsed    | 16       |
|    total_timesteps | 2000     |
| train/             |          |
|    actor_loss      | -2.5e+03 |
|    critic_loss     | 98.7     |
|    ent_coef        | 2.7      |
|    ent_coef_loss   | -0.0495  |
|    learning_rate   | 0.0003   |
|    n_updates       | 480699   |
---------------------------------
=== Iterazione IRL 1261 ===
Loss reward (iter 1261): 6.435064315795898
=== Iterazione IRL 1262 ===
Loss reward (iter 1262): 6.742632865905762
=== Iterazione IRL 1263 ===
Loss reward (iter 1263): 6.944156169891357
=== Iterazione IRL 1264 ===
Loss reward (iter 1264): 6.76113748550415
=== Iterazione IRL 1265 ===
Loss reward (iter 1265): 6.516766548156738
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 152      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.5e+03 |
|    critic_loss     | 146      |
|    ent_coef        | 2.73     |
|    ent_coef_loss   | -0.0548  |
|    learning_rate   | 0.0003   |
|    n_updates       | 480999   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 129      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.5e+03 |
|    critic_loss     | 123      |
|    ent_coef        | 2.73     |
|    ent_coef_loss   | -0.0627  |
|    learning_rate   | 0.0003   |
|    n_updates       | 481399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 120      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -2.5e+03 |
|    critic_loss     | 147      |
|    ent_coef        | 2.74     |
|    ent_coef_loss   | 0.0796   |
|    learning_rate   | 0.0003   |
|    n_updates       | 481799   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 117       |
|    time_elapsed    | 13        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 125       |
|    ent_coef        | 2.72      |
|    ent_coef_loss   | 0.113     |
|    learning_rate   | 0.0003    |
|    n_updates       | 482199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 114       |
|    time_elapsed    | 17        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 150       |
|    ent_coef        | 2.69      |
|    ent_coef_loss   | 0.0638    |
|    learning_rate   | 0.0003    |
|    n_updates       | 482599    |
----------------------------------
=== Iterazione IRL 1266 ===
Loss reward (iter 1266): 6.640865802764893
=== Iterazione IRL 1267 ===
Loss reward (iter 1267): 6.611879825592041
=== Iterazione IRL 1268 ===
Loss reward (iter 1268): 6.507760524749756
=== Iterazione IRL 1269 ===
Loss reward (iter 1269): 6.572267532348633
=== Iterazione IRL 1270 ===
Loss reward (iter 1270): 6.281800746917725
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 139      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.5e+03 |
|    critic_loss     | 175      |
|    ent_coef        | 2.69     |
|    ent_coef_loss   | 0.053    |
|    learning_rate   | 0.0003   |
|    n_updates       | 482899   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 121       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 106       |
|    ent_coef        | 2.7       |
|    ent_coef_loss   | -0.0709   |
|    learning_rate   | 0.0003    |
|    n_updates       | 483299    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 116      |
|    time_elapsed    | 10       |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -2.5e+03 |
|    critic_loss     | 104      |
|    ent_coef        | 2.7      |
|    ent_coef_loss   | 0.0202   |
|    learning_rate   | 0.0003   |
|    n_updates       | 483699   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 113       |
|    time_elapsed    | 14        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 148       |
|    ent_coef        | 2.71      |
|    ent_coef_loss   | -0.145    |
|    learning_rate   | 0.0003    |
|    n_updates       | 484099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 112       |
|    time_elapsed    | 17        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 136       |
|    ent_coef        | 2.73      |
|    ent_coef_loss   | 0.0341    |
|    learning_rate   | 0.0003    |
|    n_updates       | 484499    |
----------------------------------
=== Iterazione IRL 1271 ===
Loss reward (iter 1271): 6.732902526855469
=== Iterazione IRL 1272 ===
Loss reward (iter 1272): 6.729396343231201
=== Iterazione IRL 1273 ===
Loss reward (iter 1273): 6.693946361541748
=== Iterazione IRL 1274 ===
Loss reward (iter 1274): 6.812354564666748
=== Iterazione IRL 1275 ===
Loss reward (iter 1275): 6.663941383361816
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 139       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 134       |
|    ent_coef        | 2.71      |
|    ent_coef_loss   | 0.0346    |
|    learning_rate   | 0.0003    |
|    n_updates       | 484799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 121       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 122       |
|    ent_coef        | 2.71      |
|    ent_coef_loss   | 0.0083    |
|    learning_rate   | 0.0003    |
|    n_updates       | 485199    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 116      |
|    time_elapsed    | 10       |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -2.5e+03 |
|    critic_loss     | 131      |
|    ent_coef        | 2.73     |
|    ent_coef_loss   | 0.0462   |
|    learning_rate   | 0.0003   |
|    n_updates       | 485599   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 113       |
|    time_elapsed    | 14        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.48e+03 |
|    critic_loss     | 196       |
|    ent_coef        | 2.71      |
|    ent_coef_loss   | -0.174    |
|    learning_rate   | 0.0003    |
|    n_updates       | 485999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 112       |
|    time_elapsed    | 17        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 86.1      |
|    ent_coef        | 2.71      |
|    ent_coef_loss   | -0.00194  |
|    learning_rate   | 0.0003    |
|    n_updates       | 486399    |
----------------------------------
=== Iterazione IRL 1276 ===
Loss reward (iter 1276): 6.318017482757568
=== Iterazione IRL 1277 ===
Loss reward (iter 1277): 6.284189224243164
=== Iterazione IRL 1278 ===
Loss reward (iter 1278): 6.501327991485596
=== Iterazione IRL 1279 ===
Loss reward (iter 1279): 6.37187385559082
=== Iterazione IRL 1280 ===
Loss reward (iter 1280): 6.561692714691162
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 154       |
|    ent_coef        | 2.71      |
|    ent_coef_loss   | 0.0103    |
|    learning_rate   | 0.0003    |
|    n_updates       | 486699    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.5e+03 |
|    critic_loss     | 186      |
|    ent_coef        | 2.69     |
|    ent_coef_loss   | -0.105   |
|    learning_rate   | 0.0003   |
|    n_updates       | 487099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 126      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -2.5e+03 |
|    critic_loss     | 125      |
|    ent_coef        | 2.71     |
|    ent_coef_loss   | -0.04    |
|    learning_rate   | 0.0003   |
|    n_updates       | 487499   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 123       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 111       |
|    ent_coef        | 2.73      |
|    ent_coef_loss   | 0.0141    |
|    learning_rate   | 0.0003    |
|    n_updates       | 487899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 120       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 111       |
|    ent_coef        | 2.73      |
|    ent_coef_loss   | -0.0237   |
|    learning_rate   | 0.0003    |
|    n_updates       | 488299    |
----------------------------------
=== Iterazione IRL 1281 ===
Loss reward (iter 1281): 6.523677349090576
=== Iterazione IRL 1282 ===
Loss reward (iter 1282): 6.590916633605957
=== Iterazione IRL 1283 ===
Loss reward (iter 1283): 6.655782222747803
=== Iterazione IRL 1284 ===
Loss reward (iter 1284): 6.4320783615112305
=== Iterazione IRL 1285 ===
Loss reward (iter 1285): 6.455142498016357
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 139       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 138       |
|    ent_coef        | 2.74      |
|    ent_coef_loss   | -0.019    |
|    learning_rate   | 0.0003    |
|    n_updates       | 488599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 121       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.49e+03 |
|    critic_loss     | 150       |
|    ent_coef        | 2.73      |
|    ent_coef_loss   | -0.142    |
|    learning_rate   | 0.0003    |
|    n_updates       | 488999    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 115      |
|    time_elapsed    | 10       |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -2.5e+03 |
|    critic_loss     | 230      |
|    ent_coef        | 2.69     |
|    ent_coef_loss   | -0.111   |
|    learning_rate   | 0.0003   |
|    n_updates       | 489399   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 113       |
|    time_elapsed    | 14        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 118       |
|    ent_coef        | 2.71      |
|    ent_coef_loss   | -0.0271   |
|    learning_rate   | 0.0003    |
|    n_updates       | 489799    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 20       |
|    fps             | 112      |
|    time_elapsed    | 17       |
|    total_timesteps | 2000     |
| train/             |          |
|    actor_loss      | -2.5e+03 |
|    critic_loss     | 222      |
|    ent_coef        | 2.69     |
|    ent_coef_loss   | -0.0248  |
|    learning_rate   | 0.0003   |
|    n_updates       | 490199   |
---------------------------------
=== Iterazione IRL 1286 ===
Loss reward (iter 1286): 6.6688055992126465
=== Iterazione IRL 1287 ===
Loss reward (iter 1287): 6.421975612640381
=== Iterazione IRL 1288 ===
Loss reward (iter 1288): 6.4496049880981445
=== Iterazione IRL 1289 ===
Loss reward (iter 1289): 5.996810436248779
=== Iterazione IRL 1290 ===
Loss reward (iter 1290): 6.494070053100586
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 139      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.5e+03 |
|    critic_loss     | 157      |
|    ent_coef        | 2.72     |
|    ent_coef_loss   | 0.0859   |
|    learning_rate   | 0.0003   |
|    n_updates       | 490499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 121      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.5e+03 |
|    critic_loss     | 125      |
|    ent_coef        | 2.73     |
|    ent_coef_loss   | 0.0712   |
|    learning_rate   | 0.0003   |
|    n_updates       | 490899   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 116       |
|    time_elapsed    | 10        |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 108       |
|    ent_coef        | 2.74      |
|    ent_coef_loss   | 0.0734    |
|    learning_rate   | 0.0003    |
|    n_updates       | 491299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 113       |
|    time_elapsed    | 14        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 153       |
|    ent_coef        | 2.71      |
|    ent_coef_loss   | -0.0943   |
|    learning_rate   | 0.0003    |
|    n_updates       | 491699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 112       |
|    time_elapsed    | 17        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 131       |
|    ent_coef        | 2.74      |
|    ent_coef_loss   | 0.18      |
|    learning_rate   | 0.0003    |
|    n_updates       | 492099    |
----------------------------------
=== Iterazione IRL 1291 ===
Loss reward (iter 1291): 6.69234561920166
=== Iterazione IRL 1292 ===
Loss reward (iter 1292): 6.565343856811523
=== Iterazione IRL 1293 ===
Loss reward (iter 1293): 6.567479133605957
=== Iterazione IRL 1294 ===
Loss reward (iter 1294): 6.7646379470825195
=== Iterazione IRL 1295 ===
Loss reward (iter 1295): 6.609373569488525
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 140      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.5e+03 |
|    critic_loss     | 129      |
|    ent_coef        | 2.72     |
|    ent_coef_loss   | -0.0657  |
|    learning_rate   | 0.0003   |
|    n_updates       | 492399   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 122      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.5e+03 |
|    critic_loss     | 94.3     |
|    ent_coef        | 2.71     |
|    ent_coef_loss   | 0.0695   |
|    learning_rate   | 0.0003   |
|    n_updates       | 492799   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 116      |
|    time_elapsed    | 10       |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -2.5e+03 |
|    critic_loss     | 165      |
|    ent_coef        | 2.71     |
|    ent_coef_loss   | 0.0352   |
|    learning_rate   | 0.0003   |
|    n_updates       | 493199   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 114       |
|    time_elapsed    | 13        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.49e+03 |
|    critic_loss     | 138       |
|    ent_coef        | 2.71      |
|    ent_coef_loss   | -0.0649   |
|    learning_rate   | 0.0003    |
|    n_updates       | 493599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 113       |
|    time_elapsed    | 17        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 122       |
|    ent_coef        | 2.73      |
|    ent_coef_loss   | -0.0171   |
|    learning_rate   | 0.0003    |
|    n_updates       | 493999    |
----------------------------------
=== Iterazione IRL 1296 ===
Loss reward (iter 1296): 6.462652206420898
=== Iterazione IRL 1297 ===
Loss reward (iter 1297): 6.802292346954346
=== Iterazione IRL 1298 ===
Loss reward (iter 1298): 6.7898077964782715
=== Iterazione IRL 1299 ===
Loss reward (iter 1299): 6.65273380279541
=== Iterazione IRL 1300 ===
Loss reward (iter 1300): 6.565643310546875
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 140       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 150       |
|    ent_coef        | 2.73      |
|    ent_coef_loss   | -0.0287   |
|    learning_rate   | 0.0003    |
|    n_updates       | 494299    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 122      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.5e+03 |
|    critic_loss     | 142      |
|    ent_coef        | 2.73     |
|    ent_coef_loss   | -0.121   |
|    learning_rate   | 0.0003   |
|    n_updates       | 494699   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 116       |
|    time_elapsed    | 10        |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 94.6      |
|    ent_coef        | 2.72      |
|    ent_coef_loss   | -0.0723   |
|    learning_rate   | 0.0003    |
|    n_updates       | 495099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 114       |
|    time_elapsed    | 13        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 159       |
|    ent_coef        | 2.71      |
|    ent_coef_loss   | -0.0623   |
|    learning_rate   | 0.0003    |
|    n_updates       | 495499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 113       |
|    time_elapsed    | 17        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 173       |
|    ent_coef        | 2.72      |
|    ent_coef_loss   | -0.0489   |
|    learning_rate   | 0.0003    |
|    n_updates       | 495899    |
----------------------------------
=== Iterazione IRL 1301 ===
Loss reward (iter 1301): 6.375419616699219
=== Iterazione IRL 1302 ===
Loss reward (iter 1302): 6.256973743438721
=== Iterazione IRL 1303 ===
Loss reward (iter 1303): 6.7913360595703125
=== Iterazione IRL 1304 ===
Loss reward (iter 1304): 6.53856897354126
=== Iterazione IRL 1305 ===
Loss reward (iter 1305): 6.612517833709717
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 140       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 130       |
|    ent_coef        | 2.71      |
|    ent_coef_loss   | 0.068     |
|    learning_rate   | 0.0003    |
|    n_updates       | 496199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 122       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 117       |
|    ent_coef        | 2.72      |
|    ent_coef_loss   | -0.09     |
|    learning_rate   | 0.0003    |
|    n_updates       | 496599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 116       |
|    time_elapsed    | 10        |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 133       |
|    ent_coef        | 2.72      |
|    ent_coef_loss   | -0.0926   |
|    learning_rate   | 0.0003    |
|    n_updates       | 496999    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 16       |
|    fps             | 114      |
|    time_elapsed    | 13       |
|    total_timesteps | 1600     |
| train/             |          |
|    actor_loss      | -2.5e+03 |
|    critic_loss     | 135      |
|    ent_coef        | 2.73     |
|    ent_coef_loss   | 0.00546  |
|    learning_rate   | 0.0003   |
|    n_updates       | 497399   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 113       |
|    time_elapsed    | 17        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 101       |
|    ent_coef        | 2.73      |
|    ent_coef_loss   | 0.0728    |
|    learning_rate   | 0.0003    |
|    n_updates       | 497799    |
----------------------------------
=== Iterazione IRL 1306 ===
Loss reward (iter 1306): 5.610624313354492
=== Iterazione IRL 1307 ===
Loss reward (iter 1307): 6.900167942047119
=== Iterazione IRL 1308 ===
Loss reward (iter 1308): 6.336185455322266
=== Iterazione IRL 1309 ===
Loss reward (iter 1309): 6.3604960441589355
=== Iterazione IRL 1310 ===
Loss reward (iter 1310): 6.350853443145752
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 140      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.5e+03 |
|    critic_loss     | 115      |
|    ent_coef        | 2.72     |
|    ent_coef_loss   | -0.0102  |
|    learning_rate   | 0.0003   |
|    n_updates       | 498099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 122      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.5e+03 |
|    critic_loss     | 135      |
|    ent_coef        | 2.72     |
|    ent_coef_loss   | 0.0925   |
|    learning_rate   | 0.0003   |
|    n_updates       | 498499   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 116       |
|    time_elapsed    | 10        |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 155       |
|    ent_coef        | 2.71      |
|    ent_coef_loss   | 0.0065    |
|    learning_rate   | 0.0003    |
|    n_updates       | 498899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 114       |
|    time_elapsed    | 13        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 124       |
|    ent_coef        | 2.71      |
|    ent_coef_loss   | 0.0825    |
|    learning_rate   | 0.0003    |
|    n_updates       | 499299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 113       |
|    time_elapsed    | 17        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 109       |
|    ent_coef        | 2.72      |
|    ent_coef_loss   | 0.0482    |
|    learning_rate   | 0.0003    |
|    n_updates       | 499699    |
----------------------------------
=== Iterazione IRL 1311 ===
Loss reward (iter 1311): 6.703570365905762
=== Iterazione IRL 1312 ===
Loss reward (iter 1312): 6.867319107055664
=== Iterazione IRL 1313 ===
Loss reward (iter 1313): 6.5598835945129395
=== Iterazione IRL 1314 ===
Loss reward (iter 1314): 6.440119743347168
=== Iterazione IRL 1315 ===
Loss reward (iter 1315): 6.638459205627441
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 140       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 156       |
|    ent_coef        | 2.72      |
|    ent_coef_loss   | -0.00306  |
|    learning_rate   | 0.0003    |
|    n_updates       | 499999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 122       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.48e+03 |
|    critic_loss     | 187       |
|    ent_coef        | 2.73      |
|    ent_coef_loss   | -0.161    |
|    learning_rate   | 0.0003    |
|    n_updates       | 500399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 116       |
|    time_elapsed    | 10        |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 145       |
|    ent_coef        | 2.73      |
|    ent_coef_loss   | -0.107    |
|    learning_rate   | 0.0003    |
|    n_updates       | 500799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 114       |
|    time_elapsed    | 13        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 122       |
|    ent_coef        | 2.75      |
|    ent_coef_loss   | -0.129    |
|    learning_rate   | 0.0003    |
|    n_updates       | 501199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 113       |
|    time_elapsed    | 17        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 110       |
|    ent_coef        | 2.74      |
|    ent_coef_loss   | 0.218     |
|    learning_rate   | 0.0003    |
|    n_updates       | 501599    |
----------------------------------
=== Iterazione IRL 1316 ===
Loss reward (iter 1316): 6.804667949676514
=== Iterazione IRL 1317 ===
Loss reward (iter 1317): 6.648755073547363
=== Iterazione IRL 1318 ===
Loss reward (iter 1318): 6.584989070892334
=== Iterazione IRL 1319 ===
Loss reward (iter 1319): 6.662092208862305
=== Iterazione IRL 1320 ===
Loss reward (iter 1320): 6.763617515563965
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 140       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 98.7      |
|    ent_coef        | 2.74      |
|    ent_coef_loss   | -0.0165   |
|    learning_rate   | 0.0003    |
|    n_updates       | 501899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 122       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 129       |
|    ent_coef        | 2.74      |
|    ent_coef_loss   | -0.025    |
|    learning_rate   | 0.0003    |
|    n_updates       | 502299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 116       |
|    time_elapsed    | 10        |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 89.6      |
|    ent_coef        | 2.73      |
|    ent_coef_loss   | 0.117     |
|    learning_rate   | 0.0003    |
|    n_updates       | 502699    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 16       |
|    fps             | 114      |
|    time_elapsed    | 13       |
|    total_timesteps | 1600     |
| train/             |          |
|    actor_loss      | -2.5e+03 |
|    critic_loss     | 176      |
|    ent_coef        | 2.72     |
|    ent_coef_loss   | 0.0666   |
|    learning_rate   | 0.0003   |
|    n_updates       | 503099   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 20       |
|    fps             | 113      |
|    time_elapsed    | 17       |
|    total_timesteps | 2000     |
| train/             |          |
|    actor_loss      | -2.5e+03 |
|    critic_loss     | 150      |
|    ent_coef        | 2.73     |
|    ent_coef_loss   | -0.0725  |
|    learning_rate   | 0.0003   |
|    n_updates       | 503499   |
---------------------------------
=== Iterazione IRL 1321 ===
Loss reward (iter 1321): 6.6139445304870605
=== Iterazione IRL 1322 ===
Loss reward (iter 1322): 6.294795513153076
=== Iterazione IRL 1323 ===
Loss reward (iter 1323): 6.412516117095947
=== Iterazione IRL 1324 ===
Loss reward (iter 1324): 6.145354270935059
=== Iterazione IRL 1325 ===
Loss reward (iter 1325): 6.652681350708008
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 140       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 151       |
|    ent_coef        | 2.75      |
|    ent_coef_loss   | -0.0319   |
|    learning_rate   | 0.0003    |
|    n_updates       | 503799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 122       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 101       |
|    ent_coef        | 2.75      |
|    ent_coef_loss   | 0.0137    |
|    learning_rate   | 0.0003    |
|    n_updates       | 504199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 116       |
|    time_elapsed    | 10        |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 114       |
|    ent_coef        | 2.74      |
|    ent_coef_loss   | -0.0184   |
|    learning_rate   | 0.0003    |
|    n_updates       | 504599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 114       |
|    time_elapsed    | 13        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 130       |
|    ent_coef        | 2.73      |
|    ent_coef_loss   | 0.0463    |
|    learning_rate   | 0.0003    |
|    n_updates       | 504999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 113       |
|    time_elapsed    | 17        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 123       |
|    ent_coef        | 2.74      |
|    ent_coef_loss   | -0.0326   |
|    learning_rate   | 0.0003    |
|    n_updates       | 505399    |
----------------------------------
=== Iterazione IRL 1326 ===
Loss reward (iter 1326): 6.617073059082031
=== Iterazione IRL 1327 ===
Loss reward (iter 1327): 6.734496116638184
=== Iterazione IRL 1328 ===
Loss reward (iter 1328): 6.625654697418213
=== Iterazione IRL 1329 ===
Loss reward (iter 1329): 6.323920726776123
=== Iterazione IRL 1330 ===
Loss reward (iter 1330): 6.842288494110107
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 140       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 92.7      |
|    ent_coef        | 2.74      |
|    ent_coef_loss   | -0.0498   |
|    learning_rate   | 0.0003    |
|    n_updates       | 505699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 122       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 115       |
|    ent_coef        | 2.76      |
|    ent_coef_loss   | -0.114    |
|    learning_rate   | 0.0003    |
|    n_updates       | 506099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 116       |
|    time_elapsed    | 10        |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 107       |
|    ent_coef        | 2.76      |
|    ent_coef_loss   | 0.0332    |
|    learning_rate   | 0.0003    |
|    n_updates       | 506499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 114       |
|    time_elapsed    | 13        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 82.6      |
|    ent_coef        | 2.76      |
|    ent_coef_loss   | 0.00713   |
|    learning_rate   | 0.0003    |
|    n_updates       | 506899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 113       |
|    time_elapsed    | 17        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 95.3      |
|    ent_coef        | 2.75      |
|    ent_coef_loss   | 0.0912    |
|    learning_rate   | 0.0003    |
|    n_updates       | 507299    |
----------------------------------
=== Iterazione IRL 1331 ===
Loss reward (iter 1331): 6.873965263366699
=== Iterazione IRL 1332 ===
Loss reward (iter 1332): 5.953283309936523
=== Iterazione IRL 1333 ===
Loss reward (iter 1333): 7.1967573165893555
=== Iterazione IRL 1334 ===
Loss reward (iter 1334): 6.676747798919678
=== Iterazione IRL 1335 ===
Loss reward (iter 1335): 6.56215763092041
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 139       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 151       |
|    ent_coef        | 2.74      |
|    ent_coef_loss   | 0.00694   |
|    learning_rate   | 0.0003    |
|    n_updates       | 507599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 121       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 115       |
|    ent_coef        | 2.72      |
|    ent_coef_loss   | -0.0142   |
|    learning_rate   | 0.0003    |
|    n_updates       | 507999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 115       |
|    time_elapsed    | 10        |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 93.1      |
|    ent_coef        | 2.73      |
|    ent_coef_loss   | 0.0165    |
|    learning_rate   | 0.0003    |
|    n_updates       | 508399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 113       |
|    time_elapsed    | 14        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 114       |
|    ent_coef        | 2.74      |
|    ent_coef_loss   | 0.112     |
|    learning_rate   | 0.0003    |
|    n_updates       | 508799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 112       |
|    time_elapsed    | 17        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 99.8      |
|    ent_coef        | 2.73      |
|    ent_coef_loss   | -0.13     |
|    learning_rate   | 0.0003    |
|    n_updates       | 509199    |
----------------------------------
=== Iterazione IRL 1336 ===
Loss reward (iter 1336): 6.8476457595825195
=== Iterazione IRL 1337 ===
Loss reward (iter 1337): 6.845265865325928
=== Iterazione IRL 1338 ===
Loss reward (iter 1338): 6.844468593597412
=== Iterazione IRL 1339 ===
Loss reward (iter 1339): 6.524299621582031
=== Iterazione IRL 1340 ===
Loss reward (iter 1340): 6.775303840637207
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 139       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 107       |
|    ent_coef        | 2.75      |
|    ent_coef_loss   | 0.136     |
|    learning_rate   | 0.0003    |
|    n_updates       | 509499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 121       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 86.1      |
|    ent_coef        | 2.73      |
|    ent_coef_loss   | 0.11      |
|    learning_rate   | 0.0003    |
|    n_updates       | 509899    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 116      |
|    time_elapsed    | 10       |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -2.5e+03 |
|    critic_loss     | 120      |
|    ent_coef        | 2.74     |
|    ent_coef_loss   | 0.0362   |
|    learning_rate   | 0.0003   |
|    n_updates       | 510299   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 114       |
|    time_elapsed    | 14        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 102       |
|    ent_coef        | 2.73      |
|    ent_coef_loss   | 0.113     |
|    learning_rate   | 0.0003    |
|    n_updates       | 510699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 112       |
|    time_elapsed    | 17        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 102       |
|    ent_coef        | 2.75      |
|    ent_coef_loss   | -0.0369   |
|    learning_rate   | 0.0003    |
|    n_updates       | 511099    |
----------------------------------
=== Iterazione IRL 1341 ===
Loss reward (iter 1341): 7.002739906311035
=== Iterazione IRL 1342 ===
Loss reward (iter 1342): 6.692127704620361
=== Iterazione IRL 1343 ===
Loss reward (iter 1343): 6.893656253814697
=== Iterazione IRL 1344 ===
Loss reward (iter 1344): 6.7551398277282715
=== Iterazione IRL 1345 ===
Loss reward (iter 1345): 6.922370910644531
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 140       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 105       |
|    ent_coef        | 2.74      |
|    ent_coef_loss   | -0.0363   |
|    learning_rate   | 0.0003    |
|    n_updates       | 511399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 122       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 115       |
|    ent_coef        | 2.73      |
|    ent_coef_loss   | -0.0867   |
|    learning_rate   | 0.0003    |
|    n_updates       | 511799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 116       |
|    time_elapsed    | 10        |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 137       |
|    ent_coef        | 2.7       |
|    ent_coef_loss   | -0.0624   |
|    learning_rate   | 0.0003    |
|    n_updates       | 512199    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 16       |
|    fps             | 114      |
|    time_elapsed    | 13       |
|    total_timesteps | 1600     |
| train/             |          |
|    actor_loss      | -2.5e+03 |
|    critic_loss     | 187      |
|    ent_coef        | 2.73     |
|    ent_coef_loss   | 0.0223   |
|    learning_rate   | 0.0003   |
|    n_updates       | 512599   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 113       |
|    time_elapsed    | 17        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 102       |
|    ent_coef        | 2.74      |
|    ent_coef_loss   | -0.0928   |
|    learning_rate   | 0.0003    |
|    n_updates       | 512999    |
----------------------------------
=== Iterazione IRL 1346 ===
Loss reward (iter 1346): 6.35279655456543
=== Iterazione IRL 1347 ===
Loss reward (iter 1347): 6.593846321105957
=== Iterazione IRL 1348 ===
Loss reward (iter 1348): 6.650325775146484
=== Iterazione IRL 1349 ===
Loss reward (iter 1349): 6.563492774963379
=== Iterazione IRL 1350 ===
Loss reward (iter 1350): 6.333531379699707
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 140      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.5e+03 |
|    critic_loss     | 86.3     |
|    ent_coef        | 2.74     |
|    ent_coef_loss   | -0.0492  |
|    learning_rate   | 0.0003   |
|    n_updates       | 513299   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 122       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 196       |
|    ent_coef        | 2.71      |
|    ent_coef_loss   | -0.0656   |
|    learning_rate   | 0.0003    |
|    n_updates       | 513699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 116       |
|    time_elapsed    | 10        |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 131       |
|    ent_coef        | 2.74      |
|    ent_coef_loss   | -0.0882   |
|    learning_rate   | 0.0003    |
|    n_updates       | 514099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 114       |
|    time_elapsed    | 13        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 94.4      |
|    ent_coef        | 2.73      |
|    ent_coef_loss   | 0.0116    |
|    learning_rate   | 0.0003    |
|    n_updates       | 514499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 113       |
|    time_elapsed    | 17        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 90.5      |
|    ent_coef        | 2.74      |
|    ent_coef_loss   | 0.0746    |
|    learning_rate   | 0.0003    |
|    n_updates       | 514899    |
----------------------------------
=== Iterazione IRL 1351 ===
Loss reward (iter 1351): 7.157187461853027
=== Iterazione IRL 1352 ===
Loss reward (iter 1352): 6.820122241973877
=== Iterazione IRL 1353 ===
Loss reward (iter 1353): 6.730129718780518
=== Iterazione IRL 1354 ===
Loss reward (iter 1354): 6.579414367675781
=== Iterazione IRL 1355 ===
Loss reward (iter 1355): 6.3974289894104
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 140       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 115       |
|    ent_coef        | 2.72      |
|    ent_coef_loss   | 0.0363    |
|    learning_rate   | 0.0003    |
|    n_updates       | 515199    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 122      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.5e+03 |
|    critic_loss     | 103      |
|    ent_coef        | 2.75     |
|    ent_coef_loss   | 0.0931   |
|    learning_rate   | 0.0003   |
|    n_updates       | 515599   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 117       |
|    time_elapsed    | 10        |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 125       |
|    ent_coef        | 2.75      |
|    ent_coef_loss   | 0.0495    |
|    learning_rate   | 0.0003    |
|    n_updates       | 515999    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 16       |
|    fps             | 114      |
|    time_elapsed    | 13       |
|    total_timesteps | 1600     |
| train/             |          |
|    actor_loss      | -2.5e+03 |
|    critic_loss     | 149      |
|    ent_coef        | 2.76     |
|    ent_coef_loss   | -0.0876  |
|    learning_rate   | 0.0003   |
|    n_updates       | 516399   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 113       |
|    time_elapsed    | 17        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 133       |
|    ent_coef        | 2.77      |
|    ent_coef_loss   | 0.0513    |
|    learning_rate   | 0.0003    |
|    n_updates       | 516799    |
----------------------------------
=== Iterazione IRL 1356 ===
Loss reward (iter 1356): 6.578868865966797
=== Iterazione IRL 1357 ===
Loss reward (iter 1357): 6.367867469787598
=== Iterazione IRL 1358 ===
Loss reward (iter 1358): 6.31245231628418
=== Iterazione IRL 1359 ===
Loss reward (iter 1359): 6.8610992431640625
=== Iterazione IRL 1360 ===
Loss reward (iter 1360): 6.4549736976623535
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 140       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 111       |
|    ent_coef        | 2.75      |
|    ent_coef_loss   | -0.0151   |
|    learning_rate   | 0.0003    |
|    n_updates       | 517099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 122       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 150       |
|    ent_coef        | 2.76      |
|    ent_coef_loss   | 0.0702    |
|    learning_rate   | 0.0003    |
|    n_updates       | 517499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 117       |
|    time_elapsed    | 10        |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 109       |
|    ent_coef        | 2.75      |
|    ent_coef_loss   | 0.0125    |
|    learning_rate   | 0.0003    |
|    n_updates       | 517899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 114       |
|    time_elapsed    | 13        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 80.7      |
|    ent_coef        | 2.75      |
|    ent_coef_loss   | 0.043     |
|    learning_rate   | 0.0003    |
|    n_updates       | 518299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 112       |
|    time_elapsed    | 17        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 126       |
|    ent_coef        | 2.74      |
|    ent_coef_loss   | -0.0269   |
|    learning_rate   | 0.0003    |
|    n_updates       | 518699    |
----------------------------------
=== Iterazione IRL 1361 ===
Loss reward (iter 1361): 6.402731895446777
=== Iterazione IRL 1362 ===
Loss reward (iter 1362): 6.501398086547852
=== Iterazione IRL 1363 ===
Loss reward (iter 1363): 6.538887023925781
=== Iterazione IRL 1364 ===
Loss reward (iter 1364): 6.560710906982422
=== Iterazione IRL 1365 ===
Loss reward (iter 1365): 6.2949910163879395
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 139       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 143       |
|    ent_coef        | 2.75      |
|    ent_coef_loss   | -0.0692   |
|    learning_rate   | 0.0003    |
|    n_updates       | 518999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 121       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 128       |
|    ent_coef        | 2.74      |
|    ent_coef_loss   | -0.0208   |
|    learning_rate   | 0.0003    |
|    n_updates       | 519399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 116       |
|    time_elapsed    | 10        |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 169       |
|    ent_coef        | 2.75      |
|    ent_coef_loss   | 0.0206    |
|    learning_rate   | 0.0003    |
|    n_updates       | 519799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 113       |
|    time_elapsed    | 14        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 117       |
|    ent_coef        | 2.75      |
|    ent_coef_loss   | 0.105     |
|    learning_rate   | 0.0003    |
|    n_updates       | 520199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 112       |
|    time_elapsed    | 17        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 115       |
|    ent_coef        | 2.75      |
|    ent_coef_loss   | 0.0924    |
|    learning_rate   | 0.0003    |
|    n_updates       | 520599    |
----------------------------------
=== Iterazione IRL 1366 ===
Loss reward (iter 1366): 6.6407670974731445
=== Iterazione IRL 1367 ===
Loss reward (iter 1367): 6.631481170654297
=== Iterazione IRL 1368 ===
Loss reward (iter 1368): 6.580318927764893
=== Iterazione IRL 1369 ===
Loss reward (iter 1369): 6.681507110595703
=== Iterazione IRL 1370 ===
Loss reward (iter 1370): 6.799602031707764
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 139       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 162       |
|    ent_coef        | 2.73      |
|    ent_coef_loss   | -0.0432   |
|    learning_rate   | 0.0003    |
|    n_updates       | 520899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 121       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 108       |
|    ent_coef        | 2.76      |
|    ent_coef_loss   | -0.00669  |
|    learning_rate   | 0.0003    |
|    n_updates       | 521299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 116       |
|    time_elapsed    | 10        |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 136       |
|    ent_coef        | 2.78      |
|    ent_coef_loss   | 0.0149    |
|    learning_rate   | 0.0003    |
|    n_updates       | 521699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 113       |
|    time_elapsed    | 14        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 110       |
|    ent_coef        | 2.77      |
|    ent_coef_loss   | 0.183     |
|    learning_rate   | 0.0003    |
|    n_updates       | 522099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 112       |
|    time_elapsed    | 17        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 116       |
|    ent_coef        | 2.76      |
|    ent_coef_loss   | 0.0953    |
|    learning_rate   | 0.0003    |
|    n_updates       | 522499    |
----------------------------------
=== Iterazione IRL 1371 ===
Loss reward (iter 1371): 4.848845958709717
=== Iterazione IRL 1372 ===
Loss reward (iter 1372): 7.270039081573486
=== Iterazione IRL 1373 ===
Loss reward (iter 1373): 6.7187042236328125
=== Iterazione IRL 1374 ===
Loss reward (iter 1374): 6.570102691650391
=== Iterazione IRL 1375 ===
Loss reward (iter 1375): 7.177678108215332
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 139      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.5e+03 |
|    critic_loss     | 184      |
|    ent_coef        | 2.76     |
|    ent_coef_loss   | -0.00632 |
|    learning_rate   | 0.0003   |
|    n_updates       | 522799   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 121       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 273       |
|    ent_coef        | 2.75      |
|    ent_coef_loss   | 0.0595    |
|    learning_rate   | 0.0003    |
|    n_updates       | 523199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 116       |
|    time_elapsed    | 10        |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 154       |
|    ent_coef        | 2.76      |
|    ent_coef_loss   | -0.0204   |
|    learning_rate   | 0.0003    |
|    n_updates       | 523599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 113       |
|    time_elapsed    | 14        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 170       |
|    ent_coef        | 2.74      |
|    ent_coef_loss   | 0.0154    |
|    learning_rate   | 0.0003    |
|    n_updates       | 523999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 112       |
|    time_elapsed    | 17        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 124       |
|    ent_coef        | 2.74      |
|    ent_coef_loss   | -0.0325   |
|    learning_rate   | 0.0003    |
|    n_updates       | 524399    |
----------------------------------
=== Iterazione IRL 1376 ===
Loss reward (iter 1376): 6.740083694458008
=== Iterazione IRL 1377 ===
Loss reward (iter 1377): 6.712801933288574
=== Iterazione IRL 1378 ===
Loss reward (iter 1378): 6.635148048400879
=== Iterazione IRL 1379 ===
Loss reward (iter 1379): 5.873900890350342
=== Iterazione IRL 1380 ===
Loss reward (iter 1380): 7.11480188369751
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 139      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.5e+03 |
|    critic_loss     | 107      |
|    ent_coef        | 2.74     |
|    ent_coef_loss   | -0.101   |
|    learning_rate   | 0.0003   |
|    n_updates       | 524699   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 121       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 157       |
|    ent_coef        | 2.75      |
|    ent_coef_loss   | -0.12     |
|    learning_rate   | 0.0003    |
|    n_updates       | 525099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 116       |
|    time_elapsed    | 10        |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 109       |
|    ent_coef        | 2.74      |
|    ent_coef_loss   | 0.0307    |
|    learning_rate   | 0.0003    |
|    n_updates       | 525499    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 16       |
|    fps             | 113      |
|    time_elapsed    | 14       |
|    total_timesteps | 1600     |
| train/             |          |
|    actor_loss      | -2.5e+03 |
|    critic_loss     | 141      |
|    ent_coef        | 2.73     |
|    ent_coef_loss   | -0.0456  |
|    learning_rate   | 0.0003   |
|    n_updates       | 525899   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 112       |
|    time_elapsed    | 17        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 103       |
|    ent_coef        | 2.75      |
|    ent_coef_loss   | 0.0201    |
|    learning_rate   | 0.0003    |
|    n_updates       | 526299    |
----------------------------------
=== Iterazione IRL 1381 ===
Loss reward (iter 1381): 6.636938571929932
=== Iterazione IRL 1382 ===
Loss reward (iter 1382): 6.5193963050842285
=== Iterazione IRL 1383 ===
Loss reward (iter 1383): 6.594424247741699
=== Iterazione IRL 1384 ===
Loss reward (iter 1384): 6.202037811279297
=== Iterazione IRL 1385 ===
Loss reward (iter 1385): 7.0814595222473145
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 139       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 115       |
|    ent_coef        | 2.78      |
|    ent_coef_loss   | -0.0402   |
|    learning_rate   | 0.0003    |
|    n_updates       | 526599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 121       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 105       |
|    ent_coef        | 2.76      |
|    ent_coef_loss   | 0.0484    |
|    learning_rate   | 0.0003    |
|    n_updates       | 526999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 116       |
|    time_elapsed    | 10        |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 167       |
|    ent_coef        | 2.76      |
|    ent_coef_loss   | -0.141    |
|    learning_rate   | 0.0003    |
|    n_updates       | 527399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 115       |
|    time_elapsed    | 13        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 159       |
|    ent_coef        | 2.76      |
|    ent_coef_loss   | -0.0499   |
|    learning_rate   | 0.0003    |
|    n_updates       | 527799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 115       |
|    time_elapsed    | 17        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 178       |
|    ent_coef        | 2.75      |
|    ent_coef_loss   | -0.021    |
|    learning_rate   | 0.0003    |
|    n_updates       | 528199    |
----------------------------------
=== Iterazione IRL 1386 ===
Loss reward (iter 1386): 7.068883895874023
=== Iterazione IRL 1387 ===
Loss reward (iter 1387): 6.890244483947754
=== Iterazione IRL 1388 ===
Loss reward (iter 1388): 6.760452747344971
=== Iterazione IRL 1389 ===
Loss reward (iter 1389): 6.923738956451416
=== Iterazione IRL 1390 ===
Loss reward (iter 1390): 7.202389717102051
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 154       |
|    ent_coef        | 2.75      |
|    ent_coef_loss   | -0.0516   |
|    learning_rate   | 0.0003    |
|    n_updates       | 528499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 157       |
|    ent_coef        | 2.75      |
|    ent_coef_loss   | -0.126    |
|    learning_rate   | 0.0003    |
|    n_updates       | 528899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 112       |
|    ent_coef        | 2.73      |
|    ent_coef_loss   | 0.128     |
|    learning_rate   | 0.0003    |
|    n_updates       | 529299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 123       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 96.8      |
|    ent_coef        | 2.74      |
|    ent_coef_loss   | 0.0799    |
|    learning_rate   | 0.0003    |
|    n_updates       | 529699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 175       |
|    ent_coef        | 2.75      |
|    ent_coef_loss   | -0.0837   |
|    learning_rate   | 0.0003    |
|    n_updates       | 530099    |
----------------------------------
=== Iterazione IRL 1391 ===
Loss reward (iter 1391): 6.88697624206543
=== Iterazione IRL 1392 ===
Loss reward (iter 1392): 6.7358269691467285
=== Iterazione IRL 1393 ===
Loss reward (iter 1393): 6.521032333374023
=== Iterazione IRL 1394 ===
Loss reward (iter 1394): 6.521496295928955
=== Iterazione IRL 1395 ===
Loss reward (iter 1395): 7.034064292907715
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 241       |
|    ent_coef        | 2.75      |
|    ent_coef_loss   | 0.0442    |
|    learning_rate   | 0.0003    |
|    n_updates       | 530399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 146       |
|    ent_coef        | 2.74      |
|    ent_coef_loss   | 0.112     |
|    learning_rate   | 0.0003    |
|    n_updates       | 530799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 90.7      |
|    ent_coef        | 2.74      |
|    ent_coef_loss   | 0.034     |
|    learning_rate   | 0.0003    |
|    n_updates       | 531199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 123       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 168       |
|    ent_coef        | 2.75      |
|    ent_coef_loss   | 0.0839    |
|    learning_rate   | 0.0003    |
|    n_updates       | 531599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 171       |
|    ent_coef        | 2.72      |
|    ent_coef_loss   | 0.0718    |
|    learning_rate   | 0.0003    |
|    n_updates       | 531999    |
----------------------------------
=== Iterazione IRL 1396 ===
Loss reward (iter 1396): 6.4025654792785645
=== Iterazione IRL 1397 ===
Loss reward (iter 1397): 6.476504325866699
=== Iterazione IRL 1398 ===
Loss reward (iter 1398): 6.62308931350708
=== Iterazione IRL 1399 ===
Loss reward (iter 1399): 6.383357048034668
=== Iterazione IRL 1400 ===
Loss reward (iter 1400): 6.479068279266357
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 100       |
|    ent_coef        | 2.73      |
|    ent_coef_loss   | 0.0194    |
|    learning_rate   | 0.0003    |
|    n_updates       | 532299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 97.4      |
|    ent_coef        | 2.76      |
|    ent_coef_loss   | 0.108     |
|    learning_rate   | 0.0003    |
|    n_updates       | 532699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 114       |
|    ent_coef        | 2.72      |
|    ent_coef_loss   | 0.126     |
|    learning_rate   | 0.0003    |
|    n_updates       | 533099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 123       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 108       |
|    ent_coef        | 2.72      |
|    ent_coef_loss   | 0.0369    |
|    learning_rate   | 0.0003    |
|    n_updates       | 533499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 131       |
|    ent_coef        | 2.74      |
|    ent_coef_loss   | 0.0183    |
|    learning_rate   | 0.0003    |
|    n_updates       | 533899    |
----------------------------------
=== Iterazione IRL 1401 ===
Loss reward (iter 1401): 7.025386810302734
=== Iterazione IRL 1402 ===
Loss reward (iter 1402): 6.673076629638672
=== Iterazione IRL 1403 ===
Loss reward (iter 1403): 6.886234283447266
=== Iterazione IRL 1404 ===
Loss reward (iter 1404): 6.860779285430908
=== Iterazione IRL 1405 ===
Loss reward (iter 1405): 6.713093280792236
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 168       |
|    ent_coef        | 2.75      |
|    ent_coef_loss   | -0.0514   |
|    learning_rate   | 0.0003    |
|    n_updates       | 534199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 111       |
|    ent_coef        | 2.74      |
|    ent_coef_loss   | -0.0244   |
|    learning_rate   | 0.0003    |
|    n_updates       | 534599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 143       |
|    ent_coef        | 2.74      |
|    ent_coef_loss   | -0.0101   |
|    learning_rate   | 0.0003    |
|    n_updates       | 534999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 123       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 116       |
|    ent_coef        | 2.74      |
|    ent_coef_loss   | -0.0433   |
|    learning_rate   | 0.0003    |
|    n_updates       | 535399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 94.1      |
|    ent_coef        | 2.79      |
|    ent_coef_loss   | -0.305    |
|    learning_rate   | 0.0003    |
|    n_updates       | 535799    |
----------------------------------
=== Iterazione IRL 1406 ===
Loss reward (iter 1406): 6.873779296875
=== Iterazione IRL 1407 ===
Loss reward (iter 1407): 6.947689056396484
=== Iterazione IRL 1408 ===
Loss reward (iter 1408): 6.776528835296631
=== Iterazione IRL 1409 ===
Loss reward (iter 1409): 6.5991411209106445
=== Iterazione IRL 1410 ===
Loss reward (iter 1410): 6.564579010009766
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 148       |
|    ent_coef        | 2.78      |
|    ent_coef_loss   | -0.0322   |
|    learning_rate   | 0.0003    |
|    n_updates       | 536099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 137       |
|    ent_coef        | 2.77      |
|    ent_coef_loss   | -0.0444   |
|    learning_rate   | 0.0003    |
|    n_updates       | 536499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 122       |
|    ent_coef        | 2.77      |
|    ent_coef_loss   | -0.07     |
|    learning_rate   | 0.0003    |
|    n_updates       | 536899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 123       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 116       |
|    ent_coef        | 2.79      |
|    ent_coef_loss   | 0.067     |
|    learning_rate   | 0.0003    |
|    n_updates       | 537299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 83.6      |
|    ent_coef        | 2.76      |
|    ent_coef_loss   | 0.0869    |
|    learning_rate   | 0.0003    |
|    n_updates       | 537699    |
----------------------------------
=== Iterazione IRL 1411 ===
Loss reward (iter 1411): 6.80592155456543
=== Iterazione IRL 1412 ===
Loss reward (iter 1412): 6.753813743591309
=== Iterazione IRL 1413 ===
Loss reward (iter 1413): 6.661524772644043
=== Iterazione IRL 1414 ===
Loss reward (iter 1414): 6.612915992736816
=== Iterazione IRL 1415 ===
Loss reward (iter 1415): 6.634068489074707
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 114       |
|    ent_coef        | 2.76      |
|    ent_coef_loss   | 0.0238    |
|    learning_rate   | 0.0003    |
|    n_updates       | 537999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 135       |
|    ent_coef        | 2.76      |
|    ent_coef_loss   | -0.00552  |
|    learning_rate   | 0.0003    |
|    n_updates       | 538399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 157       |
|    ent_coef        | 2.76      |
|    ent_coef_loss   | -0.0277   |
|    learning_rate   | 0.0003    |
|    n_updates       | 538799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 123       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 80.2      |
|    ent_coef        | 2.74      |
|    ent_coef_loss   | -0.0784   |
|    learning_rate   | 0.0003    |
|    n_updates       | 539199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 135       |
|    ent_coef        | 2.76      |
|    ent_coef_loss   | -0.0571   |
|    learning_rate   | 0.0003    |
|    n_updates       | 539599    |
----------------------------------
=== Iterazione IRL 1416 ===
Loss reward (iter 1416): 6.645374298095703
=== Iterazione IRL 1417 ===
Loss reward (iter 1417): 6.447229385375977
=== Iterazione IRL 1418 ===
Loss reward (iter 1418): 6.4643874168396
=== Iterazione IRL 1419 ===
Loss reward (iter 1419): 6.787511825561523
=== Iterazione IRL 1420 ===
Loss reward (iter 1420): 6.680577754974365
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 109       |
|    ent_coef        | 2.77      |
|    ent_coef_loss   | -0.0472   |
|    learning_rate   | 0.0003    |
|    n_updates       | 539899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 107       |
|    ent_coef        | 2.75      |
|    ent_coef_loss   | 0.145     |
|    learning_rate   | 0.0003    |
|    n_updates       | 540299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 125       |
|    ent_coef        | 2.72      |
|    ent_coef_loss   | -0.00764  |
|    learning_rate   | 0.0003    |
|    n_updates       | 540699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 123       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 120       |
|    ent_coef        | 2.73      |
|    ent_coef_loss   | 0.0612    |
|    learning_rate   | 0.0003    |
|    n_updates       | 541099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 93.5      |
|    ent_coef        | 2.73      |
|    ent_coef_loss   | -0.039    |
|    learning_rate   | 0.0003    |
|    n_updates       | 541499    |
----------------------------------
=== Iterazione IRL 1421 ===
Loss reward (iter 1421): 6.727664947509766
=== Iterazione IRL 1422 ===
Loss reward (iter 1422): 6.694272518157959
=== Iterazione IRL 1423 ===
Loss reward (iter 1423): 6.845810890197754
=== Iterazione IRL 1424 ===
Loss reward (iter 1424): 6.8329901695251465
=== Iterazione IRL 1425 ===
Loss reward (iter 1425): 6.3946146965026855
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 125       |
|    ent_coef        | 2.74      |
|    ent_coef_loss   | -0.0631   |
|    learning_rate   | 0.0003    |
|    n_updates       | 541799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 112       |
|    ent_coef        | 2.72      |
|    ent_coef_loss   | -0.0651   |
|    learning_rate   | 0.0003    |
|    n_updates       | 542199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 86.7      |
|    ent_coef        | 2.74      |
|    ent_coef_loss   | -0.111    |
|    learning_rate   | 0.0003    |
|    n_updates       | 542599    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 16       |
|    fps             | 123      |
|    time_elapsed    | 12       |
|    total_timesteps | 1600     |
| train/             |          |
|    actor_loss      | -2.5e+03 |
|    critic_loss     | 120      |
|    ent_coef        | 2.73     |
|    ent_coef_loss   | -0.00248 |
|    learning_rate   | 0.0003   |
|    n_updates       | 542999   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 109       |
|    ent_coef        | 2.74      |
|    ent_coef_loss   | -0.0109   |
|    learning_rate   | 0.0003    |
|    n_updates       | 543399    |
----------------------------------
=== Iterazione IRL 1426 ===
Loss reward (iter 1426): 6.978745937347412
=== Iterazione IRL 1427 ===
Loss reward (iter 1427): 7.179666042327881
=== Iterazione IRL 1428 ===
Loss reward (iter 1428): 6.6759724617004395
=== Iterazione IRL 1429 ===
Loss reward (iter 1429): 7.303821563720703
=== Iterazione IRL 1430 ===
Loss reward (iter 1430): 6.7776055335998535
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 159       |
|    ent_coef        | 2.73      |
|    ent_coef_loss   | -0.153    |
|    learning_rate   | 0.0003    |
|    n_updates       | 543699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 75.1      |
|    ent_coef        | 2.73      |
|    ent_coef_loss   | 0.0279    |
|    learning_rate   | 0.0003    |
|    n_updates       | 544099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 121       |
|    ent_coef        | 2.72      |
|    ent_coef_loss   | -0.11     |
|    learning_rate   | 0.0003    |
|    n_updates       | 544499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 123       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 109       |
|    ent_coef        | 2.73      |
|    ent_coef_loss   | -0.119    |
|    learning_rate   | 0.0003    |
|    n_updates       | 544899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 161       |
|    ent_coef        | 2.73      |
|    ent_coef_loss   | 0.0133    |
|    learning_rate   | 0.0003    |
|    n_updates       | 545299    |
----------------------------------
=== Iterazione IRL 1431 ===
Loss reward (iter 1431): 7.0345635414123535
=== Iterazione IRL 1432 ===
Loss reward (iter 1432): 6.671489715576172
=== Iterazione IRL 1433 ===
Loss reward (iter 1433): 7.05648136138916
=== Iterazione IRL 1434 ===
Loss reward (iter 1434): 6.952010154724121
=== Iterazione IRL 1435 ===
Loss reward (iter 1435): 6.63637638092041
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 106       |
|    ent_coef        | 2.74      |
|    ent_coef_loss   | -0.0666   |
|    learning_rate   | 0.0003    |
|    n_updates       | 545599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.53e+03 |
|    critic_loss     | 111       |
|    ent_coef        | 2.72      |
|    ent_coef_loss   | 0.0867    |
|    learning_rate   | 0.0003    |
|    n_updates       | 545999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 115       |
|    ent_coef        | 2.74      |
|    ent_coef_loss   | 0.0614    |
|    learning_rate   | 0.0003    |
|    n_updates       | 546399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 123       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 136       |
|    ent_coef        | 2.73      |
|    ent_coef_loss   | -0.0483   |
|    learning_rate   | 0.0003    |
|    n_updates       | 546799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 102       |
|    ent_coef        | 2.73      |
|    ent_coef_loss   | 0.031     |
|    learning_rate   | 0.0003    |
|    n_updates       | 547199    |
----------------------------------
=== Iterazione IRL 1436 ===
Loss reward (iter 1436): 6.369839668273926
=== Iterazione IRL 1437 ===
Loss reward (iter 1437): 6.933111667633057
=== Iterazione IRL 1438 ===
Loss reward (iter 1438): 6.963957786560059
=== Iterazione IRL 1439 ===
Loss reward (iter 1439): 6.615353584289551
=== Iterazione IRL 1440 ===
Loss reward (iter 1440): 6.632538318634033
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 151      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.5e+03 |
|    critic_loss     | 202      |
|    ent_coef        | 2.74     |
|    ent_coef_loss   | 0.0921   |
|    learning_rate   | 0.0003   |
|    n_updates       | 547499   |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 131      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.5e+03 |
|    critic_loss     | 164      |
|    ent_coef        | 2.74     |
|    ent_coef_loss   | -0.179   |
|    learning_rate   | 0.0003   |
|    n_updates       | 547899   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 99.3      |
|    ent_coef        | 2.74      |
|    ent_coef_loss   | 0.078     |
|    learning_rate   | 0.0003    |
|    n_updates       | 548299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 123       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 105       |
|    ent_coef        | 2.74      |
|    ent_coef_loss   | 0.0457    |
|    learning_rate   | 0.0003    |
|    n_updates       | 548699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 115       |
|    ent_coef        | 2.73      |
|    ent_coef_loss   | 0.171     |
|    learning_rate   | 0.0003    |
|    n_updates       | 549099    |
----------------------------------
=== Iterazione IRL 1441 ===
Loss reward (iter 1441): 6.452097415924072
=== Iterazione IRL 1442 ===
Loss reward (iter 1442): 6.878292560577393
=== Iterazione IRL 1443 ===
Loss reward (iter 1443): 6.642980098724365
=== Iterazione IRL 1444 ===
Loss reward (iter 1444): 6.744114875793457
=== Iterazione IRL 1445 ===
Loss reward (iter 1445): 6.531238079071045
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 151       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 85.4      |
|    ent_coef        | 2.71      |
|    ent_coef_loss   | 0.1       |
|    learning_rate   | 0.0003    |
|    n_updates       | 549399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 131       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.53e+03 |
|    critic_loss     | 87.9      |
|    ent_coef        | 2.77      |
|    ent_coef_loss   | 0.0633    |
|    learning_rate   | 0.0003    |
|    n_updates       | 549799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 114       |
|    ent_coef        | 2.77      |
|    ent_coef_loss   | -0.0946   |
|    learning_rate   | 0.0003    |
|    n_updates       | 550199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 123       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 124       |
|    ent_coef        | 2.75      |
|    ent_coef_loss   | 0.131     |
|    learning_rate   | 0.0003    |
|    n_updates       | 550599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 141       |
|    ent_coef        | 2.76      |
|    ent_coef_loss   | -0.0668   |
|    learning_rate   | 0.0003    |
|    n_updates       | 550999    |
----------------------------------
=== Iterazione IRL 1446 ===
Loss reward (iter 1446): 6.524783611297607
=== Iterazione IRL 1447 ===
Loss reward (iter 1447): 6.927401542663574
=== Iterazione IRL 1448 ===
Loss reward (iter 1448): 6.733717441558838
=== Iterazione IRL 1449 ===
Loss reward (iter 1449): 6.550517559051514
=== Iterazione IRL 1450 ===
Loss reward (iter 1450): 6.722441673278809
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 153       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 136       |
|    ent_coef        | 2.74      |
|    ent_coef_loss   | -0.0244   |
|    learning_rate   | 0.0003    |
|    n_updates       | 551299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 110       |
|    ent_coef        | 2.75      |
|    ent_coef_loss   | -0.0447   |
|    learning_rate   | 0.0003    |
|    n_updates       | 551699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 124       |
|    ent_coef        | 2.77      |
|    ent_coef_loss   | 0.148     |
|    learning_rate   | 0.0003    |
|    n_updates       | 552099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 127       |
|    ent_coef        | 2.75      |
|    ent_coef_loss   | 0.00147   |
|    learning_rate   | 0.0003    |
|    n_updates       | 552499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 123       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 119       |
|    ent_coef        | 2.75      |
|    ent_coef_loss   | -0.0393   |
|    learning_rate   | 0.0003    |
|    n_updates       | 552899    |
----------------------------------
=== Iterazione IRL 1451 ===
Loss reward (iter 1451): 6.535171985626221
=== Iterazione IRL 1452 ===
Loss reward (iter 1452): 6.757480144500732
=== Iterazione IRL 1453 ===
Loss reward (iter 1453): 6.6707444190979
=== Iterazione IRL 1454 ===
Loss reward (iter 1454): 6.738825798034668
=== Iterazione IRL 1455 ===
Loss reward (iter 1455): 6.623473167419434
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 153       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 106       |
|    ent_coef        | 2.75      |
|    ent_coef_loss   | -0.178    |
|    learning_rate   | 0.0003    |
|    n_updates       | 553199    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 133      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.5e+03 |
|    critic_loss     | 132      |
|    ent_coef        | 2.75     |
|    ent_coef_loss   | -0.0388  |
|    learning_rate   | 0.0003   |
|    n_updates       | 553599   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 144       |
|    ent_coef        | 2.75      |
|    ent_coef_loss   | 0.103     |
|    learning_rate   | 0.0003    |
|    n_updates       | 553999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 161       |
|    ent_coef        | 2.74      |
|    ent_coef_loss   | 0.0567    |
|    learning_rate   | 0.0003    |
|    n_updates       | 554399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 123       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 123       |
|    ent_coef        | 2.72      |
|    ent_coef_loss   | -0.0326   |
|    learning_rate   | 0.0003    |
|    n_updates       | 554799    |
----------------------------------
=== Iterazione IRL 1456 ===
Loss reward (iter 1456): 6.584497451782227
=== Iterazione IRL 1457 ===
Loss reward (iter 1457): 6.688381195068359
=== Iterazione IRL 1458 ===
Loss reward (iter 1458): 6.6948161125183105
=== Iterazione IRL 1459 ===
Loss reward (iter 1459): 6.624093055725098
=== Iterazione IRL 1460 ===
Loss reward (iter 1460): 6.745667457580566
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 153       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 111       |
|    ent_coef        | 2.74      |
|    ent_coef_loss   | 0.00354   |
|    learning_rate   | 0.0003    |
|    n_updates       | 555099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 110       |
|    ent_coef        | 2.75      |
|    ent_coef_loss   | 0.0485    |
|    learning_rate   | 0.0003    |
|    n_updates       | 555499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 99.2      |
|    ent_coef        | 2.73      |
|    ent_coef_loss   | 0.0342    |
|    learning_rate   | 0.0003    |
|    n_updates       | 555899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.53e+03 |
|    critic_loss     | 125       |
|    ent_coef        | 2.72      |
|    ent_coef_loss   | 0.0396    |
|    learning_rate   | 0.0003    |
|    n_updates       | 556299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 123       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 185       |
|    ent_coef        | 2.74      |
|    ent_coef_loss   | 0.0473    |
|    learning_rate   | 0.0003    |
|    n_updates       | 556699    |
----------------------------------
=== Iterazione IRL 1461 ===
Loss reward (iter 1461): 6.892457962036133
=== Iterazione IRL 1462 ===
Loss reward (iter 1462): 6.691499710083008
=== Iterazione IRL 1463 ===
Loss reward (iter 1463): 6.683765888214111
=== Iterazione IRL 1464 ===
Loss reward (iter 1464): 6.874725341796875
=== Iterazione IRL 1465 ===
Loss reward (iter 1465): 6.744600296020508
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 153       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 118       |
|    ent_coef        | 2.74      |
|    ent_coef_loss   | -0.0875   |
|    learning_rate   | 0.0003    |
|    n_updates       | 556999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 129       |
|    ent_coef        | 2.73      |
|    ent_coef_loss   | -0.152    |
|    learning_rate   | 0.0003    |
|    n_updates       | 557399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 76.5      |
|    ent_coef        | 2.73      |
|    ent_coef_loss   | 0.262     |
|    learning_rate   | 0.0003    |
|    n_updates       | 557799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 108       |
|    ent_coef        | 2.75      |
|    ent_coef_loss   | 0.00838   |
|    learning_rate   | 0.0003    |
|    n_updates       | 558199    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 123       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 113       |
|    ent_coef        | 2.72      |
|    ent_coef_loss   | 0.109     |
|    learning_rate   | 0.0003    |
|    n_updates       | 558599    |
----------------------------------
=== Iterazione IRL 1466 ===
Loss reward (iter 1466): 6.6015167236328125
=== Iterazione IRL 1467 ===
Loss reward (iter 1467): 5.793797492980957
=== Iterazione IRL 1468 ===
Loss reward (iter 1468): 7.164966106414795
=== Iterazione IRL 1469 ===
Loss reward (iter 1469): 6.809606075286865
=== Iterazione IRL 1470 ===
Loss reward (iter 1470): 6.835709571838379
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 153      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -2.5e+03 |
|    critic_loss     | 126      |
|    ent_coef        | 2.72     |
|    ent_coef_loss   | -0.153   |
|    learning_rate   | 0.0003   |
|    n_updates       | 558899   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 99.6      |
|    ent_coef        | 2.76      |
|    ent_coef_loss   | 0.0481    |
|    learning_rate   | 0.0003    |
|    n_updates       | 559299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 127       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 129       |
|    ent_coef        | 2.74      |
|    ent_coef_loss   | -0.0224   |
|    learning_rate   | 0.0003    |
|    n_updates       | 559699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 125       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 81.6      |
|    ent_coef        | 2.74      |
|    ent_coef_loss   | -0.0469   |
|    learning_rate   | 0.0003    |
|    n_updates       | 560099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 123       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.53e+03 |
|    critic_loss     | 78.9      |
|    ent_coef        | 2.72      |
|    ent_coef_loss   | 0.101     |
|    learning_rate   | 0.0003    |
|    n_updates       | 560499    |
----------------------------------
=== Iterazione IRL 1471 ===
Loss reward (iter 1471): 6.76671838760376
=== Iterazione IRL 1472 ===
Loss reward (iter 1472): 6.616293907165527
=== Iterazione IRL 1473 ===
Loss reward (iter 1473): 6.481807708740234
=== Iterazione IRL 1474 ===
Loss reward (iter 1474): 6.787670612335205
=== Iterazione IRL 1475 ===
Loss reward (iter 1475): 6.618016719818115
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 153       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 107       |
|    ent_coef        | 2.74      |
|    ent_coef_loss   | -0.075    |
|    learning_rate   | 0.0003    |
|    n_updates       | 560799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 133       |
|    time_elapsed    | 5         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 150       |
|    ent_coef        | 2.72      |
|    ent_coef_loss   | -0.0298   |
|    learning_rate   | 0.0003    |
|    n_updates       | 561199    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 127      |
|    time_elapsed    | 9        |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -2.5e+03 |
|    critic_loss     | 116      |
|    ent_coef        | 2.71     |
|    ent_coef_loss   | -0.0139  |
|    learning_rate   | 0.0003   |
|    n_updates       | 561599   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 122       |
|    ent_coef        | 2.73      |
|    ent_coef_loss   | -0.0173   |
|    learning_rate   | 0.0003    |
|    n_updates       | 561999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 123       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 154       |
|    ent_coef        | 2.73      |
|    ent_coef_loss   | -0.104    |
|    learning_rate   | 0.0003    |
|    n_updates       | 562399    |
----------------------------------
=== Iterazione IRL 1476 ===
Loss reward (iter 1476): 6.17680549621582
=== Iterazione IRL 1477 ===
Loss reward (iter 1477): 6.419697284698486
=== Iterazione IRL 1478 ===
Loss reward (iter 1478): 6.659239292144775
=== Iterazione IRL 1479 ===
Loss reward (iter 1479): 7.123053073883057
=== Iterazione IRL 1480 ===
Loss reward (iter 1480): 6.55409574508667
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 91.1      |
|    ent_coef        | 2.72      |
|    ent_coef_loss   | 0.152     |
|    learning_rate   | 0.0003    |
|    n_updates       | 562699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 95.7      |
|    ent_coef        | 2.72      |
|    ent_coef_loss   | -0.0155   |
|    learning_rate   | 0.0003    |
|    n_updates       | 563099    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 102       |
|    ent_coef        | 2.72      |
|    ent_coef_loss   | 0.127     |
|    learning_rate   | 0.0003    |
|    n_updates       | 563499    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 123       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 82.3      |
|    ent_coef        | 2.72      |
|    ent_coef_loss   | 0.113     |
|    learning_rate   | 0.0003    |
|    n_updates       | 563899    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 107       |
|    ent_coef        | 2.72      |
|    ent_coef_loss   | -0.0972   |
|    learning_rate   | 0.0003    |
|    n_updates       | 564299    |
----------------------------------
=== Iterazione IRL 1481 ===
Loss reward (iter 1481): 6.824352741241455
=== Iterazione IRL 1482 ===
Loss reward (iter 1482): 6.761813640594482
=== Iterazione IRL 1483 ===
Loss reward (iter 1483): 6.6685943603515625
=== Iterazione IRL 1484 ===
Loss reward (iter 1484): 6.940515518188477
=== Iterazione IRL 1485 ===
Loss reward (iter 1485): 6.8428521156311035
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 140       |
|    ent_coef        | 2.72      |
|    ent_coef_loss   | -0.00686  |
|    learning_rate   | 0.0003    |
|    n_updates       | 564599    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 115       |
|    ent_coef        | 2.71      |
|    ent_coef_loss   | -0.0247   |
|    learning_rate   | 0.0003    |
|    n_updates       | 564999    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 196       |
|    ent_coef        | 2.73      |
|    ent_coef_loss   | -0.0293   |
|    learning_rate   | 0.0003    |
|    n_updates       | 565399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 123       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 108       |
|    ent_coef        | 2.75      |
|    ent_coef_loss   | -0.0688   |
|    learning_rate   | 0.0003    |
|    n_updates       | 565799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 121       |
|    ent_coef        | 2.75      |
|    ent_coef_loss   | 0.00285   |
|    learning_rate   | 0.0003    |
|    n_updates       | 566199    |
----------------------------------
=== Iterazione IRL 1486 ===
Loss reward (iter 1486): 6.611324787139893
=== Iterazione IRL 1487 ===
Loss reward (iter 1487): 6.650336742401123
=== Iterazione IRL 1488 ===
Loss reward (iter 1488): 6.785580635070801
=== Iterazione IRL 1489 ===
Loss reward (iter 1489): 6.521111965179443
=== Iterazione IRL 1490 ===
Loss reward (iter 1490): 6.719611167907715
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 151       |
|    ent_coef        | 2.75      |
|    ent_coef_loss   | -0.0304   |
|    learning_rate   | 0.0003    |
|    n_updates       | 566499    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 132      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -2.5e+03 |
|    critic_loss     | 177      |
|    ent_coef        | 2.72     |
|    ent_coef_loss   | 0.0549   |
|    learning_rate   | 0.0003   |
|    n_updates       | 566899   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 118       |
|    ent_coef        | 2.71      |
|    ent_coef_loss   | -0.0533   |
|    learning_rate   | 0.0003    |
|    n_updates       | 567299    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 124       |
|    time_elapsed    | 12        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 110       |
|    ent_coef        | 2.73      |
|    ent_coef_loss   | -0.103    |
|    learning_rate   | 0.0003    |
|    n_updates       | 567699    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 109       |
|    ent_coef        | 2.73      |
|    ent_coef_loss   | -0.0747   |
|    learning_rate   | 0.0003    |
|    n_updates       | 568099    |
----------------------------------
=== Iterazione IRL 1491 ===
Loss reward (iter 1491): 6.6840901374816895
=== Iterazione IRL 1492 ===
Loss reward (iter 1492): 6.355052947998047
=== Iterazione IRL 1493 ===
Loss reward (iter 1493): 6.744421482086182
=== Iterazione IRL 1494 ===
Loss reward (iter 1494): 6.802907943725586
=== Iterazione IRL 1495 ===
Loss reward (iter 1495): 6.57553768157959
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 152       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 92.9      |
|    ent_coef        | 2.73      |
|    ent_coef_loss   | -0.00973  |
|    learning_rate   | 0.0003    |
|    n_updates       | 568399    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 132       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 139       |
|    ent_coef        | 2.74      |
|    ent_coef_loss   | -0.0362   |
|    learning_rate   | 0.0003    |
|    n_updates       | 568799    |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 126       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.52e+03 |
|    critic_loss     | 138       |
|    ent_coef        | 2.7       |
|    ent_coef_loss   | 0.0228    |
|    learning_rate   | 0.0003    |
|    n_updates       | 569199    |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 16       |
|    fps             | 123      |
|    time_elapsed    | 12       |
|    total_timesteps | 1600     |
| train/             |          |
|    actor_loss      | -2.5e+03 |
|    critic_loss     | 106      |
|    ent_coef        | 2.71     |
|    ent_coef_loss   | -0.0185  |
|    learning_rate   | 0.0003   |
|    n_updates       | 569599   |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 122       |
|    time_elapsed    | 16        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.51e+03 |
|    critic_loss     | 91.9      |
|    ent_coef        | 2.72      |
|    ent_coef_loss   | 0.231     |
|    learning_rate   | 0.0003    |
|    n_updates       | 569999    |
----------------------------------
=== Iterazione IRL 1496 ===
Loss reward (iter 1496): 6.483656406402588
=== Iterazione IRL 1497 ===
Loss reward (iter 1497): 6.665078163146973
=== Iterazione IRL 1498 ===
Loss reward (iter 1498): 6.625261306762695
=== Iterazione IRL 1499 ===
Loss reward (iter 1499): 6.576867580413818
Modello SAC salvato.
