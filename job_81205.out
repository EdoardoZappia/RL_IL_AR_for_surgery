Inizio training IRL
Using device: cuda
Using cuda device
=== Iterazione IRL 0 ===
Loss reward (iter 0): 4.721258163452148
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 128      |
|    time_elapsed    | 3        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -9.09    |
|    critic_loss     | 0.25     |
|    ent_coef        | 0.917    |
|    ent_coef_loss   | -0.266   |
|    learning_rate   | 0.0003   |
|    n_updates       | 299      |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 115      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -15.8    |
|    critic_loss     | 0.196    |
|    ent_coef        | 0.817    |
|    ent_coef_loss   | -0.596   |
|    learning_rate   | 0.0003   |
|    n_updates       | 699      |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 111      |
|    time_elapsed    | 10       |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -23.6    |
|    critic_loss     | 0.18     |
|    ent_coef        | 0.728    |
|    ent_coef_loss   | -0.854   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1099     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 16       |
|    fps             | 109      |
|    time_elapsed    | 14       |
|    total_timesteps | 1600     |
| train/             |          |
|    actor_loss      | -30.6    |
|    critic_loss     | 0.184    |
|    ent_coef        | 0.651    |
|    ent_coef_loss   | -1.13    |
|    learning_rate   | 0.0003   |
|    n_updates       | 1499     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 20       |
|    fps             | 108      |
|    time_elapsed    | 18       |
|    total_timesteps | 2000     |
| train/             |          |
|    actor_loss      | -38.6    |
|    critic_loss     | 0.131    |
|    ent_coef        | 0.585    |
|    ent_coef_loss   | -1.14    |
|    learning_rate   | 0.0003   |
|    n_updates       | 1899     |
---------------------------------
=== Iterazione IRL 1 ===
Loss reward (iter 1): 5.791036605834961
=== Iterazione IRL 2 ===
Loss reward (iter 2): 1.454429030418396
=== Iterazione IRL 3 ===
Loss reward (iter 3): -3.527785301208496
=== Iterazione IRL 4 ===
Loss reward (iter 4): -11.544662475585938
=== Iterazione IRL 5 ===
Loss reward (iter 5): -24.67987060546875
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 136      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -90.6    |
|    critic_loss     | 117      |
|    ent_coef        | 0.611    |
|    ent_coef_loss   | 1.28     |
|    learning_rate   | 0.0003   |
|    n_updates       | 2199     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 118      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -211     |
|    critic_loss     | 93.9     |
|    ent_coef        | 0.701    |
|    ent_coef_loss   | 1.27     |
|    learning_rate   | 0.0003   |
|    n_updates       | 2599     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 12       |
|    fps             | 113      |
|    time_elapsed    | 10       |
|    total_timesteps | 1200     |
| train/             |          |
|    actor_loss      | -330     |
|    critic_loss     | 80.3     |
|    ent_coef        | 0.811    |
|    ent_coef_loss   | 0.799    |
|    learning_rate   | 0.0003   |
|    n_updates       | 2999     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 16       |
|    fps             | 111      |
|    time_elapsed    | 14       |
|    total_timesteps | 1600     |
| train/             |          |
|    actor_loss      | -488     |
|    critic_loss     | 67.7     |
|    ent_coef        | 0.936    |
|    ent_coef_loss   | 0.23     |
|    learning_rate   | 0.0003   |
|    n_updates       | 3399     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 20       |
|    fps             | 110      |
|    time_elapsed    | 18       |
|    total_timesteps | 2000     |
| train/             |          |
|    actor_loss      | -670     |
|    critic_loss     | 89.6     |
|    ent_coef        | 1.07     |
|    ent_coef_loss   | -0.271   |
|    learning_rate   | 0.0003   |
|    n_updates       | 3799     |
---------------------------------
=== Iterazione IRL 6 ===
Loss reward (iter 6): 64.52737426757812
=== Iterazione IRL 7 ===
Loss reward (iter 7): 31.628873825073242
=== Iterazione IRL 8 ===
Loss reward (iter 8): 10.220787048339844
=== Iterazione IRL 9 ===
Loss reward (iter 9): -5.234528064727783
=== Iterazione IRL 10 ===
Loss reward (iter 10): -21.4285888671875
>>> Aggiorno la policy con SAC
---------------------------------
| time/              |          |
|    episodes        | 4        |
|    fps             | 143      |
|    time_elapsed    | 2        |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -790     |
|    critic_loss     | 380      |
|    ent_coef        | 1.17     |
|    ent_coef_loss   | -0.563   |
|    learning_rate   | 0.0003   |
|    n_updates       | 4099     |
---------------------------------
---------------------------------
| time/              |          |
|    episodes        | 8        |
|    fps             | 125      |
|    time_elapsed    | 6        |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -1e+03   |
|    critic_loss     | 413      |
|    ent_coef        | 1.33     |
|    ent_coef_loss   | -0.958   |
|    learning_rate   | 0.0003   |
|    n_updates       | 4499     |
---------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 119       |
|    time_elapsed    | 10        |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.19e+03 |
|    critic_loss     | 452       |
|    ent_coef        | 1.49      |
|    ent_coef_loss   | -1.35     |
|    learning_rate   | 0.0003    |
|    n_updates       | 4899      |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 117       |
|    time_elapsed    | 13        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.35e+03 |
|    critic_loss     | 389       |
|    ent_coef        | 1.68      |
|    ent_coef_loss   | -1.74     |
|    learning_rate   | 0.0003    |
|    n_updates       | 5299      |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 116       |
|    time_elapsed    | 17        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.56e+03 |
|    critic_loss     | 481       |
|    ent_coef        | 1.88      |
|    ent_coef_loss   | -1.86     |
|    learning_rate   | 0.0003    |
|    n_updates       | 5699      |
----------------------------------
=== Iterazione IRL 11 ===
Loss reward (iter 11): 71.50765991210938
=== Iterazione IRL 12 ===
Loss reward (iter 12): 38.86485290527344
=== Iterazione IRL 13 ===
Loss reward (iter 13): 18.249780654907227
=== Iterazione IRL 14 ===
Loss reward (iter 14): 6.322384357452393
=== Iterazione IRL 15 ===
Loss reward (iter 15): -1.762224793434143
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 144       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.73e+03 |
|    critic_loss     | 848       |
|    ent_coef        | 2.04      |
|    ent_coef_loss   | -2.6      |
|    learning_rate   | 0.0003    |
|    n_updates       | 5999      |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 125       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.82e+03 |
|    critic_loss     | 1.19e+03  |
|    ent_coef        | 2.28      |
|    ent_coef_loss   | -2.4      |
|    learning_rate   | 0.0003    |
|    n_updates       | 6399      |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 120       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.93e+03 |
|    critic_loss     | 1.48e+03  |
|    ent_coef        | 2.55      |
|    ent_coef_loss   | -2.68     |
|    learning_rate   | 0.0003    |
|    n_updates       | 6799      |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 117       |
|    time_elapsed    | 13        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.05e+03 |
|    critic_loss     | 1.67e+03  |
|    ent_coef        | 2.83      |
|    ent_coef_loss   | -2.76     |
|    learning_rate   | 0.0003    |
|    n_updates       | 7199      |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 116       |
|    time_elapsed    | 17        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -2.21e+03 |
|    critic_loss     | 1.81e+03  |
|    ent_coef        | 3.13      |
|    ent_coef_loss   | -2.66     |
|    learning_rate   | 0.0003    |
|    n_updates       | 7599      |
----------------------------------
=== Iterazione IRL 16 ===
Loss reward (iter 16): -4.772952079772949
=== Iterazione IRL 17 ===
Loss reward (iter 17): -8.711906433105469
=== Iterazione IRL 18 ===
Loss reward (iter 18): -15.517496109008789
=== Iterazione IRL 19 ===
Loss reward (iter 19): -24.996063232421875
=== Iterazione IRL 20 ===
Loss reward (iter 20): -35.640869140625
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 144       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.25e+03 |
|    critic_loss     | 1.95e+03  |
|    ent_coef        | 3.33      |
|    ent_coef_loss   | -1.47     |
|    learning_rate   | 0.0003    |
|    n_updates       | 7899      |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 125       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.33e+03 |
|    critic_loss     | 2.08e+03  |
|    ent_coef        | 3.6       |
|    ent_coef_loss   | -2.42     |
|    learning_rate   | 0.0003    |
|    n_updates       | 8299      |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 120       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.34e+03 |
|    critic_loss     | 1.99e+03  |
|    ent_coef        | 3.92      |
|    ent_coef_loss   | -2.24     |
|    learning_rate   | 0.0003    |
|    n_updates       | 8699      |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 117       |
|    time_elapsed    | 13        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.36e+03 |
|    critic_loss     | 1.97e+03  |
|    ent_coef        | 4.27      |
|    ent_coef_loss   | -1.94     |
|    learning_rate   | 0.0003    |
|    n_updates       | 9099      |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 20       |
|    fps             | 116      |
|    time_elapsed    | 17       |
|    total_timesteps | 2000     |
| train/             |          |
|    actor_loss      | -2.5e+03 |
|    critic_loss     | 1.9e+03  |
|    ent_coef        | 4.73     |
|    ent_coef_loss   | -2.68    |
|    learning_rate   | 0.0003   |
|    n_updates       | 9499     |
---------------------------------
=== Iterazione IRL 21 ===
Loss reward (iter 21): -49.193485260009766
=== Iterazione IRL 22 ===
Loss reward (iter 22): -71.80130767822266
=== Iterazione IRL 23 ===
Loss reward (iter 23): -106.58148193359375
=== Iterazione IRL 24 ===
Loss reward (iter 24): -155.3981170654297
=== Iterazione IRL 25 ===
Loss reward (iter 25): -206.47021484375
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 144       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -2.56e+03 |
|    critic_loss     | 2.56e+03  |
|    ent_coef        | 5.1       |
|    ent_coef_loss   | -2.02     |
|    learning_rate   | 0.0003    |
|    n_updates       | 9799      |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 125       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -2.68e+03 |
|    critic_loss     | 4.58e+03  |
|    ent_coef        | 5.55      |
|    ent_coef_loss   | -1.22     |
|    learning_rate   | 0.0003    |
|    n_updates       | 10199     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 120       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -2.71e+03 |
|    critic_loss     | 2.7e+03   |
|    ent_coef        | 5.89      |
|    ent_coef_loss   | -1.14     |
|    learning_rate   | 0.0003    |
|    n_updates       | 10599     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 118       |
|    time_elapsed    | 13        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -2.85e+03 |
|    critic_loss     | 4.1e+03   |
|    ent_coef        | 6.17      |
|    ent_coef_loss   | -0.99     |
|    learning_rate   | 0.0003    |
|    n_updates       | 10999     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 116       |
|    time_elapsed    | 17        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -3.05e+03 |
|    critic_loss     | 4.06e+03  |
|    ent_coef        | 6.84      |
|    ent_coef_loss   | -2.22     |
|    learning_rate   | 0.0003    |
|    n_updates       | 11399     |
----------------------------------
=== Iterazione IRL 26 ===
Loss reward (iter 26): 270.4750671386719
=== Iterazione IRL 27 ===
Loss reward (iter 27): 124.52803039550781
=== Iterazione IRL 28 ===
Loss reward (iter 28): 60.081382751464844
=== Iterazione IRL 29 ===
Loss reward (iter 29): -9.735122680664062
=== Iterazione IRL 30 ===
Loss reward (iter 30): -45.16253662109375
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 144       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -3.21e+03 |
|    critic_loss     | 4.62e+03  |
|    ent_coef        | 7.45      |
|    ent_coef_loss   | -2.05     |
|    learning_rate   | 0.0003    |
|    n_updates       | 11699     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 125       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -3.49e+03 |
|    critic_loss     | 6.68e+03  |
|    ent_coef        | 8.31      |
|    ent_coef_loss   | -1.87     |
|    learning_rate   | 0.0003    |
|    n_updates       | 12099     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 120       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -3.76e+03 |
|    critic_loss     | 6.94e+03  |
|    ent_coef        | 9.25      |
|    ent_coef_loss   | -2.07     |
|    learning_rate   | 0.0003    |
|    n_updates       | 12499     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 118       |
|    time_elapsed    | 13        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -4.06e+03 |
|    critic_loss     | 6.1e+03   |
|    ent_coef        | 10.2      |
|    ent_coef_loss   | -2.36     |
|    learning_rate   | 0.0003    |
|    n_updates       | 12899     |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 20       |
|    fps             | 116      |
|    time_elapsed    | 17       |
|    total_timesteps | 2000     |
| train/             |          |
|    actor_loss      | -4.3e+03 |
|    critic_loss     | 7.28e+03 |
|    ent_coef        | 11.2     |
|    ent_coef_loss   | -2.27    |
|    learning_rate   | 0.0003   |
|    n_updates       | 13299    |
---------------------------------
=== Iterazione IRL 31 ===
Loss reward (iter 31): -86.15231323242188
=== Iterazione IRL 32 ===
Loss reward (iter 32): -165.6471710205078
=== Iterazione IRL 33 ===
Loss reward (iter 33): -210.57513427734375
=== Iterazione IRL 34 ===
Loss reward (iter 34): -313.84173583984375
=== Iterazione IRL 35 ===
Loss reward (iter 35): -405.8523254394531
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 144       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -4.57e+03 |
|    critic_loss     | 7.74e+03  |
|    ent_coef        | 11.8      |
|    ent_coef_loss   | -1.4      |
|    learning_rate   | 0.0003    |
|    n_updates       | 13599     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 125       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -4.78e+03 |
|    critic_loss     | 1.05e+04  |
|    ent_coef        | 12.4      |
|    ent_coef_loss   | -1.21     |
|    learning_rate   | 0.0003    |
|    n_updates       | 13999     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 120       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -5.04e+03 |
|    critic_loss     | 1.17e+04  |
|    ent_coef        | 12.9      |
|    ent_coef_loss   | -0.693    |
|    learning_rate   | 0.0003    |
|    n_updates       | 14399     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 118       |
|    time_elapsed    | 13        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -5.22e+03 |
|    critic_loss     | 7.01e+03  |
|    ent_coef        | 13.2      |
|    ent_coef_loss   | 0.706     |
|    learning_rate   | 0.0003    |
|    n_updates       | 14799     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 116       |
|    time_elapsed    | 17        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -5.24e+03 |
|    critic_loss     | 1.6e+04   |
|    ent_coef        | 12.9      |
|    ent_coef_loss   | 0.705     |
|    learning_rate   | 0.0003    |
|    n_updates       | 15199     |
----------------------------------
=== Iterazione IRL 36 ===
Loss reward (iter 36): 362.0643615722656
=== Iterazione IRL 37 ===
Loss reward (iter 37): 319.130615234375
=== Iterazione IRL 38 ===
Loss reward (iter 38): 99.67134094238281
=== Iterazione IRL 39 ===
Loss reward (iter 39): 57.4377326965332
=== Iterazione IRL 40 ===
Loss reward (iter 40): 24.091001510620117
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 144       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -5.44e+03 |
|    critic_loss     | 1.18e+04  |
|    ent_coef        | 12.4      |
|    ent_coef_loss   | 1.03      |
|    learning_rate   | 0.0003    |
|    n_updates       | 15499     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 126       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -6.29e+03 |
|    critic_loss     | 1.29e+04  |
|    ent_coef        | 12.2      |
|    ent_coef_loss   | -0.81     |
|    learning_rate   | 0.0003    |
|    n_updates       | 15899     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 120       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -6.27e+03 |
|    critic_loss     | 1.21e+04  |
|    ent_coef        | 12.3      |
|    ent_coef_loss   | -0.63     |
|    learning_rate   | 0.0003    |
|    n_updates       | 16299     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 118       |
|    time_elapsed    | 13        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -6.53e+03 |
|    critic_loss     | 2.03e+04  |
|    ent_coef        | 12.6      |
|    ent_coef_loss   | 0.0287    |
|    learning_rate   | 0.0003    |
|    n_updates       | 16699     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 116       |
|    time_elapsed    | 17        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -7.02e+03 |
|    critic_loss     | 1.75e+04  |
|    ent_coef        | 13.1      |
|    ent_coef_loss   | -0.628    |
|    learning_rate   | 0.0003    |
|    n_updates       | 17099     |
----------------------------------
=== Iterazione IRL 41 ===
Loss reward (iter 41): 428.14031982421875
=== Iterazione IRL 42 ===
Loss reward (iter 42): 292.18927001953125
=== Iterazione IRL 43 ===
Loss reward (iter 43): 179.88052368164062
=== Iterazione IRL 44 ===
Loss reward (iter 44): 133.36903381347656
=== Iterazione IRL 45 ===
Loss reward (iter 45): 75.62943267822266
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 144       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -7.37e+03 |
|    critic_loss     | 1.88e+04  |
|    ent_coef        | 13.5      |
|    ent_coef_loss   | -0.725    |
|    learning_rate   | 0.0003    |
|    n_updates       | 17399     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 126       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -7.52e+03 |
|    critic_loss     | 2.55e+04  |
|    ent_coef        | 14.1      |
|    ent_coef_loss   | 0.668     |
|    learning_rate   | 0.0003    |
|    n_updates       | 17799     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 120       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -8.18e+03 |
|    critic_loss     | 1.96e+04  |
|    ent_coef        | 14.8      |
|    ent_coef_loss   | -0.309    |
|    learning_rate   | 0.0003    |
|    n_updates       | 18199     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 118       |
|    time_elapsed    | 13        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -8.48e+03 |
|    critic_loss     | 2.11e+04  |
|    ent_coef        | 15.7      |
|    ent_coef_loss   | 0.452     |
|    learning_rate   | 0.0003    |
|    n_updates       | 18599     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 117       |
|    time_elapsed    | 17        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -8.65e+03 |
|    critic_loss     | 2.07e+04  |
|    ent_coef        | 16.2      |
|    ent_coef_loss   | -1.04     |
|    learning_rate   | 0.0003    |
|    n_updates       | 18999     |
----------------------------------
=== Iterazione IRL 46 ===
Loss reward (iter 46): 32.90586853027344
=== Iterazione IRL 47 ===
Loss reward (iter 47): 17.76242446899414
=== Iterazione IRL 48 ===
Loss reward (iter 48): 17.95209312438965
=== Iterazione IRL 49 ===
Loss reward (iter 49): 15.10942268371582
=== Iterazione IRL 50 ===
Loss reward (iter 50): 11.930808067321777
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 144       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -8.91e+03 |
|    critic_loss     | 1.97e+04  |
|    ent_coef        | 16.5      |
|    ent_coef_loss   | -0.846    |
|    learning_rate   | 0.0003    |
|    n_updates       | 19299     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 126       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -9.26e+03 |
|    critic_loss     | 2.04e+04  |
|    ent_coef        | 17.1      |
|    ent_coef_loss   | -0.569    |
|    learning_rate   | 0.0003    |
|    n_updates       | 19699     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 120       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -9.64e+03 |
|    critic_loss     | 2.63e+04  |
|    ent_coef        | 17.3      |
|    ent_coef_loss   | 0.00886   |
|    learning_rate   | 0.0003    |
|    n_updates       | 20099     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 118       |
|    time_elapsed    | 13        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -9.93e+03 |
|    critic_loss     | 2.94e+04  |
|    ent_coef        | 17        |
|    ent_coef_loss   | -0.113    |
|    learning_rate   | 0.0003    |
|    n_updates       | 20499     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 117       |
|    time_elapsed    | 17        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.01e+04 |
|    critic_loss     | 2.72e+04  |
|    ent_coef        | 16.7      |
|    ent_coef_loss   | 0.584     |
|    learning_rate   | 0.0003    |
|    n_updates       | 20899     |
----------------------------------
=== Iterazione IRL 51 ===
Loss reward (iter 51): 12.226972579956055
=== Iterazione IRL 52 ===
Loss reward (iter 52): 7.942468643188477
=== Iterazione IRL 53 ===
Loss reward (iter 53): 5.520550727844238
=== Iterazione IRL 54 ===
Loss reward (iter 54): 2.2011070251464844
=== Iterazione IRL 55 ===
Loss reward (iter 55): 2.26432466506958
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 144       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.08e+04 |
|    critic_loss     | 2.44e+04  |
|    ent_coef        | 16.1      |
|    ent_coef_loss   | -0.0816   |
|    learning_rate   | 0.0003    |
|    n_updates       | 21199     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 125       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.06e+04 |
|    critic_loss     | 3.46e+04  |
|    ent_coef        | 15.7      |
|    ent_coef_loss   | 0.078     |
|    learning_rate   | 0.0003    |
|    n_updates       | 21599     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 120       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.09e+04 |
|    critic_loss     | 3.02e+04  |
|    ent_coef        | 16        |
|    ent_coef_loss   | 0.791     |
|    learning_rate   | 0.0003    |
|    n_updates       | 21999     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 118       |
|    time_elapsed    | 13        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.12e+04 |
|    critic_loss     | 3.21e+04  |
|    ent_coef        | 15.6      |
|    ent_coef_loss   | 0.347     |
|    learning_rate   | 0.0003    |
|    n_updates       | 22399     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 116       |
|    time_elapsed    | 17        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.11e+04 |
|    critic_loss     | 3.19e+04  |
|    ent_coef        | 14.9      |
|    ent_coef_loss   | -0.0329   |
|    learning_rate   | 0.0003    |
|    n_updates       | 22799     |
----------------------------------
=== Iterazione IRL 56 ===
Loss reward (iter 56): 6.309356689453125
=== Iterazione IRL 57 ===
Loss reward (iter 57): 7.827157020568848
=== Iterazione IRL 58 ===
Loss reward (iter 58): 5.086440086364746
=== Iterazione IRL 59 ===
Loss reward (iter 59): 4.947675704956055
=== Iterazione IRL 60 ===
Loss reward (iter 60): 4.9238433837890625
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 145       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.14e+04 |
|    critic_loss     | 3.16e+04  |
|    ent_coef        | 14.7      |
|    ent_coef_loss   | 0.267     |
|    learning_rate   | 0.0003    |
|    n_updates       | 23099     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 126       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.16e+04 |
|    critic_loss     | 3.07e+04  |
|    ent_coef        | 13.3      |
|    ent_coef_loss   | 0.515     |
|    learning_rate   | 0.0003    |
|    n_updates       | 23499     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 121       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.16e+04 |
|    critic_loss     | 2.82e+04  |
|    ent_coef        | 12.7      |
|    ent_coef_loss   | -0.21     |
|    learning_rate   | 0.0003    |
|    n_updates       | 23899     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 118       |
|    time_elapsed    | 13        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.18e+04 |
|    critic_loss     | 3.44e+04  |
|    ent_coef        | 12.3      |
|    ent_coef_loss   | -0.757    |
|    learning_rate   | 0.0003    |
|    n_updates       | 24299     |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 20       |
|    fps             | 117      |
|    time_elapsed    | 17       |
|    total_timesteps | 2000     |
| train/             |          |
|    actor_loss      | -1.2e+04 |
|    critic_loss     | 3.32e+04 |
|    ent_coef        | 12.1     |
|    ent_coef_loss   | 0.297    |
|    learning_rate   | 0.0003   |
|    n_updates       | 24699    |
---------------------------------
=== Iterazione IRL 61 ===
Loss reward (iter 61): 11.344171524047852
=== Iterazione IRL 62 ===
Loss reward (iter 62): 6.761782169342041
=== Iterazione IRL 63 ===
Loss reward (iter 63): 8.678711891174316
=== Iterazione IRL 64 ===
Loss reward (iter 64): 6.678713798522949
=== Iterazione IRL 65 ===
Loss reward (iter 65): 5.450164318084717
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 144       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.18e+04 |
|    critic_loss     | 3.19e+04  |
|    ent_coef        | 12.4      |
|    ent_coef_loss   | -0.61     |
|    learning_rate   | 0.0003    |
|    n_updates       | 24999     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 126       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.23e+04 |
|    critic_loss     | 4.59e+04  |
|    ent_coef        | 12.6      |
|    ent_coef_loss   | 0.175     |
|    learning_rate   | 0.0003    |
|    n_updates       | 25399     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 121       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.23e+04 |
|    critic_loss     | 3.28e+04  |
|    ent_coef        | 12.5      |
|    ent_coef_loss   | -0.186    |
|    learning_rate   | 0.0003    |
|    n_updates       | 25799     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 118       |
|    time_elapsed    | 13        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.23e+04 |
|    critic_loss     | 3.3e+04   |
|    ent_coef        | 12.3      |
|    ent_coef_loss   | -0.438    |
|    learning_rate   | 0.0003    |
|    n_updates       | 26199     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 20        |
|    fps             | 117       |
|    time_elapsed    | 17        |
|    total_timesteps | 2000      |
| train/             |           |
|    actor_loss      | -1.22e+04 |
|    critic_loss     | 3.79e+04  |
|    ent_coef        | 12.2      |
|    ent_coef_loss   | 0.663     |
|    learning_rate   | 0.0003    |
|    n_updates       | 26599     |
----------------------------------
=== Iterazione IRL 66 ===
Loss reward (iter 66): 4.323343276977539
=== Iterazione IRL 67 ===
Loss reward (iter 67): 0.6760581731796265
=== Iterazione IRL 68 ===
Loss reward (iter 68): -2.790595054626465
=== Iterazione IRL 69 ===
Loss reward (iter 69): -4.802657604217529
=== Iterazione IRL 70 ===
Loss reward (iter 70): -10.954707145690918
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 144       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.26e+04 |
|    critic_loss     | 2.78e+04  |
|    ent_coef        | 12.1      |
|    ent_coef_loss   | 0.238     |
|    learning_rate   | 0.0003    |
|    n_updates       | 26899     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 8         |
|    fps             | 126       |
|    time_elapsed    | 6         |
|    total_timesteps | 800       |
| train/             |           |
|    actor_loss      | -1.24e+04 |
|    critic_loss     | 2.78e+04  |
|    ent_coef        | 11.8      |
|    ent_coef_loss   | 0.622     |
|    learning_rate   | 0.0003    |
|    n_updates       | 27299     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 12        |
|    fps             | 121       |
|    time_elapsed    | 9         |
|    total_timesteps | 1200      |
| train/             |           |
|    actor_loss      | -1.27e+04 |
|    critic_loss     | 2.4e+04   |
|    ent_coef        | 11.5      |
|    ent_coef_loss   | -0.867    |
|    learning_rate   | 0.0003    |
|    n_updates       | 27699     |
----------------------------------
----------------------------------
| time/              |           |
|    episodes        | 16        |
|    fps             | 118       |
|    time_elapsed    | 13        |
|    total_timesteps | 1600      |
| train/             |           |
|    actor_loss      | -1.29e+04 |
|    critic_loss     | 2.6e+04   |
|    ent_coef        | 11.6      |
|    ent_coef_loss   | -0.22     |
|    learning_rate   | 0.0003    |
|    n_updates       | 28099     |
----------------------------------
---------------------------------
| time/              |          |
|    episodes        | 20       |
|    fps             | 117      |
|    time_elapsed    | 17       |
|    total_timesteps | 2000     |
| train/             |          |
|    actor_loss      | -1.3e+04 |
|    critic_loss     | 3.23e+04 |
|    ent_coef        | 11       |
|    ent_coef_loss   | -0.0204  |
|    learning_rate   | 0.0003   |
|    n_updates       | 28499    |
---------------------------------
=== Iterazione IRL 71 ===
Loss reward (iter 71): -13.32874870300293
=== Iterazione IRL 72 ===
Loss reward (iter 72): -35.86171340942383
=== Iterazione IRL 73 ===
Loss reward (iter 73): -52.646453857421875
=== Iterazione IRL 74 ===
Loss reward (iter 74): -78.45063781738281
=== Iterazione IRL 75 ===
Loss reward (iter 75): -73.81391906738281
>>> Aggiorno la policy con SAC
----------------------------------
| time/              |           |
|    episodes        | 4         |
|    fps             | 145       |
|    time_elapsed    | 2         |
|    total_timesteps | 400       |
| train/             |           |
|    actor_loss      | -1.29e+04 |
|    critic_loss     | 3.27e+04  |
|    ent_coef        | 10.3      |
|    ent_coef_loss   | 0.225     |
|    learning_rate   | 0.0003    |
|    n_updates       | 28799     |
----------------------------------
